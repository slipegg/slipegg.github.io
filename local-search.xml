<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>【Nano-vLLM源码分析（二）】关键类实现</title>
    <link href="/2026/01/12/nano-vllm-main-class/"/>
    <url>/2026/01/12/nano-vllm-main-class/</url>
    
    <content type="html"><![CDATA[<h1 id="Block相关"><a href="#Block相关" class="headerlink" title="Block相关"></a>Block相关</h1><ul><li><p>Block Manger是实现vLLM中提出的PagedAttention的关键，PagedAttention通过对KV Cache实现类似虚拟页表的逻辑Block分区与物理Block分区的划分来实现更灵活的显存管理。</p></li><li><p>BlockManager 相关代码主要负责 <strong>KV cache 的 block 分配&#x2F;回收</strong>，以及 <strong>prefix cache（前缀块复用）</strong>：把“完整的 block（长度&#x3D;block_size 的 token 段）”做 hash，后续遇到相同前缀就直接复用同一段 KV cache，从而跳过重复 prefill。</p></li></ul><h2 id="分配基本单元Block类"><a href="#分配基本单元Block类" class="headerlink" title="分配基本单元Block类"></a>分配基本单元Block类</h2><ul><li>基础Block的代码如下所示：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Block</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, block_id</span>):<br>        self.block_id = block_id<br>        self.ref_count = <span class="hljs-number">0</span><br>        self.<span class="hljs-built_in">hash</span> = -<span class="hljs-number">1</span><br>        self.token_ids = []<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, <span class="hljs-built_in">hash</span>: <span class="hljs-built_in">int</span>, token_ids: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>]</span>):<br>        self.<span class="hljs-built_in">hash</span> = <span class="hljs-built_in">hash</span><br>        self.token_ids = token_ids<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset</span>(<span class="hljs-params">self</span>):<br>        self.ref_count = <span class="hljs-number">1</span><br>        self.<span class="hljs-built_in">hash</span> = -<span class="hljs-number">1</span><br>        self.token_ids = []<br></code></pre></td></tr></table></figure><ul><li><p>Block代表的是对物理显存的划分，一个block存储block_size个kv_cache</p></li><li><p>其记录了Block的一些基本属性，包括</p><ul><li><p><code>block_id</code>：块编号，对应 KV cache 里的一个 block 槽位。</p></li><li><p><code>ref_count</code>：引用计数。多个序列如果共享同一前缀块（prefix cache 命中），会共同引用同一个 block；只有当引用计数降到 0 才能回收。</p></li><li><p><code>hash</code>：该 block 对应 token 内容的哈希（<strong>仅对“满块”有效</strong>；最后不满的块通常 hash&#x3D;-1）。</p></li><li><p><code>token_ids</code>：该 block 的 token 序列副本，用来做安全校验（避免 hash 冲突导致错误复用）。</p></li></ul></li><li><p>其主要的方法函数如下：</p><ul><li><p><code>reset()</code>：把 block 变成“已分配但尚未形成可缓存键”的状态：<code>ref_count=1, hash=-1, token_ids=[]</code></p></li><li><p><code>update(hash, token_ids)</code>：当一个 block 变成“满块”时，记录它的 hash 和 token_ids，允许后续复用。</p></li></ul></li></ul><h2 id="BlockManager定义"><a href="#BlockManager定义" class="headerlink" title="BlockManager定义"></a>BlockManager定义</h2><ul><li><code>BlockManager</code>的代码如下所示，其主要是维护了一个自己的block列表，然后各个seq按需就自己的block与这里的block建立或销毁映射关系</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BlockManager</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_blocks: <span class="hljs-built_in">int</span>, block_size: <span class="hljs-built_in">int</span></span>):<br>        self.block_size = block_size<br>        self.blocks: <span class="hljs-built_in">list</span>[Block] = [Block(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_blocks)]<br>        self.hash_to_block_id: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>] = <span class="hljs-built_in">dict</span>()<br>        self.free_block_ids: deque[<span class="hljs-built_in">int</span>] = deque(<span class="hljs-built_in">range</span>(num_blocks))<br>        self.used_block_ids: <span class="hljs-built_in">set</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-built_in">set</span>()<br><br><span class="hljs-meta">    @classmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_hash</span>(<span class="hljs-params">cls, token_ids: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>], prefix: <span class="hljs-built_in">int</span> = -<span class="hljs-number">1</span></span>):<br>        h = xxhash.xxh64()<br>        <span class="hljs-keyword">if</span> prefix != -<span class="hljs-number">1</span>:<br>            h.update(prefix.to_bytes(<span class="hljs-number">8</span>, <span class="hljs-string">&quot;little&quot;</span>))<br>        h.update(np.array(token_ids).tobytes())<br>        <span class="hljs-keyword">return</span> h.intdigest()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_allocate_block</span>(<span class="hljs-params">self, block_id: <span class="hljs-built_in">int</span></span>) -&gt; Block:<br>        block = self.blocks[block_id]<br>        <span class="hljs-keyword">assert</span> block.ref_count == <span class="hljs-number">0</span><br>        block.reset()<br>        self.free_block_ids.remove(block_id)<br>        self.used_block_ids.add(block_id)<br>        <span class="hljs-keyword">return</span> self.blocks[block_id]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_deallocate_block</span>(<span class="hljs-params">self, block_id: <span class="hljs-built_in">int</span></span>) -&gt; Block:<br>        <span class="hljs-keyword">assert</span> self.blocks[block_id].ref_count == <span class="hljs-number">0</span><br>        self.used_block_ids.remove(block_id)<br>        self.free_block_ids.append(block_id)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">can_allocate</span>(<span class="hljs-params">self, seq: <span class="hljs-type">Sequence</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.free_block_ids) &gt;= seq.num_blocks<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">allocate</span>(<span class="hljs-params">self, seq: <span class="hljs-type">Sequence</span></span>):<br>        <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> seq.block_table<br>        h = -<span class="hljs-number">1</span><br>        cache_miss = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(seq.num_blocks):<br>            token_ids = seq.block(i)<br>            h = self.compute_hash(token_ids, h) <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(token_ids) == self.block_size <span class="hljs-keyword">else</span> -<span class="hljs-number">1</span><br>            block_id = self.hash_to_block_id.get(h, -<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> block_id == -<span class="hljs-number">1</span> <span class="hljs-keyword">or</span> self.blocks[block_id].token_ids != token_ids:<br>                cache_miss = <span class="hljs-literal">True</span><br>            <span class="hljs-keyword">if</span> cache_miss:<br>                block_id = self.free_block_ids[<span class="hljs-number">0</span>]<br>                block = self._allocate_block(block_id)<br>            <span class="hljs-keyword">else</span>:<br>                seq.num_cached_tokens += self.block_size<br>                <span class="hljs-keyword">if</span> block_id <span class="hljs-keyword">in</span> self.used_block_ids:<br>                    block = self.blocks[block_id]<br>                    block.ref_count += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">else</span>:<br>                    block = self._allocate_block(block_id)<br>            <span class="hljs-keyword">if</span> h != -<span class="hljs-number">1</span>:<br>                block.update(h, token_ids)<br>                self.hash_to_block_id[h] = block_id<br>            seq.block_table.append(block_id)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">deallocate</span>(<span class="hljs-params">self, seq: <span class="hljs-type">Sequence</span></span>):<br>        <span class="hljs-keyword">for</span> block_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(seq.block_table):<br>            block = self.blocks[block_id]<br>            block.ref_count -= <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> block.ref_count == <span class="hljs-number">0</span>:<br>                self._deallocate_block(block_id)<br>        seq.num_cached_tokens = <span class="hljs-number">0</span><br>        seq.block_table.clear()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">can_append</span>(<span class="hljs-params">self, seq: <span class="hljs-type">Sequence</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.free_block_ids) &gt;= (<span class="hljs-built_in">len</span>(seq) % self.block_size == <span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">may_append</span>(<span class="hljs-params">self, seq: <span class="hljs-type">Sequence</span></span>):<br>        block_table = seq.block_table<br>        last_block = self.blocks[block_table[-<span class="hljs-number">1</span>]]<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(seq) % self.block_size == <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">assert</span> last_block.<span class="hljs-built_in">hash</span> != -<span class="hljs-number">1</span><br>            block_id = self.free_block_ids[<span class="hljs-number">0</span>]<br>            self._allocate_block(block_id)<br>            block_table.append(block_id)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">len</span>(seq) % self.block_size == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">assert</span> last_block.<span class="hljs-built_in">hash</span> == -<span class="hljs-number">1</span><br>            token_ids = seq.block(seq.num_blocks-<span class="hljs-number">1</span>)<br>            prefix = self.blocks[block_table[-<span class="hljs-number">2</span>]].<span class="hljs-built_in">hash</span> <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(block_table) &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> -<span class="hljs-number">1</span><br>            h = self.compute_hash(token_ids, prefix)<br>            last_block.update(h, token_ids)<br>            self.hash_to_block_id[h] = last_block.block_id<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">assert</span> last_block.<span class="hljs-built_in">hash</span> == -<span class="hljs-number">1</span><br><br></code></pre></td></tr></table></figure><ul><li><p>其在初始化时，传入的参数包含了配置一个<code>BlockManager</code>所管理的单个Block的大小，以及block的数量。以此为基础初始化了以下几个变量：</p><ul><li><p><code>self.blocks</code>: num_blocks 个 Block 元信息对象</p></li><li><p><code>hash_to_block_id</code>: hash -&gt; block_id 的索引，用于 prefix cache 查找</p></li><li><p><code>free_block_ids</code>: 空闲 block id 队列（deque），分配时总是取队首 free_block_ids[0]</p></li><li><p><code>used_block_ids</code>: 已被使用的 block id 集合（注意：可能出现“hash 命中但 block_id 不在 used 中”的情况，代码专门处理了）</p></li></ul></li><li><p>首先看如何计算hash的：</p><ul><li><code>compute_hash(cls, token_ids: list[int], prefix: int = -1)：</code>其首先需要传入一个prefix，这是上一个block的hash，如果没有上一个block就传入-1，就不管，然后在计算hash的时候需要在这个prefix的基础上再加上各个token_id，进行计算。从而保证一个seq的hash命中的时候这个seq前面的block也是完全相同的</li></ul></li><li><p>再看allocate相关的函数，主要负责为一个新序列分配 block 表，并尽可能复用已有的相同块：</p><ul><li><p><code>can_allocate(self, seq: Sequence) -&gt; bool</code>：其判断依据是seq需要的<code>num_blocks</code>是否小于<code>free_block_ids</code></p></li><li><p><code>allocate(self, seq: Sequence)</code>：</p><ul><li><p>其遍历seq的所有block，如果当前这个block的<code>token_ids</code>数量等于<code>block_size</code>，那么就说明是一个满的block，就通过<code>compute_hash</code>来计算该block的hash值，然后通过<code>hash_to_block_id</code>以及<code>token_ids</code>的比较来判断是否确实之前这个block已经存在了。</p><ul><li><p>如果cache未命中，那就取<code>self.free_block_ids[0]</code>作为要分配出去的block_id，然后调用<code>_allocate_block</code>分配该block。</p><ul><li><code>_allocate_block</code>实际做的事情也比较简单，就是调用<code>block.reset()</code>来重置该block，然后将其从<code>free_block_ids</code>中移除，并加入到<code>used_block_ids</code>中</li></ul></li><li><p>如果cache命中，更新<code>seq.num_cached_tokens += self.block_size</code>，然后将命中的block的<code>ref_count+1</code></p></li></ul></li><li><p>只要这是一个满的block，就对刚刚命中获取到的或未命中获取的block进行update，并且记录到<code>hash_to_block_id</code>这个map中</p></li><li><p>最后执行<code>seq.block_table.append(block_id)</code>记录对应的block_id</p></li></ul></li></ul></li><li><p>再看deallocate相关的函数，主要负责释放序列占用的块（引用计数归零才回收）：</p><ul><li><p><code>deallocate(self, seq: Sequence)</code></p><ul><li><p>倒序遍历<code>seq.block_table</code>，使得对应<code>block.ref_count -= 1</code>，如果这时<code>block.ref_count == 0</code>，那么就将其从<code>used_block_ids</code>中去除，然后添加到<code>free_block_ids</code>中</p></li><li><p>处理完blocks后设置<code>seq.num_cached_tokens = 0</code>以及<code>seq.block_table.clear()</code></p></li></ul></li></ul></li><li><p>再看append相关函数，主要负责decode 阶段追加 token 时的块扩容与封存（形成可缓存 hash）：</p><ul><li><p><code>can_append(self, seq: Sequence) -&gt; bool</code>：其代码就是<code>len(self.free_block_ids) &gt;= (len(seq) % self.block_size == 1)</code>，为true的条件是目前seq的长度刚好是<code>self.block_size</code>整数倍+1，并且<code>self.free_block_ids</code>有大于等于一个空闲block</p></li><li><p><code>may_append(self, seq: Sequence)</code>:按<code> len(seq) % block_size </code>分三种情况：</p><ol><li><p><code>== 1</code>：刚进入新块</p><ul><li><p>断言 last_block.hash !&#x3D; -1：说明上一个块必须已经封存为满块并有 hash</p></li><li><p>分配一个新物理块，block_table.append(new_block_id)</p></li><li><p>新块 reset() 后 hash&#x3D;-1，因为还没满</p></li></ul></li><li><p><code>== 0</code>：刚好填满一个块（封存）</p><ul><li><p>断言 last_block.hash &#x3D;&#x3D; -1：未封存才能封存</p></li><li><p>取刚填满的最后一个逻辑块 token_ids</p></li><li><p>取前一个块的 hash 作为 prefix（如果存在）</p></li><li><p>计算新 hash，<code>last_block.update(h, token_ids)</code> 并写入 hash_to_block_id</p></li><li><p>这一步使得“这个完整块”可被未来的请求复用（prefix cache）</p></li></ul></li></ol><ol start="3"><li><p>其它：块内部追加（既没开新块、也没填满）</p><ul><li>断言 last_block.hash &#x3D;&#x3D; -1：未封存状态维持</li></ul></li></ol></li></ul></li></ul><h1 id="Scheduler相关"><a href="#Scheduler相关" class="headerlink" title="Scheduler相关"></a>Scheduler相关</h1><ul><li>Scheduler负责调度出当前需要处理哪些seq，其主要包含两个队列，一个队列是waiting队列，一个是正在处理的running队列，队列中的基本任务单元是seq</li></ul><h2 id="调度基本单元Sequence类"><a href="#调度基本单元Sequence类" class="headerlink" title="调度基本单元Sequence类"></a>调度基本单元Sequence类</h2><ul><li><p>Sequence类的相关代码如下所示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> copy <span class="hljs-keyword">import</span> copy<br><span class="hljs-keyword">from</span> enum <span class="hljs-keyword">import</span> Enum, auto<br><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> count<br><br><span class="hljs-keyword">from</span> nanovllm.sampling_params <span class="hljs-keyword">import</span> SamplingParams<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SequenceStatus</span>(<span class="hljs-title class_ inherited__">Enum</span>):<br>    WAITING = auto()<br>    RUNNING = auto()<br>    FINISHED = auto()<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sequence</span>:<br>    block_size = <span class="hljs-number">256</span><br>    counter = count()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, token_ids: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>], sampling_params = SamplingParams(<span class="hljs-params"></span>)</span>):<br>        self.seq_id = <span class="hljs-built_in">next</span>(<span class="hljs-type">Sequence</span>.counter)<br>        self.status = SequenceStatus.WAITING<br>        self.token_ids = copy(token_ids)<br>        self.last_token = token_ids[-<span class="hljs-number">1</span>]<br>        self.num_tokens = <span class="hljs-built_in">len</span>(self.token_ids)<br>        self.num_prompt_tokens = <span class="hljs-built_in">len</span>(token_ids)<br>        self.num_cached_tokens = <span class="hljs-number">0</span><br>        self.block_table = []<br>        self.temperature = sampling_params.temperature<br>        self.max_tokens = sampling_params.max_tokens<br>        self.ignore_eos = sampling_params.ignore_eos<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.num_tokens<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, key</span>):<br>        <span class="hljs-keyword">return</span> self.token_ids[key]<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">is_finished</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.status == SequenceStatus.FINISHED<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">num_completion_tokens</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.num_tokens - self.num_prompt_tokens<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">prompt_token_ids</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.token_ids[:self.num_prompt_tokens]<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">completion_token_ids</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.token_ids[self.num_prompt_tokens:]<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">num_cached_blocks</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.num_cached_tokens // self.block_size<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">num_blocks</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> (self.num_tokens + self.block_size - <span class="hljs-number">1</span>) // self.block_size<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">last_block_num_tokens</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.num_tokens - (self.num_blocks - <span class="hljs-number">1</span>) * self.block_size<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">block</span>(<span class="hljs-params">self, i</span>):<br>        <span class="hljs-keyword">assert</span> <span class="hljs-number">0</span> &lt;= i &lt; self.num_blocks<br>        <span class="hljs-keyword">return</span> self.token_ids[i*self.block_size: (i+<span class="hljs-number">1</span>)*self.block_size]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">append_token</span>(<span class="hljs-params">self, token_id: <span class="hljs-built_in">int</span></span>):<br>        self.token_ids.append(token_id)<br>        self.last_token = token_id<br>        self.num_tokens += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getstate__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> (self.num_tokens, self.num_prompt_tokens, self.num_cached_tokens, self.block_table,<br>                self.token_ids <span class="hljs-keyword">if</span> self.num_completion_tokens == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> self.last_token)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__setstate__</span>(<span class="hljs-params">self, state</span>):<br>        self.num_tokens, self.num_prompt_tokens, self.num_cached_tokens, self.block_table = state[:-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> self.num_completion_tokens == <span class="hljs-number">0</span>:<br>            self.token_ids = state[-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">else</span>:<br>            self.last_token = state[-<span class="hljs-number">1</span>]<br><br></code></pre></td></tr></table></figure></li><li><p>在状态流转方面其主要就是最开始的WAITING ，然后被调度上台运行后变为RUNNING态，如果在RUNNINF态被抢占，就会转回WAITING ，如果完成就变为FINISHED态</p></li><li><p>在初始化时会将状态标记为WAITING，然后初始化时tokens都会被记录为num_prompt_tokens，此外也会有token_ids 进行记录，此外初始化时会标记<code>num_cached_tokens = 0</code></p></li><li><p>然后还有一些简单的函数，比较关键的有：</p><ul><li><p><code>num_blocks(self)</code>：block的数量，直接使用<code>(self.num_tokens + self.block_size - 1) // self.block_size</code>向上取整获得</p></li><li><p><code>last_block_num_tokens(self)</code>：最后一个block上的token数量，<code>self.num_tokens - (self.num_blocks - 1) * self.block_size</code></p></li><li><p><code>block(self, i)</code>：block i上的tokens：self.token_ids[i*self.block_size: (i+1)*self.block_size]</p></li><li><p><code>append_token(self, token_id: int)</code>：添加生成的token，添加时<code>self.token_ids.append(token_id)</code>，<code>self.last_token = token_id</code>，<code>self.last_token = token_id</code></p></li></ul></li></ul><h2 id="Scheduler类"><a href="#Scheduler类" class="headerlink" title="Scheduler类"></a>Scheduler类</h2><ul><li><p>Scheduler类的代码如下所示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> deque<br><br><span class="hljs-keyword">from</span> nanovllm.config <span class="hljs-keyword">import</span> Config<br><span class="hljs-keyword">from</span> nanovllm.engine.sequence <span class="hljs-keyword">import</span> <span class="hljs-type">Sequence</span>, SequenceStatus<br><span class="hljs-keyword">from</span> nanovllm.engine.block_manager <span class="hljs-keyword">import</span> BlockManager<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Scheduler</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: Config</span>):<br>        self.max_num_seqs = config.max_num_seqs<br>        self.max_num_batched_tokens = config.max_num_batched_tokens<br>        self.eos = config.eos<br>        self.block_manager = BlockManager(config.num_kvcache_blocks, config.kvcache_block_size)<br>        self.waiting: deque[<span class="hljs-type">Sequence</span>] = deque()<br>        self.running: deque[<span class="hljs-type">Sequence</span>] = deque()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">is_finished</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">not</span> self.waiting <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.running<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">add</span>(<span class="hljs-params">self, seq: <span class="hljs-type">Sequence</span></span>):<br>        self.waiting.append(seq)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">schedule</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">tuple</span>[<span class="hljs-built_in">list</span>[<span class="hljs-type">Sequence</span>], <span class="hljs-built_in">bool</span>]:<br>        <span class="hljs-comment"># prefill</span><br>        scheduled_seqs = []<br>        num_seqs = <span class="hljs-number">0</span><br>        num_batched_tokens = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">while</span> self.waiting <span class="hljs-keyword">and</span> num_seqs &lt; self.max_num_seqs:<br>            seq = self.waiting[<span class="hljs-number">0</span>]<br>            <span class="hljs-keyword">if</span> num_batched_tokens + <span class="hljs-built_in">len</span>(seq) &gt; self.max_num_batched_tokens <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> self.block_manager.can_allocate(seq):<br>                <span class="hljs-keyword">break</span><br>            num_seqs += <span class="hljs-number">1</span><br>            self.block_manager.allocate(seq)<br>            num_batched_tokens += <span class="hljs-built_in">len</span>(seq) - seq.num_cached_tokens<br>            seq.status = SequenceStatus.RUNNING<br>            self.waiting.popleft()<br>            self.running.append(seq)<br>            scheduled_seqs.append(seq)<br>        <span class="hljs-keyword">if</span> scheduled_seqs:<br>            <span class="hljs-keyword">return</span> scheduled_seqs, <span class="hljs-literal">True</span><br><br>        <span class="hljs-comment"># decode</span><br>        <span class="hljs-keyword">while</span> self.running <span class="hljs-keyword">and</span> num_seqs &lt; self.max_num_seqs:<br>            seq = self.running.popleft()<br>            <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> self.block_manager.can_append(seq):<br>                <span class="hljs-keyword">if</span> self.running:<br>                    self.preempt(self.running.pop())<br>                <span class="hljs-keyword">else</span>:<br>                    self.preempt(seq)<br>                    <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">else</span>:<br>                num_seqs += <span class="hljs-number">1</span><br>                self.block_manager.may_append(seq)<br>                scheduled_seqs.append(seq)<br>        <span class="hljs-keyword">assert</span> scheduled_seqs<br>        self.running.extendleft(<span class="hljs-built_in">reversed</span>(scheduled_seqs))<br>        <span class="hljs-keyword">return</span> scheduled_seqs, <span class="hljs-literal">False</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">preempt</span>(<span class="hljs-params">self, seq: <span class="hljs-type">Sequence</span></span>):<br>        seq.status = SequenceStatus.WAITING<br>        self.block_manager.deallocate(seq)<br>        self.waiting.appendleft(seq)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">postprocess</span>(<span class="hljs-params">self, seqs: <span class="hljs-built_in">list</span>[<span class="hljs-type">Sequence</span>], token_ids: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">bool</span>]:<br>        <span class="hljs-keyword">for</span> seq, token_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(seqs, token_ids):<br>            seq.append_token(token_id)<br>            <span class="hljs-keyword">if</span> (<span class="hljs-keyword">not</span> seq.ignore_eos <span class="hljs-keyword">and</span> token_id == self.eos) <span class="hljs-keyword">or</span> seq.num_completion_tokens == seq.max_tokens:<br>                seq.status = SequenceStatus.FINISHED<br>                self.block_manager.deallocate(seq)<br>                self.running.remove(seq)<br><br></code></pre></td></tr></table></figure></li><li><p>在初始化时，其初始化了block_manager，因为scheduler需要block_manager来判断当前还有多少空间可以用来做cache，从而调度出对应的请求。其还依据配置初始化了<code>max_num_seqs</code>用来限制一次最多同时处理多少seq，还用了<code>max_num_batched_tokens</code>来限制了一次最多处理的token数量。其还用双端队列初deque始化了running和waiting队列</p></li><li><p>其关键的功能函数有以下这些：</p><ul><li><p><code>is_finished(self)</code>：判断是不是所有seq都处理完了</p></li><li><p><code>add(self, seq: Sequence)</code>：添加新seq到waiting队列中</p></li><li><p><code>preempt(self, seq: Sequence)</code>：被抢占的seq状态需要转回WAITING并加入到waiting队列的队头，然后使用block_manager释放seq占据的block</p></li><li><p><code>postprocess(self, seqs: list[Sequence], token_ids: list[int]) -&gt; list[bool]</code>：每执行完一轮都会运行这个函数，目的是为各个seq添加最新生成的token，然后如果这个seq生成完了就将其状态转为FINISHED，然后block_manager释放seq占据的block，并且将其从running队列中删除</p></li><li><p><code>schedule(self) -&gt; tuple[list[Sequence], bool]</code>：</p><ul><li><p>首先处理waiting队列的prefill请求，如果waiting队列不为空，并且没有超过scheduler自身的<code>self.max_num_seqs</code>及<code>max_num_batched_tokens</code>限制以及block manager的限制，就从waiting队列的队头中不断取出seq放入到running队列中，然后为seq在block manager中分配block，并将seq放入到结果<code>scheduled_seqs</code>中</p></li><li><p>如果没有取到prefill请求就尝试取出decode队列，如果running队列有请求，并且没有超过scheduler自身的<code>self.max_num_seqs</code>限制以及block manager的限制就从running队列的队头不断取出seq，然后为seq在block manager中按需分配block，并将seq放入到结果<code>scheduled_seqs</code>中；如果超过了block manager的限制就尝试去抢占running队列中队尾的旧seq，如果没有旧seq可以抢占，就抢占自己，也就是把自己再放回waiting队列中。最后将这些<code>scheduled_seqs</code>再按序放入到running的队头，这样子保证下一轮调度时还是基本按照这一次的顺序继续处理，从而高效利用cache</p></li></ul></li></ul></li></ul><h1 id="Attention中的KV-Cache"><a href="#Attention中的KV-Cache" class="headerlink" title="Attention中的KV Cache"></a>Attention中的KV Cache</h1><ul><li><p>Nano-vLLM中的Attention的相关代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">import</span> triton<br><span class="hljs-keyword">import</span> triton.language <span class="hljs-keyword">as</span> tl<br><br><span class="hljs-keyword">from</span> flash_attn <span class="hljs-keyword">import</span> flash_attn_varlen_func, flash_attn_with_kvcache<br><span class="hljs-keyword">from</span> nanovllm.utils.context <span class="hljs-keyword">import</span> get_context<br><br><span class="hljs-meta">@triton.jit</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">store_kvcache_kernel</span>(<span class="hljs-params"></span><br><span class="hljs-params">    key_ptr,</span><br><span class="hljs-params">    key_stride,</span><br><span class="hljs-params">    value_ptr,</span><br><span class="hljs-params">    value_stride,</span><br><span class="hljs-params">    k_cache_ptr,</span><br><span class="hljs-params">    v_cache_ptr,</span><br><span class="hljs-params">    slot_mapping_ptr,</span><br><span class="hljs-params">    D: tl.constexpr,</span><br><span class="hljs-params"></span>):<br>    idx = tl.program_id(<span class="hljs-number">0</span>)<br>    slot = tl.load(slot_mapping_ptr + idx)<br>    <span class="hljs-keyword">if</span> slot == -<span class="hljs-number">1</span>: <span class="hljs-keyword">return</span><br>    key_offsets = idx * key_stride + tl.arange(<span class="hljs-number">0</span>, D)<br>    value_offsets = idx * value_stride + tl.arange(<span class="hljs-number">0</span>, D)<br>    key = tl.load(key_ptr + key_offsets)<br>    value = tl.load(value_ptr + value_offsets)<br>    cache_offsets = slot * D + tl.arange(<span class="hljs-number">0</span>, D)<br>    tl.store(k_cache_ptr + cache_offsets, key)<br>    tl.store(v_cache_ptr + cache_offsets, value)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">store_kvcache</span>(<span class="hljs-params">key: torch.Tensor, value: torch.Tensor, k_cache: torch.Tensor, v_cache: torch.Tensor, slot_mapping: torch.Tensor</span>):<br>    N, num_heads, head_dim = key.shape<br>    D = num_heads * head_dim<br>    <span class="hljs-keyword">assert</span> key.stride(-<span class="hljs-number">1</span>) == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> value.stride(-<span class="hljs-number">1</span>) == <span class="hljs-number">1</span><br>    <span class="hljs-keyword">assert</span> key.stride(<span class="hljs-number">1</span>) == head_dim <span class="hljs-keyword">and</span> value.stride(<span class="hljs-number">1</span>) == head_dim<br>    <span class="hljs-keyword">assert</span> k_cache.stride(<span class="hljs-number">1</span>) == D <span class="hljs-keyword">and</span> v_cache.stride(<span class="hljs-number">1</span>) == D<br>    <span class="hljs-keyword">assert</span> slot_mapping.numel() == N<br>    store_kvcache_kernel[(N,)](key, key.stride(<span class="hljs-number">0</span>), value, value.stride(<span class="hljs-number">0</span>), k_cache, v_cache, slot_mapping, D)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Attention</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        num_heads,</span><br><span class="hljs-params">        head_dim,</span><br><span class="hljs-params">        scale,</span><br><span class="hljs-params">        num_kv_heads,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.num_heads = num_heads<br>        self.head_dim = head_dim<br>        self.scale = scale<br>        self.num_kv_heads = num_kv_heads<br>        self.k_cache = self.v_cache = torch.tensor([])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor</span>):<br>        context = get_context()<br>        k_cache, v_cache = self.k_cache, self.v_cache<br>        <span class="hljs-keyword">if</span> k_cache.numel() <span class="hljs-keyword">and</span> v_cache.numel():<br>            store_kvcache(k, v, k_cache, v_cache, context.slot_mapping)<br>        <span class="hljs-keyword">if</span> context.is_prefill:<br>            <span class="hljs-keyword">if</span> context.block_tables <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:    <span class="hljs-comment"># prefix cache</span><br>                k, v = k_cache, v_cache<br>            o = flash_attn_varlen_func(q, k, v,<br>                                       max_seqlen_q=context.max_seqlen_q, cu_seqlens_q=context.cu_seqlens_q,<br>                                       max_seqlen_k=context.max_seqlen_k, cu_seqlens_k=context.cu_seqlens_k,<br>                                       softmax_scale=self.scale, causal=<span class="hljs-literal">True</span>, block_table=context.block_tables)<br>        <span class="hljs-keyword">else</span>:    <span class="hljs-comment"># decode</span><br>            o = flash_attn_with_kvcache(q.unsqueeze(<span class="hljs-number">1</span>), k_cache, v_cache,<br>                                        cache_seqlens=context.context_lens, block_table=context.block_tables, <br>                                        softmax_scale=self.scale, causal=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">return</span> o<br><br></code></pre></td></tr></table></figure><ul><li><p>注意上述的Attention初始化的时候k_cache与v_cache都是空，其实际的初始化是在<code>ModelRunner</code>初始化时的<code>allocate_kv_cache</code>函数中，代码如下所示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">allocate_kv_cache</span>(<span class="hljs-params">self</span>):<br>    config = self.config<br>    hf_config = config.hf_config<br>    free, total = torch.cuda.mem_get_info()<br>    used = total - free<br>    peak = torch.cuda.memory_stats()[<span class="hljs-string">&quot;allocated_bytes.all.peak&quot;</span>]<br>    current = torch.cuda.memory_stats()[<span class="hljs-string">&quot;allocated_bytes.all.current&quot;</span>]<br>    num_kv_heads = hf_config.num_key_value_heads // self.world_size<br>    head_dim = <span class="hljs-built_in">getattr</span>(hf_config, <span class="hljs-string">&quot;head_dim&quot;</span>, hf_config.hidden_size // hf_config.num_attention_heads)<br>    block_bytes = <span class="hljs-number">2</span> * hf_config.num_hidden_layers * self.block_size * num_kv_heads * head_dim * hf_config.torch_dtype.itemsize<br>    config.num_kvcache_blocks = <span class="hljs-built_in">int</span>(total * config.gpu_memory_utilization - used - peak + current) // block_bytes<br>    <span class="hljs-keyword">assert</span> config.num_kvcache_blocks &gt; <span class="hljs-number">0</span><br>    self.kv_cache = torch.empty(<span class="hljs-number">2</span>, hf_config.num_hidden_layers, config.num_kvcache_blocks, self.block_size, num_kv_heads, head_dim)<br>    layer_id = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> self.model.modules():<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(module, <span class="hljs-string">&quot;k_cache&quot;</span>) <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(module, <span class="hljs-string">&quot;v_cache&quot;</span>):<br>            module.k_cache = self.kv_cache[<span class="hljs-number">0</span>, layer_id]<br>            module.v_cache = self.kv_cache[<span class="hljs-number">1</span>, layer_id]<br>            layer_id += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><ul><li><p>其首先计算了在TP并行下各个worker的head数量</p></li><li><p>然后其初始化了kv_cache tensor，其形状为：<code>[2, num_layers, num_kvcache_blocks, block_size, num_kv_heads_per_rank, head_dim]</code>，其中 <code>2</code> 表示 K 和 V 两份，从这里也能看出一个block中的一个元素指的是一个Attention layer中的一个token的所有head的K或V对应的tensor，即大小是<code>num_kv_heads_per_rank×head_dim</code></p></li><li><p>然后回将对应的kv cache绑定给每一层的k_cache与v_cache上</p></li></ul></li><li><p>在Attention的Forward中，只要经过上述的<code>allocate_kv_cache</code>，就会进入到<code>store_kvcache(k, v, k_cache, v_cache, context.slot_mapping)</code>中，其中<code>slot_mapping</code>是一个一维Tensor，其记录了每个k或v应该具体放置的相对位置，其数量与当前的k的数量<code>N</code>完全一致，故<code>store_kvcache</code>中，其会启用<code>N</code>个warp去根据<code>slot_mapping</code>提供的各个k、v元素需要存储的相对位置与目前计算出来的k、v存储在<code>k_cache</code>与<code>v_cache</code>中</p></li><li><p>然后如果是prefill阶段，就使用flash_attn_varlen_func计算结果</p></li><li><p>如果是decode阶段，就使用flash_attn_with_kvcache计算结果</p></li><li><p>具体在计算时如何使用kv cache这里都被封装了起来，但是整体而言其行为是可预见的，因为<code>context.block_tables</code>中记录了各个seq目前已经生成k、v所在的block id，所以获取一个seq前面的k与v的时候直接依据对应的block id去kv cache中按序读取对应的数据即可</p></li></ul></li></ul><h1 id="ModelRunner相关"><a href="#ModelRunner相关" class="headerlink" title="ModelRunner相关"></a>ModelRunner相关</h1><h2 id="Prefill"><a href="#Prefill" class="headerlink" title="Prefill"></a>Prefill</h2><ul><li><p>ModelRunner的<code>prepare_prefill</code>代码如下所示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_prefill</span>(<span class="hljs-params">self, seqs: <span class="hljs-built_in">list</span>[<span class="hljs-type">Sequence</span>]</span>):<br>    input_ids = []<br>    positions = []<br>    cu_seqlens_q = [<span class="hljs-number">0</span>]<br>    cu_seqlens_k = [<span class="hljs-number">0</span>]<br>    max_seqlen_q = <span class="hljs-number">0</span><br>    max_seqlen_k = <span class="hljs-number">0</span><br>    slot_mapping = []<br>    block_tables = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">for</span> seq <span class="hljs-keyword">in</span> seqs:<br>        seqlen = <span class="hljs-built_in">len</span>(seq)<br>        input_ids.extend(seq[seq.num_cached_tokens:])<br>        positions.extend(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(seq.num_cached_tokens, seqlen)))<br>        seqlen_q = seqlen - seq.num_cached_tokens<br>        seqlen_k = seqlen<br>        cu_seqlens_q.append(cu_seqlens_q[-<span class="hljs-number">1</span>] + seqlen_q)<br>        cu_seqlens_k.append(cu_seqlens_k[-<span class="hljs-number">1</span>] + seqlen_k)<br>        max_seqlen_q = <span class="hljs-built_in">max</span>(seqlen_q, max_seqlen_q)<br>        max_seqlen_k = <span class="hljs-built_in">max</span>(seqlen_k, max_seqlen_k)<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> seq.block_table:    <span class="hljs-comment"># warmup</span><br>            <span class="hljs-keyword">continue</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(seq.num_cached_blocks, seq.num_blocks):<br>            start = seq.block_table[i] * self.block_size<br>            <span class="hljs-keyword">if</span> i != seq.num_blocks - <span class="hljs-number">1</span>:<br>                end = start + self.block_size<br>            <span class="hljs-keyword">else</span>:<br>                end = start + seq.last_block_num_tokens <br>            slot_mapping.extend(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(start, end)))<br>    <span class="hljs-keyword">if</span> cu_seqlens_k[-<span class="hljs-number">1</span>] &gt; cu_seqlens_q[-<span class="hljs-number">1</span>]:    <span class="hljs-comment"># prefix cache</span><br>        block_tables = self.prepare_block_tables(seqs)<br>    input_ids = torch.tensor(input_ids, dtype=torch.int64, pin_memory=<span class="hljs-literal">True</span>).cuda(non_blocking=<span class="hljs-literal">True</span>)<br>    positions = torch.tensor(positions, dtype=torch.int64, pin_memory=<span class="hljs-literal">True</span>).cuda(non_blocking=<span class="hljs-literal">True</span>)<br>    cu_seqlens_q = torch.tensor(cu_seqlens_q, dtype=torch.int32, pin_memory=<span class="hljs-literal">True</span>).cuda(non_blocking=<span class="hljs-literal">True</span>)<br>    cu_seqlens_k = torch.tensor(cu_seqlens_k, dtype=torch.int32, pin_memory=<span class="hljs-literal">True</span>).cuda(non_blocking=<span class="hljs-literal">True</span>)<br>    slot_mapping = torch.tensor(slot_mapping, dtype=torch.int32, pin_memory=<span class="hljs-literal">True</span>).cuda(non_blocking=<span class="hljs-literal">True</span>)<br>    set_context(<span class="hljs-literal">True</span>, cu_seqlens_q, cu_seqlens_k, max_seqlen_q, max_seqlen_k, slot_mapping, <span class="hljs-literal">None</span>, block_tables)<br>    <span class="hljs-keyword">return</span> input_ids, positions<br><br></code></pre></td></tr></table></figure><ul><li><p>其会遍历未缓存的<code>range(seq.num_cached_blocks, seq.num_blocks)</code>，第i个block的起始位置就是<code>seq.block_table[i] * self.block_size</code>，也就是<code>对应block_id * self.block_size</code>，第i个block的结束位置会与是不是这个seq的最后一个block有关，如果不是那么就是满的block，那么就是<code>start + self.block_size</code>，否则就是<code>start + seq.last_block_num_tokens</code>，最终会将其存储在<code>slot_mapping</code>中，<code>slot_mapping</code>的格式就是<code>[block_1_start_idx, block_1_start_idx+1, ..., block_1_end_idx-1,block_2_start_idx, block_2_start_idx+1, ..., block_2_end_idx-1,...]</code></p></li><li><p>如果<code>cu_seqlens_k[-1] &gt; cu_seqlens_q[-1]</code>那么就说明有命中block缓存的token，需要专门执行<code>prepare_block_tables</code>，这是为了后续给flash attention使用block table</p><ul><li><code>prepare_block_tables</code>的代码如下所示</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_block_tables</span>(<span class="hljs-params">self, seqs: <span class="hljs-built_in">list</span>[<span class="hljs-type">Sequence</span>]</span>):<br>    max_len = <span class="hljs-built_in">max</span>(<span class="hljs-built_in">len</span>(seq.block_table) <span class="hljs-keyword">for</span> seq <span class="hljs-keyword">in</span> seqs)<br>    block_tables = [seq.block_table + [-<span class="hljs-number">1</span>] * (max_len - <span class="hljs-built_in">len</span>(seq.block_table)) <span class="hljs-keyword">for</span> seq <span class="hljs-keyword">in</span> seqs]<br>    block_tables = torch.tensor(block_tables, dtype=torch.int32, pin_memory=<span class="hljs-literal">True</span>).cuda(non_blocking=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> block_tables<br></code></pre></td></tr></table></figure><ul><li>其得到目前这批token的最长block_table，对应没达到最长的block_table，将其在后面补-1，最终得到一个GPU中的table，形状为(len(seqs), max_len_block_table)</li></ul></li><li><p>这些变量都会存储在context中等待后续调用</p></li></ul></li></ul><h2 id="Decode"><a href="#Decode" class="headerlink" title="Decode"></a>Decode</h2><ul><li><p>ModelRunner的<code>prepare_decode</code>代码如下所示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_decode</span>(<span class="hljs-params">self, seqs: <span class="hljs-built_in">list</span>[<span class="hljs-type">Sequence</span>]</span>):<br>    input_ids = []<br>    positions = []<br>    slot_mapping = []<br>    context_lens = []<br>    <span class="hljs-keyword">for</span> seq <span class="hljs-keyword">in</span> seqs:<br>        input_ids.append(seq.last_token)<br>        positions.append(<span class="hljs-built_in">len</span>(seq) - <span class="hljs-number">1</span>)<br>        context_lens.append(<span class="hljs-built_in">len</span>(seq))<br>        slot_mapping.append(seq.block_table[-<span class="hljs-number">1</span>] * self.block_size + seq.last_block_num_tokens  - <span class="hljs-number">1</span>)<br>    input_ids = torch.tensor(input_ids, dtype=torch.int64, pin_memory=<span class="hljs-literal">True</span>).cuda(non_blocking=<span class="hljs-literal">True</span>)<br>    positions = torch.tensor(positions, dtype=torch.int64, pin_memory=<span class="hljs-literal">True</span>).cuda(non_blocking=<span class="hljs-literal">True</span>)<br>    slot_mapping = torch.tensor(slot_mapping, dtype=torch.int32, pin_memory=<span class="hljs-literal">True</span>).cuda(non_blocking=<span class="hljs-literal">True</span>)<br>    context_lens = torch.tensor(context_lens, dtype=torch.int32, pin_memory=<span class="hljs-literal">True</span>).cuda(non_blocking=<span class="hljs-literal">True</span>)<br>    block_tables = self.prepare_block_tables(seqs)<br>    set_context(<span class="hljs-literal">False</span>, slot_mapping=slot_mapping, context_lens=context_lens, block_tables=block_tables)<br>    <span class="hljs-keyword">return</span> input_ids, positions<br></code></pre></td></tr></table></figure><ul><li><p>在decode中，其首先需要将各seq上一步生成的新token也就是<code>seq.last_token</code>处理好，也就是将这个token append到<code>input_ids</code>，然后再将其位置也放入到<code>positions</code>，再记录一下当前seq的长度，以给后续attention计算时访问对应长度的kv cache，然后再在<code>slot_mapping</code>中记录这个新生成的token在后续计算出k v后需要放置在kv cache中的位置<code>seq.block_table[-1] * self.block_size + seq.last_block_num_tokens  - 1</code>，注意这里的<code>seq.last_block_num_tokens</code>就包括了新生成的token</p></li><li><p>然后将计算的结果存储在context中，并且注意同样会使用<code>prepare_block_tables</code>来将<code>block_tables</code>向量化</p></li></ul></li></ul><h2 id="计算结果"><a href="#计算结果" class="headerlink" title="计算结果"></a>计算结果</h2><ul><li><p>在<code>prepare_prefill</code>或<code>prepare_decode</code>后会通过<code>run_model</code>来调用model的前向传播并计算logits</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">run_model</span>(<span class="hljs-params">self, input_ids: torch.Tensor, positions: torch.Tensor, is_prefill: <span class="hljs-built_in">bool</span></span>):<br>    <span class="hljs-keyword">if</span> is_prefill <span class="hljs-keyword">or</span> self.enforce_eager <span class="hljs-keyword">or</span> input_ids.size(<span class="hljs-number">0</span>) &gt; <span class="hljs-number">512</span>:<br>        <span class="hljs-keyword">return</span> self.model.compute_logits(self.model(input_ids, positions))<br></code></pre></td></tr></table></figure><ul><li>其中就涉及到上述说的用KV Cache计算attention，最终得到的logits就是下一个token的概率分布</li></ul></li><li><p>在得到<code>logits</code>后需要借助Sampler进行采样，从而得到最终输出的token</p><ul><li><p>实际调用的代码为：<code>token_ids = self.sampler(logits, temperatures).tolist() if self.rank == 0 else None</code></p></li><li><p>Sampler定义的代码如下：</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Sampler</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br><span class="hljs-meta">    @torch.compile</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, logits: torch.Tensor, temperatures: torch.Tensor</span>):<br>        logits = logits.<span class="hljs-built_in">float</span>().div_(temperatures.unsqueeze(dim=<span class="hljs-number">1</span>))<br>        probs = torch.softmax(logits, dim=-<span class="hljs-number">1</span>)<br>        sample_tokens = probs.div_(torch.empty_like(probs).exponential_(<span class="hljs-number">1</span>).clamp_min_(<span class="hljs-number">1e-10</span>)).argmax(dim=-<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> sample_tokens<br></code></pre></td></tr></table></figure><ul><li><p>其首先除以<code>temperatures</code>，然后通过<code>softmax</code>得到概率分布</p></li><li><p>用Gumbel-Max Trick从 softmax 分布中采样一个 token，其实现等价于按概率采样，但是更加适合GPU进行并行化</p></li></ul></li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>下面总结绘制了2个流程图：</p><ul><li>从调度到prefill结果的过程如下所示：</li></ul><p><img src="/2026/01/12/nano-vllm-main-class/diagram.png"></p><ul><li>prefill结束后进行decode的过程如下所示：</li></ul><p><img src="/2026/01/12/nano-vllm-main-class/diagram-1.png"></p>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>vLLM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>vLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Nano-vLLM源码分析（一）】环境配置及整体流程概览</title>
    <link href="/2026/01/10/nano-vllm-process/"/>
    <url>/2026/01/10/nano-vllm-process/</url>
    
    <content type="html"><![CDATA[<h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><ul><li><p>整体环境还是很干净的，跟着readme应该能很快配置起来。</p></li><li><p>不过我这里是下载了源代码，然后使用了容器<code>nvcr.io/nvidia/pytorch:25.04-py3</code>来挂载文件夹运行，在容器内还需要<code>pip3 install transformers xxhash</code>，这样就配置好了基本的python环境。</p></li><li><p>然后还需要下载Qwen3模型，因为Nano-vLLM目前只专门适配了它。这里readme使用的是比较老的<code>huggingface-cli</code>来下载，如果使用最新的版本，直接使用如下的代码即可将模型下载好，注意这里下载的位置是<code>./huggingface/Qwen3-0.6B</code>。</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain&#x20;text">hf download   Qwen/Qwen3-0.6B   --force-download   --local-dir ./huggingface/Qwen3-0.6B<br></code></pre></td></tr></table></figure><ul><li>然后修改<code>example.py</code>中的模型位置为<code>./huggingface/Qwen3-0.6B</code>再直接运行<code>python3 example.py</code>就可以运行了。运行结果如下：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@830fe60dca79:/workspace/nano-vllm<span class="hljs-comment"># python3 example.py </span><br>`torch_dtype` is deprecated! Use `dtype` instead!<br>[<span class="hljs-string">&#x27;&lt;|im_start|&gt;user\nintroduce yourself&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&#x27;</span>, <span class="hljs-string">&#x27;&lt;|im_start|&gt;user\nlist all prime numbers within 100&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&#x27;</span>]<br>Generating: 100%|█████████████████████████| 2/2 [00:12&lt;00:00,  6.24s/it, Prefill=24tok/s, Decode=30tok/s]<br><br><br>Prompt: <span class="hljs-string">&#x27;&lt;|im_start|&gt;user\nintroduce yourself&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&#x27;</span><br>Completion: <span class="hljs-string">&quot;&lt;think&gt;\nOkay, the user asked me to introduce myself. I need to make sure I respond in a friendly and helpful manner. Let me start by acknowledging their question. I should mention that I&#x27;m an AI assistant designed to help with various tasks. It&#x27;s important to add a bit about my capabilities, like answering questions, providing information, etc. I should also offer assistance in a conversational tone. Let me check if I&#x27;m using the right structure and if there&#x27;s anything else they might need. Alright, I think that covers it.\n&lt;/think&gt;\n\nHello! I&#x27;m an AI assistant designed to help you with a wide range of questions and tasks. I can answer questions, provide information, offer recommendations, and even assist with writing or other creative tasks. How can I help you today?&lt;|im_end|&gt;&quot;</span><br><br><br>Prompt: <span class="hljs-string">&#x27;&lt;|im_start|&gt;user\nlist all prime numbers within 100&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&#x27;</span><br>Completion: <span class="hljs-string">&quot;&lt;think&gt;\nOkay, so I need to list all the prime numbers between 100. Let me think. First, I remember that a prime number is a number greater than 1 that has no divisors other than 1 and itself. So, starting from 100, I need to check each number to see if it&#x27;s prime. \n\nLet me start by recalling some prime numbers. The primes less than 100 are 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, and 83. Wait, but the question is about numbers between 100. So, starting from 100 upwards. \n\nLet me check each number. Starting with 100. Is 100 a prime? Well, 100 is even, so it&#x27;s divisible by 2. So 100 is not prime. Next is 1&quot;</span><br></code></pre></td></tr></table></figure><ul><li>注意到在运行过程中由于其使用的是GPU 0，故会分配GPU 0的大量显存，主要占用显存的地方是模型以及kv cache</li></ul><p><img src="/2026/01/10/nano-vllm-process/image.png"></p><h1 id="流程概览"><a href="#流程概览" class="headerlink" title="流程概览"></a>流程概览</h1><h2 id="项目文件"><a href="#项目文件" class="headerlink" title="项目文件"></a>项目文件</h2><p>首先整体概览一下项目的文件结构，如下所示，添加了一些介绍</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs markdown">.<br>├── assets                          # 项目展示资源（如 logo），不参与核心逻辑<br>│   └── logo.png<br>├── bench.py                        # 简单的推理性能测试脚本（吞吐 / 延迟）<br>├── example.py                      # 最小可运行示例，展示 Nano-vLLM 的基本使用方式<br>├── huggingface                     # 本地 HuggingFace 模型目录<br>│   └── Qwen3-0.6B                  # Qwen3-0.6B 模型权重与 tokenizer 文件<br>│       ├── config.json             # 模型结构与超参数配置<br>│       ├── generation<span class="hljs-emphasis">_config.json  # 默认生成参数</span><br><span class="hljs-emphasis">│       ├── LICENSE</span><br><span class="hljs-emphasis">│       ├── merges.txt              # BPE 合并规则</span><br><span class="hljs-emphasis">│       ├── model.safetensors       # 模型权重文件</span><br><span class="hljs-emphasis">│       ├── README.md</span><br><span class="hljs-emphasis">│       ├── tokenizer_</span>config.json   # tokenizer 配置<br>│       ├── tokenizer.json          # tokenizer 定义<br>│       └── vocab.json              # 词表<br>├── LICENSE<br>├── nanovllm                        # Nano-vLLM 核心代码<br>│   ├── config.py                   # 全局配置定义（设备、dtype、模型路径等）<br>│   ├── engine                      # 推理引擎核心实现（vLLM 风格）<br>│   │   ├── block<span class="hljs-emphasis">_manager.py        # KV Cache 的 block 管理与分配逻辑</span><br><span class="hljs-emphasis">│   │   ├── llm_</span>engine.py           # 推理主循环，驱动调度与模型执行<br>│   │   ├── model<span class="hljs-emphasis">_runner.py         # 封装模型 forward 的执行逻辑</span><br><span class="hljs-emphasis">│   │   ├── scheduler.py            # 序列级调度器，决定每一步执行哪些请求</span><br><span class="hljs-emphasis">│   │   └── sequence.py             # 生成序列的状态表示与管理</span><br><span class="hljs-emphasis">│   ├── <span class="hljs-strong">__init__</span>.py</span><br><span class="hljs-emphasis">│   ├── layers                      # Transformer 推理所需的基础算子</span><br><span class="hljs-emphasis">│   │   ├── activation.py           # 激活函数实现</span><br><span class="hljs-emphasis">│   │   ├── attention.py            # 自注意力计算与 KV Cache 访问</span><br><span class="hljs-emphasis">│   │   ├── embed_</span>head.py           # Embedding 与 LM Head<br>│   │   ├── layernorm.py            # LayerNorm 实现<br>│   │   ├── linear.py               # 线性层实现<br>│   │   ├── rotary<span class="hljs-emphasis">_embedding.py     # RoPE 位置编码</span><br><span class="hljs-emphasis">│   │   └── sampler.py              # 从 logits 中采样下一个 token</span><br><span class="hljs-emphasis">│   ├── llm.py                      # 对外暴露的 LLM 接口（继承自LLMEngine，目前里面是pass）</span><br><span class="hljs-emphasis">│   ├── models                      # 模型结构定义</span><br><span class="hljs-emphasis">│   │   └── qwen3.py                # Qwen3 模型的最小实现</span><br><span class="hljs-emphasis">│   ├── sampling_</span>params.py          # 文本生成的采样参数定义<br>│   └── utils                       # 工具模块<br>│       ├── context.py              # 推理上下文与辅助状态管理<br>│       └── loader.py               # HuggingFace 权重加载与映射<br>├── pyproject.toml                  # Python 项目与依赖配置<br>└── README.md                       # 项目整体说明<br><br></code></pre></td></tr></table></figure><h2 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h2><p><code>example.py</code>的代码如下所示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> nanovllm <span class="hljs-keyword">import</span> LLM, SamplingParams<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    path = os.path.expanduser(<span class="hljs-string">&quot;./huggingface/Qwen3-0.6B/&quot;</span>)<br>    tokenizer = AutoTokenizer.from_pretrained(path)<br>    llm = LLM(path, enforce_eager=<span class="hljs-literal">True</span>, tensor_parallel_size=<span class="hljs-number">1</span>)<br><br>    sampling_params = SamplingParams(temperature=<span class="hljs-number">0.6</span>, max_tokens=<span class="hljs-number">256</span>)<br>    prompts = [<br>        <span class="hljs-string">&quot;introduce yourself&quot;</span>,<br>        <span class="hljs-string">&quot;list all prime numbers within 100&quot;</span>,<br>    ]<br>    prompts = [<br>        tokenizer.apply_chat_template(<br>            [&#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: prompt&#125;],<br>            tokenize=<span class="hljs-literal">False</span>,<br>            add_generation_prompt=<span class="hljs-literal">True</span>,<br>        )<br>        <span class="hljs-keyword">for</span> prompt <span class="hljs-keyword">in</span> prompts<br>    ]<br>    outputs = llm.generate(prompts, sampling_params)<br><br>    <span class="hljs-keyword">for</span> prompt, output <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prompts, outputs):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n&quot;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Prompt: <span class="hljs-subst">&#123;prompt!r&#125;</span>&quot;</span>)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Completion: <span class="hljs-subst">&#123;output[<span class="hljs-string">&#x27;text&#x27;</span>]!r&#125;</span>&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br><br></code></pre></td></tr></table></figure><p>现在跟着example的执行过程大致梳理一下整体流程：</p><ol><li><p>根据<code>Qwen3-0.6B</code>文件夹内容初始化Tokenizer以及LLM模型</p><ol><li><code>LLM</code>是对<code>LLMEngine</code>的包装，主要是为了对齐vLLM的行为，其代码如下所示</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nanovllm.engine.llm_engine <span class="hljs-keyword">import</span> LLMEngine<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LLM</span>(<span class="hljs-title class_ inherited__">LLMEngine</span>):<br>    <span class="hljs-keyword">pass</span><br><br></code></pre></td></tr></table></figure><ul><li>LLMEngine初始化的代码如下所示，其读取了配置文件，然后其支持TP并行，会加载TP个进程，每个进程运行对应的<code>ModelRunner</code>。其还配置了对应的tokenizer与scheduler</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LLMEngine</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, **kwargs</span>):<br>        config_fields = &#123;field.name <span class="hljs-keyword">for</span> field <span class="hljs-keyword">in</span> fields(Config)&#125;<br>        config_kwargs = &#123;k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> kwargs.items() <span class="hljs-keyword">if</span> k <span class="hljs-keyword">in</span> config_fields&#125;<br>        config = Config(model, **config_kwargs)<br>        self.ps = []<br>        self.events = []<br>        ctx = mp.get_context(<span class="hljs-string">&quot;spawn&quot;</span>)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, config.tensor_parallel_size):<br>            event = ctx.Event()<br>            process = ctx.Process(target=ModelRunner, args=(config, i, event))<br>            process.start()<br>            self.ps.append(process)<br>            self.events.append(event)<br>        self.model_runner = ModelRunner(config, <span class="hljs-number">0</span>, self.events)<br>        self.tokenizer = AutoTokenizer.from_pretrained(config.model, use_fast=<span class="hljs-literal">True</span>)<br>        config.eos = self.tokenizer.eos_token_id<br>        self.scheduler = Scheduler(config)<br>        atexit.register(self.exit)<br></code></pre></td></tr></table></figure></li><li><p>配置sampling_params</p><ol><li>其代码如下所示：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@dataclass</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SamplingParams</span>:<br>    temperature: <span class="hljs-built_in">float</span> = <span class="hljs-number">1.0</span><br>    max_tokens: <span class="hljs-built_in">int</span> = <span class="hljs-number">64</span><br>    ignore_eos: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__post_init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">assert</span> self.temperature &gt; <span class="hljs-number">1e-10</span>, <span class="hljs-string">&quot;greedy sampling is not permitted&quot;</span><br><br></code></pre></td></tr></table></figure><ul><li><p>主要是支持配置：</p><ol><li><p><code>temperature</code>： $$p_i &#x3D; \mathrm{softmax}!\left(\frac{z_i}{T}\right)$$，T负责控制缩放程度，T小于1.0分布会更尖锐，大于1.0分布会更加平缓。其通过<code>assert self.temperature &gt; 1e-10</code>避免了完全确定性的输出</p></li><li><p><code>max_tokens</code>：最多生成的token数量</p></li><li><p><code>ignore_eos</code>：如果为<code>true</code>，即使生成 EOS，也继续生成直到达到 <code>max_tokens</code></p></li></ol></li></ul></li><li><p>使用tokenizer模板转换prompt</p><ol><li>经过chat模板转换后，得到的<code>prompts</code>结果如下</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-string">&#x27;&lt;|im_start|&gt;user\nintroduce yourself&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&#x27;</span>, <span class="hljs-string">&#x27;&lt;|im_start|&gt;user\nlist all prime numbers within 100&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&#x27;</span>]<br></code></pre></td></tr></table></figure></li><li><p>通过<code>llm.generate(prompts, sampling_params)</code>调用得到生成结果</p><ol><li><code>generate</code>的相关代码如下所示：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    prompts: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">str</span>] | <span class="hljs-built_in">list</span>[<span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>]],</span><br><span class="hljs-params">    sampling_params: SamplingParams | <span class="hljs-built_in">list</span>[SamplingParams],</span><br><span class="hljs-params">    use_tqdm: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">str</span>]:<br>    <span class="hljs-keyword">if</span> use_tqdm:<br>        pbar = tqdm(total=<span class="hljs-built_in">len</span>(prompts), desc=<span class="hljs-string">&quot;Generating&quot;</span>, dynamic_ncols=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(sampling_params, <span class="hljs-built_in">list</span>):<br>        sampling_params = [sampling_params] * <span class="hljs-built_in">len</span>(prompts)<br>    <span class="hljs-keyword">for</span> prompt, sp <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prompts, sampling_params):<br>        self.add_request(prompt, sp)<br>    outputs = &#123;&#125;<br>    prefill_throughput = decode_throughput = <span class="hljs-number">0.</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> self.is_finished():<br>        t = perf_counter()<br>        output, num_tokens = self.step()<br>        <span class="hljs-keyword">if</span> use_tqdm:<br>            <span class="hljs-keyword">if</span> num_tokens &gt; <span class="hljs-number">0</span>:<br>                prefill_throughput = num_tokens / (perf_counter() - t)<br>            <span class="hljs-keyword">else</span>:<br>                decode_throughput = -num_tokens / (perf_counter() - t)<br>            pbar.set_postfix(&#123;<br>                <span class="hljs-string">&quot;Prefill&quot;</span>: <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-built_in">int</span>(prefill_throughput)&#125;</span>tok/s&quot;</span>,<br>                <span class="hljs-string">&quot;Decode&quot;</span>: <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-built_in">int</span>(decode_throughput)&#125;</span>tok/s&quot;</span>,<br>            &#125;)<br>        <span class="hljs-keyword">for</span> seq_id, token_ids <span class="hljs-keyword">in</span> output:<br>            outputs[seq_id] = token_ids<br>            <span class="hljs-keyword">if</span> use_tqdm:<br>                pbar.update(<span class="hljs-number">1</span>)<br>    outputs = [outputs[seq_id] <span class="hljs-keyword">for</span> seq_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">sorted</span>(outputs.keys())]<br>    outputs = [&#123;<span class="hljs-string">&quot;text&quot;</span>: self.tokenizer.decode(token_ids), <span class="hljs-string">&quot;token_ids&quot;</span>: token_ids&#125; <span class="hljs-keyword">for</span> token_ids <span class="hljs-keyword">in</span> outputs]<br>    <span class="hljs-keyword">if</span> use_tqdm:<br>        pbar.close()<br>    <span class="hljs-keyword">return</span> outputs<br><br></code></pre></td></tr></table></figure><ul><li><p>首先会将各prompt与sampling_param组合为<code>Sequence</code>然后加入到scheduler中的waiting队列中</p><ol><li><code>add_request</code>的代码如下所示</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_request</span>(<span class="hljs-params">self, prompt: <span class="hljs-built_in">str</span> | <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>], sampling_params: SamplingParams</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(prompt, <span class="hljs-built_in">str</span>):<br>        prompt = self.tokenizer.encode(prompt)<br>    seq = <span class="hljs-type">Sequence</span>(prompt, sampling_params)<br>    self.scheduler.add(seq)<br></code></pre></td></tr></table></figure><ul><li><p>其主要会将prompt转换为token id列表，如这里的<code>&#39;&lt;|im_start|&gt;user\nintroduce yourself&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&#39;</code>会被转化为<code>[151644, 872, 198, 396, 47845, 6133, 151645, 198, 151644, 77091, 198]</code></p></li><li><p>然后scheduler.add的代码如下所示，就是直接append到waiting队列中</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">add</span>(<span class="hljs-params">self, seq: <span class="hljs-type">Sequence</span></span>):<br>    self.waiting.append(seq)<br></code></pre></td></tr></table></figure></li><li><p>然后进入while循环，循环结束的条件是<code>self.is_finished()</code></p><ol><li>LLM_Engine的<code>is_finished()</code>的代码如下所示：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">is_finished</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-keyword">return</span> self.scheduler.is_finished()<br></code></pre></td></tr></table></figure><ul><li><code>Scheduler</code>的is_finished代码如下所示，即介绍的标准就是两个队列中都没有请求</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">is_finished</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-keyword">not</span> self.waiting <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.running<br></code></pre></td></tr></table></figure></li><li><p>在while循环中主要的就是不断执行<code>self.step()</code>，然后更新进度条，根据<code>perf_counter()</code>得到的运算时间以及生成的token数量<code>num_tokens</code>计算吞吐并将输出结果存储在<code>outputs</code>中</p><ol><li><code>self.step()</code>的代码如下所示</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self</span>):<br>    seqs, is_prefill = self.scheduler.schedule()<br>    token_ids = self.model_runner.call(<span class="hljs-string">&quot;run&quot;</span>, seqs, is_prefill)<br>    self.scheduler.postprocess(seqs, token_ids)<br>    outputs = [(seq.seq_id, seq.completion_token_ids) <span class="hljs-keyword">for</span> seq <span class="hljs-keyword">in</span> seqs <span class="hljs-keyword">if</span> seq.is_finished]<br>    num_tokens = <span class="hljs-built_in">sum</span>(<span class="hljs-built_in">len</span>(seq) <span class="hljs-keyword">for</span> seq <span class="hljs-keyword">in</span> seqs) <span class="hljs-keyword">if</span> is_prefill <span class="hljs-keyword">else</span> -<span class="hljs-built_in">len</span>(seqs)<br>    <span class="hljs-keyword">return</span> outputs, num_tokens<br></code></pre></td></tr></table></figure><ul><li><p>其主要就是从scheduler中获取到要运行的seqs</p><ol><li><code>scheduler.schedule()</code>的代码如下所示</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">schedule</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">tuple</span>[<span class="hljs-built_in">list</span>[<span class="hljs-type">Sequence</span>], <span class="hljs-built_in">bool</span>]:<br>    <span class="hljs-comment"># prefill</span><br>    scheduled_seqs = []<br>    num_seqs = <span class="hljs-number">0</span><br>    num_batched_tokens = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">while</span> self.waiting <span class="hljs-keyword">and</span> num_seqs &lt; self.max_num_seqs:<br>        seq = self.waiting[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">if</span> num_batched_tokens + <span class="hljs-built_in">len</span>(seq) &gt; self.max_num_batched_tokens <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> self.block_manager.can_allocate(seq):<br>            <span class="hljs-keyword">break</span><br>        num_seqs += <span class="hljs-number">1</span><br>        self.block_manager.allocate(seq)<br>        num_batched_tokens += <span class="hljs-built_in">len</span>(seq) - seq.num_cached_tokens<br>        seq.status = SequenceStatus.RUNNING<br>        self.waiting.popleft()<br>        self.running.append(seq)<br>        scheduled_seqs.append(seq)<br>    <span class="hljs-keyword">if</span> scheduled_seqs:<br>        <span class="hljs-keyword">return</span> scheduled_seqs, <span class="hljs-literal">True</span><br><br>    <span class="hljs-comment"># decode</span><br>    <span class="hljs-keyword">while</span> self.running <span class="hljs-keyword">and</span> num_seqs &lt; self.max_num_seqs:<br>        seq = self.running.popleft()<br>        <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> self.block_manager.can_append(seq):<br>            <span class="hljs-keyword">if</span> self.running:<br>                self.preempt(self.running.pop())<br>            <span class="hljs-keyword">else</span>:<br>                self.preempt(seq)<br>                <span class="hljs-keyword">break</span><br>        <span class="hljs-keyword">else</span>:<br>            num_seqs += <span class="hljs-number">1</span><br>            self.block_manager.may_append(seq)<br>            scheduled_seqs.append(seq)<br>    <span class="hljs-keyword">assert</span> scheduled_seqs<br>    self.running.extendleft(<span class="hljs-built_in">reversed</span>(scheduled_seqs))<br>    <span class="hljs-keyword">return</span> scheduled_seqs, <span class="hljs-literal">False</span><br><br></code></pre></td></tr></table></figure><ul><li><p>其划分为了两大阶段的处理，首先是对多个请求进行<code>prefill</code>处理：</p><ol><li><p>进入<code>prefill</code>的while循环的要求是<code>waiting</code>队列中有请求，并且现在获取的seq数量没有超过<code>max_num_seqs</code>限制。此外还要求新获取的seq叠加的<code>num_batched_tokens</code>要小于<code>self.max_num_batched_tokens</code>，并且<code>block_manager</code>还有足够的显存进行分配</p></li><li><p>然后正常在while循环中处理的主要流程就是<code>block_manager</code>为新的seq分配cache，seq状态修改为running，<code>num_batched_tokens</code>添加<code> len(seq) - seq.num_cached_tokens</code>，<code>waiting</code>队列<code>popleft</code>，<code>running</code>队列<code>append</code>，<code>scheduled_seqs</code>结果<code>append</code></p></li><li><p>如果最后<code>scheduled_seqs</code>确实获取了seq，就直接返回<code>scheduled_seqs</code>及True标明这一次调度只处理了<code>prefill</code></p></li></ol></li><li><p>如果没有prefill请求了，就去处理<code>decode</code>：</p><ol><li><p>进入<code>decode</code>的while循环要求<code>running</code>队列中有请求，并且现在获取的seq数量没有超过<code>max_num_seqs</code>限制</p></li><li><p>在循环中需要从<code>running</code>队列中<code>popleft</code>获取seq，然后通过<code>block_manager</code>查看是否还有足够的显存进行分配，如果没有就需要去抢占其他seq，优先抢占<code>running</code>队列最新来的seq，如果都抢占完了再抢占自己，不过如果抢占了自己就直接退出while循环了，因为确实没有显存了。如果有足够的显存或者抢占得到了足够的显存，那么就调用<code>block_manager</code>进行<code>may_append</code>分配显存，并且结果队列<code>scheduled_seqs</code> <code>append</code>该seq</p></li><li><p>最后返回结果队列<code>scheduled_seqs</code> 及False标明是decode处理</p></li></ol></li></ul></li><li><p>然后调用<code>model_runner</code>运行模型推理</p><ol><li>这里调用的是<code>model_runner</code>的run函数，如下所示</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>(<span class="hljs-params">self, seqs: <span class="hljs-built_in">list</span>[<span class="hljs-type">Sequence</span>], is_prefill: <span class="hljs-built_in">bool</span></span>) -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>]:<br>    input_ids, positions = self.prepare_prefill(seqs) <span class="hljs-keyword">if</span> is_prefill <span class="hljs-keyword">else</span> self.prepare_decode(seqs)<br>    temperatures = self.prepare_sample(seqs) <span class="hljs-keyword">if</span> self.rank == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>    logits = self.run_model(input_ids, positions, is_prefill)<br>    token_ids = self.sampler(logits, temperatures).tolist() <span class="hljs-keyword">if</span> self.rank == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>    reset_context()<br>    <span class="hljs-keyword">return</span> token_ids<br></code></pre></td></tr></table></figure><ul><li><p>其主要流程是依据是否是prefill来做一些kv cache的准备工作</p></li><li><p>然后运行模型得到运行结果</p></li><li><p>最后采样得到<code>token_ids</code>并返回</p></li></ul></li><li><p>然后执行scheduler的一些后处理流程</p><ol><li><code>postprocess</code>的相关代码如下所示，</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">postprocess</span>(<span class="hljs-params">self, seqs: <span class="hljs-built_in">list</span>[<span class="hljs-type">Sequence</span>], token_ids: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">list</span>[<span class="hljs-built_in">bool</span>]:<br>    <span class="hljs-keyword">for</span> seq, token_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(seqs, token_ids):<br>        seq.append_token(token_id)<br>        <span class="hljs-keyword">if</span> (<span class="hljs-keyword">not</span> seq.ignore_eos <span class="hljs-keyword">and</span> token_id == self.eos) <span class="hljs-keyword">or</span> seq.num_completion_tokens == seq.max_tokens:<br>            seq.status = SequenceStatus.FINISHED<br>            self.block_manager.deallocate(seq)<br>            self.running.remove(seq)<br></code></pre></td></tr></table></figure><ul><li>其主要是逐个遍历<code>token_ids</code>，并将推理结果<code>token_id</code>添加到seq中，然后检查如果没有忽略eos而当前就是eos或者已经到达最大长度了，就将seq状态修改为<code>FINISHED</code>，然后block_manager释放掉这个seq对应的显存，并且也将其从running队列中删除</li></ul></li><li><p>如果seq是已经推理结束了是<code>FINISHED</code>状态就将其记录到outputs中，并计算中共处理了多少tokens，然后返回</p></li></ul></li><li><p>最后将<code>outputs</code>使用<code>tokenizer.decode</code>解码得到text，并将token_id与text都返回</p></li></ul></li><li><p>打印生成结果</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>vLLM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>vLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读】Efficient Memory Management for Large Language  Model Serving with PagedAttention（vLLM论文）</title>
    <link href="/2026/01/09/vLLM-paper-note/"/>
    <url>/2026/01/09/vLLM-paper-note/</url>
    
    <content type="html"><![CDATA[<ul><li><p><strong>论文链接</strong>：<a href="https://dl.acm.org/doi/abs/10.1145/3600006.3613165">https://dl.acm.org/doi/abs/10.1145/3600006.3613165</a></p></li><li><p><strong>开源链接</strong>：<a href="https://github.com/vllm-project/vllm">https://github.com/vllm-project/vllm</a></p></li><li><p><strong>发表会议</strong>：SOSP’23(CCF-A)</p></li><li><p><strong>团队</strong>：UC Berkeley</p></li></ul><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><ul><li><p>大模型服务化带来了高昂的运行成本，需要进行推理优化提高吞吐、降低成本</p></li><li><p>LLM推理过程是一个自回归模型，关键的注意力机制模块需要进行KV Cache，而现有GPU的显存有限，<strong>留给KV Cache的空间不多</strong>，需要进行细致的KV Cache显存管理。例如，对于13B参数OPT模型，单个Token的KV缓存需要800KB的空间，计算为2（键和值向量）×5120（隐藏状态大小）×40（层数）×2（每个FP16字节）。由于OPT最多可以生成2048个令牌的序列，因此存储一个请求的KV缓存所需的内存可以高达1.6GB，而GPU的显存容量在几十GB的水平。</p></li></ul><p><img src="/2026/01/09/vLLM-paper-note/image.png"></p><ul><li><p>现有的LLM服务无法有效地管理KV Cache，因为它们将KV Cache存储在连续的内存空间中（因为大多数深度学习框架要求张量存储在连续空间中）。而LLM推理因为自回归的特性，KV Cache随着模型生成新的Token，可能会逐渐增长，也可能随着结束Token的出现而消除，并且它的长度与生命周期难以提前获知。这导致现有系统在以下几个方面低效：</p><p><img src="/2026/01/09/vLLM-paper-note/image-1.png"></p><ul><li><p>现有系统往往按照最大可能长度进行显存的<strong>静态分配</strong>，这导致其存在了3个部分的显存浪费：</p><ol><li><p>分配给未来Token生成使用的<strong>预留显存</strong>（Reserved）</p></li><li><p>提前分配了但是实际却不需要用到的<strong>内部显存碎片</strong>（Internal fragmentation）</p></li><li><p>系统中因为频繁静态分配导致的过小无法被静态分配使用的<strong>外部显存碎片</strong>（External fragmentation）</p></li></ol></li><li><p>现有系统也<strong>不能利用显存共享</strong>的机会。LLM服务通常会通过并行采样、波束搜索的方式为每个请求生成多个输出，这里的就可以部分共享其前置的KV Cache，但是现有系统因为KV Cache缓存在单独的连续空间所以不能做到共享</p></li></ul><p><img src="/2026/01/09/vLLM-paper-note/image-2.png"></p></li><li><p>为了高效的管理显存，本文提出了一种受操作系统中经典虚拟内存和分页技术启发的高效注意力算力PagedAttention，并以此为基础构建了LLM推理服务vLLM</p></li></ul><h1 id="方法思路"><a href="#方法思路" class="headerlink" title="方法思路"></a>方法思路</h1><h2 id="PagedAttention"><a href="#PagedAttention" class="headerlink" title="PagedAttention"></a>PagedAttention</h2><p>PagedAttention将K、V进行了分块，每块的K、V数量为B，每块会在物理存储中连续存储。各块之间并不保证连续，如下图所示，其前面的K、V被存储到了3个Block中。</p><p><img src="/2026/01/09/vLLM-paper-note/image-3.png"></p><p>在此分块的背景下，计算Attention的方式也转变为了以Block为单位的分块计算，如下所示。</p><p><img src="/2026/01/09/vLLM-paper-note/image-4.png"></p><ul><li><p>在计算 $Attention&#x3D; \mathrm{softmax}!\left(\frac{Q K^\top}{\sqrt{d_k}}\right)$时，将当前最新的$q_i$与第j个Block中的 $K_j$直接进行矩阵计算就可以批量得到该Block中各个K对于$q_i$的Attention，即$A_{ij}$。</p></li><li><p>然后在计算最新输出$o_i$时，对于第j个Block，需要将刚得到的$A_{ij}$与原本缓存该Block的 $V_j$进行矩阵乘，然后进行累加即可得到$o_i$</p></li></ul><h2 id="KV-Cache管理"><a href="#KV-Cache管理" class="headerlink" title="KV Cache管理"></a>KV Cache管理</h2><ul><li><p>系统中存在逻辑 Block和物理Block两个概念，其通过Block Table进行对应，Block Table记录了逻辑Block与物理Block的对应关系以及当前物理Block具体已填充的数量</p></li><li><p>每个物理Block具有固定的大小，物理Block可以按需灵活分配与释放</p></li></ul><p><img src="/2026/01/09/vLLM-paper-note/image-5.png"></p><p><img src="/2026/01/09/vLLM-paper-note/image-6.png"></p><h2 id="复杂解码算法适配"><a href="#复杂解码算法适配" class="headerlink" title="复杂解码算法适配"></a>复杂解码算法适配</h2><ul><li><strong>平行采样示例：</strong>&#x53EF;以让两个Sample一开始的prompt都共享同一个物理KV Cache block，并且注意对该Block设置引用计数器为2，然后当一个Sample开始生成并写入新的Token时，因为观察到引用计数器大于1，所以就采用<strong>Copy-on-write</strong>的处理方法，将其复制一份再写，同时将原本的引用计数器减一</li></ul><p><img src="/2026/01/09/vLLM-paper-note/image-7.png"></p><ul><li><strong>波束搜索示例：</strong>&#x5176;保留了top k个最可能的序列，各序列除了prompt外，已生成的序列也可能会共享，通过PagedAttention的技术可以对其做到共享，并且因为block分块，所以在Copy-on-write复制的时候只会复制最新生成的token所在的一个block。</li></ul><p><img src="/2026/01/09/vLLM-paper-note/image-8.png"></p><ul><li><strong>共享前缀实例：</strong>&#x591A;个请求共享同一个前缀，所以可以直接使用同一个前缀的物理块</li></ul><p><img src="/2026/01/09/vLLM-paper-note/image-9.png"></p><h2 id="调度与抢占"><a href="#调度与抢占" class="headerlink" title="调度与抢占"></a>调度与抢占</h2><ul><li><p>vLLM对所有请求采用<strong>first-come-first-serve（FCFS）调度策略</strong>，确保公平，防止饥饿。</p></li><li><p>当vLLM需要抢占请求时，它确保最早到达的请求被优先服务，最新的请求被优先抢占。</p><ul><li><p>具体在抢占时采用“全有或全无”的策略，驱逐序列块时将序列中的所有块都驱逐，对于波束搜索这类一个请求中包含多个序列的，也是倾向于一起被抢占或重调度</p></li><li><p>而在恢复时有两种方法，如下图所示，，这两种方法的效率取决Block size以及系统swap速度和计算速度等</p><ul><li><p>与主机CPU内存进行交换</p></li><li><p>将K、V进行重计算</p></li></ul><p><img src="/2026/01/09/vLLM-paper-note/image-10.png"></p></li></ul></li></ul><h2 id="分布式执行"><a href="#分布式执行" class="headerlink" title="分布式执行"></a>分布式执行</h2><ul><li><p>对于TP并行，其采用的是Megatron-LM风格的TP并行，在分割多头注意力时，每个worker都至少保留完整的一个注意力头</p></li><li><p>并且整体也适配SPMD的执行策略，虽然访问的是同一个逻辑Block，但是不同worker上对应的物理Block是不同的</p></li></ul><h2 id="实现细节优化"><a href="#实现细节优化" class="headerlink" title="实现细节优化"></a>实现细节优化</h2><ul><li><p>内核级优化：</p><ul><li><p>Fused reshape and block write：KV Cache需要被reshape为block的格式然后被存储到物理显存中，为了加速将其融合为了一个kernel</p></li><li><p>Fusing block read and attention：其调整FasterTransform中的注意力内核，根据块表读取KV缓存，并动态执行注意力操作。为了确保合并内存访问，还分配一个GPU warp来读取每个块</p></li><li><p>Fused block copy：其实现了一个内核，将不同块的复制操作批处理到单个内核启动中，以避免大量小数据移动的调用</p></li></ul></li><li><p>vLLM使用三种关键方法实现各种解码算法：</p><ul><li><p>Fork: 从现有序列中创建一个新序列。</p></li><li><p>Append: 将一个新TOken附加到序列中</p></li><li><p>Free: 删除序列</p></li></ul></li></ul><h1 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h1><ul><li>由于vLLM出色的显存管理能力，其对比当时其他的FasterTransformer与Orca在单序列生成、多序列生成方面都取得了更好的效果</li></ul><p><img src="/2026/01/09/vLLM-paper-note/image-11.png"></p><p><img src="/2026/01/09/vLLM-paper-note/image-12.png"></p><p><img src="/2026/01/09/vLLM-paper-note/image-13.png"></p><ul><li>整体在ChatBot应用上的表现是Orca的两倍以上</li></ul><p><img src="/2026/01/09/vLLM-paper-note/image-14.png"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li><p>虚拟页表技术是原本CPU内存管理的经典技术了，被挪用到大模型推理场景下进行显存管理确实有点大道至简的感觉</p></li><li><p>整体思路很好理解，效果也理所当然的很强。不过感觉确实最关键的是将其实现出来并能够支持上到生产环境去使用</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>vLLM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>LLM</tag>
      
      <tag>vLLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Megatron-LM源码分析（五）】-Tensor并行</title>
    <link href="/2026/01/08/megatron-lm-tp/"/>
    <url>/2026/01/08/megatron-lm-tp/</url>
    
    <content type="html"><![CDATA[<h1 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h1><ul><li><p>基础的理论分析可以见之前写的内容：<a href="https://slipegg.github.io/2025/06/07/Picotron-Tutorial%20Tensor%20Parallel/%EF%BC%8Chttps://slipegg.github.io/2025/12/07/Megatron-LM-paper-note/">https://slipegg.github.io/2025/06/07/Picotron-Tutorial%20Tensor%20Parallel/，https://slipegg.github.io/2025/12/07/Megatron-LM-paper-note/</a></p></li><li><p>简单来说就是存在<strong>行并行</strong>与<strong>列并行</strong>两种Tensor并行方式。</p></li><li><p>上述的前向传播好理解，但是反向传播会稍微复杂一些，这里可以直接访问ChatGPT查看其介绍，介绍的还是很详细的：<a href="https://chatgpt.com/share/6953c681-7cd0-8011-8dfe-1d2281834b08">https://chatgpt.com/share/6953c681-7cd0-8011-8dfe-1d2281834b08</a></p><ul><li><p>结论就是在列并行中求 $\frac{\partial L}{\partial X}$时需要All Reduce(Sum)操作，而求$\frac{\partial L}{\partial W}$不需要，在行并行中求偏导的时候都不需要额外通信操作。</p></li><li><p>由于在MLP层我们采取的是先列并行再行并行的形式，从而减少前向传播过程中的通信量，故在反向传播过程中在反向传播到列并行时也需要进行一次All Reduce(Sum)操作。</p></li></ul></li></ul><h1 id="训练数据获取"><a href="#训练数据获取" class="headerlink" title="训练数据获取"></a>训练数据获取</h1><p>在<code>pretrain_gpt.py</code>文件中的<code>get_batch</code>函数可以看到有专门的tp数据处理，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_batch</span>(<span class="hljs-params">data_iterator</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Generate a batch.&quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> this is pretty hacky, find a better way</span><br>    <span class="hljs-keyword">if</span> (<span class="hljs-keyword">not</span> parallel_state.is_pipeline_first_stage(ignore_virtual=<span class="hljs-literal">True</span>)) <span class="hljs-keyword">and</span> (<br>        <span class="hljs-keyword">not</span> parallel_state.is_pipeline_last_stage(ignore_virtual=<span class="hljs-literal">True</span>)<br>    ):<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># get batches based on the TP rank you are on</span><br>    batch = get_batch_on_this_tp_rank(data_iterator)<br><br>    <span class="hljs-comment"># slice batch along sequence dimension for context parallelism</span><br>    batch = get_batch_on_this_cp_rank(batch)<br><br>    <span class="hljs-keyword">return</span> batch.values()<br><br></code></pre></td></tr></table></figure><p>进一步的，查看<code>get_batch_on_this_tp_rank</code>函数如下所示，tp rank为0的worker会从data loader中获取一份micro_batch的数据，然后组成batch格式，将其broadcast到tp组的其他worker中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_batch_on_this_tp_rank</span>(<span class="hljs-params">data_iterator</span>):<br><br>    args = get_args()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_broadcast</span>(<span class="hljs-params">item</span>):<br>        <span class="hljs-keyword">if</span> item <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            torch.distributed.broadcast(<br>                item,<br>                mpu.get_tensor_model_parallel_src_rank(),<br>                group=mpu.get_tensor_model_parallel_group(),<br>            )<br><br>    <span class="hljs-keyword">if</span> mpu.get_tensor_model_parallel_rank() == <span class="hljs-number">0</span>:<br><br>        <span class="hljs-keyword">if</span> data_iterator <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            data = <span class="hljs-built_in">next</span>(data_iterator)<br>        <span class="hljs-keyword">else</span>:<br>            data = <span class="hljs-literal">None</span><br><br>        batch = &#123;<br>            <span class="hljs-string">&#x27;tokens&#x27;</span>: data[<span class="hljs-string">&quot;tokens&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>            <span class="hljs-string">&#x27;labels&#x27;</span>: data[<span class="hljs-string">&quot;labels&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>            <span class="hljs-string">&#x27;loss_mask&#x27;</span>: data[<span class="hljs-string">&quot;loss_mask&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>            <span class="hljs-string">&#x27;attention_mask&#x27;</span>: (<br>                <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;attention_mask&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> data<br>                <span class="hljs-keyword">else</span> data[<span class="hljs-string">&quot;attention_mask&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>)<br>            ),<br>            <span class="hljs-string">&#x27;position_ids&#x27;</span>: data[<span class="hljs-string">&quot;position_ids&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>        &#125;<br><br>        <span class="hljs-keyword">if</span> args.pipeline_model_parallel_size == <span class="hljs-number">1</span>:<br>            _broadcast(batch[<span class="hljs-string">&#x27;tokens&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;labels&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;loss_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;position_ids&#x27;</span>])<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_first_stage():<br>            _broadcast(batch[<span class="hljs-string">&#x27;tokens&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;position_ids&#x27;</span>])<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_last_stage():<br>            <span class="hljs-comment"># Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.</span><br>            <span class="hljs-comment"># Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need</span><br>            <span class="hljs-comment"># to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.</span><br>            <span class="hljs-keyword">if</span> args.mtp_num_layers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                _broadcast(batch[<span class="hljs-string">&#x27;tokens&#x27;</span>])<br>                _broadcast(batch[<span class="hljs-string">&#x27;position_ids&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;labels&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;loss_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])<br><br>    <span class="hljs-keyword">else</span>:<br><br>        tokens = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.int64,<br>            device=torch.cuda.current_device(),<br>        )<br>        labels = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.int64,<br>            device=torch.cuda.current_device(),<br>        )<br>        loss_mask = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.float32,<br>            device=torch.cuda.current_device(),<br>        )<br>        <span class="hljs-keyword">if</span> args.create_attention_mask_in_dataloader:<br>            attention_mask = torch.empty(<br>                (args.micro_batch_size, <span class="hljs-number">1</span>, args.seq_length, args.seq_length),<br>                dtype=torch.<span class="hljs-built_in">bool</span>,<br>                device=torch.cuda.current_device(),<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            attention_mask = <span class="hljs-literal">None</span><br>        position_ids = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.int64,<br>            device=torch.cuda.current_device(),<br>        )<br><br>        <span class="hljs-keyword">if</span> args.pipeline_model_parallel_size == <span class="hljs-number">1</span>:<br>            _broadcast(tokens)<br>            _broadcast(labels)<br>            _broadcast(loss_mask)<br>            _broadcast(attention_mask)<br>            _broadcast(position_ids)<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_first_stage():<br>            labels = <span class="hljs-literal">None</span><br>            loss_mask = <span class="hljs-literal">None</span><br><br>            _broadcast(tokens)<br>            _broadcast(attention_mask)<br>            _broadcast(position_ids)<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_last_stage():<br>            <span class="hljs-comment"># Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.</span><br>            <span class="hljs-comment"># Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need</span><br>            <span class="hljs-comment"># to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.</span><br>            <span class="hljs-keyword">if</span> args.mtp_num_layers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                _broadcast(tokens)<br>                _broadcast(position_ids)<br>            <span class="hljs-keyword">else</span>:<br>                tokens = <span class="hljs-literal">None</span><br>                position_ids = <span class="hljs-literal">None</span><br><br>            _broadcast(labels)<br>            _broadcast(loss_mask)<br>            _broadcast(attention_mask)<br><br>        batch = &#123;<br>            <span class="hljs-string">&#x27;tokens&#x27;</span>: tokens,<br>            <span class="hljs-string">&#x27;labels&#x27;</span>: labels,<br>            <span class="hljs-string">&#x27;loss_mask&#x27;</span>: loss_mask,<br>            <span class="hljs-string">&#x27;attention_mask&#x27;</span>: attention_mask,<br>            <span class="hljs-string">&#x27;position_ids&#x27;</span>: position_ids,<br>        &#125;<br><br>    <span class="hljs-keyword">return</span> batch<br><br></code></pre></td></tr></table></figure><p>上述可以看到其主要broadcast了tokens、labels、loss_mask、attention_mask、position_ids这五分数据，如下图的torch profiler所示，也确实发生了5次的broadcast。</p><p><img src="/2026/01/08/megatron-lm-tp/image.png"></p><h1 id="Tensor并行相关代码"><a href="#Tensor并行相关代码" class="headerlink" title="Tensor并行相关代码"></a>Tensor并行相关代码</h1><h2 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h2><p>model构建的入口函数在<code>pretrain_gpt.py</code>的<code>model_provider</code>函数中，其默认执行路线如下所示（去除了一些不必要的分支）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_provider</span>(<span class="hljs-params"></span><br><span class="hljs-params">    pre_process=<span class="hljs-literal">True</span>, post_process=<span class="hljs-literal">True</span>, vp_stage: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span></span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Union</span>[GPTModel, megatron.legacy.model.GPTModel]:<br>    <span class="hljs-string">&quot;&quot;&quot;Builds the model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    If you set the use_legacy_models to True, it will return the legacy GPT model and if not the mcore GPT model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        pre_process (bool, optional): Set to true if you need to compute embedings. Defaults to True.</span><br><span class="hljs-string">        post_process (bool, optional): Set to true if you need to want to compute output logits/loss. Defaults to True.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        Union[GPTModel, megatron.legacy.model.GPTModel]: The returned model</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    args = get_args()<br><br>    use_te = args.transformer_impl == <span class="hljs-string">&quot;transformer_engine&quot;</span><br><br>    print_rank_0(<span class="hljs-string">&#x27;building GPT model ...&#x27;</span>)<br>    <span class="hljs-comment"># Experimental loading arguments from yaml</span><br>    <span class="hljs-keyword">if</span> args.yaml_cfg <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config = core_transformer_config_from_yaml(args, <span class="hljs-string">&quot;language_model&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        config = core_transformer_config_from_args(args)<br><br>                <span class="hljs-comment"># Define the decoder layer spec</span><br>                transformer_layer_spec = _get_transformer_layer_spec(use_te, config)<br><br>        model = GPTModel(<br>            config=config,<br>            transformer_layer_spec=transformer_layer_spec,<br>            vocab_size=args.padded_vocab_size,<br>            max_sequence_length=args.max_position_embeddings,<br>            pre_process=pre_process,<br>            post_process=post_process,<br>            fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,<br>            parallel_output=<span class="hljs-literal">True</span>,<br>            share_embeddings_and_output_weights=<span class="hljs-keyword">not</span> args.untie_embeddings_and_output_weights,<br>            position_embedding_type=args.position_embedding_type,<br>            rotary_percent=args.rotary_percent,<br>            rotary_base=args.rotary_base,<br>            rope_scaling=args.use_rope_scaling,<br>            mtp_block_spec=mtp_block_spec,<br>            vp_stage=vp_stage,<br>        )<br><br>    <span class="hljs-keyword">return</span> model<br><br></code></pre></td></tr></table></figure><p>对于<code>_get_transformer_layer_spec</code>函数，其实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_transformer_layer_spec</span>(<span class="hljs-params">use_te, config</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Get transformer layer specification based on configuration.</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        use_te (bool): Whether to use Transformer Engine</span><br><span class="hljs-string">        args: Training arguments</span><br><span class="hljs-string">        config: Model configuration</span><br><span class="hljs-string">        </span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        transformer_layer_spec: The transformer layer specification</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    args = get_args()<br>    <span class="hljs-keyword">if</span> use_te:<br>        <span class="hljs-keyword">return</span> get_gpt_layer_with_transformer_engine_spec(<br>            args.num_experts,<br>            args.moe_grouped_gemm,<br>            args.qk_layernorm,<br>            args.multi_latent_attention,<br>            moe_use_legacy_grouped_gemm=args.moe_use_legacy_grouped_gemm,<br>            qk_l2_norm=args.qk_l2_norm,<br>            use_kitchen=config.use_kitchen,<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> get_gpt_layer_local_spec(<br>            args.num_experts,<br>            args.moe_grouped_gemm,<br>            args.qk_layernorm,<br>            args.multi_latent_attention,<br>            moe_use_legacy_grouped_gemm=args.moe_use_legacy_grouped_gemm,<br>            normalization=args.normalization,<br>            use_kitchen=config.use_kitchen,<br>        )<br><br></code></pre></td></tr></table></figure><p>默认参数中<code>use_te</code>为<code>True</code>，即使用了具有算子融合等优化的<code>transformer_engine</code>，故走到了<code>get_gpt_layer_with_transformer_engine_spec</code>分支，而不是Megatron-LM本地的<code>get_gpt_layer_local_spec</code>分支，<code>get_gpt_layer_with_transformer_engine_spec</code>如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_gpt_layer_with_transformer_engine_spec</span>(<span class="hljs-params"></span><br><span class="hljs-params">    num_experts: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    moe_grouped_gemm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    qk_layernorm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    multi_latent_attention: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    fp8: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># pylint: disable=unused-argument</span></span><br><span class="hljs-params">    moe_use_legacy_grouped_gemm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    qk_l2_norm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    use_te_op_fuser: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    use_kitchen: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params"></span>) -&gt; ModuleSpec:<br>    <span class="hljs-string">&quot;&quot;&quot;Use this spec to use lower-level Transformer Engine modules (required for fp8 training).</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        num_experts (int, optional): Number of experts. Defaults to None.</span><br><span class="hljs-string">        moe_grouped_gemm (bool, optional): To use Grouped GEMM. Defaults to False.</span><br><span class="hljs-string">        qk_layernorm (bool, optional): To use layernorm for queries/keys. Defaults to False.</span><br><span class="hljs-string">        fp8 (str, optional): Deprecated. For temporary Nemo compatibility.</span><br><span class="hljs-string">        moe_use_legacy_grouped_gemm (bool, optional): Force use the legacy GroupedMLP.</span><br><span class="hljs-string">                                                      Defaults to False.</span><br><span class="hljs-string">        qk_l2_norm (bool, optional): To use l2 norm for queries/keys. Defaults to False.</span><br><span class="hljs-string">        use_te_op_fuser (bool, optional): Use Transformer Engine&#x27;s operation-based API, which may</span><br><span class="hljs-string">                                          enable certain operation fusions. Defaults to False.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        ModuleSpec: Module specification with TE modules</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> fp8 <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        warnings.warn(<br>            <span class="hljs-string">&#x27;The fp8 argument in &quot;get_gpt_layer_with_transformer_engine_spec&quot; has been deprecated&#x27;</span><br>            <span class="hljs-string">&quot; and will be removed soon. Please update your code accordingly.&quot;</span><br>        )<br><br>    <span class="hljs-keyword">if</span> use_kitchen:<br>        <span class="hljs-keyword">assert</span> HAVE_KITCHEN<br>        backend: BackendSpecProvider = KitchenSpecProvider(fallback=TESpecProvider())<br>        <span class="hljs-keyword">if</span> use_te_op_fuser:<br>            <span class="hljs-keyword">raise</span> AssertionError(<span class="hljs-string">&quot;use_te_op_fuser not compatible with using kitchen in mlp.&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        backend = TESpecProvider()<br><br>    mlp = get_mlp_module_spec_for_backend(<br>        backend=backend,<br>        num_experts=num_experts,<br>        moe_grouped_gemm=moe_grouped_gemm,<br>        moe_use_legacy_grouped_gemm=moe_use_legacy_grouped_gemm,<br>        use_te_op_fuser=use_te_op_fuser,<br>    )<br><br>    <span class="hljs-keyword">if</span> multi_latent_attention:<br>        <span class="hljs-keyword">assert</span> qk_l2_norm <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>, <span class="hljs-string">&quot;qk_l2_norm is not supported with MLA.&quot;</span><br>        linear_q_up_proj = (<br>            backend.column_parallel_layer_norm_linear()<br>            <span class="hljs-keyword">if</span> qk_layernorm<br>            <span class="hljs-keyword">else</span> backend.column_parallel_linear()<br>        )<br>        linear_kv_up_proj = (<br>            backend.column_parallel_layer_norm_linear()<br>            <span class="hljs-keyword">if</span> qk_layernorm<br>            <span class="hljs-keyword">else</span> backend.column_parallel_linear()<br>        )<br>        <span class="hljs-keyword">return</span> ModuleSpec(<br>            module=TransformerLayer,<br>            submodules=TransformerLayerSubmodules(<br>                input_layernorm=backend.layer_norm(),<br>                self_attention=ModuleSpec(<br>                    module=MLASelfAttention,<br>                    params=&#123;<span class="hljs-string">&quot;attn_mask_type&quot;</span>: AttnMaskType.causal&#125;,<br>                    submodules=MLASelfAttentionSubmodules(<br>                        linear_q_proj=backend.column_parallel_linear(),<br>                        linear_q_down_proj=backend.linear(),<br>                        linear_q_up_proj=linear_q_up_proj,<br>                        linear_kv_down_proj=backend.linear(),<br>                        linear_kv_up_proj=linear_kv_up_proj,<br>                        core_attention=backend.core_attention(),<br>                        linear_proj=backend.row_parallel_linear(),<br>                        q_layernorm=IdentityOp,<br>                        kv_layernorm=IdentityOp,<br>                    ),<br>                ),<br>                self_attn_bda=get_bias_dropout_add,<br>                pre_mlp_layernorm=backend.layer_norm() <span class="hljs-keyword">if</span> num_experts <span class="hljs-keyword">else</span> IdentityOp,<br>                mlp=mlp,<br>                mlp_bda=get_bias_dropout_add,<br>            ),<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        qk_norm = backend.layer_norm(for_qk=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">return</span> ModuleSpec(<br>            module=TransformerLayer,<br>            submodules=TransformerLayerSubmodules(<br>                self_attention=ModuleSpec(<br>                    module=SelfAttention,<br>                    params=&#123;<span class="hljs-string">&quot;attn_mask_type&quot;</span>: AttnMaskType.causal&#125;,<br>                    submodules=SelfAttentionSubmodules(<br>                        linear_qkv=backend.column_parallel_layer_norm_linear(),<br>                        core_attention=backend.core_attention(),<br>                        linear_proj=backend.row_parallel_linear(),<br>                        q_layernorm=(<br>                            L2Norm <span class="hljs-keyword">if</span> qk_l2_norm <span class="hljs-keyword">else</span> (qk_norm <span class="hljs-keyword">if</span> qk_layernorm <span class="hljs-keyword">else</span> IdentityOp)<br>                        ),<br>                        k_layernorm=(<br>                            L2Norm <span class="hljs-keyword">if</span> qk_l2_norm <span class="hljs-keyword">else</span> (qk_norm <span class="hljs-keyword">if</span> qk_layernorm <span class="hljs-keyword">else</span> IdentityOp)<br>                        ),<br>                    ),<br>                ),<br>                self_attn_bda=get_bias_dropout_add,<br>                pre_mlp_layernorm=backend.layer_norm() <span class="hljs-keyword">if</span> num_experts <span class="hljs-keyword">else</span> IdentityOp,<br>                mlp=mlp,<br>                mlp_bda=get_bias_dropout_add,<br>                sharded_state_dict_keys_map=&#123;<br>                    <span class="hljs-string">&quot;mlp.0.weight&quot;</span>: <span class="hljs-string">&quot;mlp.linear_fc1.layer_norm_weight&quot;</span>,<br>                    <span class="hljs-string">&quot;mlp.0.bias&quot;</span>: <span class="hljs-string">&quot;mlp.linear_fc1.layer_norm_bias&quot;</span>,<br>                    <span class="hljs-string">&quot;mlp.1.basic_ops.0.weight&quot;</span>: <span class="hljs-string">&quot;mlp.linear_fc1.weight&quot;</span>,<br>                    <span class="hljs-string">&quot;mlp.1.basic_ops.1.bias&quot;</span>: <span class="hljs-string">&quot;mlp.linear_fc1.bias&quot;</span>,<br>                    <span class="hljs-string">&quot;mlp.3.basic_ops.0.weight&quot;</span>: <span class="hljs-string">&quot;mlp.linear_fc2.weight&quot;</span>,<br>                    <span class="hljs-string">&quot;mlp.3.basic_ops.1.bias&quot;</span>: <span class="hljs-string">&quot;mlp.linear_fc2.bias&quot;</span>,<br>                &#125;,<br>            ),<br>        )<br><br></code></pre></td></tr></table></figure><p>注意上述的<code>use_kitchen</code>很重要，而默认情况下其为False，故<code>backend = TESpecProvider()</code>，即使用的是<code>transformer_engine</code>来生成TransformerLayer，而不是用 NVIDIA Kitchen作为后端来提供（部分）Transformer 子模块的实现&#x2F;spec。而Megatron-LM还进一步对<code>transformer_engine</code>的相关模块进行了简单封装以使其可以支持Tensor并行等功能。</p><h3 id="Megatron-LM本地实现gpt-layer"><a href="#Megatron-LM本地实现gpt-layer" class="headerlink" title="Megatron-LM本地实现gpt_layer"></a>Megatron-LM本地实现gpt_layer</h3><p>由于<code>transformer_engine</code>是专有封装过于复杂，所以我们转而去查看Megatron-LM的本地实现，<code>get_gpt_layer_with_transformer_engine_spec</code>如下所示，我们查看的backend为<code>LocalSpecProvider</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_gpt_layer_local_spec</span>(<span class="hljs-params"></span><br><span class="hljs-params">    num_experts: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    moe_grouped_gemm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    qk_layernorm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    multi_latent_attention: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    fp8: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># pylint: disable=unused-argument</span></span><br><span class="hljs-params">    moe_use_legacy_grouped_gemm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    normalization: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    qk_l2_norm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    use_kitchen: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params"></span>) -&gt; ModuleSpec:<br>    <span class="hljs-string">&quot;&quot;&quot;Use this spec for an implementation using only modules in Megatron-Core.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        num_experts (int, optional): Number of experts. Defaults to None.</span><br><span class="hljs-string">        moe_grouped_gemm (bool, optional): To use Grouped GEMM. Defaults to False.</span><br><span class="hljs-string">        qk_layernorm (bool, optional): To use layernorm for queries/keys. Defaults to False.</span><br><span class="hljs-string">        fp8 (str, optional): Deprecated. For temporary Nemo compatibility.</span><br><span class="hljs-string">        moe_use_legacy_grouped_gemm (bool, optional): Force use the legacy GroupedMLP.</span><br><span class="hljs-string">                                                      Defaults to False.</span><br><span class="hljs-string">        qk_l2_norm (bool, optional): To use l2 norm for queries/keys. Defaults to False.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        ModuleSpec: Module specification with Megatron-Core modules</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">if</span> use_kitchen:<br>        <span class="hljs-keyword">assert</span> HAVE_KITCHEN<br>        backend = KitchenSpecProvider(fallback=LocalSpecProvider())<br>    <span class="hljs-keyword">else</span>:<br>        backend = LocalSpecProvider()<br>    <span class="hljs-comment"># Adjust for RMS norm.</span><br>    <span class="hljs-keyword">if</span> normalization == <span class="hljs-string">&quot;RMSNorm&quot;</span>:<br>        layer_norm = backend.layer_norm(rms_norm=<span class="hljs-literal">True</span>, for_qk=<span class="hljs-literal">False</span>)<br>        qk_norm = backend.layer_norm(rms_norm=<span class="hljs-literal">True</span>, for_qk=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">else</span>:<br>        layer_norm = backend.layer_norm(rms_norm=<span class="hljs-literal">False</span>, for_qk=<span class="hljs-literal">False</span>)<br>        qk_norm = backend.layer_norm(rms_norm=<span class="hljs-literal">False</span>, for_qk=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">if</span> fp8 <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        warnings.warn(<br>            <span class="hljs-string">&#x27;The fp8 argument in &quot;get_gpt_layer_local_spec&quot; has been deprecated&#x27;</span><br>            <span class="hljs-string">&quot; and will be removed soon. Please update your code accordingly.&quot;</span><br>        )<br><br>    mlp = get_mlp_module_spec_for_backend(<br>        backend=backend,<br>        num_experts=num_experts,<br>        moe_grouped_gemm=moe_grouped_gemm,<br>        moe_use_legacy_grouped_gemm=moe_use_legacy_grouped_gemm,<br>    )<br><br>    <span class="hljs-keyword">if</span> multi_latent_attention:<br>        <span class="hljs-keyword">assert</span> qk_l2_norm <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>, <span class="hljs-string">&quot;qk_l2_norm is not supported with MLA.&quot;</span><br>        <span class="hljs-keyword">return</span> ModuleSpec(<br>            module=TransformerLayer,<br>            submodules=TransformerLayerSubmodules(<br>                input_layernorm=layer_norm,<br>                self_attention=ModuleSpec(<br>                    module=MLASelfAttention,<br>                    params=&#123;<span class="hljs-string">&quot;attn_mask_type&quot;</span>: AttnMaskType.causal&#125;,<br>                    submodules=MLASelfAttentionSubmodules(<br>                        linear_q_proj=backend.column_parallel_linear(),<br>                        linear_q_down_proj=backend.column_parallel_linear(),<br>                        linear_q_up_proj=backend.column_parallel_linear(),<br>                        linear_kv_down_proj=backend.column_parallel_linear(),<br>                        linear_kv_up_proj=backend.column_parallel_linear(),<br>                        core_attention=backend.core_attention(),<br>                        linear_proj=backend.row_parallel_linear(),<br>                        q_layernorm=qk_norm <span class="hljs-keyword">if</span> qk_layernorm <span class="hljs-keyword">else</span> IdentityOp,<br>                        kv_layernorm=qk_norm <span class="hljs-keyword">if</span> qk_layernorm <span class="hljs-keyword">else</span> IdentityOp,<br>                    ),<br>                ),<br>                self_attn_bda=get_bias_dropout_add,<br>                pre_mlp_layernorm=layer_norm,<br>                mlp=mlp,<br>                mlp_bda=get_bias_dropout_add,<br>            ),<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> ModuleSpec(<br>            module=TransformerLayer,<br>            submodules=TransformerLayerSubmodules(<br>                input_layernorm=layer_norm,<br>                self_attention=ModuleSpec(<br>                    module=SelfAttention,<br>                    params=&#123;<span class="hljs-string">&quot;attn_mask_type&quot;</span>: AttnMaskType.causal&#125;,<br>                    submodules=SelfAttentionSubmodules(<br>                        linear_qkv=backend.column_parallel_linear(),<br>                        core_attention=backend.core_attention(),<br>                        linear_proj=backend.row_parallel_linear(),<br>                        q_layernorm=(<br>                            L2Norm <span class="hljs-keyword">if</span> qk_l2_norm <span class="hljs-keyword">else</span> (qk_norm <span class="hljs-keyword">if</span> qk_layernorm <span class="hljs-keyword">else</span> IdentityOp)<br>                        ),<br>                        k_layernorm=(<br>                            L2Norm <span class="hljs-keyword">if</span> qk_l2_norm <span class="hljs-keyword">else</span> (qk_norm <span class="hljs-keyword">if</span> qk_layernorm <span class="hljs-keyword">else</span> IdentityOp)<br>                        ),<br>                    ),<br>                ),<br>                self_attn_bda=get_bias_dropout_add,<br>                pre_mlp_layernorm=layer_norm,<br>                mlp=mlp,<br>                mlp_bda=get_bias_dropout_add,<br>                sharded_state_dict_keys_map=&#123;<br>                    <span class="hljs-string">&quot;input_layernorm.&quot;</span>: <span class="hljs-string">&quot;self_attention.linear_qkv.layer_norm_&quot;</span>,<br>                    <span class="hljs-string">&quot;pre_mlp_layernorm.&quot;</span>: <span class="hljs-string">&quot;mlp.linear_fc1.layer_norm_&quot;</span>,<br>                &#125;,<br>            ),<br>        )<br><br></code></pre></td></tr></table></figure><ul><li><p>其使用的是<code>TransformerLayer</code>来组装，初始化代码如下所示，初始化的模块依次为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    config: TransformerConfig,</span><br><span class="hljs-params">    submodules: TransformerLayerSubmodules,</span><br><span class="hljs-params">    layer_number: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">    hidden_dropout: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">float</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    model_comm_pgs: <span class="hljs-type">Optional</span>[ModelCommProcessGroups] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    vp_stage: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-built_in">super</span>().__init__(config=config)<br><br>    <span class="hljs-comment"># Enable cuda graphs.</span><br>    <span class="hljs-keyword">if</span> (<br>        config.enable_cuda_graph <span class="hljs-keyword">and</span> config.cuda_graph_scope != <span class="hljs-string">&quot;full_iteration&quot;</span><br>    ) <span class="hljs-keyword">or</span> config.external_cuda_graph:<br>        <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> (<br>            config.enable_cuda_graph <span class="hljs-keyword">and</span> config.external_cuda_graph<br>        ), <span class="hljs-string">&quot;Cudagraphs and external cudagraphs cannot be enabled at the same time&quot;</span><br>        <span class="hljs-keyword">if</span> config.enable_cuda_graph <span class="hljs-keyword">and</span> config.cuda_graph_scope != <span class="hljs-string">&quot;full_iteration&quot;</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.training:<br>                <span class="hljs-comment"># Cudagraphs for inference are only enabled with the flash decoding kernel</span><br>                <span class="hljs-keyword">assert</span> (<br>                    self.config.flash_decode<br>                ), <span class="hljs-string">&quot;--flash-decode is required to use CUDA graphs during inference&quot;</span><br>            self.cudagraph_manager = CudaGraphManager(config, vp_stage=vp_stage)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># List to store CUDA graphs. A list of `N` CUDA graphs for this layer where N is</span><br>            <span class="hljs-comment"># the number of microbatches. Multiple CUDA graphs per layer is required to support</span><br>            <span class="hljs-comment"># pipelining which requires running FWD graph of multiple microbatches before BWD</span><br>            <span class="hljs-comment"># graph. To enable CUDA graph, this list should be populated in the model training</span><br>            <span class="hljs-comment"># script with the graphs returned by make_graphed_callables API before the first</span><br>            <span class="hljs-comment"># training step.</span><br>            self.cuda_graphs = []<br>            <span class="hljs-comment"># List to store forward pre-hooks. Forward pre-hooks are not captured into CUDA</span><br>            <span class="hljs-comment"># graphs. Those hooks and args are collected in this list and should be manually</span><br>            <span class="hljs-comment"># triggered before CUDA Graph running. This is required to ensure the correct param</span><br>            <span class="hljs-comment"># all-gather overlap with forward compute.</span><br>            self.cuda_graph_manual_hooks = []<br>            self.current_microbatch = -<span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">if</span> model_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        model_comm_pgs = ModelCommProcessGroups.use_mpu_process_groups()<br><br>    self.submodules_config = submodules<br>    self.layer_number = layer_number + get_transformer_layer_offset(self.config, vp_stage)<br>    self.hidden_dropout = config.hidden_dropout <span class="hljs-keyword">if</span> hidden_dropout <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> hidden_dropout<br><br>    <span class="hljs-comment"># [Module 1: Input Layernorm] Optional Layernorm on the input data</span><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> add pytorch only layernorm</span><br>    self.input_layernorm = build_module(<br>        submodules.input_layernorm,<br>        config=self.config,<br>        hidden_size=self.config.hidden_size,<br>        eps=self.config.layernorm_epsilon,<br>    )<br><br>    attention_optional_kwargs = &#123;&#125;<br>    <span class="hljs-keyword">if</span> config.context_parallel_size &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> config.cp_comm_type <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(config.cp_comm_type, <span class="hljs-built_in">list</span>):<br>            attention_optional_kwargs[<span class="hljs-string">&quot;cp_comm_type&quot;</span>] = config.cp_comm_type[self.layer_number]<br>        <span class="hljs-keyword">else</span>:<br>            attention_optional_kwargs[<span class="hljs-string">&quot;cp_comm_type&quot;</span>] = config.cp_comm_type<br><br>    attention_optional_kwargs[<span class="hljs-string">&quot;model_comm_pgs&quot;</span>] = model_comm_pgs<br><br>    <span class="hljs-comment"># [Module 2: SelfAttention]</span><br>    self.self_attention = build_module(<br>        submodules.self_attention,<br>        config=self.config,<br>        layer_number=self.layer_number,<br>        **attention_optional_kwargs,<br>    )<br><br>    <span class="hljs-comment"># [Module 3: BiasDropoutFusion]</span><br>    self.self_attn_bda = build_module(submodules.self_attn_bda)<br><br>    <span class="hljs-comment"># [Module 4: Post SelfAttention] Optional Layernorm after self-attn</span><br>    self.pre_cross_attn_layernorm = build_module(<br>        submodules.pre_cross_attn_layernorm,<br>        config=self.config,<br>        hidden_size=self.config.hidden_size,<br>        eps=self.config.layernorm_epsilon,<br>    )<br><br>    <span class="hljs-comment"># [Module 5: CrossAttention]</span><br>    self.cross_attention = build_module(<br>        submodules.cross_attention,<br>        config=self.config,<br>        layer_number=self.layer_number,<br>        **attention_optional_kwargs,<br>    )<br><br>    <span class="hljs-comment"># [Module 6: BiasDropoutFusion]</span><br>    self.cross_attn_bda = build_module(submodules.cross_attn_bda, config=self.config)<br><br>    <span class="hljs-comment"># [Module 7: Pre MLP] Optional Layernorm before MLP</span><br>    self.pre_mlp_layernorm = build_module(<br>        submodules.pre_mlp_layernorm,<br>        config=self.config,<br>        hidden_size=self.config.hidden_size,<br>        eps=self.config.layernorm_epsilon,<br>    )<br>    <span class="hljs-comment"># [Module 8: MLP block]</span><br>    additional_mlp_kwargs = &#123;&#125;<br>    <span class="hljs-comment"># import here to avoid circular import</span><br>    <span class="hljs-keyword">from</span> megatron.core.extensions.transformer_engine <span class="hljs-keyword">import</span> TEFusedMLP<br>    <span class="hljs-keyword">from</span> megatron.core.transformer.moe.experts <span class="hljs-keyword">import</span> GroupedMLP, SequentialMLP, TEGroupedMLP<br>    <span class="hljs-keyword">from</span> megatron.core.transformer.moe.moe_layer <span class="hljs-keyword">import</span> MoELayer<br><br>    <span class="hljs-comment"># MLP expects tp_group but MoELayer expects model_comm_pgs to be passed in.</span><br>    <span class="hljs-comment"># We can change MLP to accept model_comm_pgs but it makes the logic implicit</span><br>    <span class="hljs-comment"># The conditional below is to make the logic explicit</span><br>    <span class="hljs-comment"># if submodules.mlp is not a ModuleSpec,we dont have to handle passing additional kwargs</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(submodules.mlp, ModuleSpec):<br>        <span class="hljs-keyword">if</span> submodules.mlp.module <span class="hljs-keyword">in</span> (MoELayer, GroupedMLP, TEGroupedMLP, SequentialMLP):<br>            additional_mlp_kwargs[<span class="hljs-string">&quot;model_comm_pgs&quot;</span>] = model_comm_pgs<br>        <span class="hljs-keyword">elif</span> submodules.mlp.module == MLP:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(<br>                model_comm_pgs, <span class="hljs-string">&#x27;tp&#x27;</span><br>            ), <span class="hljs-string">&#x27;TP process group is required for MLP in TransformerLayer&#x27;</span><br>            additional_mlp_kwargs[<span class="hljs-string">&quot;tp_group&quot;</span>] = model_comm_pgs.tp<br>        <span class="hljs-keyword">elif</span> TEFusedMLP <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> submodules.mlp.module == TEFusedMLP:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(<br>                model_comm_pgs, <span class="hljs-string">&#x27;tp&#x27;</span><br>            ), <span class="hljs-string">&#x27;TP process group is required for TEFusedMLP in TransformerLayer&#x27;</span><br>            additional_mlp_kwargs[<span class="hljs-string">&quot;tp_group&quot;</span>] = model_comm_pgs.tp<br>        <span class="hljs-keyword">else</span>:<br>            log_single_rank(<br>                logger,<br>                logging.WARNING,<br>                <span class="hljs-string">f&quot;Unknown MLP type: <span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(submodules.mlp)&#125;</span>. Using default kwargs.&quot;</span>,<br>            )<br>    self.mlp = build_module(submodules.mlp, config=self.config, **additional_mlp_kwargs)<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(self.mlp, <span class="hljs-string">&#x27;set_layer_number&#x27;</span>):<br>        self.mlp.set_layer_number(self.layer_number)<br><br>    <span class="hljs-comment"># [Module 9: BiasDropoutFusion]</span><br>    self.mlp_bda = build_module(submodules.mlp_bda)<br><br>    self.recompute_input_layernorm = <span class="hljs-literal">False</span><br>    self.recompute_pre_mlp_layernorm = <span class="hljs-literal">False</span><br>    self.recompute_mlp = <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">if</span> self.config.recompute_granularity == <span class="hljs-string">&#x27;selective&#x27;</span>:<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;layernorm&quot;</span> <span class="hljs-keyword">in</span> self.config.recompute_modules:<br>            <span class="hljs-keyword">if</span> (<br>                <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.input_layernorm, IdentityOp)<br>                <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.config.external_cuda_graph<br>            ):<br>                self.recompute_input_layernorm = <span class="hljs-literal">True</span><br>                <span class="hljs-keyword">if</span> self.config.fp8:<br>                    self.self_attention.set_for_recompute_input_layernorm()<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.pre_mlp_layernorm, IdentityOp):<br>                self.recompute_pre_mlp_layernorm = <span class="hljs-literal">True</span><br>                <span class="hljs-keyword">if</span> self.config.fp8:<br>                    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(self.mlp, MoELayer):<br>                        self.mlp.set_for_recompute_pre_mlp_layernorm()<br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-keyword">from</span> megatron.core.extensions.transformer_engine <span class="hljs-keyword">import</span> (<br>                            set_save_original_input,<br>                        )<br><br>                        set_save_original_input(self.mlp.linear_fc1)<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;mlp&quot;</span> <span class="hljs-keyword">in</span> self.config.recompute_modules:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.mlp, MoELayer):<br>                self.recompute_mlp = <span class="hljs-literal">True</span><br><br>    <span class="hljs-comment"># @jcasper how should we handle nvfuser?</span><br>    <span class="hljs-comment"># Set bias+dropout+add fusion grad_enable execution handler.</span><br>    <span class="hljs-comment"># TORCH_MAJOR = int(torch.__version__.split(&#x27;.&#x27;)[0])</span><br>    <span class="hljs-comment"># TORCH_MINOR = int(torch.__version__.split(&#x27;.&#x27;)[1])</span><br>    <span class="hljs-comment"># use_nvfuser = TORCH_MAJOR &gt; 1 or (TORCH_MAJOR == 1 and TORCH_MINOR &gt;= 10)</span><br>    <span class="hljs-comment"># self.bias_dropout_add_exec_handler = nullcontext if use_nvfuser else torch.enable_grad</span><br>    self.bias_dropout_add_exec_handler = torch.enable_grad<br><br></code></pre></td></tr></table></figure><ol><li><p>Input Layernorm</p></li><li><p>SelfAttention</p></li><li><p>BiasDropoutFusion</p></li><li><p>Post SelfAttention</p></li><li><p>CrossAttention</p></li><li><p>BiasDropoutFusion</p></li><li><p>Pre MLP</p></li><li><p>MLP block</p></li><li><p>BiasDropoutFusion</p></li></ol></li><li><p>其前向传播也是一些比较标准的实现，代码如下所示</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Perform a forward pass through the transformer layer.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    This method calls the core computation of a transformer layer, including</span><br><span class="hljs-string">    self-attention, cross-attention (if applicable), and feed-forward operations.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    hidden_states, context = self._forward_attention(*args, **kwargs)<br>    output = self._forward_mlp(hidden_states, kwargs.get(<span class="hljs-string">&quot;inference_context&quot;</span>, <span class="hljs-literal">None</span>))<br>    <span class="hljs-keyword">return</span> output, context<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_attention</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    hidden_states: Tensor,</span><br><span class="hljs-params">    attention_mask: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    context: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    context_mask: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    rotary_pos_emb: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    rotary_pos_cos: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    rotary_pos_sin: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    attention_bias: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    inference_context: <span class="hljs-type">Optional</span>[<span class="hljs-type">Any</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    packed_seq_params: <span class="hljs-type">Optional</span>[PackedSeqParams] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    sequence_len_offset: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    *,</span><br><span class="hljs-params">    inference_params: <span class="hljs-type">Optional</span>[<span class="hljs-type">Any</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Perform a forward pass through the attention layer and the layernorms before and after</span><br><span class="hljs-string">    the attention operations.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        hidden_states (Tensor): Input tensor of shape [s, b, h] where s is sequence length,</span><br><span class="hljs-string">            b is batch size, and h is hidden size.</span><br><span class="hljs-string">        attention_mask (Tensor): Mask tensor for self-attention.</span><br><span class="hljs-string">        context (Tensor, optional): Context tensor for cross-attention.</span><br><span class="hljs-string">        context_mask (Tensor, optional): Mask tensor for cross-attention.</span><br><span class="hljs-string">        rotary_pos_emb (Tensor, optional): Rotary positional embeddings.</span><br><span class="hljs-string">        attention_bias (Tensor, optional): Bias tensor for Q * K.T.</span><br><span class="hljs-string">        inference_context (object, optional): Parameters for inference-time optimizations.</span><br><span class="hljs-string">        packed_seq_params (object, optional): Parameters for packed sequence processing.</span><br><span class="hljs-string">        sequence_len_offset (Tensor, optional): Offset along sequence dimension</span><br><span class="hljs-string">            during inference.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        Tuple[Tensor, Tensor]: A tuple containing:</span><br><span class="hljs-string">            hidden_states (Tensor): Transformed hidden states before the MLP layernorm.</span><br><span class="hljs-string">            context (Tensor): Updated context tensor if cross-attention is used,</span><br><span class="hljs-string">            otherwise None.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    inference_context = deprecate_inference_params(inference_context, inference_params)<br><br>    <span class="hljs-comment"># Residual connection.</span><br>    residual = hidden_states<br><br>    <span class="hljs-comment"># Optional Input Layer norm</span><br>    <span class="hljs-keyword">if</span> self.recompute_input_layernorm:<br>        self.input_layernorm_checkpoint = tensor_parallel.CheckpointWithoutOutput()<br>        input_layernorm_output = self.input_layernorm_checkpoint.checkpoint(<br>            self.input_layernorm, hidden_states<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        input_layernorm_output = self.input_layernorm(hidden_states)<br><br>    <span class="hljs-comment"># Self attention.</span><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;self_attention&quot;</span>)<br>    attention_output_with_bias = self.self_attention(<br>        input_layernorm_output,<br>        attention_mask=attention_mask,<br>        inference_context=inference_context,<br>        rotary_pos_emb=rotary_pos_emb,<br>        rotary_pos_cos=rotary_pos_cos,<br>        rotary_pos_sin=rotary_pos_sin,<br>        attention_bias=attention_bias,<br>        packed_seq_params=packed_seq_params,<br>        sequence_len_offset=sequence_len_offset,<br>    )<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;self_attention&quot;</span>)<br><br>    <span class="hljs-keyword">if</span> self.recompute_input_layernorm:<br>        <span class="hljs-comment"># discard the output of the input layernorm and register the recompute</span><br>        <span class="hljs-comment"># as a gradient hook of attention_output_with_bias[0]</span><br>        self.input_layernorm_checkpoint.discard_output_and_register_recompute(<br>            attention_output_with_bias[<span class="hljs-number">0</span>]<br>        )<br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> could we move `bias_dropout_add_exec_handler` itself</span><br>    <span class="hljs-comment"># inside the module provided in the `bias_dropout_add_spec` module?</span><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;self_attn_bda&quot;</span>)<br>    <span class="hljs-keyword">with</span> self.bias_dropout_add_exec_handler():<br>        hidden_states = self.self_attn_bda(self.training, self.config.bias_dropout_fusion)(<br>            attention_output_with_bias, residual, self.hidden_dropout<br>        )<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;self_attn_bda&quot;</span>)<br><br>    <span class="hljs-comment"># Residual connection.</span><br>    residual = hidden_states<br><br>    <span class="hljs-comment"># Optional Layer norm after self-attention</span><br>    pre_cross_attn_layernorm_output = self.pre_cross_attn_layernorm(hidden_states)<br><br>    <span class="hljs-comment"># Cross attention.</span><br>    attention_output_with_bias = self.cross_attention(<br>        pre_cross_attn_layernorm_output,<br>        attention_mask=context_mask,<br>        key_value_states=context,<br>        inference_context=inference_context,<br>    )<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(attention_output_with_bias, <span class="hljs-built_in">dict</span>) <span class="hljs-keyword">and</span> <span class="hljs-string">&quot;context&quot;</span> <span class="hljs-keyword">in</span> attention_output_with_bias:<br>        context = attention_output_with_bias[<span class="hljs-string">&quot;context&quot;</span>]<br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> could we move `bias_dropout_add_exec_handler` itself</span><br>    <span class="hljs-comment"># inside the module provided in the `bias_dropout_add_spec` module?</span><br>    <span class="hljs-keyword">with</span> self.bias_dropout_add_exec_handler():<br>        hidden_states = self.cross_attn_bda(self.training, self.config.bias_dropout_fusion)(<br>            attention_output_with_bias, residual, self.hidden_dropout<br>        )<br><br>    <span class="hljs-keyword">return</span> hidden_states, context<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_mlp</span>(<span class="hljs-params">self, hidden_states, inference_context=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Perform a forward pass through the feed-forward layer.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        hidden_states (Tensor): Transformed hidden states before the MLP layernorm.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        output (Tensor): Transformed hidden states of shape [s, b, h].</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># Residual connection.</span><br>    residual = hidden_states<br><br>    <span class="hljs-comment"># Optional Layer norm post the cross-attention.</span><br>    <span class="hljs-keyword">if</span> self.recompute_pre_mlp_layernorm:<br>        self.pre_mlp_norm_checkpoint = tensor_parallel.CheckpointWithoutOutput()<br>        pre_mlp_layernorm_output = self.pre_mlp_norm_checkpoint.checkpoint(<br>            self.pre_mlp_layernorm, hidden_states<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        pre_mlp_layernorm_output = self.pre_mlp_layernorm(hidden_states)<br><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;mlp&quot;</span>)<br>    <span class="hljs-comment"># Potentially chunk the MLP computation during prefill to minimize the peak activation size</span><br>    should_chunk_mlp_for_prefill = (<br>        self.config.mlp_chunks_for_prefill &gt; <span class="hljs-number">1</span><br>        <span class="hljs-keyword">and</span> inference_context <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> inference_context.is_decode_only()<br>        <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.mlp, IdentityOp)<br>    )<br><br>    <span class="hljs-keyword">if</span> self.recompute_mlp:<br>        <span class="hljs-keyword">if</span> self.config.fp8:<br>            <span class="hljs-comment"># import here to avoid circular import</span><br>            <span class="hljs-keyword">from</span> megatron.core.extensions.transformer_engine <span class="hljs-keyword">import</span> te_checkpoint<br><br>            mlp_output_with_bias = te_checkpoint(<br>                self.mlp,<br>                <span class="hljs-literal">False</span>,<br>                tensor_parallel.random.get_cuda_rng_tracker,<br>                parallel_state.get_tensor_model_parallel_group(),<br>                pre_mlp_layernorm_output,<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            mlp_output_with_bias = tensor_parallel.checkpoint(<br>                self.mlp, <span class="hljs-literal">False</span>, pre_mlp_layernorm_output<br>            )<br>    <span class="hljs-keyword">elif</span> should_chunk_mlp_for_prefill:<br>        <span class="hljs-comment"># Chunk input along sequence dimension</span><br>        num_chunks = <span class="hljs-built_in">min</span>(self.config.mlp_chunks_for_prefill, pre_mlp_layernorm_output.shape[<span class="hljs-number">0</span>])<br>        chunks = pre_mlp_layernorm_output.chunk(num_chunks, dim=<span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># Compute outputs for each chunk</span><br>        outputs = [self.mlp(chunk) <span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> chunks]<br><br>        <span class="hljs-comment"># Aggregate chunk outputs</span><br>        mlp_output = torch.cat([out <span class="hljs-keyword">for</span> out, _ <span class="hljs-keyword">in</span> outputs], dim=<span class="hljs-number">0</span>)<br>        bias_chunks = [bias <span class="hljs-keyword">for</span> _, bias <span class="hljs-keyword">in</span> outputs <span class="hljs-keyword">if</span> bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>]<br>        bias_output = torch.stack(bias_chunks, dim=<span class="hljs-number">0</span>).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">0</span>) <span class="hljs-keyword">if</span> bias_chunks <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>        mlp_output_with_bias = (mlp_output, bias_output)<br><br>    <span class="hljs-keyword">else</span>:<br>        mlp_output_with_bias = self.mlp(pre_mlp_layernorm_output)<br><br>    <span class="hljs-keyword">if</span> self.recompute_pre_mlp_layernorm:<br>        <span class="hljs-comment"># discard the output of the pre-mlp layernorm and register the recompute</span><br>        <span class="hljs-comment"># as a gradient hook of mlp_output_with_bias[0]</span><br>        self.pre_mlp_norm_checkpoint.discard_output_and_register_recompute(<br>            mlp_output_with_bias[<span class="hljs-number">0</span>]<br>        )<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;mlp&quot;</span>)<br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> could we move `bias_dropout_add_exec_handler` itself</span><br>    <span class="hljs-comment"># inside the module provided in the `bias_dropout_add_spec` module?</span><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;mlp_bda&quot;</span>)<br>    <span class="hljs-keyword">with</span> self.bias_dropout_add_exec_handler():<br>        hidden_states = self.mlp_bda(self.training, self.config.bias_dropout_fusion)(<br>            mlp_output_with_bias, residual, self.hidden_dropout<br>        )<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;mlp_bda&quot;</span>)<br><br>    <span class="hljs-comment"># Jit compiled function creates &#x27;view&#x27; tensor. This tensor</span><br>    <span class="hljs-comment"># potentially gets saved in the MPU checkpoint function context,</span><br>    <span class="hljs-comment"># which rejects view tensors. While making a viewless tensor here</span><br>    <span class="hljs-comment"># won&#x27;t result in memory savings (like the data loader, or</span><br>    <span class="hljs-comment"># p2p_communication), it serves to document the origin of this</span><br>    <span class="hljs-comment"># &#x27;view&#x27; tensor.</span><br>    output = make_viewless_tensor(<br>        inp=hidden_states, requires_grad=hidden_states.requires_grad, keep_graph=<span class="hljs-literal">True</span><br>    )<br><br>    <span class="hljs-keyword">return</span> output<br><br></code></pre></td></tr></table></figure><h4 id="MLP模块"><a href="#MLP模块" class="headerlink" title="MLP模块"></a>MLP模块</h4><p>MLP模块中往往是先进行一次全连接计算，在使用类似gelu的激活函数，再使用一次全连接计算，在TP并行中往往采用的是对前一次采用列并行对后一次采用行并行的方式。</p><p>本地模块获取MLP的相关代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_mlp_module_spec_for_backend</span>(<span class="hljs-params"></span><br><span class="hljs-params">    backend: BackendSpecProvider,</span><br><span class="hljs-params">    num_experts: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    moe_grouped_gemm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    moe_use_legacy_grouped_gemm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    use_te_op_fuser: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params"></span>) -&gt; ModuleSpec:<br>    <span class="hljs-string">&quot;&quot;&quot;Helper function to get module spec for MLP/MoE&quot;&quot;&quot;</span><br><br>    linear_fc2 = backend.row_parallel_linear()<br><br>    <span class="hljs-keyword">if</span> num_experts <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># Dense MLP w/ or w/o TE modules.</span><br>        <span class="hljs-keyword">if</span> use_te_op_fuser:<br>            <span class="hljs-keyword">return</span> ModuleSpec(module=TEFusedMLP)<br>        <span class="hljs-keyword">elif</span> backend.fuse_layernorm_and_linear():<br>            linear_fc1 = backend.column_parallel_layer_norm_linear()<br>            <span class="hljs-keyword">assert</span> linear_fc1 <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">else</span>:<br>            linear_fc1 = backend.column_parallel_linear()<br>        <span class="hljs-keyword">return</span> ModuleSpec(<br>            module=MLP, submodules=MLPSubmodules(linear_fc1=linear_fc1, linear_fc2=linear_fc2)<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># Mixture of experts with modules in megatron core.</span><br>        <span class="hljs-keyword">return</span> get_moe_module_spec_for_backend(<br>            backend=backend,<br>            num_experts=num_experts,<br>            moe_grouped_gemm=moe_grouped_gemm,<br>            moe_use_legacy_grouped_gemm=moe_use_legacy_grouped_gemm,<br>        )<br><br></code></pre></td></tr></table></figure><p>一般情况下两个linear层分别为<code>column_parallel_linear</code>与<code>row_parallel_linear</code>，然后以此为基础构建了<code>MLP</code>模块，<code>MLP</code>模块的相关代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MLP</span>(<span class="hljs-title class_ inherited__">MegatronModule</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    MLP will take the input with h hidden state, project it to 4*h</span><br><span class="hljs-string">    hidden dimension, perform nonlinear transformation, and project the</span><br><span class="hljs-string">    state back into h hidden dimension.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns an output and a bias to be added to the output.</span><br><span class="hljs-string">    If config.add_bias_linear is False, the bias returned is None.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    We use the following notation:</span><br><span class="hljs-string">     h: hidden size</span><br><span class="hljs-string">     p: number of tensor model parallel partitions</span><br><span class="hljs-string">     b: batch size</span><br><span class="hljs-string">     s: sequence length</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        config: TransformerConfig,</span><br><span class="hljs-params">        submodules: MLPSubmodules,</span><br><span class="hljs-params">        is_expert: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        input_size: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        ffn_hidden_size: <span class="hljs-built_in">int</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        tp_group: <span class="hljs-type">Optional</span>[torch.distributed.ProcessGroup] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__(config=config)<br><br>        self.config: TransformerConfig = config<br><br>        self.input_size = input_size <span class="hljs-keyword">if</span> input_size != <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> self.config.hidden_size<br><br>        tp_group = get_tensor_model_parallel_group_if_none(tp_group, is_expert=is_expert)<br>        <span class="hljs-keyword">if</span> ffn_hidden_size <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> is_expert:<br>                <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;MoE MLP requires `ffn_hidden_size`, but it was not provided.&quot;</span>)<br>            warnings.warn(<br>                <span class="hljs-string">&quot;MLP requires ffn_hidden_size, but it was not provided. Using \</span><br><span class="hljs-string">                    config.ffn_hidden_size by default.&quot;</span>,<br>                DeprecationWarning,<br>                stacklevel=<span class="hljs-number">2</span>,<br>            )<br>            ffn_hidden_size = self.config.ffn_hidden_size<br><br>        <span class="hljs-comment"># If this is a gated linear unit we double the output width</span><br>        <span class="hljs-comment"># see https://arxiv.org/pdf/2002.05202.pdf</span><br>        <span class="hljs-keyword">if</span> self.config.gated_linear_unit:<br>            ffn_hidden_size *= <span class="hljs-number">2</span><br><br>        self.linear_fc1 = build_module(<br>            submodules.linear_fc1,<br>            self.input_size,<br>            ffn_hidden_size,<br>            config=self.config,<br>            init_method=self.config.init_method,<br>            gather_output=<span class="hljs-literal">False</span>,<br>            bias=self.config.add_bias_linear,<br>            skip_bias_add=<span class="hljs-literal">True</span>,<br>            is_expert=is_expert,<br>            tp_comm_buffer_name=<span class="hljs-string">&quot;fc1&quot;</span>,<br>            tp_group=tp_group,<br>        )<br><br>        self.activation_func = self.config.activation_func<br><br>        self.linear_fc2 = build_module(<br>            submodules.linear_fc2,<br>            self.config.ffn_hidden_size,<br>            self.config.hidden_size,<br>            config=self.config,<br>            init_method=self.config.output_layer_init_method,<br>            bias=self.config.add_bias_linear,<br>            input_is_parallel=<span class="hljs-literal">True</span>,<br>            skip_bias_add=<span class="hljs-literal">True</span>,<br>            is_expert=is_expert,<br>            tp_comm_buffer_name=<span class="hljs-string">&quot;fc2&quot;</span>,<br>            tp_group=tp_group,<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states, per_token_scale=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Perform the forward pass through the MLP block.&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># [s, b, 4 * h/p]</span><br>        nvtx_range_push(suffix=<span class="hljs-string">&quot;linear_fc1&quot;</span>)<br>        intermediate_parallel, bias_parallel = self.linear_fc1(hidden_states)<br>        nvtx_range_pop(suffix=<span class="hljs-string">&quot;linear_fc1&quot;</span>)<br><br>        nvtx_range_push(suffix=<span class="hljs-string">&quot;activation&quot;</span>)<br>        <span class="hljs-keyword">if</span> self.config.bias_activation_fusion:<br>            <span class="hljs-keyword">if</span> per_token_scale <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">if</span> self.activation_func == F.silu <span class="hljs-keyword">and</span> self.config.gated_linear_unit:<br>                    <span class="hljs-comment"># dtype is handled inside the fused kernel</span><br>                    intermediate_parallel = weighted_bias_swiglu_impl(<br>                        intermediate_parallel,<br>                        bias_parallel,<br>                        per_token_scale.unsqueeze(-<span class="hljs-number">1</span>),<br>                        self.config.activation_func_fp8_input_store,<br>                    )<br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Only support fusion of swiglu with per_token_scale in MLP.&quot;</span>)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">if</span> self.activation_func == F.gelu:<br>                    <span class="hljs-keyword">if</span> self.config.gated_linear_unit:<br>                        intermediate_parallel = bias_geglu_impl(<br>                            intermediate_parallel, bias_parallel<br>                        )<br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-keyword">assert</span> self.config.add_bias_linear <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span><br>                        intermediate_parallel = bias_gelu_impl(intermediate_parallel, bias_parallel)<br>                <span class="hljs-keyword">elif</span> self.activation_func == F.silu <span class="hljs-keyword">and</span> self.config.gated_linear_unit:<br>                    intermediate_parallel = bias_swiglu_impl(<br>                        intermediate_parallel,<br>                        bias_parallel,<br>                        self.config.activation_func_fp8_input_store,<br>                        self.config.cpu_offloading<br>                        <span class="hljs-keyword">and</span> self.config.cpu_offloading_activations<br>                        <span class="hljs-keyword">and</span> HAVE_TE,<br>                    )<br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Only support fusion of gelu and swiglu&quot;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> bias_parallel <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                intermediate_parallel = intermediate_parallel + bias_parallel<br>            <span class="hljs-keyword">if</span> self.config.gated_linear_unit:<br><br>                <span class="hljs-keyword">def</span> <span class="hljs-title function_">glu</span>(<span class="hljs-params">x</span>):<br>                    x = torch.chunk(x, <span class="hljs-number">2</span>, dim=-<span class="hljs-number">1</span>)<br>                    <span class="hljs-keyword">return</span> self.config.activation_func(x[<span class="hljs-number">0</span>]) * x[<span class="hljs-number">1</span>]<br><br>                intermediate_parallel = glu(intermediate_parallel)<br>            <span class="hljs-keyword">else</span>:<br>                intermediate_parallel = self.activation_func(intermediate_parallel)<br><br>            <span class="hljs-keyword">if</span> per_token_scale <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                original_dtype = intermediate_parallel.dtype<br>                intermediate_parallel = intermediate_parallel * per_token_scale.unsqueeze(-<span class="hljs-number">1</span>)<br>                intermediate_parallel = intermediate_parallel.to(original_dtype)<br>        nvtx_range_pop(suffix=<span class="hljs-string">&quot;activation&quot;</span>)<br><br>        <span class="hljs-comment"># [s, b, h]</span><br>        nvtx_range_push(suffix=<span class="hljs-string">&quot;linear_fc2&quot;</span>)<br>        output, output_bias = self.linear_fc2(intermediate_parallel)<br>        nvtx_range_pop(suffix=<span class="hljs-string">&quot;linear_fc2&quot;</span>)<br><br>        <span class="hljs-keyword">if</span> per_token_scale <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">assert</span> output_bias <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;Bias is not supported with per_token_scale&quot;</span><br><br>        <span class="hljs-keyword">return</span> output, output_bias<br><br>    <span class="hljs-comment"># pylint: disable=missing-function-docstring</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sharded_state_dict</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, prefix: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;&quot;</span>, sharded_offsets: <span class="hljs-built_in">tuple</span> = (<span class="hljs-params"></span>), metadata: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">dict</span>] = <span class="hljs-literal">None</span></span><br><span class="hljs-params">    </span>) -&gt; ShardedStateDict:<br>        sharded_state_dict = &#123;&#125;<br>        singleton_local_shards = (metadata <span class="hljs-keyword">or</span> &#123;&#125;).get(<span class="hljs-string">&#x27;singleton_local_shards&#x27;</span>, <span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">for</span> name, module <span class="hljs-keyword">in</span> self._modules.items():<br>            sub_sd = module.sharded_state_dict(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;prefix&#125;</span><span class="hljs-subst">&#123;name&#125;</span>.&quot;</span>, sharded_offsets, metadata)<br>            <span class="hljs-keyword">if</span> self.config.gated_linear_unit <span class="hljs-keyword">and</span> name == <span class="hljs-string">&quot;linear_fc1&quot;</span>:<br>                <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> sub_sd.items():<br>                    <span class="hljs-keyword">if</span> k <span class="hljs-keyword">in</span> (<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;prefix&#125;</span><span class="hljs-subst">&#123;name&#125;</span>.weight&quot;</span>, <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;prefix&#125;</span><span class="hljs-subst">&#123;name&#125;</span>.bias&quot;</span>):<br>                        sub_sd[k] = apply_swiglu_sharded_factory(<br>                            v, sharded_offsets, singleton_local_shards<br>                        )<br>            sharded_state_dict.update(sub_sd)<br>        <span class="hljs-keyword">return</span> sharded_state_dict<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward_dw</span>(<span class="hljs-params">self</span>):<br>        self.linear_fc2.backward_dw()<br>        self.linear_fc1.backward_dw()<br><br></code></pre></td></tr></table></figure><ul><li><p>在初始化时：</p><ul><li><p>其读取配置得到了<code>ffn_hidden_size</code>以及<code>tp_group</code>等参数</p></li><li><p>然后构建了<code>column_parallel_linear</code>类型的fc1以及<code>row_parallel_linear</code>类型的fc2，还要按配置所需的<code>activation_func</code></p></li></ul></li><li><p>在Forward时：</p><ul><li><p>其整个流程为了方便Nsys分析使用<code>nvtx_range_push</code>进行了准确的划分</p></li><li><p>先调用<code>linear_fc1</code>，再调用<code>activation</code>计算，再调用<code>linear_fc2</code>计算</p></li></ul></li></ul><h5 id="column-parallel-linear"><a href="#column-parallel-linear" class="headerlink" title="column_parallel_linear"></a><code>column_parallel_linear</code></h5><p>Megatron-LM本地写的<code>ColumnParallelLinear</code>如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ColumnParallelLinear</span>(torch.nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Linear layer with column parallelism.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    The linear layer is defined as Y = XA + b. A is parallelized along</span><br><span class="hljs-string">    its second dimension as A = [A_1, ..., A_p].</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        input_size:</span><br><span class="hljs-string">            first dimension of matrix A.</span><br><span class="hljs-string">        output_size:</span><br><span class="hljs-string">            second dimension of matrix A.</span><br><span class="hljs-string">        bias:</span><br><span class="hljs-string">            If true, add bias</span><br><span class="hljs-string">        gather_output:</span><br><span class="hljs-string">            If true, call all-gather on output and make Y available to all GPUs,</span><br><span class="hljs-string">            otherwise, every GPU will have its output which is Y_i = XA_i</span><br><span class="hljs-string">        init_method:</span><br><span class="hljs-string">            method to initialize weights. Note that bias is always set to zero.</span><br><span class="hljs-string">        stride:</span><br><span class="hljs-string">            For the strided linear layers.</span><br><span class="hljs-string">        keep_master_weight_for_test:</span><br><span class="hljs-string">            This was added for testing and should be set to False. It</span><br><span class="hljs-string">            returns the master weights used for initialization.</span><br><span class="hljs-string">        skip_bias_add:</span><br><span class="hljs-string">            If True, do not add the bias term, instead return it to be added by the</span><br><span class="hljs-string">            caller. This enables performance optimations where bias can be fused with other</span><br><span class="hljs-string">            elementwise operations.</span><br><span class="hljs-string">        skip_weight_param_allocation:</span><br><span class="hljs-string">            If True, weight parameter is not allocated and must be passed</span><br><span class="hljs-string">            as a keyword argument `weight` during the forward pass. Note that this does not</span><br><span class="hljs-string">            affect bias, which will be allocated if bias is True. Defaults to False.</span><br><span class="hljs-string">        embedding_activation_buffer:</span><br><span class="hljs-string">            This buffer holds the input activations of the final embedding</span><br><span class="hljs-string">            linear layer on the last pipeline stage when defer_embedding_wgrad_compute is enabled.</span><br><span class="hljs-string">        grad_output_buffer:</span><br><span class="hljs-string">            This buffer holds the gradient outputs of the final embedding linear</span><br><span class="hljs-string">            layer on the last pipeline stage when defer_embedding_wgrad_compute is enabled.</span><br><span class="hljs-string">        is_expert:</span><br><span class="hljs-string">            If True, the layer is treated as an MoE expert layer.</span><br><span class="hljs-string">        config:</span><br><span class="hljs-string">            ModelParallelConfig object</span><br><span class="hljs-string">        tp_comm_buffer_name:</span><br><span class="hljs-string">            Communication buffer name is not used in non-Transformer-Engine modules.</span><br><span class="hljs-string">        disable_grad_reduce:</span><br><span class="hljs-string">            If True, reduction of output gradients across tensor-parallel ranks</span><br><span class="hljs-string">            will be disabled. Defaults to False. This feature is used by Lora Adapter in Nemo to</span><br><span class="hljs-string">            delay and fuse reduction along with other gradients for performance optimization.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        input_size,</span><br><span class="hljs-params">        output_size,</span><br><span class="hljs-params">        *,</span><br><span class="hljs-params">        config: ModelParallelConfig,</span><br><span class="hljs-params">        init_method: <span class="hljs-type">Callable</span>,</span><br><span class="hljs-params">        bias=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        gather_output=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        stride=<span class="hljs-number">1</span>,</span><br><span class="hljs-params">        keep_master_weight_for_test=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        skip_bias_add=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        skip_weight_param_allocation: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        embedding_activation_buffer: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[torch.Tensor]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        grad_output_buffer: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[torch.Tensor]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        is_expert: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        tp_comm_buffer_name: <span class="hljs-built_in">str</span> = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># Not used</span></span><br><span class="hljs-params">        disable_grad_reduce: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        tp_group: <span class="hljs-type">Optional</span>[torch.distributed.ProcessGroup] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>(ColumnParallelLinear, self).__init__()<br><br>        <span class="hljs-comment"># Keep input parameters</span><br>        self.input_size = input_size<br>        self.output_size = output_size<br>        self.gather_output = gather_output<br>        <span class="hljs-comment"># Divide the weight matrix along the last dimension.</span><br>        self.skip_bias_add = skip_bias_add<br>        self.is_expert = is_expert<br>        self.expert_parallel = config.expert_model_parallel_size &gt; <span class="hljs-number">1</span><br>        self.embedding_activation_buffer = embedding_activation_buffer<br>        self.grad_output_buffer = grad_output_buffer<br>        self.config = config<br>        self.disable_grad_reduce = disable_grad_reduce<br>        self.tp_group = tp_group<br><br>        self.tp_group = get_tensor_model_parallel_group_if_none(<br>            self.tp_group, is_expert=self.is_expert<br>        )<br>        world_size = get_pg_size(self.tp_group)<br>        rank = get_pg_rank(self.tp_group)<br>        self.explicit_expert_comm = self.is_expert <span class="hljs-keyword">and</span> (world_size &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> self.expert_parallel)<br>        self.output_size_per_partition = divide(output_size, world_size)<br><br>        <span class="hljs-comment"># Parameters.</span><br>        <span class="hljs-comment"># Note: torch.nn.functional.linear performs XA^T + b and as a result</span><br>        <span class="hljs-comment"># we allocate the transpose.</span><br>        <span class="hljs-comment"># Initialize weight.</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> skip_weight_param_allocation:<br>            <span class="hljs-keyword">if</span> config.use_cpu_initialization:<br>                self.weight = Parameter(<br>                    torch.empty(<br>                        self.output_size_per_partition, self.input_size, dtype=config.params_dtype<br>                    )<br>                )<br>                <span class="hljs-keyword">if</span> config.perform_initialization:<br>                    self.master_weight = _initialize_affine_weight_cpu(<br>                        self.weight,<br>                        self.output_size,<br>                        self.input_size,<br>                        self.output_size_per_partition,<br>                        <span class="hljs-number">0</span>,<br>                        init_method,<br>                        stride=stride,<br>                        return_master_weight=keep_master_weight_for_test,<br>                        rank=rank,<br>                        world_size=world_size,<br>                    )<br>            <span class="hljs-keyword">else</span>:<br>                self.weight = Parameter(<br>                    torch.empty(<br>                        self.output_size_per_partition,<br>                        self.input_size,<br>                        device=torch.cuda.current_device(),<br>                        dtype=config.params_dtype,<br>                    )<br>                )<br>                <span class="hljs-keyword">if</span> config.perform_initialization:<br>                    _initialize_affine_weight_gpu(<br>                        self.weight,<br>                        init_method,<br>                        partition_dim=<span class="hljs-number">0</span>,<br>                        stride=stride,<br>                        is_expert=self.is_expert,<br>                    )<br><br>            <span class="hljs-built_in">setattr</span>(self.weight, <span class="hljs-string">&quot;allreduce&quot;</span>, <span class="hljs-keyword">not</span> (self.is_expert <span class="hljs-keyword">and</span> self.expert_parallel))<br>        <span class="hljs-keyword">else</span>:<br>            self.weight = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> bias:<br>            <span class="hljs-keyword">if</span> config.use_cpu_initialization:<br>                self.bias = Parameter(<br>                    torch.empty(self.output_size_per_partition, dtype=config.params_dtype)<br>                )<br>            <span class="hljs-keyword">else</span>:<br>                self.bias = Parameter(<br>                    torch.empty(<br>                        self.output_size_per_partition,<br>                        device=torch.cuda.current_device(),<br>                        dtype=config.params_dtype,<br>                    )<br>                )<br>            set_tensor_model_parallel_attributes(self.bias, <span class="hljs-literal">True</span>, <span class="hljs-number">0</span>, stride)<br>            <span class="hljs-keyword">if</span> config.perform_initialization:<br>                <span class="hljs-comment"># Always initialize bias to zero.</span><br>                <span class="hljs-keyword">with</span> torch.no_grad():<br>                    self.bias.zero_()<br>            <span class="hljs-built_in">setattr</span>(self.bias, <span class="hljs-string">&quot;allreduce&quot;</span>, <span class="hljs-keyword">not</span> (self.is_expert <span class="hljs-keyword">and</span> self.expert_parallel))<br>        <span class="hljs-keyword">else</span>:<br>            self.register_parameter(<span class="hljs-string">&quot;bias&quot;</span>, <span class="hljs-literal">None</span>)<br><br>        self.sequence_parallel = config.sequence_parallel<br>        <span class="hljs-keyword">if</span> self.sequence_parallel <span class="hljs-keyword">and</span> world_size &lt;= <span class="hljs-number">1</span>:<br>            warnings.warn(<br>                <span class="hljs-string">&quot;`sequence_parallel` is set to `True`, but tensor model parallel size &quot;</span><br>                <span class="hljs-string">f&quot;is <span class="hljs-subst">&#123;world_size&#125;</span>. Disabling sequence parallel.&quot;</span><br>            )<br>            self.sequence_parallel = <span class="hljs-literal">False</span><br><br>        self.allreduce_dgrad = (<br>            world_size &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.sequence_parallel <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.disable_grad_reduce<br>        )<br><br>        <span class="hljs-keyword">if</span> config.gradient_accumulation_fusion <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> _grad_accum_fusion_available:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<br>                <span class="hljs-string">&quot;ColumnParallelLinear was called with gradient_accumulation_fusion set &quot;</span><br>                <span class="hljs-string">&quot;to True but the custom CUDA extension fused_weight_gradient_mlp_cuda &quot;</span><br>                <span class="hljs-string">&quot;module is not found. To use gradient_accumulation_fusion you must &quot;</span><br>                <span class="hljs-string">&quot;install APEX with --cpp_ext and --cuda_ext. For example: &quot;</span><br>                <span class="hljs-string">&#x27;pip install --global-option=&quot;--cpp_ext&quot; --global-option=&quot;--cuda_ext .&quot; &#x27;</span><br>                <span class="hljs-string">&quot;Note that the extension requires CUDA&gt;=11. Otherwise, you must turn off &quot;</span><br>                <span class="hljs-string">&quot;gradient accumulation fusion.&quot;</span><br>            )<br>        self.gradient_accumulation_fusion = config.gradient_accumulation_fusion<br><br>        <span class="hljs-keyword">if</span> self.allreduce_dgrad <span class="hljs-keyword">and</span> self.sequence_parallel:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<br>                <span class="hljs-string">&quot;`allreduce_dgrad` and `sequence_parallel` cannot be enabled at the same time.&quot;</span><br>            )<br><br>        <span class="hljs-comment"># Hook adding a default empty _extra_state for state dict</span><br>        self._register_load_state_dict_pre_hook(<br>            <span class="hljs-keyword">lambda</span> state_dict, prefix, *args, **kwargs: state_dict.setdefault(<br>                <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;prefix&#125;</span>_extra_state&quot;</span><br>            )<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_impl</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span>, weight, *args, **kwargs</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> weight.requires_grad:<br>            <span class="hljs-keyword">return</span> linear_with_frozen_weight(<span class="hljs-built_in">input</span>, weight, *args, **kwargs)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> linear_with_grad_accumulation_and_async_allreduce(<span class="hljs-built_in">input</span>, weight, *args, **kwargs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        input_: torch.Tensor,</span><br><span class="hljs-params">        weight: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        runtime_gather_output: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Forward of ColumnParallelLinear</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            input_:</span><br><span class="hljs-string">                3D tensor whose order of dimension is [sequence, batch, hidden]</span><br><span class="hljs-string">            weight (optional):</span><br><span class="hljs-string">                weight tensor to use, compulsory when skip_weight_param_allocation is True.</span><br><span class="hljs-string">            runtime_gather_output (bool): Gather output at runtime. Default None means</span><br><span class="hljs-string">                `gather_output` arg in the constructor will be used.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            - output</span><br><span class="hljs-string">            - bias</span><br><span class="hljs-string"></span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> weight <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> self.weight <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">raise</span> RuntimeError(<br>                    <span class="hljs-string">&quot;weight was not supplied to ColumnParallelLinear forward pass &quot;</span><br>                    <span class="hljs-string">&quot;and skip_weight_param_allocation is True.&quot;</span><br>                )<br>            weight = self.weight<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># Check the weight passed in is the correct shape</span><br>            expected_shape = (self.output_size_per_partition, self.input_size)<br>            <span class="hljs-keyword">if</span> weight.shape != expected_shape:<br>                <span class="hljs-keyword">raise</span> RuntimeError(<br>                    <span class="hljs-string">f&quot;supplied weight&#x27;s shape is <span class="hljs-subst">&#123;<span class="hljs-built_in">tuple</span>(weight.shape)&#125;</span>, &quot;</span><br>                    <span class="hljs-string">f&quot;not <span class="hljs-subst">&#123;expected_shape&#125;</span> as expected&quot;</span><br>                )<br><br>        bias = self.bias <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.skip_bias_add <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> (<br>            self.allreduce_dgrad<br>            <span class="hljs-keyword">or</span> self.sequence_parallel<br>            <span class="hljs-keyword">or</span> self.explicit_expert_comm<br>            <span class="hljs-keyword">or</span> self.disable_grad_reduce<br>        ):<br>            input_parallel = input_<br>        <span class="hljs-keyword">else</span>:<br>            input_parallel = copy_to_tensor_model_parallel_region(input_, group=self.tp_group)<br><br>        <span class="hljs-keyword">if</span> self.config.defer_embedding_wgrad_compute:<br>            <span class="hljs-keyword">if</span> (<br>                self.config.wgrad_deferral_limit == <span class="hljs-number">0</span><br>                <span class="hljs-keyword">or</span> <span class="hljs-built_in">len</span>(self.embedding_activation_buffer) &lt; self.config.wgrad_deferral_limit<br>            ):<br>                self.embedding_activation_buffer.append(input_parallel)<br><br>        <span class="hljs-comment"># Matrix multiply.</span><br>        allreduce_dgrad = <span class="hljs-literal">False</span> <span class="hljs-keyword">if</span> self.explicit_expert_comm <span class="hljs-keyword">else</span> self.allreduce_dgrad<br><br>        <span class="hljs-keyword">if</span> self.config._cpu_offloading_context <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> self.config._cpu_offloading_context.inside_context <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span>:<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> HAVE_TE:<br>                    <span class="hljs-keyword">assert</span> (<br>                        self.config.cpu_offloading <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span><br>                    ), <span class="hljs-string">&quot;CPU Offloading cannot be enabled while TE is not present&quot;</span><br>                <span class="hljs-keyword">else</span>:<br>                    input_parallel.activation_offloading = self.config.cpu_offloading_activations<br><br>        output_parallel = self._forward_impl(<br>            <span class="hljs-built_in">input</span>=input_parallel,<br>            weight=weight,<br>            bias=bias,<br>            gradient_accumulation_fusion=self.gradient_accumulation_fusion,<br>            allreduce_dgrad=allreduce_dgrad,<br>            sequence_parallel=<span class="hljs-literal">False</span> <span class="hljs-keyword">if</span> self.explicit_expert_comm <span class="hljs-keyword">else</span> self.sequence_parallel,<br>            grad_output_buffer=(<br>                self.grad_output_buffer <span class="hljs-keyword">if</span> self.config.defer_embedding_wgrad_compute <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>            ),<br>            wgrad_deferral_limit=(<br>                self.config.wgrad_deferral_limit<br>                <span class="hljs-keyword">if</span> self.config.defer_embedding_wgrad_compute<br>                <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>            ),<br>            tp_group=self.tp_group,<br>        )<br><br>        gather_output = self.gather_output<br>        <span class="hljs-comment"># Use the runtime gather output if it&#x27;s set explicitly.</span><br>        <span class="hljs-keyword">if</span> runtime_gather_output <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            gather_output = runtime_gather_output<br><br>        <span class="hljs-keyword">if</span> gather_output:<br>            <span class="hljs-comment"># All-gather across the partitions.</span><br>            output = gather_from_tensor_model_parallel_region(output_parallel, group=self.tp_group)<br>        <span class="hljs-keyword">else</span>:<br>            output = output_parallel<br>        output_bias = self.bias <span class="hljs-keyword">if</span> self.skip_bias_add <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">return</span> output, output_bias<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sharded_state_dict</span>(<span class="hljs-params">self, prefix=<span class="hljs-string">&quot;&quot;</span>, sharded_offsets=(<span class="hljs-params"></span>), metadata=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Sharding along axis 0, bias sharded&quot;&quot;&quot;</span><br>        state_dict = self.state_dict(prefix=<span class="hljs-string">&quot;&quot;</span>, keep_vars=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">return</span> make_sharded_tensors_for_checkpoint(<br>            state_dict, prefix, &#123;<span class="hljs-string">&quot;weight&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;bias&quot;</span>: <span class="hljs-number">0</span>&#125;, sharded_offsets<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">set_extra_state</span>(<span class="hljs-params">self, state: <span class="hljs-type">Any</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Extra state is ignored&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_extra_state</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Keep compatibility with TE state dict.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__repr__</span>(<span class="hljs-params">self</span>):<br>        tp = self.output_size // self.output_size_per_partition<br>        use_bias = self.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> self.bias <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">return</span> (<br>            <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(self).__name__&#125;</span>(in_features=<span class="hljs-subst">&#123;self.input_size&#125;</span>, &quot;</span><br>            <span class="hljs-string">f&quot;out_features=<span class="hljs-subst">&#123;self.output_size&#125;</span>, bias=<span class="hljs-subst">&#123;use_bias&#125;</span>, TP=<span class="hljs-subst">&#123;tp&#125;</span>)&quot;</span><br>        )<br><br></code></pre></td></tr></table></figure><ul><li><p>在初始化时：</p><ul><li><p>其首先计算出在TP列并行下<code>self.output_size_per_partition = divide(output_size, world_size)</code>，并以此为基础初始化权重<code>self.weight = Parameter(torch.empty(self.output_size_per_partition, self.input_size, ...))</code></p></li><li><p>此外还标记了计算梯度时是否需要<code>allreduce_dgrad</code>，需要的条件是<code>world_size &gt; 1 and not self.sequence_parallel and not self.disable_grad_reduce</code>，因为<code>sequence_parallel </code>与梯度并行有冲突。</p></li></ul></li><li><p>在Forward时，流程如下：</p><ol><li><p>首先如果没有<code>weight</code>参数就使用自身初始化的<code>weight</code>，然后检查形状。</p></li><li><p>对于列并行而言，典型的实现是输入在所有 TP ranks 上一致（复制一份），每个 rank 用自己的 <code>W_i</code> 计算 <code>Y_i = X @ W_i^T</code>。<code>copy_to_tensor_model_parallel_region</code>在 TP&gt;1 时会涉及通信&#x2F;广播式的“让 input 在 TP ranks 上一致”，但如果启用了某些模式（sequence_parallel &#x2F; allreduce_dgrad &#x2F; expert 显式通信 &#x2F; disable_grad_reduce），这里会选择不走 copy 路径（因为这些模式下输入已经按其它语义准备好了，或者通信由别处负责），直接使用传入的<code>input_</code></p></li><li><p>然后其调用了<code>_forward_impl</code>计算结果，这里进行了多层包装，主要是为了应对<code>sequence_parallel</code>的情况，因为如果<code>sequence_parallel</code>为True，那么其会使用<code>All gather</code>获取input完整序列再做Gemm。</p><ul><li><p>注意其这里也定义了在 backward 中：</p><ul><li><p>如果 ctx.allreduce_dgrad&#x3D;True：会 torch.distributed.all_reduce(grad_input, async_op&#x3D;True)<br>这是 TP 下典型的 dgrad 通信重叠。</p></li><li><p>如果 ctx.sequence_parallel&#x3D;True：会 <code>reduce_scatter</code> 把 grad_input 分发回 sequence-parallel 格式。</p></li></ul></li></ul></li><li><p>然后其还需要根据<code>runtime_gather_output</code>参数来判断是否需要执行All Gather来复原所有结果。注意在上述的MLP Forward计算时并没有配置<code>runtime_gather_output</code>，所以没有执行All Gather，这也符合TP并行的需要</p></li><li><p>最后返回<code>output</code>, <code>output_bias</code></p></li></ol></li></ul><blockquote><p>注意这里并没有直接定义backward的行为，但是正如我们前面所分析的，列并行在反向传播时求 $$\frac{\partial L}{\partial X}$$时需要All Reduce(Sum)操作，这部分backward的行为是Pytorch自动生成的</p></blockquote><h5 id="row-parallel-linear"><a href="#row-parallel-linear" class="headerlink" title="row_parallel_linear"></a><code>row_parallel_linear</code></h5><p><code>row_parallel_linear</code>代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RowParallelLinear</span>(torch.nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Linear layer with row parallelism.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    The linear layer is defined as Y = XA + b. A is parallelized along its first dimension and X</span><br><span class="hljs-string">    along its second dimension. A = transpose([A_1 .. A_p]) X = [X_1, ..., X_p]</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        input_size:</span><br><span class="hljs-string">            first dimension of matrix A.</span><br><span class="hljs-string">        output_size:</span><br><span class="hljs-string">            second dimension of matrix A.</span><br><span class="hljs-string">        bias:</span><br><span class="hljs-string">            If true, add bias. Note that bias is not parallelized.</span><br><span class="hljs-string">        input_is_parallel:</span><br><span class="hljs-string">            If true, we assume that the input is already split across the GPUs</span><br><span class="hljs-string">            and we do not split again.</span><br><span class="hljs-string">        init_method:</span><br><span class="hljs-string">            method to initialize weights. Note that bias is always set to zero.</span><br><span class="hljs-string">        stride:</span><br><span class="hljs-string">            For the strided linear layers.</span><br><span class="hljs-string">        keep_master_weight_for_test:</span><br><span class="hljs-string">            This was added for testing and should be set to False. It returns the master weights</span><br><span class="hljs-string">            used for initialization.</span><br><span class="hljs-string">        skip_bias_add:</span><br><span class="hljs-string">            If True, do not add the bias term, instead return it to be added by the</span><br><span class="hljs-string">            caller. This enables performance optimations where bias can be fused with other</span><br><span class="hljs-string">            elementwise operations.</span><br><span class="hljs-string">        is_expert:</span><br><span class="hljs-string">            If True, the layer is treated as an MoE expert layer</span><br><span class="hljs-string">        tp_comm_buffer_name:</span><br><span class="hljs-string">            Communication buffer name. Not used in non-Transformer-Engine modules.</span><br><span class="hljs-string">        config:</span><br><span class="hljs-string">            ModelParallelConfig object</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        input_size: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        output_size: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        *,</span><br><span class="hljs-params">        config: ModelParallelConfig,</span><br><span class="hljs-params">        init_method: <span class="hljs-type">Callable</span>,</span><br><span class="hljs-params">        bias: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        input_is_parallel: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        skip_bias_add: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        stride: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">        keep_master_weight_for_test: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        is_expert: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        tp_comm_buffer_name: <span class="hljs-built_in">str</span> = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># Not used</span></span><br><span class="hljs-params">        tp_group: <span class="hljs-type">Optional</span>[torch.distributed.ProcessGroup] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>(RowParallelLinear, self).__init__()<br><br>        <span class="hljs-comment"># Keep input parameters</span><br>        self.input_size = input_size<br>        self.output_size = output_size<br>        self.input_is_parallel = input_is_parallel<br>        self.skip_bias_add = skip_bias_add<br>        self.config = config<br>        self.is_expert = is_expert<br>        self.expert_parallel = config.expert_model_parallel_size &gt; <span class="hljs-number">1</span><br>        self.gradient_accumulation_fusion = config.gradient_accumulation_fusion<br>        self.sequence_parallel = config.sequence_parallel<br>        self.tp_group = tp_group<br><br>        <span class="hljs-keyword">if</span> self.sequence_parallel <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.input_is_parallel:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&quot;To enable `sequence_parallel`, `input_is_parallel` must be `True`&quot;</span>)<br><br>        <span class="hljs-comment"># Divide the weight matrix along the last dimension.</span><br>        self.tp_group = get_tensor_model_parallel_group_if_none(<br>            self.tp_group, is_expert=self.is_expert<br>        )<br><br>        world_size = get_pg_size(self.tp_group)<br>        rank = get_pg_rank(self.tp_group)<br>        self.explicit_expert_comm = self.is_expert <span class="hljs-keyword">and</span> (world_size &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> self.expert_parallel)<br><br>        self.input_size_per_partition = divide(input_size, world_size)<br><br>        <span class="hljs-comment"># Parameters.</span><br>        <span class="hljs-comment"># Note: torch.nn.functional.linear performs XA^T + b and as a result</span><br>        <span class="hljs-comment"># we allocate the transpose.</span><br>        <span class="hljs-comment"># Initialize weight.</span><br>        <span class="hljs-keyword">if</span> config.use_cpu_initialization:<br>            self.weight = Parameter(<br>                torch.empty(<br>                    self.output_size, self.input_size_per_partition, dtype=config.params_dtype<br>                )<br>            )<br>            <span class="hljs-keyword">if</span> config.perform_initialization:<br>                self.master_weight = _initialize_affine_weight_cpu(<br>                    self.weight,<br>                    self.output_size,<br>                    self.input_size,<br>                    self.input_size_per_partition,<br>                    <span class="hljs-number">1</span>,<br>                    init_method,<br>                    stride=stride,<br>                    return_master_weight=keep_master_weight_for_test,<br>                    params_dtype=config.params_dtype,<br>                    rank=rank,<br>                    world_size=world_size,<br>                )<br>        <span class="hljs-keyword">else</span>:<br>            self.weight = Parameter(<br>                torch.empty(<br>                    self.output_size,<br>                    self.input_size_per_partition,<br>                    device=torch.cuda.current_device(),<br>                    dtype=config.params_dtype,<br>                )<br>            )<br>            <span class="hljs-keyword">if</span> config.perform_initialization:<br>                _initialize_affine_weight_gpu(<br>                    self.weight,<br>                    init_method,<br>                    partition_dim=<span class="hljs-number">1</span>,<br>                    stride=stride,<br>                    is_expert=self.is_expert,<br>                )<br>        <span class="hljs-built_in">setattr</span>(self.weight, <span class="hljs-string">&quot;allreduce&quot;</span>, <span class="hljs-keyword">not</span> (self.is_expert <span class="hljs-keyword">and</span> self.expert_parallel))<br><br>        <span class="hljs-keyword">if</span> bias:<br>            <span class="hljs-keyword">if</span> config.use_cpu_initialization:<br>                self.bias = Parameter(torch.empty(self.output_size, dtype=config.params_dtype))<br>            <span class="hljs-keyword">else</span>:<br>                self.bias = Parameter(<br>                    torch.empty(<br>                        self.output_size,<br>                        device=torch.cuda.current_device(),<br>                        dtype=config.params_dtype,<br>                    )<br>                )<br><br>            <span class="hljs-keyword">if</span> config.perform_initialization:<br>                <span class="hljs-comment"># Always initialize bias to zero.</span><br>                <span class="hljs-keyword">with</span> torch.no_grad():<br>                    self.bias.zero_()<br>            <span class="hljs-built_in">setattr</span>(self.bias, <span class="hljs-string">&quot;allreduce&quot;</span>, <span class="hljs-keyword">not</span> (self.is_expert <span class="hljs-keyword">and</span> self.expert_parallel))<br>            <span class="hljs-built_in">setattr</span>(self.bias, <span class="hljs-string">&quot;sequence_parallel&quot;</span>, self.sequence_parallel)<br>        <span class="hljs-keyword">else</span>:<br>            self.register_parameter(<span class="hljs-string">&quot;bias&quot;</span>, <span class="hljs-literal">None</span>)<br><br>        <span class="hljs-comment"># Hook adding a default empty _extra_state for state dict</span><br>        self._register_load_state_dict_pre_hook(<br>            <span class="hljs-keyword">lambda</span> state_dict, prefix, *args, **kwargs: state_dict.setdefault(<br>                <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;prefix&#125;</span>_extra_state&quot;</span><br>            )<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_impl</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span>, weight, *args, **kwargs</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> weight.requires_grad:<br>            <span class="hljs-keyword">return</span> linear_with_frozen_weight(<span class="hljs-built_in">input</span>, weight, *args, **kwargs)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> linear_with_grad_accumulation_and_async_allreduce(<span class="hljs-built_in">input</span>, weight, *args, **kwargs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input_</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Forward of RowParallelLinear</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            input_: 3D tensor whose order of dimension is [sequence, batch, hidden]</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            - output</span><br><span class="hljs-string">            - bias</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># Set up backprop all-reduce.</span><br>        <span class="hljs-keyword">if</span> self.input_is_parallel:<br>            input_parallel = input_<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> self.sequence_parallel<br>            input_parallel = scatter_to_tensor_model_parallel_region(input_, group=self.tp_group)<br>        <span class="hljs-comment"># Matrix multiply.</span><br>        allreduce_dgrad = <span class="hljs-literal">False</span><br><br>        <span class="hljs-keyword">if</span> self.config._cpu_offloading_context <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> self.config._cpu_offloading_context.inside_context <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span>:<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> HAVE_TE:<br>                    <span class="hljs-keyword">assert</span> (<br>                        self.config.cpu_offloading <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span><br>                    ), <span class="hljs-string">&quot;CPU Offloading cannot be enabled while TE is not present&quot;</span><br>                <span class="hljs-keyword">else</span>:<br>                    input_parallel.activation_offloading = self.config.cpu_offloading_activations<br><br>        output_parallel = self._forward_impl(<br>            <span class="hljs-built_in">input</span>=input_parallel,<br>            weight=self.weight,<br>            bias=<span class="hljs-literal">None</span>,<br>            gradient_accumulation_fusion=self.gradient_accumulation_fusion,<br>            allreduce_dgrad=allreduce_dgrad,<br>            sequence_parallel=<span class="hljs-literal">False</span>,<br>            tp_group=<span class="hljs-literal">None</span>,<br>            grad_output_buffer=<span class="hljs-literal">None</span>,<br>        )<br><br>        <span class="hljs-comment"># All-reduce across all the partitions.</span><br>        <span class="hljs-keyword">if</span> self.explicit_expert_comm:<br>            <span class="hljs-keyword">assert</span> self.skip_bias_add<br>            output_ = output_parallel<br>        <span class="hljs-keyword">elif</span> self.sequence_parallel:<br>            output_ = reduce_scatter_to_sequence_parallel_region(<br>                output_parallel, group=self.tp_group<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            output_ = reduce_from_tensor_model_parallel_region(output_parallel, group=self.tp_group)<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.skip_bias_add:<br>            output = (output_ + self.bias) <span class="hljs-keyword">if</span> self.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> output_<br>            output_bias = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">else</span>:<br>            output = output_<br>            output_bias = self.bias<br>        <span class="hljs-keyword">return</span> output, output_bias<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sharded_state_dict</span>(<span class="hljs-params">self, prefix=<span class="hljs-string">&quot;&quot;</span>, sharded_offsets=(<span class="hljs-params"></span>), metadata=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Sharding along axis 1, bias not sharded&quot;&quot;&quot;</span><br>        state_dict = self.state_dict(prefix=<span class="hljs-string">&quot;&quot;</span>, keep_vars=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">return</span> make_sharded_tensors_for_checkpoint(<br>            state_dict, prefix, &#123;<span class="hljs-string">&quot;weight&quot;</span>: <span class="hljs-number">1</span>&#125;, sharded_offsets<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">set_extra_state</span>(<span class="hljs-params">self, state: <span class="hljs-type">Any</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Extra state is ignored&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_extra_state</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Keep compatibility with TE state dict.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__repr__</span>(<span class="hljs-params">self</span>):<br>        tp = self.input_size // self.input_size_per_partition<br>        use_bias = self.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> self.bias <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">return</span> (<br>            <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(self).__name__&#125;</span>(in_features=<span class="hljs-subst">&#123;self.input_size&#125;</span>, &quot;</span><br>            <span class="hljs-string">f&quot;out_features=<span class="hljs-subst">&#123;self.output_size&#125;</span>, bias=<span class="hljs-subst">&#123;use_bias&#125;</span>, TP=<span class="hljs-subst">&#123;tp&#125;</span>)&quot;</span><br>        )<br><br></code></pre></td></tr></table></figure><ul><li><p>在初始化时：</p><ul><li><p>参数设置整体与<code>row_parallel_linear</code>类似，不同点在于其包含参数<code>input_is_parallel</code>记录输出是否已经被并行切分，并且存在约束如果设置了<code>self.sequence_parallel</code>，那么<code>self.input_is_parallel</code>必须为True。</p></li><li><p>其切分权重时也是对输入维度进行切分（input_size_per_partition &#x3D; input_size &#x2F; tp_world_size）</p></li></ul></li><li><p>在Forward时，流程如下：</p><ol><li><p>其计查看参数<code>input_is_parallel</code>，如果没有切分就调用scatter在TP组内进行划分</p></li><li><p>然后其调用<code>_forward_impl</code>来实现具体计算，与<code>ColumnParallelLinear</code>计算类似，如果使用了<code>sequence_parallel</code>会先All Gather获取对应输入数据</p></li><li><p>然后对局部输出做对应通信得到<code>output</code>：</p><ul><li><p><strong>普通情况（非 expert、非 sequence_parallel）</strong>：<br>调用 <code>reduce_from_tensor_model_parallel_region</code><br>=&gt; 本质是 **TP all-reduce(sum)**，把各 rank 的 <code>Y_i</code> 求和得到完整 <code>Y</code>（每个 rank 都得到同样的 <code>Y</code>）。</p></li><li><p><strong>sequence_parallel&#x3D;True</strong>：<br>调用 <code>reduce_scatter_to_sequence_parallel_region</code><br>=&gt; 把 sum 的结果直接按 sequence parallel 需要的布局做 <strong>reduce-scatter</strong>，避免先 all-reduce 再切分的额外开销。</p></li><li><p><strong>expert 显式通信</strong>（MoE）： 不在这里做 reduce，直接返回本地 output_parallel，因为 MoE 的 token dispatcher 负责跨 rank 的聚合&#x2F;路由。</p></li></ul></li><li><p>最后返回<code>output</code>, <code>output_bias</code></p></li></ol></li></ul><h4 id="Transformer模块"><a href="#Transformer模块" class="headerlink" title="Transformer模块"></a>Transformer模块</h4><p>在具体实现Transformer模块时，其会依赖<code>multi_latent_attention</code>参数来判断GPT 的每一层 self-attention 子模块用标准SelfAttention还是用MLA（Multi‑Latent Attention）变体。</p><p>我们这里直接看最标准的实现，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerLayer</span>(MegatronModule, BaseTransformerLayer):<br>    <span class="hljs-string">&quot;&quot;&quot;A single transformer layer.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Transformer layer takes input with size [s, b, h] and returns an</span><br><span class="hljs-string">    output of the same size.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        config: TransformerConfig,</span><br><span class="hljs-params">        submodules: TransformerLayerSubmodules,</span><br><span class="hljs-params">        layer_number: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">        hidden_dropout: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">float</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        model_comm_pgs: <span class="hljs-type">Optional</span>[ModelCommProcessGroups] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        vp_stage: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__(config=config)<br><br>        <span class="hljs-comment"># Enable cuda graphs.</span><br>        <span class="hljs-keyword">if</span> (<br>            config.enable_cuda_graph <span class="hljs-keyword">and</span> config.cuda_graph_scope != <span class="hljs-string">&quot;full_iteration&quot;</span><br>        ) <span class="hljs-keyword">or</span> config.external_cuda_graph:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> (<br>                config.enable_cuda_graph <span class="hljs-keyword">and</span> config.external_cuda_graph<br>            ), <span class="hljs-string">&quot;Cudagraphs and external cudagraphs cannot be enabled at the same time&quot;</span><br>            <span class="hljs-keyword">if</span> config.enable_cuda_graph <span class="hljs-keyword">and</span> config.cuda_graph_scope != <span class="hljs-string">&quot;full_iteration&quot;</span>:<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.training:<br>                    <span class="hljs-comment"># Cudagraphs for inference are only enabled with the flash decoding kernel</span><br>                    <span class="hljs-keyword">assert</span> (<br>                        self.config.flash_decode<br>                    ), <span class="hljs-string">&quot;--flash-decode is required to use CUDA graphs during inference&quot;</span><br>                self.cudagraph_manager = CudaGraphManager(config, vp_stage=vp_stage)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># List to store CUDA graphs. A list of `N` CUDA graphs for this layer where N is</span><br>                <span class="hljs-comment"># the number of microbatches. Multiple CUDA graphs per layer is required to support</span><br>                <span class="hljs-comment"># pipelining which requires running FWD graph of multiple microbatches before BWD</span><br>                <span class="hljs-comment"># graph. To enable CUDA graph, this list should be populated in the model training</span><br>                <span class="hljs-comment"># script with the graphs returned by make_graphed_callables API before the first</span><br>                <span class="hljs-comment"># training step.</span><br>                self.cuda_graphs = []<br>                <span class="hljs-comment"># List to store forward pre-hooks. Forward pre-hooks are not captured into CUDA</span><br>                <span class="hljs-comment"># graphs. Those hooks and args are collected in this list and should be manually</span><br>                <span class="hljs-comment"># triggered before CUDA Graph running. This is required to ensure the correct param</span><br>                <span class="hljs-comment"># all-gather overlap with forward compute.</span><br>                self.cuda_graph_manual_hooks = []<br>                self.current_microbatch = -<span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">if</span> model_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            model_comm_pgs = ModelCommProcessGroups.use_mpu_process_groups()<br><br>        self.submodules_config = submodules<br>        self.layer_number = layer_number + get_transformer_layer_offset(self.config, vp_stage)<br>        self.hidden_dropout = config.hidden_dropout <span class="hljs-keyword">if</span> hidden_dropout <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> hidden_dropout<br><br>        <span class="hljs-comment"># [Module 1: Input Layernorm] Optional Layernorm on the input data</span><br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> add pytorch only layernorm</span><br>        self.input_layernorm = build_module(<br>            submodules.input_layernorm,<br>            config=self.config,<br>            hidden_size=self.config.hidden_size,<br>            eps=self.config.layernorm_epsilon,<br>        )<br><br>        attention_optional_kwargs = &#123;&#125;<br>        <span class="hljs-keyword">if</span> config.context_parallel_size &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> config.cp_comm_type <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(config.cp_comm_type, <span class="hljs-built_in">list</span>):<br>                attention_optional_kwargs[<span class="hljs-string">&quot;cp_comm_type&quot;</span>] = config.cp_comm_type[self.layer_number]<br>            <span class="hljs-keyword">else</span>:<br>                attention_optional_kwargs[<span class="hljs-string">&quot;cp_comm_type&quot;</span>] = config.cp_comm_type<br><br>        attention_optional_kwargs[<span class="hljs-string">&quot;model_comm_pgs&quot;</span>] = model_comm_pgs<br><br>        <span class="hljs-comment"># [Module 2: SelfAttention]</span><br>        self.self_attention = build_module(<br>            submodules.self_attention,<br>            config=self.config,<br>            layer_number=self.layer_number,<br>            **attention_optional_kwargs,<br>        )<br><br>        <span class="hljs-comment"># [Module 3: BiasDropoutFusion]</span><br>        self.self_attn_bda = build_module(submodules.self_attn_bda)<br><br>        <span class="hljs-comment"># [Module 4: Post SelfAttention] Optional Layernorm after self-attn</span><br>        self.pre_cross_attn_layernorm = build_module(<br>            submodules.pre_cross_attn_layernorm,<br>            config=self.config,<br>            hidden_size=self.config.hidden_size,<br>            eps=self.config.layernorm_epsilon,<br>        )<br><br>        <span class="hljs-comment"># [Module 5: CrossAttention]</span><br>        self.cross_attention = build_module(<br>            submodules.cross_attention,<br>            config=self.config,<br>            layer_number=self.layer_number,<br>            **attention_optional_kwargs,<br>        )<br><br>        <span class="hljs-comment"># [Module 6: BiasDropoutFusion]</span><br>        self.cross_attn_bda = build_module(submodules.cross_attn_bda, config=self.config)<br><br>        <span class="hljs-comment"># [Module 7: Pre MLP] Optional Layernorm before MLP</span><br>        self.pre_mlp_layernorm = build_module(<br>            submodules.pre_mlp_layernorm,<br>            config=self.config,<br>            hidden_size=self.config.hidden_size,<br>            eps=self.config.layernorm_epsilon,<br>        )<br>        <span class="hljs-comment"># [Module 8: MLP block]</span><br>        additional_mlp_kwargs = &#123;&#125;<br>        <span class="hljs-comment"># import here to avoid circular import</span><br>        <span class="hljs-keyword">from</span> megatron.core.extensions.transformer_engine <span class="hljs-keyword">import</span> TEFusedMLP<br>        <span class="hljs-keyword">from</span> megatron.core.transformer.moe.experts <span class="hljs-keyword">import</span> GroupedMLP, SequentialMLP, TEGroupedMLP<br>        <span class="hljs-keyword">from</span> megatron.core.transformer.moe.moe_layer <span class="hljs-keyword">import</span> MoELayer<br><br>        <span class="hljs-comment"># MLP expects tp_group but MoELayer expects model_comm_pgs to be passed in.</span><br>        <span class="hljs-comment"># We can change MLP to accept model_comm_pgs but it makes the logic implicit</span><br>        <span class="hljs-comment"># The conditional below is to make the logic explicit</span><br>        <span class="hljs-comment"># if submodules.mlp is not a ModuleSpec,we dont have to handle passing additional kwargs</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(submodules.mlp, ModuleSpec):<br>            <span class="hljs-keyword">if</span> submodules.mlp.module <span class="hljs-keyword">in</span> (MoELayer, GroupedMLP, TEGroupedMLP, SequentialMLP):<br>                additional_mlp_kwargs[<span class="hljs-string">&quot;model_comm_pgs&quot;</span>] = model_comm_pgs<br>            <span class="hljs-keyword">elif</span> submodules.mlp.module == MLP:<br>                <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(<br>                    model_comm_pgs, <span class="hljs-string">&#x27;tp&#x27;</span><br>                ), <span class="hljs-string">&#x27;TP process group is required for MLP in TransformerLayer&#x27;</span><br>                additional_mlp_kwargs[<span class="hljs-string">&quot;tp_group&quot;</span>] = model_comm_pgs.tp<br>            <span class="hljs-keyword">elif</span> TEFusedMLP <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> submodules.mlp.module == TEFusedMLP:<br>                <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(<br>                    model_comm_pgs, <span class="hljs-string">&#x27;tp&#x27;</span><br>                ), <span class="hljs-string">&#x27;TP process group is required for TEFusedMLP in TransformerLayer&#x27;</span><br>                additional_mlp_kwargs[<span class="hljs-string">&quot;tp_group&quot;</span>] = model_comm_pgs.tp<br>            <span class="hljs-keyword">else</span>:<br>                log_single_rank(<br>                    logger,<br>                    logging.WARNING,<br>                    <span class="hljs-string">f&quot;Unknown MLP type: <span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(submodules.mlp)&#125;</span>. Using default kwargs.&quot;</span>,<br>                )<br>        self.mlp = build_module(submodules.mlp, config=self.config, **additional_mlp_kwargs)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(self.mlp, <span class="hljs-string">&#x27;set_layer_number&#x27;</span>):<br>            self.mlp.set_layer_number(self.layer_number)<br><br>        <span class="hljs-comment"># [Module 9: BiasDropoutFusion]</span><br>        self.mlp_bda = build_module(submodules.mlp_bda)<br><br>        self.recompute_input_layernorm = <span class="hljs-literal">False</span><br>        self.recompute_pre_mlp_layernorm = <span class="hljs-literal">False</span><br>        self.recompute_mlp = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">if</span> self.config.recompute_granularity == <span class="hljs-string">&#x27;selective&#x27;</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;layernorm&quot;</span> <span class="hljs-keyword">in</span> self.config.recompute_modules:<br>                <span class="hljs-keyword">if</span> (<br>                    <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.input_layernorm, IdentityOp)<br>                    <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.config.external_cuda_graph<br>                ):<br>                    self.recompute_input_layernorm = <span class="hljs-literal">True</span><br>                    <span class="hljs-keyword">if</span> self.config.fp8:<br>                        self.self_attention.set_for_recompute_input_layernorm()<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.pre_mlp_layernorm, IdentityOp):<br>                    self.recompute_pre_mlp_layernorm = <span class="hljs-literal">True</span><br>                    <span class="hljs-keyword">if</span> self.config.fp8:<br>                        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(self.mlp, MoELayer):<br>                            self.mlp.set_for_recompute_pre_mlp_layernorm()<br>                        <span class="hljs-keyword">else</span>:<br>                            <span class="hljs-keyword">from</span> megatron.core.extensions.transformer_engine <span class="hljs-keyword">import</span> (<br>                                set_save_original_input,<br>                            )<br><br>                            set_save_original_input(self.mlp.linear_fc1)<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;mlp&quot;</span> <span class="hljs-keyword">in</span> self.config.recompute_modules:<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.mlp, MoELayer):<br>                    self.recompute_mlp = <span class="hljs-literal">True</span><br><br>        <span class="hljs-comment"># @jcasper how should we handle nvfuser?</span><br>        <span class="hljs-comment"># Set bias+dropout+add fusion grad_enable execution handler.</span><br>        <span class="hljs-comment"># TORCH_MAJOR = int(torch.__version__.split(&#x27;.&#x27;)[0])</span><br>        <span class="hljs-comment"># TORCH_MINOR = int(torch.__version__.split(&#x27;.&#x27;)[1])</span><br>        <span class="hljs-comment"># use_nvfuser = TORCH_MAJOR &gt; 1 or (TORCH_MAJOR == 1 and TORCH_MINOR &gt;= 10)</span><br>        <span class="hljs-comment"># self.bias_dropout_add_exec_handler = nullcontext if use_nvfuser else torch.enable_grad</span><br>        self.bias_dropout_add_exec_handler = torch.enable_grad<br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_layer_offset</span>(<span class="hljs-params">config: TransformerConfig</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Get the layer offset for the current pipeline stage.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Deprecated: please use `get_transformer_layer_offset` instead.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        warnings.warn(<br>            <span class="hljs-string">&quot;TransformerLayer._get_layer_offset is deprecated.&quot;</span><br>            <span class="hljs-string">&quot;Please use get_transformer_layer_offset instead.&quot;</span><br>        )<br>        <span class="hljs-keyword">return</span> get_transformer_layer_offset(config)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Perform a forward pass through the transformer layer.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This method calls the core computation of a transformer layer, including</span><br><span class="hljs-string">        self-attention, cross-attention (if applicable), and feed-forward operations.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        hidden_states, context = self._forward_attention(*args, **kwargs)<br>        output = self._forward_mlp(hidden_states, kwargs.get(<span class="hljs-string">&quot;inference_context&quot;</span>, <span class="hljs-literal">None</span>))<br>        <span class="hljs-keyword">return</span> output, context<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_attention</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        hidden_states: Tensor,</span><br><span class="hljs-params">        attention_mask: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        context: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        context_mask: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        rotary_pos_emb: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        rotary_pos_cos: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        rotary_pos_sin: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        attention_bias: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        inference_context: <span class="hljs-type">Optional</span>[<span class="hljs-type">Any</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        packed_seq_params: <span class="hljs-type">Optional</span>[PackedSeqParams] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        sequence_len_offset: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        *,</span><br><span class="hljs-params">        inference_params: <span class="hljs-type">Optional</span>[<span class="hljs-type">Any</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Perform a forward pass through the attention layer and the layernorms before and after</span><br><span class="hljs-string">        the attention operations.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            hidden_states (Tensor): Input tensor of shape [s, b, h] where s is sequence length,</span><br><span class="hljs-string">                b is batch size, and h is hidden size.</span><br><span class="hljs-string">            attention_mask (Tensor): Mask tensor for self-attention.</span><br><span class="hljs-string">            context (Tensor, optional): Context tensor for cross-attention.</span><br><span class="hljs-string">            context_mask (Tensor, optional): Mask tensor for cross-attention.</span><br><span class="hljs-string">            rotary_pos_emb (Tensor, optional): Rotary positional embeddings.</span><br><span class="hljs-string">            attention_bias (Tensor, optional): Bias tensor for Q * K.T.</span><br><span class="hljs-string">            inference_context (object, optional): Parameters for inference-time optimizations.</span><br><span class="hljs-string">            packed_seq_params (object, optional): Parameters for packed sequence processing.</span><br><span class="hljs-string">            sequence_len_offset (Tensor, optional): Offset along sequence dimension</span><br><span class="hljs-string">                during inference.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Tuple[Tensor, Tensor]: A tuple containing:</span><br><span class="hljs-string">                hidden_states (Tensor): Transformed hidden states before the MLP layernorm.</span><br><span class="hljs-string">                context (Tensor): Updated context tensor if cross-attention is used,</span><br><span class="hljs-string">                otherwise None.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        inference_context = deprecate_inference_params(inference_context, inference_params)<br><br>        <span class="hljs-comment"># Residual connection.</span><br>        residual = hidden_states<br><br>        <span class="hljs-comment"># Optional Input Layer norm</span><br>        <span class="hljs-keyword">if</span> self.recompute_input_layernorm:<br>            self.input_layernorm_checkpoint = tensor_parallel.CheckpointWithoutOutput()<br>            input_layernorm_output = self.input_layernorm_checkpoint.checkpoint(<br>                self.input_layernorm, hidden_states<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            input_layernorm_output = self.input_layernorm(hidden_states)<br><br>        <span class="hljs-comment"># Self attention.</span><br>        nvtx_range_push(suffix=<span class="hljs-string">&quot;self_attention&quot;</span>)<br>        attention_output_with_bias = self.self_attention(<br>            input_layernorm_output,<br>            attention_mask=attention_mask,<br>            inference_context=inference_context,<br>            rotary_pos_emb=rotary_pos_emb,<br>            rotary_pos_cos=rotary_pos_cos,<br>            rotary_pos_sin=rotary_pos_sin,<br>            attention_bias=attention_bias,<br>            packed_seq_params=packed_seq_params,<br>            sequence_len_offset=sequence_len_offset,<br>        )<br>        nvtx_range_pop(suffix=<span class="hljs-string">&quot;self_attention&quot;</span>)<br><br>        <span class="hljs-keyword">if</span> self.recompute_input_layernorm:<br>            <span class="hljs-comment"># discard the output of the input layernorm and register the recompute</span><br>            <span class="hljs-comment"># as a gradient hook of attention_output_with_bias[0]</span><br>            self.input_layernorm_checkpoint.discard_output_and_register_recompute(<br>                attention_output_with_bias[<span class="hljs-number">0</span>]<br>            )<br><br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> could we move `bias_dropout_add_exec_handler` itself</span><br>        <span class="hljs-comment"># inside the module provided in the `bias_dropout_add_spec` module?</span><br>        nvtx_range_push(suffix=<span class="hljs-string">&quot;self_attn_bda&quot;</span>)<br>        <span class="hljs-keyword">with</span> self.bias_dropout_add_exec_handler():<br>            hidden_states = self.self_attn_bda(self.training, self.config.bias_dropout_fusion)(<br>                attention_output_with_bias, residual, self.hidden_dropout<br>            )<br>        nvtx_range_pop(suffix=<span class="hljs-string">&quot;self_attn_bda&quot;</span>)<br><br>        <span class="hljs-comment"># Residual connection.</span><br>        residual = hidden_states<br><br>        <span class="hljs-comment"># Optional Layer norm after self-attention</span><br>        pre_cross_attn_layernorm_output = self.pre_cross_attn_layernorm(hidden_states)<br><br>        <span class="hljs-comment"># Cross attention.</span><br>        attention_output_with_bias = self.cross_attention(<br>            pre_cross_attn_layernorm_output,<br>            attention_mask=context_mask,<br>            key_value_states=context,<br>            inference_context=inference_context,<br>        )<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(attention_output_with_bias, <span class="hljs-built_in">dict</span>) <span class="hljs-keyword">and</span> <span class="hljs-string">&quot;context&quot;</span> <span class="hljs-keyword">in</span> attention_output_with_bias:<br>            context = attention_output_with_bias[<span class="hljs-string">&quot;context&quot;</span>]<br><br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> could we move `bias_dropout_add_exec_handler` itself</span><br>        <span class="hljs-comment"># inside the module provided in the `bias_dropout_add_spec` module?</span><br>        <span class="hljs-keyword">with</span> self.bias_dropout_add_exec_handler():<br>            hidden_states = self.cross_attn_bda(self.training, self.config.bias_dropout_fusion)(<br>                attention_output_with_bias, residual, self.hidden_dropout<br>            )<br><br>        <span class="hljs-keyword">return</span> hidden_states, context<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_mlp</span>(<span class="hljs-params">self, hidden_states, inference_context=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Perform a forward pass through the feed-forward layer.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            hidden_states (Tensor): Transformed hidden states before the MLP layernorm.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            output (Tensor): Transformed hidden states of shape [s, b, h].</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># Residual connection.</span><br>        residual = hidden_states<br><br>        <span class="hljs-comment"># Optional Layer norm post the cross-attention.</span><br>        <span class="hljs-keyword">if</span> self.recompute_pre_mlp_layernorm:<br>            self.pre_mlp_norm_checkpoint = tensor_parallel.CheckpointWithoutOutput()<br>            pre_mlp_layernorm_output = self.pre_mlp_norm_checkpoint.checkpoint(<br>                self.pre_mlp_layernorm, hidden_states<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            pre_mlp_layernorm_output = self.pre_mlp_layernorm(hidden_states)<br><br>        nvtx_range_push(suffix=<span class="hljs-string">&quot;mlp&quot;</span>)<br>        <span class="hljs-comment"># Potentially chunk the MLP computation during prefill to minimize the peak activation size</span><br>        should_chunk_mlp_for_prefill = (<br>            self.config.mlp_chunks_for_prefill &gt; <span class="hljs-number">1</span><br>            <span class="hljs-keyword">and</span> inference_context <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> inference_context.is_decode_only()<br>            <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.mlp, IdentityOp)<br>        )<br><br>        <span class="hljs-keyword">if</span> self.recompute_mlp:<br>            <span class="hljs-keyword">if</span> self.config.fp8:<br>                <span class="hljs-comment"># import here to avoid circular import</span><br>                <span class="hljs-keyword">from</span> megatron.core.extensions.transformer_engine <span class="hljs-keyword">import</span> te_checkpoint<br><br>                mlp_output_with_bias = te_checkpoint(<br>                    self.mlp,<br>                    <span class="hljs-literal">False</span>,<br>                    tensor_parallel.random.get_cuda_rng_tracker,<br>                    parallel_state.get_tensor_model_parallel_group(),<br>                    pre_mlp_layernorm_output,<br>                )<br>            <span class="hljs-keyword">else</span>:<br>                mlp_output_with_bias = tensor_parallel.checkpoint(<br>                    self.mlp, <span class="hljs-literal">False</span>, pre_mlp_layernorm_output<br>                )<br>        <span class="hljs-keyword">elif</span> should_chunk_mlp_for_prefill:<br>            <span class="hljs-comment"># Chunk input along sequence dimension</span><br>            num_chunks = <span class="hljs-built_in">min</span>(self.config.mlp_chunks_for_prefill, pre_mlp_layernorm_output.shape[<span class="hljs-number">0</span>])<br>            chunks = pre_mlp_layernorm_output.chunk(num_chunks, dim=<span class="hljs-number">0</span>)<br><br>            <span class="hljs-comment"># Compute outputs for each chunk</span><br>            outputs = [self.mlp(chunk) <span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> chunks]<br><br>            <span class="hljs-comment"># Aggregate chunk outputs</span><br>            mlp_output = torch.cat([out <span class="hljs-keyword">for</span> out, _ <span class="hljs-keyword">in</span> outputs], dim=<span class="hljs-number">0</span>)<br>            bias_chunks = [bias <span class="hljs-keyword">for</span> _, bias <span class="hljs-keyword">in</span> outputs <span class="hljs-keyword">if</span> bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>]<br>            bias_output = torch.stack(bias_chunks, dim=<span class="hljs-number">0</span>).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">0</span>) <span class="hljs-keyword">if</span> bias_chunks <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>            mlp_output_with_bias = (mlp_output, bias_output)<br><br>        <span class="hljs-keyword">else</span>:<br>            mlp_output_with_bias = self.mlp(pre_mlp_layernorm_output)<br><br>        <span class="hljs-keyword">if</span> self.recompute_pre_mlp_layernorm:<br>            <span class="hljs-comment"># discard the output of the pre-mlp layernorm and register the recompute</span><br>            <span class="hljs-comment"># as a gradient hook of mlp_output_with_bias[0]</span><br>            self.pre_mlp_norm_checkpoint.discard_output_and_register_recompute(<br>                mlp_output_with_bias[<span class="hljs-number">0</span>]<br>            )<br>        nvtx_range_pop(suffix=<span class="hljs-string">&quot;mlp&quot;</span>)<br><br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> could we move `bias_dropout_add_exec_handler` itself</span><br>        <span class="hljs-comment"># inside the module provided in the `bias_dropout_add_spec` module?</span><br>        nvtx_range_push(suffix=<span class="hljs-string">&quot;mlp_bda&quot;</span>)<br>        <span class="hljs-keyword">with</span> self.bias_dropout_add_exec_handler():<br>            hidden_states = self.mlp_bda(self.training, self.config.bias_dropout_fusion)(<br>                mlp_output_with_bias, residual, self.hidden_dropout<br>            )<br>        nvtx_range_pop(suffix=<span class="hljs-string">&quot;mlp_bda&quot;</span>)<br><br>        <span class="hljs-comment"># Jit compiled function creates &#x27;view&#x27; tensor. This tensor</span><br>        <span class="hljs-comment"># potentially gets saved in the MPU checkpoint function context,</span><br>        <span class="hljs-comment"># which rejects view tensors. While making a viewless tensor here</span><br>        <span class="hljs-comment"># won&#x27;t result in memory savings (like the data loader, or</span><br>        <span class="hljs-comment"># p2p_communication), it serves to document the origin of this</span><br>        <span class="hljs-comment"># &#x27;view&#x27; tensor.</span><br>        output = make_viewless_tensor(<br>            inp=hidden_states, requires_grad=hidden_states.requires_grad, keep_graph=<span class="hljs-literal">True</span><br>        )<br><br>        <span class="hljs-keyword">return</span> output<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sharded_state_dict</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, prefix: <span class="hljs-built_in">str</span> = <span class="hljs-string">&#x27;&#x27;</span>, sharded_offsets: <span class="hljs-built_in">tuple</span> = (<span class="hljs-params"></span>), metadata: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">dict</span>] = <span class="hljs-literal">None</span></span><br><span class="hljs-params">    </span>) -&gt; ShardedStateDict:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Generate a sharded state dictionary for the transformer layer.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            prefix (str, optional): Prefix to be added to all keys in the state dict.</span><br><span class="hljs-string">            sharded_offsets (tuple, optional): Tuple of sharding offsets.</span><br><span class="hljs-string">            metadata (Optional[dict], optional): Additional metadata for sharding.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            ShardedStateDict: A dictionary containing the sharded state of the transformer layer.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        sharded_state_dict = <span class="hljs-built_in">super</span>().sharded_state_dict(prefix, sharded_offsets, metadata)<br>        prefixed_map = &#123;<br>            <span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;prefix&#125;</span><span class="hljs-subst">&#123;k&#125;</span>&#x27;</span>: <span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;prefix&#125;</span><span class="hljs-subst">&#123;v&#125;</span>&#x27;</span><br>            <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> self.submodules_config.sharded_state_dict_keys_map.items()<br>        &#125;<br>        <span class="hljs-keyword">if</span> prefixed_map:<br>            apply_prefix_mapping(sharded_state_dict, prefixed_map)<br>        <span class="hljs-keyword">return</span> sharded_state_dict<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_layer_static_inputs</span>(<span class="hljs-params">self, seq_length, micro_batch_size</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Get the static inputs for the transformer layer.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Dict[str, torch.Tensor]: A dictionary containing the static inputs for the layer.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Calculate data shape related values.</span><br>        context_parallel_size = self.config.context_parallel_size<br>        slen_per_cp = seq_length // context_parallel_size<br>        sequence_parallel = self.config.sequence_parallel<br>        tensor_model_parallel_size = self.config.tensor_model_parallel_size<br>        slen_per_cptp = (<br>            slen_per_cp // tensor_model_parallel_size <span class="hljs-keyword">if</span> sequence_parallel <span class="hljs-keyword">else</span> slen_per_cp<br>        )<br><br>        static_inputs = &#123;&#125;<br>        static_inputs[<span class="hljs-string">&quot;hidden_states&quot;</span>] = torch.ones(<br>            (slen_per_cptp, micro_batch_size, self.config.hidden_size),<br>            dtype=torch.bfloat16,<br>            requires_grad=<span class="hljs-literal">True</span>,<br>            device=torch.cuda.current_device(),<br>        )<br>        static_inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>] = (<br>            ~(torch.tril(torch.ones((slen_per_cp, seq_length))).<span class="hljs-built_in">bool</span>())<br>            .to(torch.cuda.current_device())<br>            .reshape(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, slen_per_cp, seq_length)<br>            .tile(micro_batch_size, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        )<br>        <span class="hljs-keyword">return</span> static_inputs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_manual_hooks</span>(<span class="hljs-params">self, make_hook_func</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Set CUDA Graph manual hooks for the modules that contain direct parameters and are</span><br><span class="hljs-string">        covered by cudagraphs.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.cuda_graph_manual_hooks = []<br><br>        <span class="hljs-comment"># Select the modules who contain direct parameters and are covered by cudagraphs.</span><br>        <span class="hljs-comment"># Add these modules to the `cuda_graph_manual_hooks` because their hooks will not</span><br>        <span class="hljs-comment"># be automatically triggered when they go through the CUDA Graph path.</span><br>        <span class="hljs-keyword">if</span> self.config.cuda_graph_scope == <span class="hljs-string">&#x27;full&#x27;</span>:<br>            high_level_modules = [self]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">assert</span> (<br>                self.config.cuda_graph_scope == <span class="hljs-string">&#x27;attn&#x27;</span><br>            ), <span class="hljs-string">&quot;Invalid cuda_graph_scope $&#123;self.config.cuda_graph_scope&#125;&quot;</span><br>            high_level_modules = [<br>                self.input_layernorm,<br>                self.self_attention,<br>                self.pre_cross_attn_layernorm,<br>                self.cross_attention,<br>            ]<br><br>        param_modules = []<br>        <span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> high_level_modules:<br>            <span class="hljs-keyword">for</span> submodule <span class="hljs-keyword">in</span> module.modules():<br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">next</span>(submodule.parameters(recurse=<span class="hljs-literal">False</span>), <span class="hljs-literal">None</span>) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    <span class="hljs-comment"># Module contains direct parameters.</span><br>                    param_modules.append(submodule)<br>                    <span class="hljs-keyword">continue</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(param_modules) &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> param_modules:<br>                self.cuda_graph_manual_hooks.append((make_hook_func(), (module,)))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_cuda_graph_capture</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        CUDA Graph capture for this layer. There are some differences from the normal pass:</span><br><span class="hljs-string">        1. In some conditions CUDA graph cannot cover the entire layer. The `cuda_graph_scope`</span><br><span class="hljs-string">           attribute can be set to control the scope of the CUDA graph.</span><br><span class="hljs-string">        2. If context is None, it cannot be returned as output.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        hidden_states, context = self._forward_attention(*args, **kwargs)<br><br>        <span class="hljs-keyword">if</span> self.config.cuda_graph_scope == <span class="hljs-string">&quot;full&quot;</span>:<br>            hidden_states = self._forward_mlp(hidden_states)<br>        cuda_graph_outputs = [hidden_states]<br><br>        <span class="hljs-keyword">if</span> context <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            cuda_graph_outputs.append(context)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">tuple</span>(cuda_graph_outputs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_cuda_graph_replay</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        CUDA graph replay for this layer and microbatch</span><br><span class="hljs-string">        `self.current_microbatch`. TransformerEngine versions&gt;=1.10</span><br><span class="hljs-string">        allow keyword arguments with CUDA graph. However, CUDA graph</span><br><span class="hljs-string">        acccepts only Tensor inputs and Tensor outputs. Hence,</span><br><span class="hljs-string">        `inference_context` and `packed_seq_params` are excluded from</span><br><span class="hljs-string">        input list while output is limited to `hidden_states`.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">_check_cuda_graph_replay_args</span>(<span class="hljs-params">*args, **kwargs</span>):<br>            <span class="hljs-string">&quot;&quot;&quot;Helper function to get optional tensor arguments for CUDA graph.&quot;&quot;&quot;</span><br><br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(args) &lt;= <span class="hljs-number">1</span>, <span class="hljs-string">&quot;At most one positional argument `hidden_states` is expected.&quot;</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(args) == <span class="hljs-number">1</span>:<br>                hidden_states = args[<span class="hljs-number">0</span>]<br>            <span class="hljs-keyword">else</span>:<br>                hidden_states = kwargs.pop(<span class="hljs-string">&quot;hidden_states&quot;</span>)<br>            cudagraph_args = [hidden_states]<br><br>            optional_inputs = kwargs.copy()<br>            optional_inputs[<span class="hljs-string">&#x27;is_first_microbatch&#x27;</span>] = self.current_microbatch == <span class="hljs-number">0</span><br>            <span class="hljs-keyword">try</span>:<br>                <span class="hljs-keyword">import</span> transformer_engine.pytorch <span class="hljs-keyword">as</span> te  <span class="hljs-comment"># pylint: disable=unused-import</span><br><br>                <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_zero_attention_mask</span>(<span class="hljs-params">slen_per_tpcp, micro_batch_size</span>):<br>                    sequence_parallel = self.config.sequence_parallel<br>                    tensor_model_parallel_size = self.config.tensor_model_parallel_size<br>                    slen_per_cp = (<br>                        slen_per_tpcp * tensor_model_parallel_size<br>                        <span class="hljs-keyword">if</span> sequence_parallel<br>                        <span class="hljs-keyword">else</span> slen_per_tpcp<br>                    )<br>                    slen = slen_per_cp * self.config.context_parallel_size<br>                    <span class="hljs-keyword">return</span> torch.zeros(<br>                        (micro_batch_size, <span class="hljs-number">1</span>, slen_per_cp, slen),<br>                        dtype=torch.<span class="hljs-built_in">bool</span>,<br>                        device=torch.cuda.current_device(),<br>                    )<br><br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> is_te_min_version(<span class="hljs-string">&quot;1.10.0&quot;</span>):<br>                    <span class="hljs-comment"># TE version &lt; 1.10.0 does not support keyword arguments with CUDA graph.</span><br>                    <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> kwargs.items():<br>                        <span class="hljs-keyword">if</span> k == <span class="hljs-string">&quot;attention_mask&quot;</span>:<br>                            <span class="hljs-keyword">if</span> v <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                                cudagraph_args.append(v)<br>                                optional_inputs[k] = <span class="hljs-literal">None</span><br>                            <span class="hljs-keyword">else</span>:<br>                                cudagraph_args.append(<br>                                    get_zero_attention_mask(<br>                                        hidden_states.size(<span class="hljs-number">0</span>), hidden_states.size(<span class="hljs-number">1</span>)<br>                                    )<br>                                )<br>                        <span class="hljs-keyword">else</span>:<br>                            <span class="hljs-keyword">assert</span> v <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;Keyword Arguments not supported with CUDA graph.&quot;</span><br>                <span class="hljs-keyword">elif</span> optional_inputs[<span class="hljs-string">&#x27;attention_mask&#x27;</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                    <span class="hljs-comment"># The attention_mask can be None when there is no padding to the input sequence.</span><br>                    <span class="hljs-comment"># However, an attention_mask Tensor must be passed into cudagraph for replay, so</span><br>                    <span class="hljs-comment"># we create an equivalent zero Tensor as the attention_mask.</span><br>                    optional_inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>] = get_zero_attention_mask(<br>                        hidden_states.size(<span class="hljs-number">0</span>), hidden_states.size(<span class="hljs-number">1</span>)<br>                    )<br>            <span class="hljs-keyword">except</span> ImportError:<br>                <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&quot;CUDAGraph requires TransformerEngine, but not installed&quot;</span>)<br>            <span class="hljs-keyword">return</span> <span class="hljs-built_in">tuple</span>(cudagraph_args), optional_inputs<br><br>        cg_index = self.current_microbatch % <span class="hljs-built_in">len</span>(self.cuda_graphs)<br>        <span class="hljs-keyword">assert</span> (<span class="hljs-string">&#x27;inference_context&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> kwargs <span class="hljs-keyword">or</span> kwargs[<span class="hljs-string">&#x27;inference_context&#x27;</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>) <span class="hljs-keyword">and</span> (<br>            <span class="hljs-string">&#x27;packed_seq_params&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> kwargs <span class="hljs-keyword">or</span> kwargs[<span class="hljs-string">&#x27;packed_seq_params&#x27;</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>        ), <span class="hljs-string">&quot;CUDA graph accepts only Tensor inputs.&quot;</span><br>        cudagraph_args, cudagraph_kwargs = _check_cuda_graph_replay_args(*args, **kwargs)<br><br>        <span class="hljs-keyword">for</span> hook, hook_args <span class="hljs-keyword">in</span> self.cuda_graph_manual_hooks:<br>            hook(*hook_args)<br>        cuda_graph_output = self.cuda_graphs[cg_index](*cudagraph_args, **cudagraph_kwargs)<br><br>        <span class="hljs-keyword">if</span> cudagraph_kwargs.get(<span class="hljs-string">&#x27;context&#x27;</span>) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            context = cuda_graph_output[-<span class="hljs-number">1</span>]<br>            cuda_graph_output = cuda_graph_output[:-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">else</span>:<br>            context = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> self.config.cuda_graph_scope == <span class="hljs-string">&quot;attn&quot;</span>:<br>            <span class="hljs-comment"># CUDA Graph only covers the attention layer. Feed-forward</span><br>            <span class="hljs-comment"># layer still goes through the normal pass.</span><br>            output = self._forward_mlp(*cuda_graph_output)<br>        <span class="hljs-keyword">else</span>:<br>            output = cuda_graph_output[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">return</span> output, context<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-comment"># Training and validation mode CUDA graphs</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(self, <span class="hljs-string">&#x27;cudagraph_manager&#x27;</span>) <span class="hljs-keyword">and</span> kwargs.get(<span class="hljs-string">&#x27;inference_context&#x27;</span>) <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span> self.cudagraph_manager(self, args, kwargs)<br>        <span class="hljs-comment"># Inference mode. CUDA graphs are used in the decode phase only, when attn mask is None</span><br>        <span class="hljs-keyword">elif</span> <span class="hljs-keyword">not</span> self.training <span class="hljs-keyword">and</span> (<br>            <span class="hljs-built_in">hasattr</span>(self, <span class="hljs-string">&#x27;cudagraph_manager&#x27;</span>)<br>            <span class="hljs-keyword">and</span> kwargs[<span class="hljs-string">&#x27;attention_mask&#x27;</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">and</span> (<br>                (<br>                    kwargs.get(<span class="hljs-string">&#x27;inference_context&#x27;</span>) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>                    <span class="hljs-keyword">and</span> kwargs[<span class="hljs-string">&#x27;inference_context&#x27;</span>].is_decode_only()<br>                )<br>                <span class="hljs-keyword">or</span> (<br>                    kwargs.get(<span class="hljs-string">&#x27;inference_params&#x27;</span>) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>                    <span class="hljs-keyword">and</span> kwargs[<span class="hljs-string">&#x27;inference_params&#x27;</span>].is_decode_only()<br>                )<br>            )<br>        ):<br>            <span class="hljs-keyword">assert</span> (<br>                kwargs.get(<span class="hljs-string">&#x27;attention_mask&#x27;</span>) <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>            ), <span class="hljs-string">f&quot;Attention mask must not be set when using CUDA graphs for decode&quot;</span><br>            <span class="hljs-keyword">return</span> self.cudagraph_manager(self, args, kwargs)<br>        <span class="hljs-keyword">elif</span> (<br>            self.config.external_cuda_graph<br>            <span class="hljs-keyword">and</span> self.training<br>            <span class="hljs-keyword">and</span> (is_graph_capturing() <span class="hljs-keyword">or</span> self.cuda_graphs)<br>        ):<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.cuda_graphs:<br>                <span class="hljs-comment"># Do CUDA Graphs capture.</span><br>                cuda_graph_func = self._cuda_graph_capture<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># Do CUDA Graphs replay.</span><br>                cuda_graph_func = self._cuda_graph_replay<br>            <span class="hljs-keyword">return</span> cuda_graph_func(*args, **kwargs)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">super</span>(MegatronModule, self).__call__(*args, **kwargs)<br><br></code></pre></td></tr></table></figure><ul><li><p>其初始化的时候初始化了下面几个模块：</p><ol><li><p>Input Layernorm：对输入数据进行可选的层归一化</p></li><li><p>SelfAttention</p></li><li><p>BiasDropoutFusion</p></li><li><p>Post SelfAttention：自注意力后的可选层归一化</p></li><li><p>CrossAttention</p></li><li><p>BiasDropoutFusion</p></li><li><p>Pre MLP：MLP 前的可选层归一化</p></li><li><p>MLP block</p></li><li><p>BiasDropoutFusion</p></li></ol></li></ul><p>我们下面再看一下SelfAttention模块是如何设计的，尤其关注其与TP并行相关的内容</p><h5 id="SelfAttention"><a href="#SelfAttention" class="headerlink" title="SelfAttention"></a>SelfAttention</h5><p>SelfAttention的相关代码如下所示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SelfAttention</span>(<span class="hljs-title class_ inherited__">Attention</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Self-attention layer class</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Self-attention layer takes input with size [s, b, h]</span><br><span class="hljs-string">    and returns output of the same size.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        config: TransformerConfig,</span><br><span class="hljs-params">        submodules: SelfAttentionSubmodules,</span><br><span class="hljs-params">        layer_number: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        attn_mask_type=AttnMaskType.padding,</span><br><span class="hljs-params">        cp_comm_type: <span class="hljs-built_in">str</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        model_comm_pgs: ModelCommProcessGroups = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__(<br>            config=config,<br>            submodules=submodules,<br>            layer_number=layer_number,<br>            attn_mask_type=attn_mask_type,<br>            attention_type=<span class="hljs-string">&quot;self&quot;</span>,<br>            cp_comm_type=cp_comm_type,<br>            model_comm_pgs=model_comm_pgs,<br>        )<br><br>        self.linear_qkv = build_module(<br>            submodules.linear_qkv,<br>            self.config.hidden_size,<br>            self.query_projection_size + <span class="hljs-number">2</span> * self.kv_projection_size,<br>            config=self.config,<br>            init_method=self.config.init_method,<br>            gather_output=<span class="hljs-literal">False</span>,<br>            bias=self.config.add_bias_linear <span class="hljs-keyword">or</span> self.config.add_qkv_bias,<br>            skip_bias_add=<span class="hljs-literal">False</span>,<br>            is_expert=<span class="hljs-literal">False</span>,<br>            tp_comm_buffer_name=<span class="hljs-string">&#x27;qkv&#x27;</span>,<br>            tp_group=self.model_comm_pgs.tp,<br>        )<br><br>        <span class="hljs-keyword">if</span> submodules.q_layernorm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            self.q_layernorm = build_module(<br>                submodules.q_layernorm,<br>                hidden_size=self.hidden_size_per_attention_head,<br>                config=self.config,<br>                eps=self.config.layernorm_epsilon,<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            self.q_layernorm = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> submodules.k_layernorm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            self.k_layernorm = build_module(<br>                submodules.k_layernorm,<br>                hidden_size=self.hidden_size_per_attention_head,<br>                config=self.config,<br>                eps=self.config.layernorm_epsilon,<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            self.k_layernorm = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run_realtime_tests</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Performs a consistency check.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This function makes sure that tensors across devices are the same during an experiment.</span><br><span class="hljs-string">        This is often not guaranteed to be so because of silent hardware failures (eg, memory</span><br><span class="hljs-string">        corruption loading a checkpoint, network traffic corruption encountered during</span><br><span class="hljs-string">        data transmission).</span><br><span class="hljs-string"></span><br><span class="hljs-string">        (TODO) In the future, more tensors should be checked across the training run and</span><br><span class="hljs-string">        checked every X iterations. This is left for future work. Equality of tensors is probably</span><br><span class="hljs-string">        not required; transmitting hashes is sufficient.&quot;&quot;&quot;</span><br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.config.qk_layernorm:<br>            <span class="hljs-keyword">return</span><br><br>        <span class="hljs-comment"># check that all tensor parallel and data parallel ranks have the same</span><br>        <span class="hljs-comment"># Q &amp; K layernorm parameters.</span><br>        rank = get_data_parallel_rank()<br>        inputs = torch.stack(<br>            [<br>                self.q_layernorm.weight.data,<br>                self.q_layernorm.bias.data,<br>                self.k_layernorm.weight.data,<br>                self.k_layernorm.bias.data,<br>            ]<br>        )<br>        dp_list = [torch.empty_like(inputs) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(get_data_parallel_world_size())]<br>        dp_list[rank] = inputs<br>        torch.distributed.all_gather(dp_list, inputs, group=get_data_parallel_group())<br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">_compare</span>(<span class="hljs-params">srcs, tgts, names, parallelism</span>):<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(srcs) == <span class="hljs-built_in">len</span>(tgts) == <span class="hljs-built_in">len</span>(names)<br>            <span class="hljs-keyword">for</span> src, tgt, name <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(srcs, tgts, names):<br>                <span class="hljs-keyword">assert</span> torch.<span class="hljs-built_in">all</span>(src == tgt), (<br>                    <span class="hljs-string">f&quot;Discrepancy between <span class="hljs-subst">&#123;name&#125;</span> in <span class="hljs-subst">&#123;parallelism&#125;</span> ranks <span class="hljs-subst">&#123;i&#125;</span> and <span class="hljs-subst">&#123;rank&#125;</span>. &quot;</span><br>                    <span class="hljs-string">f&quot;Diff: <span class="hljs-subst">&#123;torch.norm(src - tgt)&#125;</span>&quot;</span><br>                )<br><br>        <span class="hljs-keyword">for</span> i, dp <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dp_list):<br>            q_w, q_b, k_w, k_b = torch.unbind(dp)<br>            _compare(<br>                [q_w, q_b, k_w, k_b],<br>                [<br>                    self.q_layernorm.weight.data,<br>                    self.q_layernorm.bias.data,<br>                    self.k_layernorm.weight.data,<br>                    self.k_layernorm.bias.data,<br>                ],<br>                [<span class="hljs-string">&quot;q_w&quot;</span>, <span class="hljs-string">&quot;q_b&quot;</span>, <span class="hljs-string">&quot;k_w&quot;</span>, <span class="hljs-string">&quot;k_b&quot;</span>],<br>                <span class="hljs-string">&quot;DP&quot;</span>,<br>            )<br><br>        rank = get_tensor_model_parallel_rank()<br>        tp_list = [torch.empty_like(inputs) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(get_tensor_model_parallel_world_size())]<br>        tp_list[rank] = inputs<br>        torch.distributed.all_gather(tp_list, inputs, group=get_tensor_model_parallel_group())<br><br>        <span class="hljs-keyword">for</span> i, tp <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tp_list):<br>            q_w, q_b, k_w, k_b = torch.unbind(tp)<br>            _compare(<br>                [q_w, q_b, k_w, k_b],<br>                [<br>                    self.q_layernorm.weight.data,<br>                    self.q_layernorm.bias.data,<br>                    self.k_layernorm.weight.data,<br>                    self.k_layernorm.bias.data,<br>                ],<br>                [<span class="hljs-string">&quot;q_w&quot;</span>, <span class="hljs-string">&quot;q_b&quot;</span>, <span class="hljs-string">&quot;k_w&quot;</span>, <span class="hljs-string">&quot;k_b&quot;</span>],<br>                <span class="hljs-string">&quot;TP&quot;</span>,<br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_query_key_value_tensors</span>(<span class="hljs-params">self, hidden_states, key_value_states=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Derives `query`, `key` and `value` tensors from `hidden_states`.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Attention heads [sq, b, h] --&gt; [sq, b, ng * (np/ng + 2) * hn)]</span><br>        mixed_qkv, _ = self.linear_qkv(hidden_states)<br><br>        <span class="hljs-comment"># [sq, b, hp] --&gt; [sq, b, ng, (np/ng + 2) * hn]</span><br>        new_tensor_shape = mixed_qkv.size()[:-<span class="hljs-number">1</span>] + (<br>            self.num_query_groups_per_partition,<br>            (<br>                (self.num_attention_heads_per_partition // self.num_query_groups_per_partition + <span class="hljs-number">2</span>)<br>                * self.hidden_size_per_attention_head<br>            ),<br>        )<br>        mixed_qkv = mixed_qkv.view(*new_tensor_shape)<br><br>        split_arg_list = [<br>            (<br>                self.num_attention_heads_per_partition<br>                // self.num_query_groups_per_partition<br>                * self.hidden_size_per_attention_head<br>            ),<br>            self.hidden_size_per_attention_head,<br>            self.hidden_size_per_attention_head,<br>        ]<br><br>        <span class="hljs-keyword">if</span> SplitAlongDim <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br><br>            <span class="hljs-comment"># [sq, b, ng, (np/ng + 2) * hn]</span><br>            <span class="hljs-comment"># --&gt; [sq, b, ng, np/ng * hn], [sq, b, ng, hn], [sq, b, ng, hn]</span><br>            (query, key, value) = SplitAlongDim(mixed_qkv, <span class="hljs-number">3</span>, split_arg_list)<br>        <span class="hljs-keyword">else</span>:<br><br>            <span class="hljs-comment"># [sq, b, ng, (np/ng + 2) * hn]</span><br>            <span class="hljs-comment"># --&gt; [sq, b, ng, np/ng * hn], [sq, b, ng, hn], [sq, b, ng, hn]</span><br>            (query, key, value) = torch.split(mixed_qkv, split_arg_list, dim=<span class="hljs-number">3</span>)<br><br>        <span class="hljs-comment"># [sq, b, ng, np/ng * hn] -&gt; [sq, b, np, hn]</span><br>        query = query.reshape(query.size(<span class="hljs-number">0</span>), query.size(<span class="hljs-number">1</span>), -<span class="hljs-number">1</span>, self.hidden_size_per_attention_head)<br><br>        <span class="hljs-keyword">if</span> self.q_layernorm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            query = self.q_layernorm(query)<br><br>        <span class="hljs-keyword">if</span> self.k_layernorm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            key = self.k_layernorm(key)<br><br>        <span class="hljs-keyword">if</span> self.config.test_mode:<br>            self.run_realtime_tests()<br><br>        <span class="hljs-keyword">return</span> query, key, value<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward_dw</span>(<span class="hljs-params">self</span>) -&gt; NoReturn:<br>        <span class="hljs-string">&quot;&quot;&quot;Execute weight update operations&quot;&quot;&quot;</span><br>        self._backward_qkv_proj()<br>        self._backward_output_proj()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_backward_qkv_proj</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Update weights for QKV projection layer&quot;&quot;&quot;</span><br>        self.linear_qkv.backward_dw()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_backward_output_proj</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Update weights for output projection layer&quot;&quot;&quot;</span><br>        self.linear_proj.backward_dw()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">set_for_recompute_input_layernorm</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Set the attention layer for recompute input_layernorm. Only needed for fp8.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">from</span> megatron.core.extensions.transformer_engine <span class="hljs-keyword">import</span> set_save_original_input<br><br>        set_save_original_input(self.linear_qkv)<br><br></code></pre></td></tr></table></figure><ul><li><p>在初始化时：</p><ul><li><p>尤其它是拓展了<code>Attention</code>类，所以其首先对Attention进行了初始化，Attention的初始化代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Attention</span>(MegatronModule, ABC):<br>    <span class="hljs-string">&quot;&quot;&quot;Attention layer abstract class.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    This layer only contains common modules required for the &quot;self attn&quot; and</span><br><span class="hljs-string">    &quot;cross attn&quot; specializations.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        config: TransformerConfig,</span><br><span class="hljs-params">        submodules: <span class="hljs-type">Union</span>[SelfAttentionSubmodules, CrossAttentionSubmodules],</span><br><span class="hljs-params">        layer_number: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        attn_mask_type: AttnMaskType,</span><br><span class="hljs-params">        attention_type: <span class="hljs-built_in">str</span>,</span><br><span class="hljs-params">        cp_comm_type: <span class="hljs-built_in">str</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        model_comm_pgs: ModelCommProcessGroups = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__(config=config)<br><br>        self.config = config<br>        self.layer_number = layer_number<br>        self.attn_mask_type = attn_mask_type<br>        self.attention_type = attention_type<br><br>        <span class="hljs-comment"># For normal attention without groups, num_query_groups == num_attention_heads,</span><br>        <span class="hljs-comment"># so these two will be the same</span><br>        self.query_projection_size = self.config.kv_channels * self.config.num_attention_heads<br>        self.kv_projection_size = self.config.kv_channels * self.config.num_query_groups<br><br>        <span class="hljs-keyword">if</span> model_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            model_comm_pgs = ModelCommProcessGroups.use_mpu_process_groups(<br>                required_pgs=[<span class="hljs-string">&#x27;tp&#x27;</span>, <span class="hljs-string">&#x27;cp&#x27;</span>]<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(<br>                model_comm_pgs, <span class="hljs-string">&#x27;tp&#x27;</span><br>            ), <span class="hljs-string">&quot;Attention model_comm_pgs must have tp process group&quot;</span><br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(<br>                model_comm_pgs, <span class="hljs-string">&#x27;cp&#x27;</span><br>            ), <span class="hljs-string">&quot;Attention model_comm_pgs must have cp process group&quot;</span><br>        self.model_comm_pgs = model_comm_pgs<br><br>        <span class="hljs-comment"># Per attention head and per partition values</span><br>        world_size = get_pg_size(self.model_comm_pgs.tp)<br>        self.hidden_size_per_attention_head = divide(<br>            self.query_projection_size, self.config.num_attention_heads<br>        )<br>        self.num_attention_heads_per_partition = divide(self.config.num_attention_heads, world_size)<br>        self.num_query_groups_per_partition = divide(self.config.num_query_groups, world_size)<br><br>        <span class="hljs-comment"># To support both CUDA Graphs and key value with different hidden size</span><br>        self.key_hidden_size = self.hidden_size_per_attention_head<br>        self.val_hidden_size = self.hidden_size_per_attention_head<br><br>        self.core_attention = build_module(<br>            submodules.core_attention,<br>            config=self.config,<br>            layer_number=self.layer_number,<br>            attn_mask_type=self.attn_mask_type,<br>            attention_type=self.attention_type,<br>            cp_comm_type=cp_comm_type,<br>            softmax_scale=self.config.softmax_scale,<br>            model_comm_pgs=self.model_comm_pgs,<br>        )<br><br>        self.checkpoint_core_attention = (<br>            self.config.recompute_granularity == <span class="hljs-string">&#x27;selective&#x27;</span><br>            <span class="hljs-keyword">and</span> <span class="hljs-string">&quot;core_attn&quot;</span> <span class="hljs-keyword">in</span> self.config.recompute_modules<br>        )<br><br>        <span class="hljs-comment"># Output.</span><br>        self.linear_proj = build_module(<br>            submodules.linear_proj,<br>            self.query_projection_size,<br>            self.config.hidden_size,<br>            config=self.config,<br>            init_method=self.config.output_layer_init_method,<br>            bias=self.config.add_bias_linear,<br>            input_is_parallel=<span class="hljs-literal">True</span>,<br>            skip_bias_add=<span class="hljs-literal">True</span>,<br>            is_expert=<span class="hljs-literal">False</span>,<br>            tp_comm_buffer_name=<span class="hljs-string">&#x27;proj&#x27;</span>,<br>            tp_group=self.model_comm_pgs.tp,<br>        )<br><br>        <span class="hljs-keyword">if</span> (<br>            HAVE_TE<br>            <span class="hljs-keyword">and</span> self.config.fp8<br>            <span class="hljs-keyword">and</span> self.config.fp8_recipe != <span class="hljs-string">&#x27;delayed&#x27;</span><br>            <span class="hljs-keyword">and</span> is_te_min_version(<span class="hljs-string">&quot;2.6.0dev0&quot;</span>)<br>            <span class="hljs-keyword">and</span> <span class="hljs-built_in">isinstance</span>(self.linear_proj, TELinear)<br>        ):<br>            <span class="hljs-comment"># For fp8 training, the output of the fused core_attn is saved by itself, and</span><br>            <span class="hljs-comment"># linear_proj also saves the quantized tensor of this output. Here we set the</span><br>            <span class="hljs-comment"># linear_proj to save the original input tensors to avoid the extra memory usage of</span><br>            <span class="hljs-comment"># the quantized tensor.</span><br>            set_save_original_input(self.linear_proj)<br><br></code></pre></td></tr></table></figure><ul><li><p>在计算q、k、v的输出维度时，其单独计算了q的维度（<code>self.query_projection_size = self.config.kv_channels * self.config.num_attention_heads</code>），再计算了k与v的维度（<code>self.kv_projection_size = self.config.kv_channels * self.config.num_query_groups</code>），因为在类似在GQA&#x2F;MQA中<code>self.config.num_attention_heads</code>与<code>self.config.num_query_groups</code>可能不同</p></li><li><p>然后基于TP并行度切分了q所对应的<code>self.config.num_attention_heads</code>个数，还切分了kv所对应的<code>self.config.num_query_groups</code>，注意这里如果不能整除的话会直接报错，所以运行起来的必然是每个TP rank都有均匀切分的q、k、v</p></li><li><p>然后其构建了core attention，在本地模式中使用的是<code>DotProductAttention</code>，代码如下所示，其主要是在Forward时负责依据传入的q、k、v、attention_mask等计算attention结果，</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DotProductAttention</span>(<span class="hljs-title class_ inherited__">MegatronModule</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Region where selective activation recomputation is applied.</span><br><span class="hljs-string">    This region is memory intensive but less compute intensive which</span><br><span class="hljs-string">    makes activation checkpointing more efficient for LLMs (20B+).</span><br><span class="hljs-string">    See Reducing Activation Recomputation in Large Transformer Models:</span><br><span class="hljs-string">    https://arxiv.org/abs/2205.05198 for more details.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    We use the following notation:</span><br><span class="hljs-string">     h: hidden size</span><br><span class="hljs-string">     n: number of attention heads</span><br><span class="hljs-string">     p: number of tensor model parallel partitions</span><br><span class="hljs-string">     b: batch size</span><br><span class="hljs-string">     s: sequence length</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        config: TransformerConfig,</span><br><span class="hljs-params">        layer_number: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        attn_mask_type: AttnMaskType,</span><br><span class="hljs-params">        attention_type: <span class="hljs-built_in">str</span>,</span><br><span class="hljs-params">        attention_dropout: <span class="hljs-built_in">float</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        softmax_scale: <span class="hljs-built_in">float</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        cp_comm_type: <span class="hljs-built_in">str</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        model_comm_pgs: ModelCommProcessGroups = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__(config=config)<br><br>        self.config: TransformerConfig = config<br><br>        <span class="hljs-keyword">assert</span> (<br>            self.config.context_parallel_size == <span class="hljs-number">1</span><br>        ), <span class="hljs-string">&quot;Context parallelism is only supported by TEDotProductAttention!&quot;</span><br><br>        <span class="hljs-keyword">assert</span> (<br>            self.config.window_size <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>        ), <span class="hljs-string">&quot;Sliding Window Attention is only supported by TEDotProductAttention!&quot;</span><br><br>        self.layer_number = <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, layer_number)<br>        self.attn_mask_type = attn_mask_type<br>        self.attention_type = attention_type  <span class="hljs-comment"># unused for now</span><br><br>        projection_size = self.config.kv_channels * self.config.num_attention_heads<br><br>        <span class="hljs-comment"># Per attention head and per partition values.</span><br>        <span class="hljs-keyword">if</span> model_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># For backward compatibility, remove in v0.14 and raise error</span><br>            <span class="hljs-comment"># raise ValueError(&quot;DotProductAttention was called without ModelCommProcessGroups&quot;)</span><br>            model_comm_pgs = ModelCommProcessGroups.use_mpu_process_groups(required_pgs=[<span class="hljs-string">&#x27;tp&#x27;</span>])<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(<br>                model_comm_pgs, <span class="hljs-string">&#x27;tp&#x27;</span><br>            ), <span class="hljs-string">&quot;DotProductAttention model_comm_pgs must have tp process group&quot;</span><br><br>        world_size = model_comm_pgs.tp.size()<br>        self.hidden_size_per_partition = divide(projection_size, world_size)<br>        self.hidden_size_per_attention_head = divide(projection_size, config.num_attention_heads)<br>        self.num_attention_heads_per_partition = divide(self.config.num_attention_heads, world_size)<br>        self.num_query_groups_per_partition = divide(self.config.num_query_groups, world_size)<br><br>        coeff = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> softmax_scale <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            self.softmax_scale = <span class="hljs-number">1.0</span> / math.sqrt(self.hidden_size_per_attention_head)<br>        <span class="hljs-keyword">else</span>:<br>            self.softmax_scale = softmax_scale<br><br>        <span class="hljs-keyword">if</span> self.config.apply_query_key_layer_scaling:<br>            coeff = self.layer_number<br>            self.softmax_scale /= coeff<br><br>        self.scale_mask_softmax = FusedScaleMaskSoftmax(<br>            input_in_fp16=self.config.fp16,<br>            input_in_bf16=self.config.bf16,<br>            attn_mask_type=self.attn_mask_type,<br>            scaled_masked_softmax_fusion=self.config.masked_softmax_fusion,<br>            mask_func=attention_mask_func,<br>            softmax_in_fp32=self.config.attention_softmax_in_fp32,<br>            scale=coeff,<br>        )<br><br>        <span class="hljs-comment"># Dropout. Note that for a single iteration, this layer will generate</span><br>        <span class="hljs-comment"># different outputs on different number of parallel partitions but</span><br>        <span class="hljs-comment"># on average it should not be partition dependent.</span><br>        self.attention_dropout = torch.nn.Dropout(<br>            self.config.attention_dropout <span class="hljs-keyword">if</span> attention_dropout <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> attention_dropout<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        query: Tensor,</span><br><span class="hljs-params">        key: Tensor,</span><br><span class="hljs-params">        value: Tensor,</span><br><span class="hljs-params">        attention_mask: Tensor,</span><br><span class="hljs-params">        attn_mask_type: AttnMaskType = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        attention_bias: Tensor = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        packed_seq_params: <span class="hljs-type">Optional</span>[PackedSeqParams] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Forward.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">assert</span> packed_seq_params <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, (<br>            <span class="hljs-string">&quot;Packed sequence is not supported by DotProductAttention.&quot;</span><br>            <span class="hljs-string">&quot;Please use TEDotProductAttention instead.&quot;</span><br>        )<br>        <span class="hljs-keyword">assert</span> attention_bias <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;Attention bias is not supported for DotProductAttention.&quot;</span><br><br>        <span class="hljs-comment"># ===================================</span><br>        <span class="hljs-comment"># Raw attention scores. [b, n/p, s, s]</span><br>        <span class="hljs-comment"># ===================================</span><br><br>        <span class="hljs-comment"># expand the key and value [sk, b, ng, hn] -&gt; [sk, b, np, hn]</span><br>        <span class="hljs-comment"># This is a noop for normal attention where ng == np. When using group query attention this</span><br>        <span class="hljs-comment"># creates a view that has the keys and values virtually repeated along their dimension to</span><br>        <span class="hljs-comment"># match the number of queries.</span><br><br>        <span class="hljs-comment"># attn_mask_type is not used.</span><br>        <span class="hljs-keyword">if</span> self.num_attention_heads_per_partition // self.num_query_groups_per_partition &gt; <span class="hljs-number">1</span>:<br>            key = key.repeat_interleave(<br>                self.num_attention_heads_per_partition // self.num_query_groups_per_partition, dim=<span class="hljs-number">2</span><br>            )<br>            value = value.repeat_interleave(<br>                self.num_attention_heads_per_partition // self.num_query_groups_per_partition, dim=<span class="hljs-number">2</span><br>            )<br><br>        <span class="hljs-comment"># [b, np, sq, sk]</span><br>        output_size = (query.size(<span class="hljs-number">1</span>), query.size(<span class="hljs-number">2</span>), query.size(<span class="hljs-number">0</span>), key.size(<span class="hljs-number">0</span>))<br><br>        <span class="hljs-comment"># [sq, b, np, hn] -&gt; [sq, b * np, hn]</span><br>        <span class="hljs-comment"># This will be a simple view when doing normal attention, but in group query attention</span><br>        <span class="hljs-comment"># the key and value tensors are repeated to match the queries so you can&#x27;t use</span><br>        <span class="hljs-comment"># simple strides to extract the queries.</span><br>        query = query.reshape(output_size[<span class="hljs-number">2</span>], output_size[<span class="hljs-number">0</span>] * output_size[<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># [sk, b, np, hn] -&gt; [sk, b * np, hn]</span><br>        key = key.view(output_size[<span class="hljs-number">3</span>], output_size[<span class="hljs-number">0</span>] * output_size[<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># preallocting input tensor: [b * np, sq, sk]</span><br>        matmul_input_buffer = parallel_state.get_global_memory_buffer().get_tensor(<br>            (output_size[<span class="hljs-number">0</span>] * output_size[<span class="hljs-number">1</span>], output_size[<span class="hljs-number">2</span>], output_size[<span class="hljs-number">3</span>]), query.dtype, <span class="hljs-string">&quot;mpu&quot;</span><br>        )<br><br>        <span class="hljs-comment"># Raw attention scores. [b * np, sq, sk]</span><br>        matmul_result = torch.baddbmm(<br>            matmul_input_buffer,<br>            query.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>),  <span class="hljs-comment"># [b * np, sq, hn]</span><br>            key.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>),  <span class="hljs-comment"># [b * np, hn, sk]</span><br>            beta=<span class="hljs-number">0.0</span>,<br>            alpha=self.softmax_scale,<br>        )<br><br>        <span class="hljs-comment"># change view to [b, np, sq, sk]</span><br>        attention_scores = matmul_result.view(*output_size)<br><br>        <span class="hljs-comment"># ===========================</span><br>        <span class="hljs-comment"># Attention probs and dropout</span><br>        <span class="hljs-comment"># ===========================</span><br><br>        <span class="hljs-comment"># attention scores and attention mask [b, np, sq, sk]</span><br>        attention_probs: Tensor = self.scale_mask_softmax(attention_scores, attention_mask)<br><br>        <span class="hljs-comment"># This is actually dropping out entire tokens to attend to, which might</span><br>        <span class="hljs-comment"># seem a bit unusual, but is taken from the original Transformer paper.</span><br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.config.sequence_parallel:<br>            <span class="hljs-keyword">with</span> tensor_parallel.get_cuda_rng_tracker().fork():<br>                attention_probs = self.attention_dropout(attention_probs)<br>        <span class="hljs-keyword">else</span>:<br>            attention_probs = self.attention_dropout(attention_probs)<br><br>        <span class="hljs-comment"># =========================</span><br>        <span class="hljs-comment"># Context layer. [sq, b, hp]</span><br>        <span class="hljs-comment"># =========================</span><br><br>        <span class="hljs-comment"># value -&gt; context layer.</span><br>        <span class="hljs-comment"># [sk, b, np, hn] --&gt; [b, np, sq, hn]</span><br><br>        <span class="hljs-comment"># context layer shape: [b, np, sq, hn]</span><br>        output_size = (value.size(<span class="hljs-number">1</span>), value.size(<span class="hljs-number">2</span>), query.size(<span class="hljs-number">0</span>), value.size(<span class="hljs-number">3</span>))<br><br>        <span class="hljs-comment"># change view [sk, b * np, hn]</span><br>        value = value.view(value.size(<span class="hljs-number">0</span>), output_size[<span class="hljs-number">0</span>] * output_size[<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># change view [b * np, sq, sk]</span><br>        attention_probs = attention_probs.view(output_size[<span class="hljs-number">0</span>] * output_size[<span class="hljs-number">1</span>], output_size[<span class="hljs-number">2</span>], -<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># matmul: [b * np, sq, hn]</span><br>        context = torch.bmm(attention_probs, value.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br><br>        <span class="hljs-comment"># change view [b, np, sq, hn]</span><br>        context = context.view(*output_size)<br><br>        <span class="hljs-comment"># [b, np, sq, hn] --&gt; [sq, b, np, hn]</span><br>        context = context.permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>).contiguous()<br><br>        <span class="hljs-comment"># [sq, b, np, hn] --&gt; [sq, b, hp]</span><br>        new_context_shape = context.size()[:-<span class="hljs-number">2</span>] + (self.hidden_size_per_partition,)<br>        context = context.view(*new_context_shape)<br><br>        <span class="hljs-keyword">return</span> context<br><br></code></pre></td></tr></table></figure><ul><li>然后其构建了<code>linear_proj</code>，注意其使用的是<code>row_parallel_linear</code>，并且它也明确在参数中指出了其输入是并行的，符合一贯的先列并行再行并行计算的结果</li></ul></li><li><p>其创建了linear_qkv：</p><ul><li><p><code>linear_qkv</code>是<code>column_parallel_linear</code>类</p></li><li><p><code>linear_qkv</code>输入维度是标准的<code>self.config.hidden_size</code>，其输出维度是<code>self.query_projection_size + 2 * self.kv_projection_size</code>，因为<code>linear_qkv</code>需要投影生成q、k、v这3个基础张量</p></li><li><p>此外值得注意的是它还专门设计了<code>gather_output</code>为False，因为其本身就希望使用列并行来多注意力头计算</p></li></ul></li><li><p>然后还构建了<code>submodules.q_layernorm</code>与<code>submodules.k_layernorm</code></p></li></ul></li><li><p>在Forward中完全走的是Attention的代码如下所示，依据<code>nvtx_range_push</code>其相关流程可以划分为：</p><ol><li><p>计算出当前Sequence的q、k、v：</p><ul><li>其代码如下所示，</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_query_key_value_tensors</span>(<span class="hljs-params">self, hidden_states, key_value_states=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Derives `query`, `key` and `value` tensors from `hidden_states`.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># Attention heads [sq, b, h] --&gt; [sq, b, ng * (np/ng + 2) * hn)]</span><br>    mixed_qkv, _ = self.linear_qkv(hidden_states)<br><br>    <span class="hljs-comment"># [sq, b, hp] --&gt; [sq, b, ng, (np/ng + 2) * hn]</span><br>    new_tensor_shape = mixed_qkv.size()[:-<span class="hljs-number">1</span>] + (<br>        self.num_query_groups_per_partition,<br>        (<br>            (self.num_attention_heads_per_partition // self.num_query_groups_per_partition + <span class="hljs-number">2</span>)<br>            * self.hidden_size_per_attention_head<br>        ),<br>    )<br>    mixed_qkv = mixed_qkv.view(*new_tensor_shape)<br><br>    split_arg_list = [<br>        (<br>            self.num_attention_heads_per_partition<br>            // self.num_query_groups_per_partition<br>            * self.hidden_size_per_attention_head<br>        ),<br>        self.hidden_size_per_attention_head,<br>        self.hidden_size_per_attention_head,<br>    ]<br><br>    <span class="hljs-keyword">if</span> SplitAlongDim <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br><br>        <span class="hljs-comment"># [sq, b, ng, (np/ng + 2) * hn]</span><br>        <span class="hljs-comment"># --&gt; [sq, b, ng, np/ng * hn], [sq, b, ng, hn], [sq, b, ng, hn]</span><br>        (query, key, value) = SplitAlongDim(mixed_qkv, <span class="hljs-number">3</span>, split_arg_list)<br>    <span class="hljs-keyword">else</span>:<br><br>        <span class="hljs-comment"># [sq, b, ng, (np/ng + 2) * hn]</span><br>        <span class="hljs-comment"># --&gt; [sq, b, ng, np/ng * hn], [sq, b, ng, hn], [sq, b, ng, hn]</span><br>        (query, key, value) = torch.split(mixed_qkv, split_arg_list, dim=<span class="hljs-number">3</span>)<br><br>    <span class="hljs-comment"># [sq, b, ng, np/ng * hn] -&gt; [sq, b, np, hn]</span><br>    query = query.reshape(query.size(<span class="hljs-number">0</span>), query.size(<span class="hljs-number">1</span>), -<span class="hljs-number">1</span>, self.hidden_size_per_attention_head)<br><br>    <span class="hljs-keyword">if</span> self.q_layernorm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        query = self.q_layernorm(query)<br><br>    <span class="hljs-keyword">if</span> self.k_layernorm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        key = self.k_layernorm(key)<br><br>    <span class="hljs-keyword">if</span> self.config.test_mode:<br>        self.run_realtime_tests()<br><br>    <span class="hljs-keyword">return</span> query, key, value<br><br></code></pre></td></tr></table></figure><ul><li><p>首先通过<code>mixed_qkv, _ = self.linear_qkv(hidden_states)</code>得到<code>mixed_qkv</code>，因为<code>self.linear_qkv</code>是列并行并且初始化时设置了<code>gather_output=False</code>，所以得到的<code>mixed_qkv</code>是被TP并行划分后的部分结果，由于前面的检查，所以它必然是q、k、v维度的整数倍。故是从[sq,b,h]转化为了[sq,b,per_tp_num_query_groups *(per_tp_num_heads &#x2F; per_tp_num_query_groups + 2) * head_dim]，结果的最后一维是q、k、v的维度和</p></li><li><p>然后会把形状进行调整，最后得到q的维度为[sq,b,per_tp_num_heads,head_dim]，最后得到k与v的维度都是[sq,b,per_tp_num_query_groups, head_dim]</p></li></ul><blockquote><p>为什么要引入 <code>num_query_groups</code> 这一维？因为它在支持 <strong>GQA&#x2F;MQA</strong> 时很关键：</p><ul><li><p>普通 attention：<code>num_query_groups== num_heads</code>，每个 group 只有 1 个 query head，对应关系很直接。</p></li><li><p>GQA：<code>num_query_groups &lt; num_heads</code>，多个 query heads 共享同一组 K&#x2F;V（在同一个 group 下）。</p></li></ul></blockquote></li><li><p>调整key值</p></li><li><p>调用<code>rotary_pos_emb</code></p></li><li><p>调用<code>core_attention</code>进行计算</p></li><li><p>调用<code>linear_proj</code>得到最终结果，因为其是一个<code>row_parallel_linear</code>，所以最后会通过all reduce得到完整的结果</p></li></ol></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    hidden_states: Tensor,</span><br><span class="hljs-params">    attention_mask: Tensor,</span><br><span class="hljs-params">    key_value_states: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    inference_context: <span class="hljs-type">Optional</span>[BaseInferenceContext] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    rotary_pos_emb: <span class="hljs-type">Optional</span>[<span class="hljs-type">Union</span>[Tensor, <span class="hljs-type">Tuple</span>[Tensor, Tensor]]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    rotary_pos_cos: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    rotary_pos_sin: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    attention_bias: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    packed_seq_params: <span class="hljs-type">Optional</span>[PackedSeqParams] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    sequence_len_offset: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    *,</span><br><span class="hljs-params">    inference_params: <span class="hljs-type">Optional</span>[BaseInferenceContext] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Tuple</span>[Tensor, Tensor]:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Perform a forward pass through the attention module.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        hidden_states (Tensor): Hidden states.</span><br><span class="hljs-string">        attention_mask (Tensor): Attention mask.</span><br><span class="hljs-string">        key_value_states (Optional[Tensor]): Key/value states (for cross attention).</span><br><span class="hljs-string">        inference_context (Optional[BaseInferenceContext]): Inference context that manages</span><br><span class="hljs-string">            KV cache.</span><br><span class="hljs-string">        rotary_pos_emb (Optional[Union[Tensor, Tuple[Tensor, Tensor]]]): Rotary</span><br><span class="hljs-string">            embedding tensor(s).</span><br><span class="hljs-string">        rotary_pos_cos (Optional[Tensor]): Rotary embedding cosine.</span><br><span class="hljs-string">        rotary_pos_sin (Optional[Tensor]): Rotary embedding sine.</span><br><span class="hljs-string">        attention_bias (Optional[Tensor]): Attention bias.</span><br><span class="hljs-string">        packed_seq_params (Optional[PackedSeqparams]): Parameters used for THD format.</span><br><span class="hljs-string">        sequence_len_offset (Optional[int]): Sequence length offset used for</span><br><span class="hljs-string">            inference CUDA graphs.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Return:</span><br><span class="hljs-string">        (Tuple[Tensor, Tensor]) Attention output and bias.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># Check if we need to skip RoPE</span><br>    <span class="hljs-comment"># no_rope is 0-indexed array and self.layer_number is 1-indexed</span><br>    no_rope = (<br>        self.config.no_rope_freq[self.layer_number - <span class="hljs-number">1</span>] <span class="hljs-keyword">if</span> self.config.no_rope_freq <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span><br>    )<br>    <span class="hljs-keyword">if</span> no_rope:<br>        rotary_pos_emb = <span class="hljs-literal">None</span><br><br>    inference_context = deprecate_inference_params(inference_context, inference_params)<br><br>    <span class="hljs-keyword">if</span> inference_context <span class="hljs-keyword">and</span> inference_context.is_dynamic_batching():<br>        <span class="hljs-keyword">assert</span> HAVE_FA3 <span class="hljs-keyword">or</span> is_fa_min_version(<br>            <span class="hljs-string">&quot;2.7.3&quot;</span><br>        ), <span class="hljs-string">&quot;flash attn verion v2.7.3 and above is required for dynamic batching.&quot;</span><br><br>    <span class="hljs-comment"># hidden_states: [sq, b, h]</span><br>    <span class="hljs-keyword">if</span> self.config.flash_decode <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.training <span class="hljs-keyword">and</span> inference_context <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        rotary_pos_emb = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">assert</span> rotary_pos_cos <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> rotary_pos_sin <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># For self attention we just duplicate the rotary_pos_emb if it isn&#x27;t already</span><br>    <span class="hljs-keyword">if</span> rotary_pos_emb <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(rotary_pos_emb, <span class="hljs-built_in">tuple</span>):<br>        rotary_pos_emb = (rotary_pos_emb,) * <span class="hljs-number">2</span><br><br>    <span class="hljs-comment"># =====================</span><br>    <span class="hljs-comment"># Query, Key, and Value</span><br>    <span class="hljs-comment"># =====================</span><br>    <span class="hljs-comment"># Get the query, key and value tensors based on the type of attention -</span><br>    <span class="hljs-comment"># self or cross attn.</span><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;qkv&quot;</span>)<br>    query, key, value = self.get_query_key_value_tensors(hidden_states, key_value_states)<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;qkv&quot;</span>)<br><br>    <span class="hljs-comment"># ===================================================</span><br>    <span class="hljs-comment"># Adjust key, value, and rotary_pos_emb for inference</span><br>    <span class="hljs-comment"># ===================================================</span><br><br>    in_decode_mode = (<br>        inference_context <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">and</span> inference_context.is_decode_only()<br>        <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.training<br>    )<br><br>    <span class="hljs-comment"># This branch only runs in the decode phase of flash decoding and returns after the linear</span><br>    <span class="hljs-comment"># projection. This conditional is not used in the prefill phase or non-flash-decoding cases.</span><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;adjust_key_value&quot;</span>)<br>    <span class="hljs-keyword">if</span> in_decode_mode <span class="hljs-keyword">and</span> self.config.flash_decode:<br>        <span class="hljs-keyword">assert</span> self.layer_number <span class="hljs-keyword">in</span> inference_context.key_value_memory_dict<br>        <span class="hljs-keyword">assert</span> inference_context.sequence_len_offset <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        inference_key_memory, inference_value_memory = inference_context.key_value_memory_dict[<br>            self.layer_number<br>        ]<br>        output = self.flash_decode(<br>            sequence_len_offset=sequence_len_offset,<br>            query_layer=query,<br>            key_layer=key,<br>            value_layer=value,<br>            inference_key_memory=inference_key_memory,<br>            inference_value_memory=inference_value_memory,<br>            rotary_cos=rotary_pos_cos,<br>            rotary_sin=rotary_pos_sin,<br>            rotary_interleaved=self.config.rotary_interleaved,<br>        )<br>        out = output.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>).contiguous()<br>        context_layer = out.view(out.size(<span class="hljs-number">0</span>), out.size(<span class="hljs-number">1</span>), -<span class="hljs-number">1</span>)<br>        output, bias = self.linear_proj(context_layer)<br>        <span class="hljs-keyword">return</span> output, bias<br><br>    <span class="hljs-keyword">if</span> (<br>        in_decode_mode<br>        <span class="hljs-keyword">and</span> self.config.enable_cuda_graph<br>        <span class="hljs-keyword">and</span> self.config.cuda_graph_scope != <span class="hljs-string">&quot;full_iteration&quot;</span><br>        <span class="hljs-keyword">and</span> inference_context.is_static_batching()<br>    ):<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;CUDA graphs must use flash decode with static batching!&quot;</span>)<br><br>    query, key, value, rotary_pos_emb, attn_mask_type, block_table = (<br>        self._adjust_key_value_for_inference(<br>            inference_context,<br>            query,<br>            key,<br>            value,<br>            rotary_pos_emb,<br>            rotary_pos_cos,<br>            rotary_pos_sin,<br>            sequence_len_offset,<br>        )<br>    )<br><br>    <span class="hljs-keyword">if</span> packed_seq_params <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        query = query.squeeze(<span class="hljs-number">1</span>)<br>        key = key.squeeze(<span class="hljs-number">1</span>)<br>        value = value.squeeze(<span class="hljs-number">1</span>)<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;adjust_key_value&quot;</span>)<br><br>    <span class="hljs-comment"># ================================================</span><br>    <span class="hljs-comment"># relative positional embedding (rotary embedding)</span><br>    <span class="hljs-comment"># ================================================</span><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;rotary_pos_emb&quot;</span>)<br>    <span class="hljs-keyword">if</span> rotary_pos_emb <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.config.flash_decode:<br>        q_pos_emb, k_pos_emb = rotary_pos_emb<br><br>        <span class="hljs-keyword">if</span> packed_seq_params <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> packed_seq_params.cu_seqlens_q_padded <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                cu_seqlens_q = packed_seq_params.cu_seqlens_q_padded<br>            <span class="hljs-keyword">else</span>:<br>                cu_seqlens_q = packed_seq_params.cu_seqlens_q<br>            <span class="hljs-keyword">if</span> packed_seq_params.cu_seqlens_kv_padded <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                cu_seqlens_kv = packed_seq_params.cu_seqlens_kv_padded<br>            <span class="hljs-keyword">else</span>:<br>                cu_seqlens_kv = packed_seq_params.cu_seqlens_kv<br>        <span class="hljs-keyword">else</span>:<br>            cu_seqlens_q = cu_seqlens_kv = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> q_pos_emb <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># TODO VIJAY: simplify</span><br>            <span class="hljs-keyword">if</span> inference_context <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> inference_context.is_static_batching():<br>                query = apply_rotary_pos_emb(<br>                    query,<br>                    q_pos_emb,<br>                    config=self.config,<br>                    cu_seqlens=cu_seqlens_q,<br>                    cp_group=self.model_comm_pgs.cp,<br>                )<br>            <span class="hljs-keyword">else</span>:<br>                query = inference_context.apply_rotary_emb_query(<br>                    query, q_pos_emb, self.config, cu_seqlens_q, self.model_comm_pgs.cp<br>                )<br>        <span class="hljs-keyword">if</span> k_pos_emb <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            key = apply_rotary_pos_emb(<br>                key,<br>                k_pos_emb,<br>                config=self.config,<br>                cu_seqlens=cu_seqlens_kv,<br>                cp_group=self.model_comm_pgs.cp,<br>            )<br><br>        <span class="hljs-comment"># TODO, can apply positional embedding to value_layer so it has</span><br>        <span class="hljs-comment"># absolute positional embedding.</span><br>        <span class="hljs-comment"># otherwise, only relative positional embedding takes effect</span><br>        <span class="hljs-comment"># value_layer = apply_rotary_pos_emb(value_layer, k_pos_emb)</span><br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;rotary_pos_emb&quot;</span>)<br><br>    <span class="hljs-comment"># ==================================</span><br>    <span class="hljs-comment"># core attention computation</span><br>    <span class="hljs-comment"># ==================================</span><br><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;core_attention&quot;</span>)<br>    <span class="hljs-keyword">if</span> self.checkpoint_core_attention <span class="hljs-keyword">and</span> self.training:<br>        core_attn_out = self._checkpointed_attention_forward(<br>            query,<br>            key,<br>            value,<br>            attention_mask,<br>            attn_mask_type=attn_mask_type,<br>            attention_bias=attention_bias,<br>            packed_seq_params=packed_seq_params,<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">if</span> inference_context <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> inference_context.is_static_batching():<br>            <span class="hljs-comment"># Static batching attention kernel.</span><br>            core_attn_out = self.core_attention(<br>                query,<br>                key,<br>                value,<br>                attention_mask,<br>                attn_mask_type=attn_mask_type,<br>                attention_bias=attention_bias,<br>                packed_seq_params=packed_seq_params,<br>            )<br><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># Dynamic batching attention kernel.</span><br>            q, k, v = (query, key, value)<br>            cu_query_lengths, max_seqlen_q = inference_context.cu_query_lengths()<br>            cu_kv_lengths, kv_lengths, max_seqlen_k = inference_context.cu_kv_lengths()<br><br>            core_attn_out = self.flash_decode_and_prefill(<br>                q,<br>                k,<br>                v,<br>                max_seqlen_q,<br>                max_seqlen_k,<br>                cu_query_lengths,<br>                cu_kv_lengths,<br>                kv_lengths,<br>                block_table,<br>            )<br>            core_attn_out = rearrange(core_attn_out, <span class="hljs-string">&#x27;s b h d -&gt; s b (h d)&#x27;</span>)<br><br>    <span class="hljs-keyword">if</span> packed_seq_params <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> packed_seq_params.qkv_format == <span class="hljs-string">&#x27;thd&#x27;</span>:<br>        <span class="hljs-comment"># reshape to same output shape as unpacked case</span><br>        <span class="hljs-comment"># (t, np, hn) -&gt; (t, b=1, h=np*hn)</span><br>        <span class="hljs-comment"># t is the pack size = sum (sq_i)</span><br>        <span class="hljs-comment"># note that batch is a dummy dimension in the packed case</span><br>        core_attn_out = core_attn_out.reshape(core_attn_out.size(<span class="hljs-number">0</span>), <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;core_attention&quot;</span>)<br><br>    <span class="hljs-comment"># =================</span><br>    <span class="hljs-comment"># Output. [sq, b, h]</span><br>    <span class="hljs-comment"># =================</span><br><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;linear_proj&quot;</span>)<br>    output, bias = self.linear_proj(core_attn_out)<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;linear_proj&quot;</span>)<br><br>    <span class="hljs-keyword">return</span> output, bias<br><br></code></pre></td></tr></table></figure><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>在构建<code>GPTModel</code>时，在初始化时对<code>embedding</code>层使用的是<code>LanguageModelEmbedding</code>进行初始化，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> self.pre_process <span class="hljs-keyword">or</span> self.mtp_process:<br>    self.embedding = LanguageModelEmbedding(<br>        config=self.config,<br>        vocab_size=self.vocab_size,<br>        max_sequence_length=self.max_sequence_length,<br>        position_embedding_type=position_embedding_type,<br>        scatter_to_sequence_parallel=scatter_embedding_sequence_parallel,<br>        tp_group=self.model_comm_pgs.tp,<br>    )<br></code></pre></td></tr></table></figure><p><code>LanguageModelEmbedding</code>也会涉及到TP并行切分，因为词表可能会难以放入一个GPU内，所以就可以进行TP切分，每个GPU只保留一部分词表 embedding，然后在Forward时每个GPU只去获取在自己范围内的token的内容，最后all reduce得到完整的embedding。</p><h4 id="LanguageModelEmbedding"><a href="#LanguageModelEmbedding" class="headerlink" title="LanguageModelEmbedding"></a><code>LanguageModelEmbedding</code></h4><p><code>LanguageModelEmbedding</code>的代码如下所示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LanguageModelEmbedding</span>(<span class="hljs-title class_ inherited__">MegatronModule</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Language model embeddings.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        config (TransformerConfig): config object with all necessary configs for TransformerBlock</span><br><span class="hljs-string">        vocab_size (int): vocabulary size</span><br><span class="hljs-string">        max_sequence_length (int): maximum size of sequence. This</span><br><span class="hljs-string">                             is used for positional embedding</span><br><span class="hljs-string">        add_position_embedding (bool): Add a position embedding.</span><br><span class="hljs-string">        embedding_dropout_prob (float): dropout probability for embeddings</span><br><span class="hljs-string">        num_tokentypes (int): Set to 0 without binary head, and 2 with a binary head. Defaults to 0.</span><br><span class="hljs-string">        scatter_to_sequence_parallel (bool): Set to False to disable scatter of embedding</span><br><span class="hljs-string">            across sequence parallel region. Defaults to True.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        config: TransformerConfig,</span><br><span class="hljs-params">        vocab_size: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        max_sequence_length: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        position_embedding_type: <span class="hljs-type">Literal</span>[<span class="hljs-string">&#x27;learned_absolute&#x27;</span>, <span class="hljs-string">&#x27;rope&#x27;</span>, <span class="hljs-string">&#x27;none&#x27;</span>] = <span class="hljs-string">&#x27;learned_absolute&#x27;</span>,</span><br><span class="hljs-params">        num_tokentypes: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span>,</span><br><span class="hljs-params">        scatter_to_sequence_parallel: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        tp_group: <span class="hljs-type">Optional</span>[torch.distributed.ProcessGroup] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__(config=config)<br><br>        self.config: TransformerConfig = config<br>        self.vocab_size: <span class="hljs-built_in">int</span> = vocab_size<br>        self.max_sequence_length: <span class="hljs-built_in">int</span> = max_sequence_length<br>        self.add_position_embedding: <span class="hljs-built_in">bool</span> = position_embedding_type == <span class="hljs-string">&#x27;learned_absolute&#x27;</span><br>        self.num_tokentypes = num_tokentypes<br>        self.scatter_to_sequence_parallel = scatter_to_sequence_parallel<br>        self.tp_group = get_tensor_model_parallel_group_if_none(tp_group)<br>        self.reduce_scatter_embeddings = (<br>            (<span class="hljs-keyword">not</span> self.add_position_embedding)<br>            <span class="hljs-keyword">and</span> self.num_tokentypes &lt;= <span class="hljs-number">0</span><br>            <span class="hljs-keyword">and</span> self.config.sequence_parallel<br>            <span class="hljs-keyword">and</span> self.scatter_to_sequence_parallel<br>        )<br><br>        <span class="hljs-comment"># Word embeddings (parallel).</span><br>        self.word_embeddings = tensor_parallel.VocabParallelEmbedding(<br>            num_embeddings=self.vocab_size,<br>            embedding_dim=self.config.hidden_size,<br>            init_method=self.config.embedding_init_method,<br>            reduce_scatter_embeddings=self.reduce_scatter_embeddings,<br>            config=self.config,<br>            tp_group=self.tp_group,<br>        )<br><br>        <span class="hljs-comment"># Position embedding (serial).</span><br>        <span class="hljs-keyword">if</span> self.add_position_embedding:<br>            self.position_embeddings = torch.nn.Embedding(<br>                self.max_sequence_length, self.config.hidden_size<br>            )<br><br>            <span class="hljs-comment"># Initialize the position embeddings.</span><br>            <span class="hljs-keyword">if</span> self.config.perform_initialization:<br>                self.config.embedding_init_method(self.position_embeddings.weight)<br><br>        <span class="hljs-keyword">if</span> self.num_tokentypes &gt; <span class="hljs-number">0</span>:<br>            self.tokentype_embeddings = torch.nn.Embedding(<br>                self.num_tokentypes, self.config.hidden_size<br>            )<br>            <span class="hljs-comment"># Initialize the token-type embeddings.</span><br>            <span class="hljs-keyword">if</span> self.config.perform_initialization:<br>                self.config.embedding_init_method(self.tokentype_embeddings.weight)<br>        <span class="hljs-keyword">else</span>:<br>            self.tokentype_embeddings = <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># Embeddings dropout</span><br>        self.embedding_dropout = torch.nn.Dropout(self.config.hidden_dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">zero_parameters</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Zero out all parameters in embedding.&quot;&quot;&quot;</span><br>        self.word_embeddings.weight.data.fill_(<span class="hljs-number">0</span>)<br>        self.word_embeddings.weight.shared = <span class="hljs-literal">True</span><br>        self.position_embeddings.weight.data.fill_(<span class="hljs-number">0</span>)<br>        self.position_embeddings.weight.shared = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">if</span> self.num_tokentypes &gt; <span class="hljs-number">0</span>:<br>            self.tokentype_embeddings.weight.data.fill_(<span class="hljs-number">0</span>)<br>            self.tokentype_embeddings.weight.shared = <span class="hljs-literal">True</span><br><br><span class="hljs-meta">    @nvtx_decorator()</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input_ids: Tensor, position_ids: Tensor, tokentype_ids: <span class="hljs-built_in">int</span> = <span class="hljs-literal">None</span></span>) -&gt; Tensor:<br>        <span class="hljs-string">&quot;&quot;&quot;Forward pass of the embedding module.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            input_ids (Tensor): The input tokens</span><br><span class="hljs-string">            position_ids (Tensor): The position id&#x27;s used to calculate position embeddings</span><br><span class="hljs-string">            tokentype_ids (int): The token type ids. Used when args.bert_binary_head is</span><br><span class="hljs-string">                set to True. Defaults to None</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Tensor: The output embeddings</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        word_embeddings = self.word_embeddings(input_ids)<br>        <span class="hljs-keyword">if</span> self.add_position_embedding:<br>            position_embeddings = self.position_embeddings(position_ids)<br>            embeddings = word_embeddings + position_embeddings<br>        <span class="hljs-keyword">else</span>:<br>            embeddings = word_embeddings<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.reduce_scatter_embeddings:<br>            <span class="hljs-comment"># Data format change to avoid explicit tranposes : [b s h] --&gt; [s b h].</span><br>            embeddings = embeddings.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>).contiguous()<br><br>        <span class="hljs-keyword">if</span> tokentype_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">assert</span> self.tokentype_embeddings <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>            <span class="hljs-comment"># [b s h] -&gt; [s b h] (So that it can be added with embeddings)</span><br>            tokentype_embedding = self.tokentype_embeddings(tokentype_ids).permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br>            embeddings = embeddings + tokentype_embedding<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">assert</span> self.tokentype_embeddings <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># If the input flag for fp32 residual connection is set, convert for float.</span><br>        <span class="hljs-keyword">if</span> self.config.fp32_residual_connection:<br>            embeddings = embeddings.<span class="hljs-built_in">float</span>()<br><br>        <span class="hljs-comment"># Dropout.</span><br>        <span class="hljs-keyword">if</span> self.config.sequence_parallel:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.reduce_scatter_embeddings <span class="hljs-keyword">and</span> self.scatter_to_sequence_parallel:<br>                embeddings = tensor_parallel.scatter_to_sequence_parallel_region(<br>                    embeddings, group=self.tp_group<br>                )<br>            <span class="hljs-comment"># `scatter_to_sequence_parallel_region` returns a view, which prevents</span><br>            <span class="hljs-comment"># the original tensor from being garbage collected. Clone to facilitate GC.</span><br>            <span class="hljs-comment"># Has a small runtime cost (~0.5%).</span><br>            <span class="hljs-keyword">if</span> self.config.clone_scatter_output_in_embedding <span class="hljs-keyword">and</span> self.scatter_to_sequence_parallel:<br>                embeddings = embeddings.clone()<br>            <span class="hljs-keyword">with</span> tensor_parallel.get_cuda_rng_tracker().fork():<br>                embeddings = self.embedding_dropout(embeddings)<br>        <span class="hljs-keyword">else</span>:<br>            embeddings = self.embedding_dropout(embeddings)<br><br>        <span class="hljs-keyword">return</span> embeddings<br><br></code></pre></td></tr></table></figure><ul><li><p>在初始化时，其使用<code>tensor_parallel.VocabParallelEmbedding</code>进行初始化，</p><ul><li><code>VocabParallelEmbedding</code>的代码如下所示</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">VocabParallelEmbedding</span>(torch.nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Embedding parallelized in the vocabulary dimension.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    This is mainly adapted from torch.nn.Embedding and all the default</span><br><span class="hljs-string">    values are kept.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        num_embeddings: vocabulary size.</span><br><span class="hljs-string">        embedding_dim: size of hidden state.</span><br><span class="hljs-string">        reduce_scatter_embeddings: Decides whether to perform ReduceScatter after embedding lookup</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Keyword Args:</span><br><span class="hljs-string">        config: A megatron.core.ModelParallelConfig object</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        num_embeddings: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        embedding_dim: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        *,</span><br><span class="hljs-params">        init_method: <span class="hljs-type">Callable</span>,</span><br><span class="hljs-params">        reduce_scatter_embeddings: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        config: ModelParallelConfig,</span><br><span class="hljs-params">        tp_group: <span class="hljs-type">Optional</span>[torch.distributed.ProcessGroup] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>(VocabParallelEmbedding, self).__init__()<br>        <span class="hljs-comment"># Keep the input dimensions.</span><br>        self.num_embeddings = num_embeddings<br>        self.embedding_dim = embedding_dim<br>        self.reduce_scatter_embeddings = reduce_scatter_embeddings<br>        self.tp_group = tp_group<br><br>        self.tp_group = get_tensor_model_parallel_group_if_none(self.tp_group)<br><br>        (self.vocab_start_index, self.vocab_end_index) = (<br>            VocabUtility.vocab_range_from_global_vocab_size(<br>                self.num_embeddings, get_pg_rank(self.tp_group), get_pg_size(self.tp_group)<br>            )<br>        )<br>        self.num_embeddings_per_partition = self.vocab_end_index - self.vocab_start_index<br>        self.deterministic_mode = config.deterministic_mode<br><br>        <span class="hljs-comment"># Allocate weights and initialize.</span><br>        <span class="hljs-keyword">if</span> config.use_cpu_initialization:<br>            self.weight = Parameter(<br>                torch.empty(<br>                    self.num_embeddings_per_partition, self.embedding_dim, dtype=config.params_dtype<br>                )<br>            )<br>            <span class="hljs-keyword">if</span> config.perform_initialization:<br>                _initialize_affine_weight_cpu(<br>                    self.weight,<br>                    self.num_embeddings,<br>                    self.embedding_dim,<br>                    self.num_embeddings_per_partition,<br>                    <span class="hljs-number">0</span>,<br>                    init_method,<br>                    params_dtype=config.params_dtype,<br>                    rank=get_pg_rank(self.tp_group),<br>                    world_size=get_pg_size(self.tp_group),<br>                )<br>        <span class="hljs-keyword">else</span>:<br>            self.weight = Parameter(<br>                torch.empty(<br>                    self.num_embeddings_per_partition,<br>                    self.embedding_dim,<br>                    device=torch.cuda.current_device(),<br>                    dtype=config.params_dtype,<br>                )<br>            )<br>            <span class="hljs-keyword">if</span> config.perform_initialization:<br>                _initialize_affine_weight_gpu(self.weight, init_method, partition_dim=<span class="hljs-number">0</span>, stride=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input_</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Forward.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            input_ (torch.Tensor): Input tensor.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> self.tp_group.size() &gt; <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># Build the mask.</span><br>            input_mask = (input_ &lt; self.vocab_start_index) | (input_ &gt;= self.vocab_end_index)<br>            <span class="hljs-comment"># Mask the input.</span><br>            masked_input = input_.clone() - self.vocab_start_index<br>            masked_input[input_mask] = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">else</span>:<br>            masked_input = input_<br>        <span class="hljs-comment"># Get the embeddings.</span><br>        <span class="hljs-keyword">if</span> self.deterministic_mode:<br>            output_parallel = self.weight[masked_input]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># F.embedding currently has a non-deterministic backward function</span><br>            output_parallel = F.embedding(masked_input, self.weight)<br>        <span class="hljs-comment"># Mask the output embedding.</span><br>        <span class="hljs-keyword">if</span> self.tp_group.size() &gt; <span class="hljs-number">1</span>:<br>            output_parallel[input_mask, :] = <span class="hljs-number">0.0</span><br><br>        <span class="hljs-keyword">if</span> self.reduce_scatter_embeddings:<br>            <span class="hljs-comment"># Data format change to avoid explicit tranposes : [b s h] --&gt; [s b h].</span><br>            output_parallel = output_parallel.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>).contiguous()<br>            output = reduce_scatter_to_sequence_parallel_region(<br>                output_parallel, group=self.tp_group<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># Reduce across all the model parallel GPUs.</span><br>            output = reduce_from_tensor_model_parallel_region(output_parallel, group=self.tp_group)<br>        <span class="hljs-keyword">return</span> output<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sharded_state_dict</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        prefix: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;&quot;</span>,</span><br><span class="hljs-params">        sharded_offsets: <span class="hljs-type">Tuple</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]] = (<span class="hljs-params"></span>),</span><br><span class="hljs-params">        metadata: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">dict</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>) -&gt; ShardedStateDict:<br>        <span class="hljs-string">&quot;&quot;&quot;Non-default implementation for embeddings due to `allow_shape_mismatch` param&quot;&quot;&quot;</span><br>        state_dict = self.state_dict(prefix=<span class="hljs-string">&quot;&quot;</span>, keep_vars=<span class="hljs-literal">True</span>)<br><br>        weight_prefix = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;prefix&#125;</span>weight&quot;</span><br>        <span class="hljs-keyword">return</span> &#123;<br>            weight_prefix: make_tp_sharded_tensor_for_checkpoint(<br>                tensor=state_dict[<span class="hljs-string">&quot;weight&quot;</span>],<br>                key=weight_prefix,<br>                allow_shape_mismatch=<span class="hljs-literal">True</span>,<br>                prepend_offsets=sharded_offsets,<br>            )<br>        &#125;<br><br></code></pre></td></tr></table></figure><ul><li><p><code>VocabParallelEmbedding</code>在初始化时首先根据TP对embedding进行分组，得到起始位置<code>self.vocab_start_index</code>,与结束<code>self.vocab_end_index</code></p></li><li><p><code>VocabParallelEmbedding</code>在Forward时：</p><ol><li><p>首先得到<code>input_mask = (input_ &lt; self.vocab_start_index) | (input_ &gt;= self.vocab_end_index)</code>，然后再得到<code>masked_input = input_.clone() - self.vocab_start_index</code>，再将不在这个范围内的置零<code>masked_input[input_mask] = 0</code></p></li><li><p><code>masked_input</code>记录了token更新后的id，然后再在<code>weight</code>中依据<code>masked_input</code>id去取对应的内容，得到<code>output_parallel</code>，并将不属于本rank的清零</p></li><li><p>然后这里依据<code>reduce_scatter_embeddings</code>有两种输出策略进行选择，注意</p><p><code>reduce_scatter_embeddings = ((not self.add_position_embedding) and self.num_tokentypes &lt;= 0 and self.config.sequence_parallel  and self.scatter_to_sequence_parallel)</code>：</p><ul><li><p><code>reduce_scatter_embeddings=True</code>（配合 sequence parallel）</p><ul><li><p>先把布局从 <code>[b, s, h]</code> 转成 <code>[s, b, h]</code>，因为 Megatron 的 sequence-parallel 通常以 <code>[seq, batch, hidden]</code> 为主（这样更容易沿 seq 维切分&#x2F;拼接）。</p></li><li><p>然后调用 <code>reduce_scatter_to_sequence_parallel_region</code>：</p><ul><li><p>语义上等价于：先对 output_parallel 在 TP 组上做 sum-reduce，再按 sequence 维把结果 scatter 给各 rank。</p></li><li><p>好处：直接产出 sequence-parallel 需要的分片输出，避免 “all-reduce 得到全量，再手动切分” 的额外开销和内存峰值。</p></li></ul></li></ul></li><li><p><code>reduce_scatter_embeddings=False</code>（默认更直观）</p><ul><li><p>用 <code>reduce_from_tensor_model_parallel_region</code>：</p><ul><li><p>语义就是对 output_parallel 在 TP 组上 all-reduce(sum)；</p></li><li><p>每个 TP rank 都拿到完整的 embedding 输出（与未切分词表时一致）。</p></li></ul></li></ul></li></ul></li></ol></li></ul></li></ul><h1 id="Tensor并行实验"><a href="#Tensor并行实验" class="headerlink" title="Tensor并行实验"></a>Tensor并行实验</h1><p>实验依据采用的是GPT3 857m的模型，运行脚本如下所示，值得注意的是在<code>GPT_MODEL_ARGS</code>参数中设置为了<code>local</code>，即不使用<code>transformer_engine</code>而是使用Megatron-LM本地实现的gpt_layer，与上述介绍对应，此外也设置TP切分维度为4</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><br><span class="hljs-comment"># Runs the &quot;857m&quot; parameter model</span><br><br><span class="hljs-built_in">export</span> CUDA_DEVICE_MAX_CONNECTIONS=1<br><br>GPUS_PER_NODE=4<br><span class="hljs-comment"># Change for multinode config</span><br>MASTER_ADDR=localhost<br>MASTER_PORT=6000<br>NUM_NODES=1<br>NODE_RANK=0<br>WORLD_SIZE=$((<span class="hljs-variable">$GPUS_PER_NODE</span>*<span class="hljs-variable">$NUM_NODES</span>))<br><br>CHECKPOINT_PATH=<span class="hljs-variable">$1</span> <span class="hljs-comment">#&lt;Specify path&gt;</span><br>TENSORBOARD_LOGS_PATH=<span class="hljs-variable">$2</span> <span class="hljs-comment">#&lt;Specify path&gt;</span><br>VOCAB_FILE=<span class="hljs-variable">$3</span> <span class="hljs-comment">#&lt;Specify path to file&gt;/gpt2-vocab.json</span><br>MERGE_FILE=<span class="hljs-variable">$4</span> <span class="hljs-comment">#&lt;Specify path to file&gt;/gpt2-merges.txt</span><br>DATA_PATH=<span class="hljs-variable">$5</span> <span class="hljs-comment">#&lt;Specify path and file prefix&gt;_text_document</span><br>USE_NSYS=0<br><span class="hljs-keyword">if</span> [[ <span class="hljs-variable">$&#123;6:-&#125;</span> == <span class="hljs-string">&quot;--nsys&quot;</span> ]]; <span class="hljs-keyword">then</span><br>  USE_NSYS=1<br><span class="hljs-keyword">fi</span><br><br>DISTRIBUTED_ARGS=(<br>    --nproc_per_node <span class="hljs-variable">$GPUS_PER_NODE</span> <br>    --nnodes <span class="hljs-variable">$NUM_NODES</span> <br>    --master_addr <span class="hljs-variable">$MASTER_ADDR</span> <br>    --master_port <span class="hljs-variable">$MASTER_PORT</span><br>)<br><br>GPT_MODEL_ARGS=(<br>    --num-layers 24 <br>    --hidden-size 1024 <br>    --num-attention-heads 16 <br>    --seq-length 2048 <br>    --max-position-embeddings 2048 <br>    --transformer-impl <span class="hljs-built_in">local</span><br>)<br><br>TRAINING_ARGS=(<br>    --micro-batch-size 4 <br>    --global-batch-size 16 <br>    <span class="hljs-comment"># --rampup-batch-size 16 16 5859375 </span><br>    --train-iters 20000 <br>    --weight-decay 0.1 <br>    --adam-beta1 0.9 <br>    --adam-beta2 0.95 <br>    --init-method-std 0.006 <br>    --clip-grad 1.0 <br>    --fp16<br>    --lr 6.0e-5 <br>    --lr-decay-style cosine <br>    --min-lr 6.0e-6<br>    --lr-warmup-fraction .001 <br>    --lr-decay-iters 20000 <br>)<br><br>MODEL_PARALLEL_ARGS=(<br>  --tensor-model-parallel-size 4 <br>  --pipeline-model-parallel-size 1 <br>)<br><br>DATA_ARGS=(<br>    --data-path <span class="hljs-variable">$DATA_PATH</span> <br>    --vocab-file <span class="hljs-variable">$VOCAB_FILE</span> <br>    --merge-file <span class="hljs-variable">$MERGE_FILE</span> <br>    --<span class="hljs-built_in">split</span> 949,50,1<br>)<br><br>EVAL_AND_LOGGING_ARGS=(<br>    --log-interval 200<br>    --save-interval 10000 <br>    --eval-interval 1000 <br>    --save <span class="hljs-variable">$CHECKPOINT_PATH</span> <br>    --load <span class="hljs-variable">$CHECKPOINT_PATH</span> <br>    --eval-iters 10<br>    --tensorboard-dir <span class="hljs-variable">$TENSORBOARD_LOGS_PATH</span> <br>)<br><br>PROFILER_ARGS=(<br>    --profile<br>    --use-pytorch-profiler<br>    --profile-step-start 110<br>    --profile-step-end 112<br>    --profile-ranks 0<br>)<br><br><span class="hljs-comment"># Build command as an array (no string concatenation)</span><br>CMD=(<br>  torchrun<br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;DISTRIBUTED_ARGS[@]&#125;</span>&quot;</span><br>  pretrain_gpt.py<br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;GPT_MODEL_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;TRAINING_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;MODEL_PARALLEL_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;DATA_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;EVAL_AND_LOGGING_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;PROFILER_ARGS[@]&#125;</span>&quot;</span><br>)<br><br><span class="hljs-keyword">if</span> [[ <span class="hljs-string">&quot;<span class="hljs-variable">$USE_NSYS</span>&quot;</span> -eq 1 ]]; <span class="hljs-keyword">then</span><br>  NSIGHT_PREFIX=<span class="hljs-string">&quot;./nsight_profile/gpt3_857m&quot;</span><br>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Running with Nsight profiling, output prefix: <span class="hljs-variable">$&#123;NSIGHT_PREFIX&#125;</span>&quot;</span><br>  <span class="hljs-built_in">exec</span> nsys profile \<br>    -s none -t nvtx,cuda \<br>    --cudabacktrace=all \<br>    --cuda-graph-trace=node \<br>    --python-backtrace=cuda \<br>    --<span class="hljs-built_in">wait</span> all \<br>    -o <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;NSIGHT_PREFIX&#125;</span>&quot;</span> \<br>    --force-overwrite <span class="hljs-literal">true</span> \<br>    --capture-range=cudaProfilerApi \<br>    --capture-range-end=stop \<br>    <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;CMD[@]&#125;</span>&quot;</span><br><span class="hljs-keyword">else</span><br>  <span class="hljs-built_in">exec</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;CMD[@]&#125;</span>&quot;</span><br><span class="hljs-keyword">fi</span><br></code></pre></td></tr></table></figure><p>运行的命令为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash examples/gpt3/train_gpt3_857m_distributed.sh     /workspace/megatron-lm/model_ckpt/gpt3_857m_tp4     /workspace/megatron-lm/tb_logs/gpt3_857m_profiler_tp4     /workspace/megatron-lm/data/tokenizer/gpt2-vocab.json     /workspace/megatron-lm/data/tokenizer/gpt2-merges.txt     /workspace/megatron-lm/data/TinyStoriesV2-GPT4-train_text_document      &gt; gpt3_857m_tp4.log 2&gt;&amp;1 &amp;<br></code></pre></td></tr></table></figure><p>运行日志如下所示：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">W0103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:27.318000</span> <span class="hljs-number">1772056</span> <span class="hljs-string">torch/distributed/run.py:766]</span> <br><span class="hljs-string">W0103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:27.318000</span> <span class="hljs-number">1772056</span> <span class="hljs-string">torch/distributed/run.py:766]</span> <span class="hljs-string">*****************************************</span><br><span class="hljs-string">W0103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:27.318000</span> <span class="hljs-number">1772056</span> <span class="hljs-string">torch/distributed/run.py:766]</span> <span class="hljs-string">Setting</span> <span class="hljs-string">OMP_NUM_THREADS</span> <span class="hljs-string">environment</span> <span class="hljs-string">variable</span> <span class="hljs-string">for</span> <span class="hljs-string">each</span> <span class="hljs-string">process</span> <span class="hljs-string">to</span> <span class="hljs-string">be</span> <span class="hljs-number">1</span> <span class="hljs-string">in</span> <span class="hljs-string">default,</span> <span class="hljs-string">to</span> <span class="hljs-string">avoid</span> <span class="hljs-string">your</span> <span class="hljs-string">system</span> <span class="hljs-string">being</span> <span class="hljs-string">overloaded,</span> <span class="hljs-string">please</span> <span class="hljs-string">further</span> <span class="hljs-string">tune</span> <span class="hljs-string">the</span> <span class="hljs-string">variable</span> <span class="hljs-string">for</span> <span class="hljs-string">optimal</span> <span class="hljs-string">performance</span> <span class="hljs-string">in</span> <span class="hljs-string">your</span> <span class="hljs-string">application</span> <span class="hljs-string">as</span> <span class="hljs-string">needed.</span> <br><span class="hljs-string">W0103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:27.318000</span> <span class="hljs-number">1772056</span> <span class="hljs-string">torch/distributed/run.py:766]</span> <span class="hljs-string">*****************************************</span><br><span class="hljs-attr">using world size:</span> <span class="hljs-number">4</span><span class="hljs-string">,</span> <span class="hljs-attr">data-parallel size:</span> <span class="hljs-number">1</span><span class="hljs-string">,</span> <span class="hljs-attr">context-parallel size:</span> <span class="hljs-number">1</span><span class="hljs-string">,</span> <span class="hljs-attr">hierarchical context-parallel sizes:</span> <span class="hljs-string">None,</span> <span class="hljs-attr">tensor-model-parallel size:</span> <span class="hljs-number">4</span><span class="hljs-string">,</span> <span class="hljs-attr">pipeline-model-parallel size:</span> <span class="hljs-number">1</span><br><span class="hljs-attr">Number of virtual stages per pipeline stage:</span> <span class="hljs-string">None</span><br><span class="hljs-attr">WARNING:</span> <span class="hljs-string">Setting</span> <span class="hljs-string">args.check_for_nan_in_loss_and_grad</span> <span class="hljs-string">to</span> <span class="hljs-literal">False</span> <span class="hljs-string">since</span> <span class="hljs-string">dynamic</span> <span class="hljs-string">loss</span> <span class="hljs-string">scaling</span> <span class="hljs-string">is</span> <span class="hljs-string">being</span> <span class="hljs-string">used</span><br><span class="hljs-string">using</span> <span class="hljs-string">torch.float16</span> <span class="hljs-string">for</span> <span class="hljs-string">parameters</span> <span class="hljs-string">...</span><br><span class="hljs-string">------------------------</span> <span class="hljs-string">arguments</span> <span class="hljs-string">------------------------</span><br>  <span class="hljs-string">account_for_embedding_in_pipeline_split</span> <span class="hljs-string">.........</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">account_for_loss_in_pipeline_split</span> <span class="hljs-string">..............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">accumulate_allreduce_grads_in_fp32</span> <span class="hljs-string">..............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">adam_beta1</span> <span class="hljs-string">......................................</span> <span class="hljs-number">0.9</span><br>  <span class="hljs-string">adam_beta2</span> <span class="hljs-string">......................................</span> <span class="hljs-number">0.95</span><br>  <span class="hljs-string">adam_eps</span> <span class="hljs-string">........................................</span> <span class="hljs-number">1e-08</span><br>  <span class="hljs-string">add_bias_linear</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">add_position_embedding</span> <span class="hljs-string">..........................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">add_qkv_bias</span> <span class="hljs-string">....................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">adlr_autoresume</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">adlr_autoresume_interval</span> <span class="hljs-string">........................</span> <span class="hljs-number">1000</span><br>  <span class="hljs-string">align_grad_reduce</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">align_param_gather</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">app_tag_run_name</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">app_tag_run_version</span> <span class="hljs-string">.............................</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><br>  <span class="hljs-string">apply_layernorm_1p</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">apply_query_key_layer_scaling</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">apply_residual_connection_post_layernorm</span> <span class="hljs-string">........</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">apply_rope_fusion</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">async_save</span> <span class="hljs-string">......................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">async_tensor_model_parallel_allreduce</span> <span class="hljs-string">...........</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">attention_backend</span> <span class="hljs-string">...............................</span> <span class="hljs-string">AttnBackend.auto</span><br>  <span class="hljs-string">attention_dropout</span> <span class="hljs-string">...............................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">attention_softmax_in_fp32</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">auto_detect_ckpt_format</span> <span class="hljs-string">.........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">barrier_with_L1_time</span> <span class="hljs-string">............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">bert_binary_head</span> <span class="hljs-string">................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">bert_embedder_type</span> <span class="hljs-string">..............................</span> <span class="hljs-string">megatron</span><br>  <span class="hljs-string">bert_load</span> <span class="hljs-string">.......................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">bf16</span> <span class="hljs-string">............................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">bias_dropout_fusion</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">bias_gelu_fusion</span> <span class="hljs-string">................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">bias_swiglu_fusion</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">biencoder_projection_dim</span> <span class="hljs-string">........................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">biencoder_shared_query_context_model</span> <span class="hljs-string">............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">block_data_path</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">cache_mla_latents</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">calc_ft_timeouts</span> <span class="hljs-string">................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">calculate_per_token_loss</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">check_for_large_grads</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">check_for_nan_in_loss_and_grad</span> <span class="hljs-string">..................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">check_for_spiky_loss</span> <span class="hljs-string">............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">check_weight_hash_across_dp_replicas_interval</span> <span class="hljs-string">...</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">ckpt_assume_constant_structure</span> <span class="hljs-string">..................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">ckpt_convert_format</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">ckpt_convert_save</span> <span class="hljs-string">...............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">ckpt_convert_update_legacy_dist_opt_format</span> <span class="hljs-string">......</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">ckpt_format</span> <span class="hljs-string">.....................................</span> <span class="hljs-string">torch_dist</span><br>  <span class="hljs-string">ckpt_fully_parallel_load</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">ckpt_fully_parallel_save</span> <span class="hljs-string">........................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">ckpt_fully_parallel_save_deprecated</span> <span class="hljs-string">.............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">ckpt_step</span> <span class="hljs-string">.......................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">classes_fraction</span> <span class="hljs-string">................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">clip_grad</span> <span class="hljs-string">.......................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">clone_scatter_output_in_embedding</span> <span class="hljs-string">...............</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">config_logger_dir</span> <span class="hljs-string">...............................</span> <br>  <span class="hljs-string">consumed_train_samples</span> <span class="hljs-string">..........................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">consumed_valid_samples</span> <span class="hljs-string">..........................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">context_parallel_size</span> <span class="hljs-string">...........................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">cp_comm_type</span> <span class="hljs-string">....................................</span> [<span class="hljs-string">&#x27;p2p&#x27;</span>]<br>  <span class="hljs-string">create_attention_mask_in_dataloader</span> <span class="hljs-string">.............</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">cross_entropy_fusion_impl</span> <span class="hljs-string">.......................</span> <span class="hljs-string">native</span><br>  <span class="hljs-string">cross_entropy_loss_fusion</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">cuda_graph_scope</span> <span class="hljs-string">................................</span> <span class="hljs-string">full</span><br>  <span class="hljs-string">cuda_graph_warmup_steps</span> <span class="hljs-string">.........................</span> <span class="hljs-number">3</span><br>  <span class="hljs-string">data_args_path</span> <span class="hljs-string">..................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">data_cache_path</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">data_parallel_random_init</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">data_parallel_sharding_strategy</span> <span class="hljs-string">.................</span> <span class="hljs-string">no_shard</span><br>  <span class="hljs-string">data_parallel_size</span> <span class="hljs-string">..............................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">data_path</span> <span class="hljs-string">.......................................</span> [<span class="hljs-string">&#x27;/workspace/megatron-lm/data/TinyStoriesV2-GPT4-train_text_document&#x27;</span>]<br>  <span class="hljs-string">data_per_class_fraction</span> <span class="hljs-string">.........................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">data_sharding</span> <span class="hljs-string">...................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">dataloader_type</span> <span class="hljs-string">.................................</span> <span class="hljs-string">single</span><br>  <span class="hljs-string">ddp_average_in_collective</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">ddp_bucket_size</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">ddp_num_buckets</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">ddp_pad_buckets_for_high_nccl_busbw</span> <span class="hljs-string">.............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">decoder_first_pipeline_num_layers</span> <span class="hljs-string">...............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">decoder_last_pipeline_num_layers</span> <span class="hljs-string">................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">decoder_num_layers</span> <span class="hljs-string">..............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">decoder_seq_length</span> <span class="hljs-string">..............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">decoupled_lr</span> <span class="hljs-string">....................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">decoupled_min_lr</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">decrease_batch_size_if_needed</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">defer_embedding_wgrad_compute</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">delay_wgrad_compute</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">deprecated_use_mcore_models</span> <span class="hljs-string">.....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">deterministic_mode</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">dino_bottleneck_size</span> <span class="hljs-string">............................</span> <span class="hljs-number">256</span><br>  <span class="hljs-string">dino_freeze_last_layer</span> <span class="hljs-string">..........................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">dino_head_hidden_size</span> <span class="hljs-string">...........................</span> <span class="hljs-number">2048</span><br>  <span class="hljs-string">dino_local_crops_number</span> <span class="hljs-string">.........................</span> <span class="hljs-number">10</span><br>  <span class="hljs-string">dino_local_img_size</span> <span class="hljs-string">.............................</span> <span class="hljs-number">96</span><br>  <span class="hljs-string">dino_norm_last_layer</span> <span class="hljs-string">............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">dino_teacher_temp</span> <span class="hljs-string">...............................</span> <span class="hljs-number">0.07</span><br>  <span class="hljs-string">dino_warmup_teacher_temp</span> <span class="hljs-string">........................</span> <span class="hljs-number">0.04</span><br>  <span class="hljs-string">dino_warmup_teacher_temp_epochs</span> <span class="hljs-string">.................</span> <span class="hljs-number">30</span><br>  <span class="hljs-string">disable_bf16_reduced_precision_matmul</span> <span class="hljs-string">...........</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">disable_mamba_mem_eff_path</span> <span class="hljs-string">......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">disable_straggler_on_startup</span> <span class="hljs-string">....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">dist_ckpt_format_deprecated</span> <span class="hljs-string">.....................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">dist_ckpt_strictness</span> <span class="hljs-string">............................</span> <span class="hljs-string">assume_ok_unexpected</span><br>  <span class="hljs-string">distribute_saved_activations</span> <span class="hljs-string">....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">distributed_backend</span> <span class="hljs-string">.............................</span> <span class="hljs-string">nccl</span><br>  <span class="hljs-string">distributed_timeout_minutes</span> <span class="hljs-string">.....................</span> <span class="hljs-number">10</span><br>  <span class="hljs-string">embedding_init_method_std</span> <span class="hljs-string">.......................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">embedding_path</span> <span class="hljs-string">..................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">empty_unused_memory_level</span> <span class="hljs-string">.......................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">enable_cuda_graph</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">enable_experimental</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">enable_ft_package</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">enable_full_sharding_in_hsdp</span> <span class="hljs-string">....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">enable_gloo_process_groups</span> <span class="hljs-string">......................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">enable_msc</span> <span class="hljs-string">......................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">enable_one_logger</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">encoder_num_layers</span> <span class="hljs-string">..............................</span> <span class="hljs-number">24</span><br>  <span class="hljs-string">encoder_seq_length</span> <span class="hljs-string">..............................</span> <span class="hljs-number">2048</span><br>  <span class="hljs-string">end_weight_decay</span> <span class="hljs-string">................................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">eod_mask_loss</span> <span class="hljs-string">...................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">error_injection_rate</span> <span class="hljs-string">............................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">error_injection_type</span> <span class="hljs-string">............................</span> <span class="hljs-string">transient_error</span><br>  <span class="hljs-string">eval_interval</span> <span class="hljs-string">...................................</span> <span class="hljs-number">1000</span><br>  <span class="hljs-string">eval_iters</span> <span class="hljs-string">......................................</span> <span class="hljs-number">10</span><br>  <span class="hljs-string">evidence_data_path</span> <span class="hljs-string">..............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">exit_duration_in_mins</span> <span class="hljs-string">...........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">exit_interval</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">exit_on_missing_checkpoint</span> <span class="hljs-string">......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">exit_signal_handler</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">exp_avg_dtype</span> <span class="hljs-string">...................................</span> <span class="hljs-string">torch.float32</span><br>  <span class="hljs-string">exp_avg_sq_dtype</span> <span class="hljs-string">................................</span> <span class="hljs-string">torch.float32</span><br>  <span class="hljs-string">expert_model_parallel_size</span> <span class="hljs-string">......................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">expert_tensor_parallel_size</span> <span class="hljs-string">.....................</span> <span class="hljs-number">4</span><br>  <span class="hljs-string">export_force_local_attention</span> <span class="hljs-string">....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">export_kd_cfg</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">export_kd_teacher_ckpt_format</span> <span class="hljs-string">...................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">export_kd_teacher_load</span> <span class="hljs-string">..........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">export_kv_cache_quant</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">export_legacy_megatron</span> <span class="hljs-string">..........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">export_model_type</span> <span class="hljs-string">...............................</span> <span class="hljs-string">GPTModel</span><br>  <span class="hljs-string">export_moe_apply_probs_on_input</span> <span class="hljs-string">.................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">export_qk_l2_norm</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">export_quant_cfg</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">export_real_quant_cfg</span> <span class="hljs-string">...........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">export_te_mcore_model</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">external_cuda_graph</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">ffn_hidden_size</span> <span class="hljs-string">.................................</span> <span class="hljs-number">4096</span><br>  <span class="hljs-string">finetune</span> <span class="hljs-string">........................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">finetune_data_split</span> <span class="hljs-string">.............................</span> <span class="hljs-string">train</span><br>  <span class="hljs-string">finetune_hf_dataset</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">first_last_layers_bf16</span> <span class="hljs-string">..........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">flash_decode</span> <span class="hljs-string">....................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">fp16</span> <span class="hljs-string">............................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">fp16_lm_cross_entropy</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">fp32_residual_connection</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">fp8</span> <span class="hljs-string">.............................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">fp8_amax_compute_algo</span> <span class="hljs-string">...........................</span> <span class="hljs-string">most_recent</span><br>  <span class="hljs-string">fp8_amax_history_len</span> <span class="hljs-string">............................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">fp8_interval</span> <span class="hljs-string">....................................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">fp8_margin</span> <span class="hljs-string">......................................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">fp8_param_gather</span> <span class="hljs-string">................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">fp8_recipe</span> <span class="hljs-string">......................................</span> <span class="hljs-string">delayed</span><br>  <span class="hljs-string">fp8_wgrad</span> <span class="hljs-string">.......................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">fsdp_double_buffer</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">full_validation</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">global_batch_size</span> <span class="hljs-string">...............................</span> <span class="hljs-number">16</span><br>  <span class="hljs-string">grad_reduce_in_bf16</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">gradient_accumulation_fusion</span> <span class="hljs-string">....................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">gradient_reduce_div_fusion</span> <span class="hljs-string">......................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">group_query_attention</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">head_lr_mult</span> <span class="hljs-string">....................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">heterogeneous_layers_config_encoded_json</span> <span class="hljs-string">........</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">heterogeneous_layers_config_path</span> <span class="hljs-string">................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">hidden_dropout</span> <span class="hljs-string">..................................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">hidden_size</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">1024</span><br>  <span class="hljs-string">hierarchical_context_parallel_sizes</span> <span class="hljs-string">.............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">high_priority_stream_groups</span> <span class="hljs-string">.....................</span> []<br>  <span class="hljs-string">hybrid_attention_ratio</span> <span class="hljs-string">..........................</span> <span class="hljs-number">0.0</span><br>  <span class="hljs-string">hybrid_mlp_ratio</span> <span class="hljs-string">................................</span> <span class="hljs-number">0.0</span><br>  <span class="hljs-string">hybrid_override_pattern</span> <span class="hljs-string">.........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">hysteresis</span> <span class="hljs-string">......................................</span> <span class="hljs-number">2</span><br>  <span class="hljs-string">ict_head_size</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">ict_load</span> <span class="hljs-string">........................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">img_h</span> <span class="hljs-string">...........................................</span> <span class="hljs-number">224</span><br>  <span class="hljs-string">img_w</span> <span class="hljs-string">...........................................</span> <span class="hljs-number">224</span><br>  <span class="hljs-string">indexer_batch_size</span> <span class="hljs-string">..............................</span> <span class="hljs-number">128</span><br>  <span class="hljs-string">indexer_log_interval</span> <span class="hljs-string">............................</span> <span class="hljs-number">1000</span><br>  <span class="hljs-string">inference_batch_times_seqlen_threshold</span> <span class="hljs-string">..........</span> <span class="hljs-number">-1</span><br>  <span class="hljs-string">inference_dynamic_batching</span> <span class="hljs-string">......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">inference_dynamic_batching_buffer_guaranteed_fraction</span>  <span class="hljs-number">0.2</span><br>  <span class="hljs-string">inference_dynamic_batching_buffer_overflow_factor</span>  <span class="hljs-string">None</span><br>  <span class="hljs-string">inference_dynamic_batching_buffer_size_gb</span> <span class="hljs-string">.......</span> <span class="hljs-number">40.0</span><br>  <span class="hljs-string">inference_dynamic_batching_chunk_size</span> <span class="hljs-string">...........</span> <span class="hljs-number">256</span><br>  <span class="hljs-string">inference_dynamic_batching_max_requests_override</span>  <span class="hljs-string">None</span><br>  <span class="hljs-string">inference_dynamic_batching_max_tokens_override</span> <span class="hljs-string">..</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">inference_dynamic_batching_num_cuda_graphs</span> <span class="hljs-string">......</span> <span class="hljs-number">16</span><br>  <span class="hljs-string">inference_max_batch_size</span> <span class="hljs-string">........................</span> <span class="hljs-number">8</span><br>  <span class="hljs-string">inference_max_seq_length</span> <span class="hljs-string">........................</span> <span class="hljs-number">2560</span><br>  <span class="hljs-string">inference_rng_tracker</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">init_method_std</span> <span class="hljs-string">.................................</span> <span class="hljs-number">0.006</span><br>  <span class="hljs-string">init_method_xavier_uniform</span> <span class="hljs-string">......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">init_model_with_meta_device</span> <span class="hljs-string">.....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">initial_loss_scale</span> <span class="hljs-string">..............................</span> <span class="hljs-number">4294967296</span><br>  <span class="hljs-string">inprocess_active_world_size</span> <span class="hljs-string">.....................</span> <span class="hljs-number">4</span><br>  <span class="hljs-string">inprocess_barrier_timeout</span> <span class="hljs-string">.......................</span> <span class="hljs-number">120</span><br>  <span class="hljs-string">inprocess_completion_timeout</span> <span class="hljs-string">....................</span> <span class="hljs-number">120</span><br>  <span class="hljs-string">inprocess_empty_cuda_cache</span> <span class="hljs-string">......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">inprocess_granularity</span> <span class="hljs-string">...........................</span> <span class="hljs-string">node</span><br>  <span class="hljs-string">inprocess_hard_timeout</span> <span class="hljs-string">..........................</span> <span class="hljs-number">90</span><br>  <span class="hljs-string">inprocess_heartbeat_interval</span> <span class="hljs-string">....................</span> <span class="hljs-number">30</span><br>  <span class="hljs-string">inprocess_heartbeat_timeout</span> <span class="hljs-string">.....................</span> <span class="hljs-number">60</span><br>  <span class="hljs-string">inprocess_last_call_wait</span> <span class="hljs-string">........................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">inprocess_max_iterations</span> <span class="hljs-string">........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">inprocess_monitor_process_interval</span> <span class="hljs-string">..............</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">inprocess_monitor_thread_interval</span> <span class="hljs-string">...............</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">inprocess_progress_watchdog_interval</span> <span class="hljs-string">............</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">inprocess_restart</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">inprocess_soft_timeout</span> <span class="hljs-string">..........................</span> <span class="hljs-number">60</span><br>  <span class="hljs-string">inprocess_termination_grace_time</span> <span class="hljs-string">................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">is_hybrid_model</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">iter_per_epoch</span> <span class="hljs-string">..................................</span> <span class="hljs-number">1250</span><br>  <span class="hljs-string">iterations_to_skip</span> <span class="hljs-string">..............................</span> []<br>  <span class="hljs-string">keep_fp8_transpose_cache</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">kitchen_config_file</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">kitchen_recipe_number</span> <span class="hljs-string">...........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">kv_channels</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">64</span><br>  <span class="hljs-string">kv_lora_rank</span> <span class="hljs-string">....................................</span> <span class="hljs-number">32</span><br>  <span class="hljs-string">lazy_mpu_init</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">load</span> <span class="hljs-string">............................................</span> <span class="hljs-string">/workspace/megatron-lm/model_ckpt/gpt3_857m_tp4</span><br>  <span class="hljs-string">load_main_params_from_ckpt</span> <span class="hljs-string">......................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">load_model_opt_format</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">local_rank</span> <span class="hljs-string">......................................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">log_energy</span> <span class="hljs-string">......................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_interval</span> <span class="hljs-string">....................................</span> <span class="hljs-number">200</span><br>  <span class="hljs-string">log_loss_scale_to_tensorboard</span> <span class="hljs-string">...................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">log_memory_to_tensorboard</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_num_zeros_in_grad</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_params_norm</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_progress</span> <span class="hljs-string">....................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_straggler</span> <span class="hljs-string">...................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_throughput</span> <span class="hljs-string">..................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_timers_to_tensorboard</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_validation_ppl_to_tensorboard</span> <span class="hljs-string">...............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_world_size_to_tensorboard</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">logging_level</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">loss_scale</span> <span class="hljs-string">......................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">loss_scale_window</span> <span class="hljs-string">...............................</span> <span class="hljs-number">1000</span><br>  <span class="hljs-string">lr</span> <span class="hljs-string">..............................................</span> <span class="hljs-number">6e-05</span><br>  <span class="hljs-string">lr_decay_iters</span> <span class="hljs-string">..................................</span> <span class="hljs-number">20000</span><br>  <span class="hljs-string">lr_decay_samples</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">lr_decay_style</span> <span class="hljs-string">..................................</span> <span class="hljs-string">cosine</span><br>  <span class="hljs-string">lr_warmup_fraction</span> <span class="hljs-string">..............................</span> <span class="hljs-number">0.001</span><br>  <span class="hljs-string">lr_warmup_init</span> <span class="hljs-string">..................................</span> <span class="hljs-number">0.0</span><br>  <span class="hljs-string">lr_warmup_iters</span> <span class="hljs-string">.................................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">lr_warmup_samples</span> <span class="hljs-string">...............................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">lr_wsd_decay_iters</span> <span class="hljs-string">..............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">lr_wsd_decay_samples</span> <span class="hljs-string">............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">lr_wsd_decay_style</span> <span class="hljs-string">..............................</span> <span class="hljs-string">exponential</span><br>  <span class="hljs-string">main_grads_dtype</span> <span class="hljs-string">................................</span> <span class="hljs-string">torch.float32</span><br>  <span class="hljs-string">main_params_dtype</span> <span class="hljs-string">...............................</span> <span class="hljs-string">torch.float32</span><br>  <span class="hljs-string">make_vocab_size_divisible_by</span> <span class="hljs-string">....................</span> <span class="hljs-number">128</span><br>  <span class="hljs-string">mamba_head_dim</span> <span class="hljs-string">..................................</span> <span class="hljs-number">64</span><br>  <span class="hljs-string">mamba_num_groups</span> <span class="hljs-string">................................</span> <span class="hljs-number">8</span><br>  <span class="hljs-string">mamba_num_heads</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">mamba_state_dim</span> <span class="hljs-string">.................................</span> <span class="hljs-number">128</span><br>  <span class="hljs-string">manual_gc</span> <span class="hljs-string">.......................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">manual_gc_eval</span> <span class="hljs-string">..................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">manual_gc_interval</span> <span class="hljs-string">..............................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">mask_factor</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">mask_prob</span> <span class="hljs-string">.......................................</span> <span class="hljs-number">0.15</span><br>  <span class="hljs-string">mask_type</span> <span class="hljs-string">.......................................</span> <span class="hljs-string">random</span><br>  <span class="hljs-string">masked_softmax_fusion</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">max_position_embeddings</span> <span class="hljs-string">.........................</span> <span class="hljs-number">2048</span><br>  <span class="hljs-string">max_tokens_to_oom</span> <span class="hljs-string">...............................</span> <span class="hljs-number">12000</span><br>  <span class="hljs-string">memory_snapshot_path</span> <span class="hljs-string">............................</span> <span class="hljs-string">snapshot.pickle</span><br>  <span class="hljs-string">merge_file</span> <span class="hljs-string">......................................</span> <span class="hljs-string">/workspace/megatron-lm/data/tokenizer/gpt2-merges.txt</span><br>  <span class="hljs-string">micro_batch_size</span> <span class="hljs-string">................................</span> <span class="hljs-number">4</span><br>  <span class="hljs-string">microbatch_group_size_per_vp_stage</span> <span class="hljs-string">..............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">mid_level_dataset_surplus</span> <span class="hljs-string">.......................</span> <span class="hljs-number">0.005</span><br>  <span class="hljs-string">min_loss_scale</span> <span class="hljs-string">..................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">min_lr</span> <span class="hljs-string">..........................................</span> <span class="hljs-number">6e-06</span><br>  <span class="hljs-string">mlp_chunks_for_prefill</span> <span class="hljs-string">..........................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">mmap_bin_files</span> <span class="hljs-string">..................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">mock_data</span> <span class="hljs-string">.......................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_apply_probs_on_input</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_aux_loss_coeff</span> <span class="hljs-string">..............................</span> <span class="hljs-number">0.0</span><br>  <span class="hljs-string">moe_deepep_num_sms</span> <span class="hljs-string">..............................</span> <span class="hljs-number">20</span><br>  <span class="hljs-string">moe_enable_deepep</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_expert_capacity_factor</span> <span class="hljs-string">......................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_extended_tp</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_ffn_hidden_size</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_grouped_gemm</span> <span class="hljs-string">................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_input_jitter_eps</span> <span class="hljs-string">............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_layer_freq</span> <span class="hljs-string">..................................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">moe_layer_recompute</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_pad_expert_input_to_capacity</span> <span class="hljs-string">................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_per_layer_logging</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_permute_fusion</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_router_bias_update_rate</span> <span class="hljs-string">.....................</span> <span class="hljs-number">0.001</span><br>  <span class="hljs-string">moe_router_dtype</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_router_enable_expert_bias</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_router_force_load_balancing</span> <span class="hljs-string">.................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_router_fusion</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_router_group_topk</span> <span class="hljs-string">...........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_router_load_balancing_type</span> <span class="hljs-string">..................</span> <span class="hljs-string">aux_loss</span><br>  <span class="hljs-string">moe_router_num_groups</span> <span class="hljs-string">...........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_router_padding_for_fp8</span> <span class="hljs-string">......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_router_pre_softmax</span> <span class="hljs-string">..........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_router_score_function</span> <span class="hljs-string">.......................</span> <span class="hljs-string">softmax</span><br>  <span class="hljs-string">moe_router_topk</span> <span class="hljs-string">.................................</span> <span class="hljs-number">2</span><br>  <span class="hljs-string">moe_router_topk_scaling_factor</span> <span class="hljs-string">..................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_shared_expert_intermediate_size</span> <span class="hljs-string">.............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_shared_expert_overlap</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_token_dispatcher_type</span> <span class="hljs-string">.......................</span> <span class="hljs-string">allgather</span><br>  <span class="hljs-string">moe_token_drop_policy</span> <span class="hljs-string">...........................</span> <span class="hljs-string">probs</span><br>  <span class="hljs-string">moe_upcycling_granularity</span> <span class="hljs-string">.......................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">moe_use_legacy_grouped_gemm</span> <span class="hljs-string">.....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_use_upcycling</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_z_loss_coeff</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">mrope_section</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">mscale</span> <span class="hljs-string">..........................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">mscale_all_dim</span> <span class="hljs-string">..................................</span> <span class="hljs-number">0.0</span><br>  <span class="hljs-string">mtp_loss_scaling_factor</span> <span class="hljs-string">.........................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">mtp_num_layers</span> <span class="hljs-string">..................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">multi_latent_attention</span> <span class="hljs-string">..........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">multiple_validation_sets</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">nccl_all_reduce_for_prefill</span> <span class="hljs-string">.....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">nccl_communicator_config_path</span> <span class="hljs-string">...................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">nccl_ub</span> <span class="hljs-string">.........................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">no_load_optim</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">no_load_rng</span> <span class="hljs-string">.....................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">no_persist_layer_norm</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">no_rope_freq</span> <span class="hljs-string">....................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">no_save_optim</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">no_save_rng</span> <span class="hljs-string">.....................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">non_persistent_ckpt_type</span> <span class="hljs-string">........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">non_persistent_global_ckpt_dir</span> <span class="hljs-string">..................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">non_persistent_local_ckpt_algo</span> <span class="hljs-string">..................</span> <span class="hljs-string">fully_parallel</span><br>  <span class="hljs-string">non_persistent_local_ckpt_dir</span> <span class="hljs-string">...................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">non_persistent_save_interval</span> <span class="hljs-string">....................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">norm_epsilon</span> <span class="hljs-string">....................................</span> <span class="hljs-number">1e-05</span><br>  <span class="hljs-string">normalization</span> <span class="hljs-string">...................................</span> <span class="hljs-string">LayerNorm</span><br>  <span class="hljs-string">num_attention_heads</span> <span class="hljs-string">.............................</span> <span class="hljs-number">16</span><br>  <span class="hljs-string">num_channels</span> <span class="hljs-string">....................................</span> <span class="hljs-number">3</span><br>  <span class="hljs-string">num_classes</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">1000</span><br>  <span class="hljs-string">num_dataset_builder_threads</span> <span class="hljs-string">.....................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">num_distributed_optimizer_instances</span> <span class="hljs-string">.............</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">num_experts</span> <span class="hljs-string">.....................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">num_layers</span> <span class="hljs-string">......................................</span> <span class="hljs-number">24</span><br>  <span class="hljs-string">num_layers_at_end_in_bf16</span> <span class="hljs-string">.......................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">num_layers_at_start_in_bf16</span> <span class="hljs-string">.....................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">num_layers_per_virtual_pipeline_stage</span> <span class="hljs-string">...........</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">num_query_groups</span> <span class="hljs-string">................................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">num_virtual_stages_per_pipeline_rank</span> <span class="hljs-string">............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">num_workers</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">2</span><br>  <span class="hljs-string">object_storage_cache_path</span> <span class="hljs-string">.......................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">one_logger_async</span> <span class="hljs-string">................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">one_logger_project</span> <span class="hljs-string">..............................</span> <span class="hljs-string">megatron-lm</span><br>  <span class="hljs-string">one_logger_run_name</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">onnx_safe</span> <span class="hljs-string">.......................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">openai_gelu</span> <span class="hljs-string">.....................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">optimizer</span> <span class="hljs-string">.......................................</span> <span class="hljs-string">adam</span><br>  <span class="hljs-string">optimizer_cpu_offload</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">optimizer_offload_fraction</span> <span class="hljs-string">......................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">output_bert_embeddings</span> <span class="hljs-string">..........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">overlap_cpu_optimizer_d2h_h2d</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">overlap_grad_reduce</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">overlap_moe_expert_parallel_comm</span> <span class="hljs-string">................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">overlap_p2p_comm</span> <span class="hljs-string">................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">overlap_p2p_comm_warmup_flush</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">overlap_param_gather</span> <span class="hljs-string">............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">overlap_param_gather_with_optimizer_step</span> <span class="hljs-string">........</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">override_opt_param_scheduler</span> <span class="hljs-string">....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">padded_vocab_size</span> <span class="hljs-string">...............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">params_dtype</span> <span class="hljs-string">....................................</span> <span class="hljs-string">torch.float16</span><br>  <span class="hljs-string">patch_dim</span> <span class="hljs-string">.......................................</span> <span class="hljs-number">16</span><br>  <span class="hljs-string">per_split_data_args_path</span> <span class="hljs-string">........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">perform_initialization</span> <span class="hljs-string">..........................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">pin_cpu_grads</span> <span class="hljs-string">...................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">pin_cpu_params</span> <span class="hljs-string">..................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">pipeline_model_parallel_comm_backend</span> <span class="hljs-string">............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">pipeline_model_parallel_layout</span> <span class="hljs-string">..................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">pipeline_model_parallel_size</span> <span class="hljs-string">....................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">position_embedding_type</span> <span class="hljs-string">.........................</span> <span class="hljs-string">learned_absolute</span><br>  <span class="hljs-string">pretrained_checkpoint</span> <span class="hljs-string">...........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">profile</span> <span class="hljs-string">.........................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">profile_ranks</span> <span class="hljs-string">...................................</span> [<span class="hljs-number">0</span>]<br>  <span class="hljs-string">profile_step_end</span> <span class="hljs-string">................................</span> <span class="hljs-number">112</span><br>  <span class="hljs-string">profile_step_start</span> <span class="hljs-string">..............................</span> <span class="hljs-number">110</span><br>  <span class="hljs-string">q_lora_rank</span> <span class="hljs-string">.....................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">qk_head_dim</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">128</span><br>  <span class="hljs-string">qk_l2_norm</span> <span class="hljs-string">......................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">qk_layernorm</span> <span class="hljs-string">....................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">qk_pos_emb_head_dim</span> <span class="hljs-string">.............................</span> <span class="hljs-number">64</span><br>  <span class="hljs-string">query_in_block_prob</span> <span class="hljs-string">.............................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">rampup_batch_size</span> <span class="hljs-string">...............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">rank</span> <span class="hljs-string">............................................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">recompute_granularity</span> <span class="hljs-string">...........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">recompute_method</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">recompute_modules</span> <span class="hljs-string">...............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">recompute_num_layers</span> <span class="hljs-string">............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">record_memory_history</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">relative_attention_max_distance</span> <span class="hljs-string">.................</span> <span class="hljs-number">128</span><br>  <span class="hljs-string">relative_attention_num_buckets</span> <span class="hljs-string">..................</span> <span class="hljs-number">32</span><br>  <span class="hljs-string">replication</span> <span class="hljs-string">.....................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">replication_factor</span> <span class="hljs-string">..............................</span> <span class="hljs-number">2</span><br>  <span class="hljs-string">replication_jump</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">rerun_mode</span> <span class="hljs-string">......................................</span> <span class="hljs-string">validate_results</span><br>  <span class="hljs-string">reset_attention_mask</span> <span class="hljs-string">............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">reset_position_ids</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">result_rejected_tracker_filename</span> <span class="hljs-string">................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">retriever_report_topk_accuracies</span> <span class="hljs-string">................</span> []<br>  <span class="hljs-string">retriever_score_scaling</span> <span class="hljs-string">.........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">retriever_seq_length</span> <span class="hljs-string">............................</span> <span class="hljs-number">256</span><br>  <span class="hljs-string">retro_add_retriever</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">retro_attention_gate</span> <span class="hljs-string">............................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">retro_cyclic_train_iters</span> <span class="hljs-string">........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">retro_encoder_attention_dropout</span> <span class="hljs-string">.................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">retro_encoder_hidden_dropout</span> <span class="hljs-string">....................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">retro_encoder_layers</span> <span class="hljs-string">............................</span> <span class="hljs-number">2</span><br>  <span class="hljs-string">retro_num_neighbors</span> <span class="hljs-string">.............................</span> <span class="hljs-number">2</span><br>  <span class="hljs-string">retro_num_retrieved_chunks</span> <span class="hljs-string">......................</span> <span class="hljs-number">2</span><br>  <span class="hljs-string">retro_project_dir</span> <span class="hljs-string">...............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">retro_verify_neighbor_count</span> <span class="hljs-string">.....................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">reuse_grad_buf_for_mxfp8_param_ag</span> <span class="hljs-string">...............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">rope_scaling_factor</span> <span class="hljs-string">.............................</span> <span class="hljs-number">8.0</span><br>  <span class="hljs-string">rope_type</span> <span class="hljs-string">.......................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">rotary_base</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">10000</span><br>  <span class="hljs-string">rotary_interleaved</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">rotary_percent</span> <span class="hljs-string">..................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">rotary_scaling_factor</span> <span class="hljs-string">...........................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">rotary_seq_len_interpolation_factor</span> <span class="hljs-string">.............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">run_workload_inspector_server</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">sample_rate</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">save</span> <span class="hljs-string">............................................</span> <span class="hljs-string">/workspace/megatron-lm/model_ckpt/gpt3_857m_tp4</span><br>  <span class="hljs-string">save_interval</span> <span class="hljs-string">...................................</span> <span class="hljs-number">10000</span><br>  <span class="hljs-string">save_retain_interval</span> <span class="hljs-string">............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">scatter_gather_tensors_in_pipeline</span> <span class="hljs-string">..............</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">seed</span> <span class="hljs-string">............................................</span> <span class="hljs-number">1234</span><br>  <span class="hljs-string">seq_length</span> <span class="hljs-string">......................................</span> <span class="hljs-number">2048</span><br>  <span class="hljs-string">sequence_parallel</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">sft</span> <span class="hljs-string">.............................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">sft_tokenizer_prompt_format</span> <span class="hljs-string">.....................</span> <span class="hljs-string">nemotron-h-aligned</span><br>  <span class="hljs-string">sgd_momentum</span> <span class="hljs-string">....................................</span> <span class="hljs-number">0.9</span><br>  <span class="hljs-string">sharp_enabled_group</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">short_seq_prob</span> <span class="hljs-string">..................................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">skip_train</span> <span class="hljs-string">......................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">skipped_train_samples</span> <span class="hljs-string">...........................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">spec</span> <span class="hljs-string">............................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">split</span> <span class="hljs-string">...........................................</span> <span class="hljs-number">949</span><span class="hljs-string">,50,1</span><br>  <span class="hljs-string">squared_relu</span> <span class="hljs-string">....................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">start_weight_decay</span> <span class="hljs-string">..............................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">straggler_ctrlr_port</span> <span class="hljs-string">............................</span> <span class="hljs-number">65535</span><br>  <span class="hljs-string">straggler_minmax_count</span> <span class="hljs-string">..........................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">strict_fsdp_dtensor_load</span> <span class="hljs-string">........................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">suggested_communication_unit_size</span> <span class="hljs-string">...............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">swiglu</span> <span class="hljs-string">..........................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">swin_backbone_type</span> <span class="hljs-string">..............................</span> <span class="hljs-string">tiny</span><br>  <span class="hljs-string">symmetric_ar_type</span> <span class="hljs-string">...............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">te_rng_tracker</span> <span class="hljs-string">..................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">tensor_model_parallel_size</span> <span class="hljs-string">......................</span> <span class="hljs-number">4</span><br>  <span class="hljs-string">tensorboard_dir</span> <span class="hljs-string">.................................</span> <span class="hljs-string">/workspace/megatron-lm/tb_logs/gpt3_857m_profiler_tp4</span><br>  <span class="hljs-string">tensorboard_log_interval</span> <span class="hljs-string">........................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">tensorboard_queue_size</span> <span class="hljs-string">..........................</span> <span class="hljs-number">1000</span><br>  <span class="hljs-string">test_data_path</span> <span class="hljs-string">..................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">test_mode</span> <span class="hljs-string">.......................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">tiktoken_num_special_tokens</span> <span class="hljs-string">.....................</span> <span class="hljs-number">1000</span><br>  <span class="hljs-string">tiktoken_pattern</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">tiktoken_special_tokens</span> <span class="hljs-string">.........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">timing_log_level</span> <span class="hljs-string">................................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">timing_log_option</span> <span class="hljs-string">...............................</span> <span class="hljs-string">minmax</span><br>  <span class="hljs-string">titles_data_path</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">tokenizer_model</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">tokenizer_type</span> <span class="hljs-string">..................................</span> <span class="hljs-string">GPT2BPETokenizer</span><br>  <span class="hljs-string">torch_fsdp2_reshard_after_forward</span> <span class="hljs-string">...............</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">tp_comm_bootstrap_backend</span> <span class="hljs-string">.......................</span> <span class="hljs-string">nccl</span><br>  <span class="hljs-string">tp_comm_bulk_dgrad</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">tp_comm_bulk_wgrad</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">tp_comm_overlap</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">tp_comm_overlap_ag</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">tp_comm_overlap_cfg</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">tp_comm_overlap_rs</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">tp_comm_overlap_rs_dgrad</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">tp_comm_split_ag</span> <span class="hljs-string">................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">tp_comm_split_rs</span> <span class="hljs-string">................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">train_data_path</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">train_iters</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">20000</span><br>  <span class="hljs-string">train_samples</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">train_sync_interval</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">transformer_impl</span> <span class="hljs-string">................................</span> <span class="hljs-string">local</span><br>  <span class="hljs-string">transformer_pipeline_model_parallel_size</span> <span class="hljs-string">........</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">untie_embeddings_and_output_weights</span> <span class="hljs-string">.............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_checkpoint_args</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_checkpoint_opt_param_scheduler</span> <span class="hljs-string">..............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_cpu_initialization</span> <span class="hljs-string">..........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">use_dist_ckpt</span> <span class="hljs-string">...................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">use_dist_ckpt_deprecated</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_distributed_optimizer</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_flash_attn</span> <span class="hljs-string">..................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_fused_weighted_squared_relu</span> <span class="hljs-string">.................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_legacy_models</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_megatron_fsdp</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_mp_args_from_checkpoint_args</span> <span class="hljs-string">................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_one_sent_docs</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_persistent_ckpt_worker</span> <span class="hljs-string">......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_precision_aware_optimizer</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_pytorch_profiler</span> <span class="hljs-string">............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">use_ring_exchange_p2p</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_rope_scaling</span> <span class="hljs-string">................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_rotary_position_embeddings</span> <span class="hljs-string">..................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_sharp</span> <span class="hljs-string">.......................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_tokenizer_model_from_checkpoint_args</span> <span class="hljs-string">........</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">use_torch_fsdp2</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_torch_optimizer_for_cpu_offload</span> <span class="hljs-string">.............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_tp_pp_dp_mapping</span> <span class="hljs-string">............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">v_head_dim</span> <span class="hljs-string">......................................</span> <span class="hljs-number">128</span><br>  <span class="hljs-string">valid_data_path</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">variable_seq_lengths</span> <span class="hljs-string">............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">virtual_pipeline_model_parallel_size</span> <span class="hljs-string">............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">vision_backbone_type</span> <span class="hljs-string">............................</span> <span class="hljs-string">vit</span><br>  <span class="hljs-string">vision_pretraining</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">vision_pretraining_type</span> <span class="hljs-string">.........................</span> <span class="hljs-string">classify</span><br>  <span class="hljs-string">vocab_extra_ids</span> <span class="hljs-string">.................................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">vocab_file</span> <span class="hljs-string">......................................</span> <span class="hljs-string">/workspace/megatron-lm/data/tokenizer/gpt2-vocab.json</span><br>  <span class="hljs-string">vocab_size</span> <span class="hljs-string">......................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">wandb_exp_name</span> <span class="hljs-string">..................................</span> <br>  <span class="hljs-string">wandb_project</span> <span class="hljs-string">...................................</span> <br>  <span class="hljs-string">wandb_save_dir</span> <span class="hljs-string">..................................</span> <br>  <span class="hljs-string">weight_decay</span> <span class="hljs-string">....................................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">weight_decay_incr_style</span> <span class="hljs-string">.........................</span> <span class="hljs-string">constant</span><br>  <span class="hljs-string">wgrad_deferral_limit</span> <span class="hljs-string">............................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">world_size</span> <span class="hljs-string">......................................</span> <span class="hljs-number">4</span><br>  <span class="hljs-string">yaml_cfg</span> <span class="hljs-string">........................................</span> <span class="hljs-string">None</span><br><span class="hljs-string">--------------------</span> <span class="hljs-string">end</span> <span class="hljs-string">of</span> <span class="hljs-string">arguments</span> <span class="hljs-string">---------------------</span><br><span class="hljs-string">INFO:megatron.core.num_microbatches_calculator:setting</span> <span class="hljs-string">number</span> <span class="hljs-string">of</span> <span class="hljs-string">microbatches</span> <span class="hljs-string">to</span> <span class="hljs-string">constant</span> <span class="hljs-number">4</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">building</span> <span class="hljs-string">GPT2BPETokenizer</span> <span class="hljs-string">tokenizer</span> <span class="hljs-string">...</span><br> <span class="hljs-string">&gt;</span> <span class="hljs-string">padded</span> <span class="hljs-string">vocab</span> <span class="hljs-string">(size:</span> <span class="hljs-number">50257</span><span class="hljs-string">)</span> <span class="hljs-string">with</span> <span class="hljs-number">431</span> <span class="hljs-string">dummy</span> <span class="hljs-string">tokens</span> <span class="hljs-string">(new</span> <span class="hljs-attr">size:</span> <span class="hljs-number">50688</span><span class="hljs-string">)</span><br><span class="hljs-string">WARNING:megatron.core.rerun_state_machine:RerunStateMachine</span> <span class="hljs-string">initialized</span> <span class="hljs-string">in</span> <span class="hljs-string">mode</span> <span class="hljs-string">RerunMode.VALIDATE_RESULTS</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">initializing</span> <span class="hljs-string">torch</span> <span class="hljs-string">distributed</span> <span class="hljs-string">...</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">initialized</span> <span class="hljs-string">tensor</span> <span class="hljs-string">model</span> <span class="hljs-string">parallel</span> <span class="hljs-string">with</span> <span class="hljs-string">size</span> <span class="hljs-number">4</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">initialized</span> <span class="hljs-string">pipeline</span> <span class="hljs-string">model</span> <span class="hljs-string">parallel</span> <span class="hljs-string">with</span> <span class="hljs-string">size</span> <span class="hljs-number">1</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">setting</span> <span class="hljs-string">random</span> <span class="hljs-string">seeds</span> <span class="hljs-string">to</span> <span class="hljs-number">1234</span> <span class="hljs-string">...</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">compiling</span> <span class="hljs-string">dataset</span> <span class="hljs-string">index</span> <span class="hljs-string">builder</span> <span class="hljs-string">...</span><br><span class="hljs-attr">make:</span> <span class="hljs-string">Entering</span> <span class="hljs-string">directory</span> <span class="hljs-string">&#x27;/workspace/megatron-lm/megatron/core/datasets&#x27;</span><br>[<span class="hljs-string">rank2</span>]<span class="hljs-string">:[W103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:34.420203100</span> <span class="hljs-string">ProcessGroupNCCL.cpp:4751]</span> [<span class="hljs-string">PG</span> <span class="hljs-string">ID</span> <span class="hljs-number">0</span> <span class="hljs-string">PG</span> <span class="hljs-string">GUID</span> <span class="hljs-number">0</span> <span class="hljs-string">Rank</span> <span class="hljs-number">2</span>]  <span class="hljs-string">using</span> <span class="hljs-string">GPU</span> <span class="hljs-number">2</span> <span class="hljs-string">as</span> <span class="hljs-string">device</span> <span class="hljs-string">used</span> <span class="hljs-string">by</span> <span class="hljs-string">this</span> <span class="hljs-string">process</span> <span class="hljs-string">is</span> <span class="hljs-string">currently</span> <span class="hljs-string">unknown.</span> <span class="hljs-string">This</span> <span class="hljs-string">can</span> <span class="hljs-string">potentially</span> <span class="hljs-string">cause</span> <span class="hljs-string">a</span> <span class="hljs-string">hang</span> <span class="hljs-string">if</span> <span class="hljs-string">this</span> <span class="hljs-string">rank</span> <span class="hljs-string">to</span> <span class="hljs-string">GPU</span> <span class="hljs-string">mapping</span> <span class="hljs-string">is</span> <span class="hljs-string">incorrect.</span> <span class="hljs-string">You</span> <span class="hljs-string">can</span> <span class="hljs-string">pecify</span> <span class="hljs-string">device_id</span> <span class="hljs-string">in</span> <span class="hljs-string">init_process_group()</span> <span class="hljs-string">to</span> <span class="hljs-string">force</span> <span class="hljs-string">use</span> <span class="hljs-string">of</span> <span class="hljs-string">a</span> <span class="hljs-string">particular</span> <span class="hljs-string">device.</span><br>[<span class="hljs-string">rank1</span>]<span class="hljs-string">:[W103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:34.605196852</span> <span class="hljs-string">ProcessGroupNCCL.cpp:4751]</span> [<span class="hljs-string">PG</span> <span class="hljs-string">ID</span> <span class="hljs-number">0</span> <span class="hljs-string">PG</span> <span class="hljs-string">GUID</span> <span class="hljs-number">0</span> <span class="hljs-string">Rank</span> <span class="hljs-number">1</span>]  <span class="hljs-string">using</span> <span class="hljs-string">GPU</span> <span class="hljs-number">1</span> <span class="hljs-string">as</span> <span class="hljs-string">device</span> <span class="hljs-string">used</span> <span class="hljs-string">by</span> <span class="hljs-string">this</span> <span class="hljs-string">process</span> <span class="hljs-string">is</span> <span class="hljs-string">currently</span> <span class="hljs-string">unknown.</span> <span class="hljs-string">This</span> <span class="hljs-string">can</span> <span class="hljs-string">potentially</span> <span class="hljs-string">cause</span> <span class="hljs-string">a</span> <span class="hljs-string">hang</span> <span class="hljs-string">if</span> <span class="hljs-string">this</span> <span class="hljs-string">rank</span> <span class="hljs-string">to</span> <span class="hljs-string">GPU</span> <span class="hljs-string">mapping</span> <span class="hljs-string">is</span> <span class="hljs-string">incorrect.</span> <span class="hljs-string">You</span> <span class="hljs-string">can</span> <span class="hljs-string">pecify</span> <span class="hljs-string">device_id</span> <span class="hljs-string">in</span> <span class="hljs-string">init_process_group()</span> <span class="hljs-string">to</span> <span class="hljs-string">force</span> <span class="hljs-string">use</span> <span class="hljs-string">of</span> <span class="hljs-string">a</span> <span class="hljs-string">particular</span> <span class="hljs-string">device.</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">setting</span> <span class="hljs-string">tensorboard</span> <span class="hljs-string">...</span><br><span class="hljs-attr">WARNING:</span> <span class="hljs-string">one_logger</span> <span class="hljs-string">package</span> <span class="hljs-string">is</span> <span class="hljs-string">required</span> <span class="hljs-string">to</span> <span class="hljs-string">enable</span> <span class="hljs-string">e2e</span> <span class="hljs-string">metrics</span> <span class="hljs-string">tracking.</span> <span class="hljs-string">please</span> <span class="hljs-string">go</span> <span class="hljs-string">to</span> <span class="hljs-string">https://confluence.nvidia.com/display/MLWFO/Package+Repositories</span> <span class="hljs-string">for</span> <span class="hljs-string">details</span> <span class="hljs-string">to</span> <span class="hljs-string">install</span> <span class="hljs-string">it</span><br><span class="hljs-attr">make:</span> <span class="hljs-string">Nothing</span> <span class="hljs-string">to</span> <span class="hljs-string">be</span> <span class="hljs-string">done</span> <span class="hljs-string">for</span> <span class="hljs-string">&#x27;default&#x27;</span><span class="hljs-string">.</span><br><span class="hljs-attr">make:</span> <span class="hljs-string">Leaving</span> <span class="hljs-string">directory</span> <span class="hljs-string">&#x27;/workspace/megatron-lm/megatron/core/datasets&#x27;</span><br><span class="hljs-string">&gt;&gt;&gt;</span> <span class="hljs-attr">done with dataset index builder. Compilation time:</span> <span class="hljs-number">0.207</span> <span class="hljs-string">seconds</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">compiling</span> <span class="hljs-string">and</span> <span class="hljs-string">loading</span> <span class="hljs-string">fused</span> <span class="hljs-string">kernels</span> <span class="hljs-string">...</span><br>[<span class="hljs-string">rank3</span>]<span class="hljs-string">:[W103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:34.713259577</span> <span class="hljs-string">ProcessGroupNCCL.cpp:4751]</span> [<span class="hljs-string">PG</span> <span class="hljs-string">ID</span> <span class="hljs-number">0</span> <span class="hljs-string">PG</span> <span class="hljs-string">GUID</span> <span class="hljs-number">0</span> <span class="hljs-string">Rank</span> <span class="hljs-number">3</span>]  <span class="hljs-string">using</span> <span class="hljs-string">GPU</span> <span class="hljs-number">3</span> <span class="hljs-string">as</span> <span class="hljs-string">device</span> <span class="hljs-string">used</span> <span class="hljs-string">by</span> <span class="hljs-string">this</span> <span class="hljs-string">process</span> <span class="hljs-string">is</span> <span class="hljs-string">currently</span> <span class="hljs-string">unknown.</span> <span class="hljs-string">This</span> <span class="hljs-string">can</span> <span class="hljs-string">potentially</span> <span class="hljs-string">cause</span> <span class="hljs-string">a</span> <span class="hljs-string">hang</span> <span class="hljs-string">if</span> <span class="hljs-string">this</span> <span class="hljs-string">rank</span> <span class="hljs-string">to</span> <span class="hljs-string">GPU</span> <span class="hljs-string">mapping</span> <span class="hljs-string">is</span> <span class="hljs-string">incorrect.</span> <span class="hljs-string">You</span> <span class="hljs-string">can</span> <span class="hljs-string">pecify</span> <span class="hljs-string">device_id</span> <span class="hljs-string">in</span> <span class="hljs-string">init_process_group()</span> <span class="hljs-string">to</span> <span class="hljs-string">force</span> <span class="hljs-string">use</span> <span class="hljs-string">of</span> <span class="hljs-string">a</span> <span class="hljs-string">particular</span> <span class="hljs-string">device.</span><br>[<span class="hljs-string">rank0</span>]<span class="hljs-string">:[W103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:34.732739062</span> <span class="hljs-string">ProcessGroupNCCL.cpp:4751]</span> [<span class="hljs-string">PG</span> <span class="hljs-string">ID</span> <span class="hljs-number">0</span> <span class="hljs-string">PG</span> <span class="hljs-string">GUID</span> <span class="hljs-number">0</span> <span class="hljs-string">Rank</span> <span class="hljs-number">0</span>]  <span class="hljs-string">using</span> <span class="hljs-string">GPU</span> <span class="hljs-number">0</span> <span class="hljs-string">as</span> <span class="hljs-string">device</span> <span class="hljs-string">used</span> <span class="hljs-string">by</span> <span class="hljs-string">this</span> <span class="hljs-string">process</span> <span class="hljs-string">is</span> <span class="hljs-string">currently</span> <span class="hljs-string">unknown.</span> <span class="hljs-string">This</span> <span class="hljs-string">can</span> <span class="hljs-string">potentially</span> <span class="hljs-string">cause</span> <span class="hljs-string">a</span> <span class="hljs-string">hang</span> <span class="hljs-string">if</span> <span class="hljs-string">this</span> <span class="hljs-string">rank</span> <span class="hljs-string">to</span> <span class="hljs-string">GPU</span> <span class="hljs-string">mapping</span> <span class="hljs-string">is</span> <span class="hljs-string">incorrect.</span> <span class="hljs-string">You</span> <span class="hljs-string">can</span> <span class="hljs-string">pecify</span> <span class="hljs-string">device_id</span> <span class="hljs-string">in</span> <span class="hljs-string">init_process_group()</span> <span class="hljs-string">to</span> <span class="hljs-string">force</span> <span class="hljs-string">use</span> <span class="hljs-string">of</span> <span class="hljs-string">a</span> <span class="hljs-string">particular</span> <span class="hljs-string">device.</span><br><span class="hljs-string">&gt;&gt;&gt;</span> <span class="hljs-attr">done with compiling and loading fused kernels. Compilation time:</span> <span class="hljs-number">0.324</span> <span class="hljs-string">seconds</span><br><span class="hljs-string">time</span> <span class="hljs-string">to</span> <span class="hljs-string">initialize</span> <span class="hljs-string">megatron</span> <span class="hljs-string">(seconds):</span> <span class="hljs-number">2.393</span><br>[<span class="hljs-string">after</span> <span class="hljs-string">megatron</span> <span class="hljs-string">is</span> <span class="hljs-string">initialized</span>] <span class="hljs-attr">datetime:</span> <span class="hljs-number">2026-01-03 04:22:36</span> <br><span class="hljs-string">building</span> <span class="hljs-string">GPT</span> <span class="hljs-string">model</span> <span class="hljs-string">...</span><br> <span class="hljs-string">&gt;</span> <span class="hljs-string">number</span> <span class="hljs-string">of</span> <span class="hljs-string">parameters</span> <span class="hljs-string">on</span> <span class="hljs-string">(tensor,</span> <span class="hljs-string">pipeline)</span> <span class="hljs-string">model</span> <span class="hljs-string">parallel</span> <span class="hljs-string">rank</span> <span class="hljs-string">(3,</span> <span class="hljs-number">0</span><span class="hljs-string">):</span> <span class="hljs-number">90763264</span><br> <span class="hljs-string">&gt;</span> <span class="hljs-string">number</span> <span class="hljs-string">of</span> <span class="hljs-string">parameters</span> <span class="hljs-string">on</span> <span class="hljs-string">(tensor,</span> <span class="hljs-string">pipeline)</span> <span class="hljs-string">model</span> <span class="hljs-string">parallel</span> <span class="hljs-string">rank</span> <span class="hljs-string">(0,</span> <span class="hljs-number">0</span><span class="hljs-string">):</span> <span class="hljs-number">90763264</span><br> <span class="hljs-string">&gt;</span> <span class="hljs-string">number</span> <span class="hljs-string">of</span> <span class="hljs-string">parameters</span> <span class="hljs-string">on</span> <span class="hljs-string">(tensor,</span> <span class="hljs-string">pipeline)</span> <span class="hljs-string">model</span> <span class="hljs-string">parallel</span> <span class="hljs-string">rank</span> <span class="hljs-string">(2,</span> <span class="hljs-number">0</span><span class="hljs-string">):</span> <span class="hljs-number">90763264</span><br> <span class="hljs-string">&gt;</span> <span class="hljs-string">number</span> <span class="hljs-string">of</span> <span class="hljs-string">parameters</span> <span class="hljs-string">on</span> <span class="hljs-string">(tensor,</span> <span class="hljs-string">pipeline)</span> <span class="hljs-string">model</span> <span class="hljs-string">parallel</span> <span class="hljs-string">rank</span> <span class="hljs-string">(1,</span> <span class="hljs-number">0</span><span class="hljs-string">):</span> <span class="hljs-number">90763264</span><br><span class="hljs-string">INFO:megatron.core.distributed.distributed_data_parallel:Setting</span> <span class="hljs-string">up</span> <span class="hljs-string">DistributedDataParallel</span> <span class="hljs-string">with</span> <span class="hljs-string">config</span> <span class="hljs-string">DistributedDataParallelConfig(grad_reduce_in_fp32=False,</span> <span class="hljs-string">overlap_grad_reduce=False,</span> <span class="hljs-string">overlap_param_gather=False,</span> <span class="hljs-string">align_param_gather=False,</span> <span class="hljs-string">use_distributed_optimizer=False,</span> <span class="hljs-string">num_distributed_optimizer_instances=1,</span> <span class="hljs-string">check_for_nan_in_grad=False,</span> <span class="hljs-string">check_for_large_grads=False,</span> <span class="hljs-string">bucket_size=None,</span> <span class="hljs-string">pad_buckets_for_high_nccl_busbw=False,</span> <span class="hljs-string">average_in_collective=False,</span> <span class="hljs-string">fp8_param_gather=False,</span> <span class="hljs-string">reuse_grad_buf_for_mxfp8_param_ag=False,</span> <span class="hljs-string">use_megatron_fsdp=False,</span> <span class="hljs-string">use_custom_fsdp=False,</span> <span class="hljs-string">data_parallel_sharding_strategy=&#x27;no_shard&#x27;,</span> <span class="hljs-string">gradient_reduce_div_fusion=True,</span> <span class="hljs-string">suggested_communication_unit_size=None,</span> <span class="hljs-string">preserve_fp32_weights=True,</span> <span class="hljs-string">keep_fp8_transpose_cache=False,</span> <span class="hljs-string">nccl_ub=False,</span> <span class="hljs-string">fsdp_double_buffer=False,</span> <span class="hljs-string">outer_dp_sharding_strategy=&#x27;no_shard&#x27;,</span> <span class="hljs-string">disable_symmetric_registration=False,</span> <span class="hljs-string">delay_wgrad_compute=False)</span><br><span class="hljs-attr">INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter:</span> <span class="hljs-number">1</span><br><span class="hljs-string">Params</span> <span class="hljs-string">for</span> <span class="hljs-string">bucket</span> <span class="hljs-number">1</span> <span class="hljs-string">(90763264</span> <span class="hljs-string">elements,</span> <span class="hljs-number">90763264</span> <span class="hljs-string">padded</span> <span class="hljs-string">size):</span><br>    <span class="hljs-string">module.decoder.layers.22.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.20.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.18.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.16.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.8.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.7.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.3.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.1.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.21.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.14.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.11.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.6.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.5.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.2.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.21.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.20.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.18.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.15.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.11.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.6.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.23.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.22.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.14.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.13.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.10.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.9.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.1.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.23.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.22.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.20.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.17.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.16.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.15.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.13.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.10.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.5.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.4.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.22.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.15.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.11.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.9.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.6.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.embedding.word_embeddings.weight</span><br>    <span class="hljs-string">module.decoder.layers.22.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.19.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.17.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.11.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.9.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.5.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.4.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.17.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.13.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.11.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.7.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.final_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.22.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.20.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.19.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.16.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.15.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.13.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.11.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.9.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.0.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.19.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.16.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.15.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.13.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.6.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.6.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.5.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.4.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.3.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.1.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.22.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.17.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.15.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.13.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.13.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.11.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.17.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.15.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.8.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.4.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.4.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.2.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.1.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.embedding.position_embeddings.weight</span><br>    <span class="hljs-string">module.decoder.layers.1.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.final_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.21.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.20.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.19.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.17.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.13.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.11.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.6.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.17.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.20.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.19.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.15.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.23.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.22.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.21.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.19.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.13.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.10.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.9.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.5.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.1.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.21.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.19.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.17.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.15.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.13.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.4.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.2.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.23.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.11.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.10.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.9.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.8.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.5.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.5.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.2.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.23.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.19.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.17.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.14.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.10.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.6.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.4.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.0.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.22.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.21.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.12.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.6.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.4.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.22.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.21.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.19.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.16.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.15.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.12.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.9.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.5.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.4.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.23.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.21.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.11.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.10.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.9.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.5.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.5.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.17.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.23.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.21.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.18.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.16.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.11.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.10.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.9.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.5.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.23.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.16.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.13.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.12.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.10.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.9.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.6.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.23.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.21.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.19.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.15.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.13.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.10.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.1.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.15.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.9.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.7.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.5.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.4.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.1.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.23.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.21.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.20.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.17.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.16.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.14.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.10.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.3.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.1.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.20.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.17.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.16.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.4.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.1.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.23.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.19.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.16.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.10.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.1.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.18.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.20.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.19.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.16.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.6.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.3.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.22.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.21.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.20.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.12.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.9.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.6.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.0.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.20.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.16.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.6.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.4.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.23.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.22.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.20.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.11.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.10.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.8.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.7.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.1.input_layernorm.bias</span><br><span class="hljs-string">INFO:megatron.core.optimizer:Setting</span> <span class="hljs-string">up</span> <span class="hljs-string">optimizer</span> <span class="hljs-string">with</span> <span class="hljs-string">config</span> <span class="hljs-string">OptimizerConfig(optimizer=&#x27;adam&#x27;,</span> <span class="hljs-string">lr=6e-05,</span> <span class="hljs-string">min_lr=6e-06,</span> <span class="hljs-string">decoupled_lr=None,</span> <span class="hljs-string">decoupled_min_lr=None,</span> <span class="hljs-string">weight_decay=0.1,</span> <span class="hljs-string">fp8_recipe=&#x27;delayed&#x27;,</span> <span class="hljs-string">fp16=True,</span> <span class="hljs-string">bf16=False,</span> <span class="hljs-string">reuse_grad_buf_for_mxfp8_param_ag=False,</span> <span class="hljs-string">params_dtype=torch.float16,</span> <span class="hljs-string">use_precision_aware_optimizer=False,</span> <span class="hljs-string">store_param_remainders=True,</span> <span class="hljs-string">main_grads_dtype=torch.float32,</span> <span class="hljs-string">main_params_dtype=torch.float32,</span> <span class="hljs-string">exp_avg_dtype=torch.float32,</span> <span class="hljs-string">exp_avg_sq_dtype=torch.float32,</span> <span class="hljs-string">loss_scale=None,</span> <span class="hljs-string">initial_loss_scale=4294967296,</span> <span class="hljs-string">min_loss_scale=1.0,</span> <span class="hljs-string">loss_scale_window=1000,</span> <span class="hljs-string">hysteresis=2,</span> <span class="hljs-string">adam_beta1=0.9,</span> <span class="hljs-string">adam_beta2=0.95,</span> <span class="hljs-string">adam_eps=1e-08,</span> <span class="hljs-string">sgd_momentum=0.9,</span> <span class="hljs-string">use_distributed_optimizer=False,</span> <span class="hljs-string">overlap_param_gather=False,</span> <span class="hljs-string">overlap_param_gather_with_optimizer_step=False,</span> <span class="hljs-string">use_megatron_fsdp=False,</span> <span class="hljs-string">optimizer_cpu_offload=False,</span> <span class="hljs-string">optimizer_offload_fraction=1.0,</span> <span class="hljs-string">use_torch_optimizer_for_cpu_offload=False,</span> <span class="hljs-string">overlap_cpu_optimizer_d2h_h2d=False,</span> <span class="hljs-string">pin_cpu_grads=True,</span> <span class="hljs-string">pin_cpu_params=True,</span> <span class="hljs-string">clip_grad=1.0,</span> <span class="hljs-string">log_num_zeros_in_grad=False,</span> <span class="hljs-string">barrier_with_L1_time=True,</span> <span class="hljs-string">timers=&lt;megatron.core.timers.Timers</span> <span class="hljs-string">object</span> <span class="hljs-string">at</span> <span class="hljs-number">0x7f59b73c1430</span><span class="hljs-string">&gt;,</span> <span class="hljs-string">config_logger_dir=&#x27;&#x27;)</span><br><span class="hljs-string">INFO:megatron.core.optimizer_param_scheduler:&gt;</span> <span class="hljs-attr">learning rate decay style:</span> <span class="hljs-string">cosine</span><br><span class="hljs-attr">WARNING:</span> <span class="hljs-string">could</span> <span class="hljs-string">not</span> <span class="hljs-string">find</span> <span class="hljs-string">the</span> <span class="hljs-string">metadata</span> <span class="hljs-string">file</span> <span class="hljs-string">/workspace/megatron-lm/model_ckpt/gpt3_857m_tp4/latest_checkpointed_iteration.txt</span><br>    <span class="hljs-string">will</span> <span class="hljs-string">not</span> <span class="hljs-string">load</span> <span class="hljs-string">any</span> <span class="hljs-string">checkpoints</span> <span class="hljs-string">and</span> <span class="hljs-string">will</span> <span class="hljs-string">start</span> <span class="hljs-string">from</span> <span class="hljs-string">random</span><br><span class="hljs-string">(min,</span> <span class="hljs-string">max)</span> <span class="hljs-string">time</span> <span class="hljs-string">across</span> <span class="hljs-string">ranks</span> <span class="hljs-string">(ms):</span><br>    <span class="hljs-attr">load-checkpoint ................................:</span> <span class="hljs-string">(0.29,</span> <span class="hljs-number">0.30</span><span class="hljs-string">)</span><br>[<span class="hljs-string">after</span> <span class="hljs-string">model</span>, <span class="hljs-string">optimizer</span>, <span class="hljs-string">and</span> <span class="hljs-string">learning</span> <span class="hljs-string">rate</span> <span class="hljs-string">scheduler</span> <span class="hljs-string">are</span> <span class="hljs-string">built</span>] <span class="hljs-attr">datetime:</span> <span class="hljs-number">2026-01-03 04:22:36</span> <br><span class="hljs-string">&gt;</span> <span class="hljs-string">building</span> <span class="hljs-string">train,</span> <span class="hljs-string">validation,</span> <span class="hljs-string">and</span> <span class="hljs-string">test</span> <span class="hljs-string">datasets</span> <span class="hljs-string">...</span><br> <span class="hljs-string">&gt;</span> <span class="hljs-string">datasets</span> <span class="hljs-string">target</span> <span class="hljs-string">sizes</span> <span class="hljs-string">(minimum</span> <span class="hljs-string">size):</span><br>    <span class="hljs-attr">train:</span>      <span class="hljs-number">320000</span><br>    <span class="hljs-attr">validation:</span> <span class="hljs-number">3360</span><br>    <span class="hljs-attr">test:</span>       <span class="hljs-number">160</span><br><span class="hljs-string">INFO:megatron.core.datasets.blended_megatron_dataset_config:Let</span> <span class="hljs-string">split_matrix</span> <span class="hljs-string">=</span> [<span class="hljs-string">(0</span>, <span class="hljs-number">0.949</span><span class="hljs-string">)</span>, <span class="hljs-string">(0.949</span>, <span class="hljs-number">0.999</span><span class="hljs-string">)</span>, <span class="hljs-string">(0.999</span>, <span class="hljs-number">1.0</span><span class="hljs-string">)</span>]<br><span class="hljs-string">&gt;</span> <span class="hljs-string">building</span> <span class="hljs-string">train,</span> <span class="hljs-string">validation,</span> <span class="hljs-string">and</span> <span class="hljs-string">test</span> <span class="hljs-string">datasets</span> <span class="hljs-string">for</span> <span class="hljs-string">GPT</span> <span class="hljs-string">...</span><br><span class="hljs-string">INFO:megatron.core.datasets.blended_megatron_dataset_builder:Building</span> <span class="hljs-string">GPTDataset</span> <span class="hljs-string">splits</span> <span class="hljs-string">with</span> <span class="hljs-string">sizes=(320000,</span> <span class="hljs-number">3360</span><span class="hljs-string">,</span> <span class="hljs-number">160</span><span class="hljs-string">)</span> <span class="hljs-string">and</span> <span class="hljs-string">config=GPTDatasetConfig(random_seed=1234,</span> <span class="hljs-string">sequence_length=2048,</span> <span class="hljs-string">blend=([&#x27;/workspace/megatron-lm/data/TinyStoriesV2-GPT4-train_text_document&#x27;],</span> <span class="hljs-string">None),</span> <span class="hljs-string">blend_per_split=None,</span> <span class="hljs-string">multiple_validation_sets=False,</span> <span class="hljs-string">full_validation=False,</span> <span class="hljs-string">split=&#x27;949,50,1&#x27;,</span> <span class="hljs-string">split_matrix=[(0,</span> <span class="hljs-number">0.949</span><span class="hljs-string">),</span> <span class="hljs-string">(0.949,</span> <span class="hljs-number">0.999</span><span class="hljs-string">),</span> <span class="hljs-string">(0.999,</span> <span class="hljs-number">1.0</span><span class="hljs-string">)],</span> <span class="hljs-string">num_dataset_builder_threads=1,</span> <span class="hljs-string">path_to_cache=None,</span> <span class="hljs-string">mmap_bin_files=True,</span> <span class="hljs-string">mock=False,</span> <span class="hljs-string">tokenizer=&lt;megatron.training.tokenizer.tokenizer._GPT2BPETokenizer</span> <span class="hljs-string">object</span> <span class="hljs-string">at</span> <span class="hljs-number">0x7f59b702e240</span><span class="hljs-string">&gt;,</span> <span class="hljs-string">mid_level_dataset_surplus=0.005,</span> <span class="hljs-string">reset_position_ids=False,</span> <span class="hljs-string">reset_attention_mask=False,</span> <span class="hljs-string">eod_mask_loss=False,</span> <span class="hljs-string">create_attention_mask=True,</span> <span class="hljs-string">drop_last_partial_validation_sequence=True,</span> <span class="hljs-string">add_extra_token_to_sequence=True,</span> <span class="hljs-string">object_storage_cache_path=None)</span><br><span class="hljs-string">INFO:megatron.core.datasets.indexed_dataset:Load</span> <span class="hljs-string">the</span> <span class="hljs-string">_IndexReader</span> <span class="hljs-string">from</span> <span class="hljs-string">/workspace/megatron-lm/data/TinyStoriesV2-GPT4-train_text_document.idx</span><br><span class="hljs-attr">INFO:megatron.core.datasets.indexed_dataset:</span>    <span class="hljs-string">Extract</span> <span class="hljs-string">the</span> <span class="hljs-string">sequence</span> <span class="hljs-string">lengths</span><br><span class="hljs-attr">INFO:megatron.core.datasets.indexed_dataset:</span>    <span class="hljs-string">Extract</span> <span class="hljs-string">the</span> <span class="hljs-string">sequence</span> <span class="hljs-string">pointers</span><br><span class="hljs-attr">INFO:megatron.core.datasets.indexed_dataset:</span>    <span class="hljs-string">Extract</span> <span class="hljs-string">the</span> <span class="hljs-string">document</span> <span class="hljs-string">indices</span><br><span class="hljs-string">INFO:megatron.core.datasets.indexed_dataset:&gt;</span> <span class="hljs-attr">total number of sequences:</span> <span class="hljs-number">14548094</span><br><span class="hljs-string">INFO:megatron.core.datasets.indexed_dataset:&gt;</span> <span class="hljs-attr">total number of documents:</span> <span class="hljs-number">14548094</span><br><span class="hljs-string">INFO:megatron.core.datasets.gpt_dataset:Load</span> <span class="hljs-string">the</span> <span class="hljs-string">GPTDataset</span> <span class="hljs-string">train</span> <span class="hljs-string">indices</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">document</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">c376e20e5de541283d4ccc974c960cb8-GPTDataset-train-document_index.npy</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">sample</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">c376e20e5de541283d4ccc974c960cb8-GPTDataset-train-sample_index.npy</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">shuffle</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">c376e20e5de541283d4ccc974c960cb8-GPTDataset-train-shuffle_index.npy</span><br><span class="hljs-string">INFO:megatron.core.datasets.gpt_dataset:&gt;</span> <span class="hljs-attr">total number of samples:</span> <span class="hljs-number">521301</span><br><span class="hljs-string">INFO:megatron.core.datasets.gpt_dataset:Load</span> <span class="hljs-string">the</span> <span class="hljs-string">GPTDataset</span> <span class="hljs-string">valid</span> <span class="hljs-string">indices</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">document</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">f975a8258f34477c465b869135b1a202-GPTDataset-valid-document_index.npy</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">sample</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">f975a8258f34477c465b869135b1a202-GPTDataset-valid-sample_index.npy</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">shuffle</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">f975a8258f34477c465b869135b1a202-GPTDataset-valid-shuffle_index.npy</span><br><span class="hljs-string">INFO:megatron.core.datasets.gpt_dataset:&gt;</span> <span class="hljs-attr">total number of samples:</span> <span class="hljs-number">13728</span><br><span class="hljs-string">INFO:megatron.core.datasets.gpt_dataset:Load</span> <span class="hljs-string">the</span> <span class="hljs-string">GPTDataset</span> <span class="hljs-string">test</span> <span class="hljs-string">indices</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">document</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">b12f62104fc19a6d3c6c6402fedd7e04-GPTDataset-test-document_index.npy</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">sample</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">b12f62104fc19a6d3c6c6402fedd7e04-GPTDataset-test-sample_index.npy</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">shuffle</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">b12f62104fc19a6d3c6c6402fedd7e04-GPTDataset-test-shuffle_index.npy</span><br><span class="hljs-string">INFO:megatron.core.datasets.gpt_dataset:&gt;</span> <span class="hljs-attr">total number of samples:</span> <span class="hljs-number">273</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">finished</span> <span class="hljs-string">creating</span> <span class="hljs-string">GPT</span> <span class="hljs-string">datasets</span> <span class="hljs-string">...</span><br>[<span class="hljs-string">after</span> <span class="hljs-string">dataloaders</span> <span class="hljs-string">are</span> <span class="hljs-string">built</span>] <span class="hljs-attr">datetime:</span> <span class="hljs-number">2026-01-03 04:22:36</span> <br><span class="hljs-string">done</span> <span class="hljs-string">with</span> <span class="hljs-string">setup</span> <span class="hljs-string">...</span><br><span class="hljs-string">(min,</span> <span class="hljs-string">max)</span> <span class="hljs-string">time</span> <span class="hljs-string">across</span> <span class="hljs-string">ranks</span> <span class="hljs-string">(ms):</span><br>    <span class="hljs-attr">model-and-optimizer-setup ......................:</span> <span class="hljs-string">(162.43,</span> <span class="hljs-number">174.52</span><span class="hljs-string">)</span><br>    <span class="hljs-attr">train/valid/test-data-iterators-setup ..........:</span> <span class="hljs-string">(26.90,</span> <span class="hljs-number">152.19</span><span class="hljs-string">)</span><br><span class="hljs-string">training</span> <span class="hljs-string">...</span><br><span class="hljs-string">Overwriting</span> <span class="hljs-string">rerun_state_machine.current_iteration</span> <span class="hljs-string">from</span> <span class="hljs-number">-1</span> <span class="hljs-string">to</span> <span class="hljs-number">0</span><span class="hljs-string">...</span><br>[<span class="hljs-string">before</span> <span class="hljs-string">the</span> <span class="hljs-string">start</span> <span class="hljs-string">of</span> <span class="hljs-string">training</span> <span class="hljs-string">step</span>] <span class="hljs-attr">datetime:</span> <span class="hljs-number">2026-01-03 04:22:36</span> <br><span class="hljs-attr">Number of parameters in transformer block in billions:</span>  <span class="hljs-number">0.30</span><br><span class="hljs-attr">Number of parameters in embedding layers in billions:</span> <span class="hljs-number">0.05</span><br><span class="hljs-attr">Total number of parameters in billions:</span> <span class="hljs-number">0.35</span><br><span class="hljs-attr">Number of parameters in most loaded shard in billions:</span> <span class="hljs-number">0.0885</span><br><span class="hljs-attr">Theoretical memory footprints:</span> <span class="hljs-string">weight</span> <span class="hljs-string">and</span> <span class="hljs-string">optimizer=1519.18</span> <span class="hljs-string">MB</span><br> [<span class="hljs-number">2026-01-03 04:29:07</span>] <span class="hljs-string">iteration</span>      <span class="hljs-number">200</span><span class="hljs-string">/</span>   <span class="hljs-number">20000</span> <span class="hljs-string">|</span> <span class="hljs-attr">consumed samples:</span>         <span class="hljs-number">3200</span> <span class="hljs-string">|</span> <span class="hljs-string">elapsed</span> <span class="hljs-string">time</span> <span class="hljs-string">per</span> <span class="hljs-string">iteration</span> <span class="hljs-string">(ms):</span> <span class="hljs-number">1952.1</span> <span class="hljs-string">|</span> <span class="hljs-attr">learning rate:</span> <span class="hljs-number">5.999146E-05</span> <span class="hljs-string">|</span> <span class="hljs-attr">global batch size:</span>    <span class="hljs-number">16</span> <span class="hljs-string">|</span> <span class="hljs-attr">lm loss:</span> <span class="hljs-number">5.665651E+00</span> <span class="hljs-string">|</span> <span class="hljs-attr">loss scale:</span> <span class="hljs-number">8192.0</span> <span class="hljs-string">|</span> <span class="hljs-attr">grad norm:</span> <span class="hljs-number">0.833</span> <span class="hljs-string">|</span> <span class="hljs-attr">number of skipped iterations:</span>  <span class="hljs-number">20</span> <span class="hljs-string">|</span> <span class="hljs-attr">number of nan iterations:</span>   <span class="hljs-number">0</span> <span class="hljs-string">|</span><br>[<span class="hljs-string">Rank</span> <span class="hljs-number">1</span>] <span class="hljs-string">(after</span> <span class="hljs-number">200</span> <span class="hljs-string">iterations)</span> <span class="hljs-string">memory</span> <span class="hljs-string">(MB)</span> <span class="hljs-string">|</span> <span class="hljs-attr">allocated:</span> <span class="hljs-number">1935.56005859375</span> <span class="hljs-string">|</span> <span class="hljs-attr">max allocated:</span> <span class="hljs-number">12972.46923828125</span> <span class="hljs-string">|</span> <span class="hljs-attr">reserved:</span> <span class="hljs-number">14548.0</span> <span class="hljs-string">|</span> <span class="hljs-attr">max reserved:</span> <span class="hljs-number">14548.0</span><br>[<span class="hljs-string">Rank</span> <span class="hljs-number">2</span>] <span class="hljs-string">(after</span> <span class="hljs-number">200</span> <span class="hljs-string">iterations)</span> <span class="hljs-string">memory</span> <span class="hljs-string">(MB)</span> <span class="hljs-string">|</span> <span class="hljs-attr">allocated:</span> <span class="hljs-number">1911.18505859375</span> <span class="hljs-string">|</span> <span class="hljs-attr">max allocated:</span> <span class="hljs-number">12948.21923828125</span> <span class="hljs-string">|</span> <span class="hljs-attr">reserved:</span> <span class="hljs-number">14516.0</span> <span class="hljs-string">|</span> <span class="hljs-attr">max reserved:</span> <span class="hljs-number">14516.0</span><br>[<span class="hljs-string">Rank</span> <span class="hljs-number">0</span>] <span class="hljs-string">(after</span> <span class="hljs-number">200</span> <span class="hljs-string">iterations)</span> <span class="hljs-string">memory</span> <span class="hljs-string">(MB)</span> <span class="hljs-string">|</span> <span class="hljs-attr">allocated:</span> <span class="hljs-number">1923.56005859375</span> <span class="hljs-string">|</span> <span class="hljs-attr">max allocated:</span> <span class="hljs-number">12960.71923828125</span> <span class="hljs-string">|</span> <span class="hljs-attr">reserved:</span> <span class="hljs-number">14812.0</span> <span class="hljs-string">|</span> <span class="hljs-attr">max reserved:</span> <span class="hljs-number">14812.0</span><br>[<span class="hljs-string">Rank</span> <span class="hljs-number">3</span>] <span class="hljs-string">(after</span> <span class="hljs-number">200</span> <span class="hljs-string">iterations)</span> <span class="hljs-string">memory</span> <span class="hljs-string">(MB)</span> <span class="hljs-string">|</span> <span class="hljs-attr">allocated:</span> <span class="hljs-number">1911.18505859375</span> <span class="hljs-string">|</span> <span class="hljs-attr">max allocated:</span> <span class="hljs-number">12947.96923828125</span> <span class="hljs-string">|</span> <span class="hljs-attr">reserved:</span> <span class="hljs-number">14532.0</span> <span class="hljs-string">|</span> <span class="hljs-attr">max reserved:</span> <span class="hljs-number">14532.0</span><br><br></code></pre></td></tr></table></figure><h2 id="profiler文件"><a href="#profiler文件" class="headerlink" title="profiler文件"></a>profiler文件</h2><p>下图就是初始的<code>LanguageModelEmbedding</code>因为TP维度是4，并且没有Sequence并行，所以后续采用<code>reduce_from_tensor_model_parallel_region</code>来进行all reduce获得token转化结果</p><p><img src="/2026/01/08/megatron-lm-tp/image-1.png"></p><p>下图是MHA计算时最后一步通过与<code>linear_proj</code>的计算将维度转换回去的计算，这里<code>linear_proj</code>是行并行最后会调用all reduce得到结果</p><p><img src="/2026/01/08/megatron-lm-tp/image-2.png"></p><p>下图是MLP模块中最后行并行后调用all reduce的地方</p><p><img src="/2026/01/08/megatron-lm-tp/image-3.png"></p>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>Megatron-LM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Megatron-LM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读】ByteScale:Efficient Scaling of LLM Training with a  2048K Context Length on More Than 12,000</title>
    <link href="/2025/12/28/ByteScale-paper-note/"/>
    <url>/2025/12/28/ByteScale-paper-note/</url>
    
    <content type="html"><![CDATA[<ul><li><p><strong>链接：</strong><a href="https://arxiv.org/abs/2502.21231">https://arxiv.org/abs/2502.21231</a></p></li><li><p><strong>团队：ByteDance</strong></p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul><li><p>为了提升大语言模型的长上下文能力，长文本的训练是有必要的，经过通过Flash Attention技术可以将Attention相关计算的显存占用降低为O(S)，但是计算量依旧为O(S^2)，这会极大的消耗显存和算力。</p></li><li><p>为了降低显存开销，传统的方法是使用静态的数据并行叠加上下文并行，即固定数据并行和上下文并行的维度，在一个数据并行内将文本均匀地切割给各个上下文并行组的worker，在Attention计算时通过P2P通信来将各个worker的K、V进行传递。</p></li><li><p>此外由于训练数据集中的序列长度往往不同，除了padding填充外，一个常见的做法是对序列进行打包，即将较短的多个序列合并为一个不超过长度限制的长序列，如下图所示。</p><p><img src="/2025/12/28/ByteScale-paper-note/image.png"></p><ul><li>训练数据集分析如下图所示，大部分序列都是短序列，但是长序列虽然数量不多但是占据了总Token的很大一比例。</li></ul></li></ul><p><img src="/2025/12/28/ByteScale-paper-note/image-1.png"></p><ul><li><p>在这种静态上下文并行的处理下，打包带来了一系列问题：</p><ul><li><p><strong>通信冗余：</strong>&#x8FD9;些短序列需要经历和长序列一样的上下文切分和通信，但实际上它们因为足够短所以其实不需要如此；并且这还要求短序列对于O(S^2)的计算与O(S)的通信重叠。</p></li><li><p><strong>计算不平衡：</strong>&#x5C3D;管Token被均匀地分发给了各个worker，内存保持了平衡，但是就计算而言，其计算复杂度与原始序列的长度有关，对于都是短的原始序列，在O(S^2)的计算复杂度下，其计算时间也更短。下图显示了不同长度下的计算时间和FLOPs分布。</p></li></ul><p><img src="/2025/12/28/ByteScale-paper-note/image-2.png"></p><ul><li>如下图所示，DP1内的序列本可以直接在一个GPU内直接完成，但是因为打包的原因的现在需要跨GPU直接做序列并行的冗余通信了。并且由于DP1的序列都更短，所以其计算也更快，但是在DP训练中，需要等待各个DP都计算完再进行梯度同步，这就导致了如下图(c)所示就出现了DP并行训练中的bubble。</li></ul><p><img src="/2025/12/28/ByteScale-paper-note/image-3.png"></p><ul><li>此外如下图所示在PP与DP并行混合中也会因为计算不平衡出现bubble。</li></ul><p><img src="/"></p></li><li><p>为了解决上述的问题，其提出了ByteScale，专为大规模长短序列混合训练而设计。主要贡献有：</p><ul><li><p>提出了一种<strong>混合数据并行方案(HDP)</strong>&#x6765;统一DP和CP，利&#x7528;<strong>[1,DP*CP]</strong>&#x8303;围内的worker<strong>灵活处理可变长序列</strong></p></li><li><p>提出了一种<strong>通信优化方法</strong>，为了减少短序列的冗余通信提供了一种<strong>数据感知的分片能力</strong>，让<strong>每个序列由最少的设备进行处理</strong>。并且还提供了<strong>选择性卸载</strong>功能以<strong>进一步压缩长序列的通信成本</strong>。</p></li><li><p>提出了一种<strong>平衡策略</strong>，为了缓解计算不平衡，其设计了一种启发式算法，根据数<strong>据和流水线并行性</strong>的特点<strong>重新组织数据分配</strong>。此外，对于那些执行时间较短的设备，分配更多的微批次，而不是在静态系统设计下分配相同的数量。</p></li><li><p>在具有超过12,000个GPU的生产集群上进行实验，将模型大小从7B扩展到141B，上下文长度从256K扩展到2048K。结果显示其训练速度是现有的训练方法的<strong>7.89倍</strong>。</p></li></ul></li></ul><h2 id="方法思路"><a href="#方法思路" class="headerlink" title="方法思路"></a>方法思路</h2><h3 id="系统概览"><a href="#系统概览" class="headerlink" title="系统概览"></a>系统概览</h3><p>ByteScale整体概览如下图所示：</p><p><img src="/2025/12/28/ByteScale-paper-note/image-5.png"></p><ul><li><p>Profiler目的是分析环境、模型配置、数据分布，并为其他组件构建成本模型。</p></li><li><p>Communication Optimizer旨在通过数据感知分片、动态通信和选择性卸载来提高短序列和长序列的通信效率。</p></li><li><p>Balance Scheduler旨在通过并行感知的数据分配来解决计算不平衡问题。</p></li></ul><h3 id="Communication-Optimizer"><a href="#Communication-Optimizer" class="headerlink" title="Communication Optimizer"></a>Communication Optimizer</h3><h4 id="数据感知分片与通信"><a href="#数据感知分片与通信" class="headerlink" title="数据感知分片与通信"></a>数据感知分片与通信</h4><ul><li><p>HDP取代了DP和CP，d_HDP&#x3D;d_DP*d_CP，HDP不同于DP与CP，它允许HDP节点之间存在异构行为:</p><ul><li><strong>更灵活的通信：</strong>&#x48;DP仅要求不同的HDP rank处理相同数量的token，HDP内的worker可以直接处理一个完整的短序列，如下图（d）所示</li></ul><p><img src="/2025/12/28/ByteScale-paper-note/image-6.png"></p><ul><li><strong>更细粒度的通信</strong>：HDP的并行度并不需要完全固定，其可以使用[1,d_HDP]范围内的任意并行度，例如当d_HDP&#x3D;d_DP*d_CP时，可以使用96个进程处理768K序列，而剩余32个进程各自单独处理32个8K长度的序列。</li></ul></li><li><p><strong>NCCL缓冲区优化：</strong>&#x521B;建过多的HDP通信组会占用GPU显存，而因为上下文并行中使用的是P2P通信，所以这里直接复用一个全局的HDP进程组来进行通信</p></li><li><p><strong>优化器状态分片：</strong>&#x4E0E;DP类似，一个HDP内是完整的一个模型，所以可以服用Zero优化技术，实践中使用了Zero-1技术来分片优化器状态，如上图（a）所示</p></li></ul><h4 id="数据感知选择性卸载"><a href="#数据感知选择性卸载" class="headerlink" title="数据感知选择性卸载"></a>数据感知选择性卸载</h4><ul><li><p>为了使用尽可能少的设备处理训练序列，显存是一大问题，因为激活的大小与序列长度成正比。</p></li><li><p>故为了更好地支持长序列训练，需要将部分激活卸载到CPU内存上。正好以下两个特性也支持了这一实现</p><ul><li><p><strong>激活值是FILO模型</strong>：激活值随着前先传播而从前向后累积，而后在反向传播中从后先前逐渐使用完毕。所以可以将前面的激活先卸载到CPU中。</p></li><li><p><strong>O(S^2)计算可与O(S)显存卸载重叠</strong>：由于GPU与CPU间的PCIe带宽有限，所以卸载时间往往很慢，难以重叠，但是在足够长的序列中，因为O(S^2)计算复杂度导致了卸载时间可以被计算重叠，并且因为是其是主机内CPU与GPU间的卸载，所以也不会额外占用显存。</p></li></ul></li><li><p><strong>选择性卸载：</strong>&#x53EA;有长序列才能让卸载与计算重叠，所以需要选择性地选择长序列进行卸载。这里提出了一些选择的方法。</p></li></ul><h3 id="Balance-Scheduler"><a href="#Balance-Scheduler" class="headerlink" title="Balance Scheduler"></a>Balance Scheduler</h3><h4 id="微批次重定义"><a href="#微批次重定义" class="headerlink" title="微批次重定义"></a>微批次重定义</h4><ul><li><p>一般的梯度累积要求每个micro batch相同，但是因为每个序列的计算消耗不同，所以这会导致各个micro batch处理的时间不平衡。</p></li><li><p>故在HDP在保持global batch相同的前提先允许每个micro batch不同，如下图（a）所示</p><p><img src="/2025/12/28/ByteScale-paper-note/image-7.png"></p></li></ul><h4 id="PP平衡"><a href="#PP平衡" class="headerlink" title="PP平衡"></a>PP平衡</h4><ul><li><p>如上图（b）所示，如果各个HDP中的PP并行下的执行时间不同，会导致最后做梯度同步前出现一段bubble，完成快的等待完成慢的。</p></li><li><p>为了减少这段bubble提高计算效率，其为执行时间较短的流水线分配更多微批次以使得最终执行时间基本相同。如下图所示。</p></li></ul><p><img src="/2025/12/28/ByteScale-paper-note/image-8.png"></p><h4 id="DP平衡"><a href="#DP平衡" class="headerlink" title="DP平衡"></a>DP平衡</h4><ul><li>如果没有PP并行，那么为了DP并行中各个DP组的完成时间对齐，一个简单的方法就是直接使得在一轮batch中各个DP均使用相同长度的序列。</li></ul><h4 id="综合平衡策略"><a href="#综合平衡策略" class="headerlink" title="综合平衡策略"></a>综合平衡策略</h4><ul><li><p>首选对global batch中的序列按长度降序排列，然后划分为若干桶，每个桶的浮点运算次数总和大致相同</p></li><li><p>然后依据是DP平衡策略还是PP平衡策略来给micro batch选择序列以使得符合前述的平衡策略，如下所示</p></li></ul><p><img src="/2025/12/28/ByteScale-paper-note/image-9.png"></p><h2 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h2><ul><li><p>实验在超过12000个GPU的大型生产级GPU集群上进行</p></li><li><p>端到端测试结果如下：</p></li></ul><p><img src="/2025/12/28/ByteScale-paper-note/image-10.png"></p><ul><li>也经过case studies进行了进一步分析</li></ul><p><img src="/2025/12/28/ByteScale-paper-note/image-11.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><p>前面分析的引言写的很好，层层入扣，也很方便阅读</p></li><li><p>混合上下文并行下的问题找的很经典，解决方案最终也围绕在解决通信冗余与计算不平衡方面的问题，解决方法很朴实但有效</p></li><li><p>个人觉得这之中的系统实现肯定也是一个大难点</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>LLM</tag>
      
      <tag>MoE</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Megatron-LM源码分析（四）】-DDP数据并行</title>
    <link href="/2025/12/28/megatron-lm-ddp/"/>
    <url>/2025/12/28/megatron-lm-ddp/</url>
    
    <content type="html"><![CDATA[<p>数据并行存在多种，最简单的就是DDP，每个DP都拥有完整的模型，然后在每个batch结束后在DP间同步梯度，最后统一进行优化器更新。再复杂一些的数据宾县会上ZERO技术，将模型、梯度、优化器状态等进行切分。</p><p>这里主要关注最简单的DDP，主要关注DP并行组如何划分，如果做个各个DP读取不同的数据，以及训练过程中如何做到梯度同步。</p><h1 id="DP并行组"><a href="#DP并行组" class="headerlink" title="DP并行组"></a>DP并行组</h1><p>查看<code>megatron/core/parallel_state.py</code>中的<code>initialize_model_parallel</code>函数，可以看到其model_size计算的公式为<code>model_size = tensor_model_parallel_size * pipeline_model_parallel_size * context_parallel_size</code>，然后其<code>data_parallel_size</code>并不是直接定义出来的，而是通过<code>data_parallel_size: int = world_size // model_size</code>计算出来。</p><p>其构建DP并行组的代码如下，这里是通过<code>decoder_rank_generator</code>将所有除所属dp不同外都相同的rank组成一个dp group通信组：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;dp&#x27;</span>):<br>    group = create_group(<br>        ranks,<br>        <span class="hljs-built_in">timeout</span>=<span class="hljs-built_in">timeout</span>,<br>        pg_options=get_nccl_options(<span class="hljs-string">&quot;dp&quot;</span>, nccl_comm_cfgs),<br>        group_desc=<span class="hljs-string">&quot;DATA_PARALLEL_GROUP&quot;</span>,<br>    )<br>    <span class="hljs-keyword">if</span> create_gloo_process_groups:<br>        group_gloo = create_group(<br>            ranks, <span class="hljs-built_in">timeout</span>=<span class="hljs-built_in">timeout</span>, backend=<span class="hljs-string">&quot;gloo&quot;</span>, group_desc=<span class="hljs-string">&quot;DATA_PARALLEL_GROUP_GLOO&quot;</span><br>        )<br>    <span class="hljs-keyword">else</span>:<br>        group_gloo = None<br>    <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>        _DATA_PARALLEL_GROUP = group<br>        _DATA_PARALLEL_GROUP_GLOO = group_gloo<br>        _DATA_PARALLEL_GLOBAL_RANKS = ranks<br></code></pre></td></tr></table></figure><h1 id="数据切分"><a href="#数据切分" class="headerlink" title="数据切分"></a>数据切分</h1><p>在DP并行下，每个dp都应该获得不同的数据。</p><h2 id="数据集读取器构造流程"><a href="#数据集读取器构造流程" class="headerlink" title="数据集读取器构造流程"></a>数据集读取器构造流程</h2><p>数据集读取器的整体构造流程为：</p><ol><li><p>用户提供数据集原始文件，以及token化所需要的merge和vocab文件，然后提供split划分train、valid、test的比例，并设置<code>micro-batch-size</code>和<code>global-batch-size</code></p></li><li><p>用户通过定义<code>BlendedMegatronDatasetBuilder</code>得到将文件转化为数据集的train_ds, valid_ds, test_ds</p></li><li><p>然后将这些数据集转化为支持迭代获取一批一批数据的rain_dataloader, valid_dataloaders, test_dataloader</p></li></ol><p>Megatron-LM构建数据集读取器的具体构造顺序如下：</p><ol><li>用户需要自定义一个<code>train_valid_test_datasets_provider</code>，在该函数中构建<code>BlendedMegatronDatasetBuilder</code>获得train_ds, valid_ds, test_ds并返回，这类ds对原始数据集文件进行了包裹，支持从中读取数据。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">is_dataset_built_on_rank</span>():<br>    <span class="hljs-keyword">return</span> (<br>        parallel_state.is_pipeline_first_stage(ignore_virtual=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">or</span> parallel_state.is_pipeline_last_stage(ignore_virtual=<span class="hljs-literal">True</span>)<br>    ) <span class="hljs-keyword">and</span> parallel_state.get_tensor_model_parallel_rank() == <span class="hljs-number">0</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">core_gpt_dataset_config_from_args</span>(<span class="hljs-params">args</span>):<br>    tokenizer = get_tokenizer()<br><br>    <span class="hljs-comment"># Sometimes --data-path is too long, instead we parse it from a file.</span><br>    blend: <span class="hljs-type">Optional</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>], <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]]]]<br>    blend_per_split: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-type">Optional</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>], <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]]]]]]<br>    blend, blend_per_split = get_blend_and_blend_per_split(args)<br><br>    <span class="hljs-keyword">return</span> GPTDatasetConfig(<br>        random_seed=args.seed,<br>        sequence_length=args.seq_length,<br>        blend=blend,<br>        blend_per_split=blend_per_split,<br>        split=args.split,<br>        multiple_validation_sets=args.multiple_validation_sets,<br>        full_validation=args.full_validation,<br>        num_dataset_builder_threads=args.num_dataset_builder_threads,<br>        path_to_cache=args.data_cache_path,<br>        mmap_bin_files=args.mmap_bin_files,<br>        tokenizer=tokenizer,<br>        reset_position_ids=args.reset_position_ids,<br>        reset_attention_mask=args.reset_attention_mask,<br>        eod_mask_loss=args.eod_mask_loss,<br>        create_attention_mask=args.create_attention_mask_in_dataloader,<br>        object_storage_cache_path=args.object_storage_cache_path,<br>        mid_level_dataset_surplus=args.mid_level_dataset_surplus,<br>    )<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_valid_test_datasets_provider</span>(<span class="hljs-params">train_val_test_num_samples</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build the train test and validation datasets.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        train_val_test_num_samples : A list containing the number of samples in train test and validation.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    args = get_args()<br><br>    config = core_gpt_dataset_config_from_args(args)<br><br>    <span class="hljs-keyword">if</span> args.sft:<br>        dataset_type = SFTDataset<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">if</span> args.mock_data:<br>            dataset_type = MockGPTDataset<br>        <span class="hljs-keyword">else</span>:<br>            dataset_type = GPTDataset<br><br>    print_rank_0(<span class="hljs-string">&quot;&gt; building train, validation, and test datasets for GPT ...&quot;</span>)<br><br>    train_ds, valid_ds, test_ds = BlendedMegatronDatasetBuilder(<br>        dataset_type, train_val_test_num_samples, is_dataset_built_on_rank, config<br>    ).build()<br><br>    print_rank_0(<span class="hljs-string">&quot;&gt; finished creating GPT datasets ...&quot;</span>)<br><br>    <span class="hljs-keyword">return</span> train_ds, valid_ds, test_ds<br><br></code></pre></td></tr></table></figure><ul><li><p>将自定义的<code>train_valid_test_datasets_provider</code>作为参数传递给<code>pretrain</code>核心训练函数，然后在<code>pretrain</code>中借助<code>build_train_valid_test_data_iterators</code>函数构造出数据集迭代器：train_data_iterator,valid_data_iterator, test_data_iterator。注意如果是pp并行中启用了vp，那么需要多个iterators。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br><br>    <span class="hljs-comment"># Temporary for transition to core datasets</span><br>    train_valid_test_datasets_provider.is_distributed = <span class="hljs-literal">True</span><br><br>    <span class="hljs-comment"># Optionally enable inprocess restart on pretrain</span><br>    pretrain, store = inprocess_restart.maybe_wrap_for_inprocess_restart(pretrain)<br><br>    pretrain(<br>        train_valid_test_datasets_provider,<br>        model_provider,<br>        ModelType.encoder_or_decoder,<br>        forward_step,<br>        args_defaults=&#123;<span class="hljs-string">&#x27;tokenizer_type&#x27;</span>: <span class="hljs-string">&#x27;GPT2BPETokenizer&#x27;</span>&#125;,<br>        extra_args_provider=add_modelopt_args <span class="hljs-keyword">if</span> has_nvidia_modelopt <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>        store=store,<br>    )<br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pretrain</span>(<span class="hljs-params">...</span>):<br>    ...<br>    <span class="hljs-keyword">if</span> args.virtual_pipeline_model_parallel_size <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        train_data_iterator = []<br>        valid_data_iterator = []<br>        test_data_iterator = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(model)):<br>            iterators = build_train_valid_test_data_iterators(train_valid_test_dataset_provider)<br>            train_data_iterator.append(iterators[<span class="hljs-number">0</span>])<br>            valid_data_iterator.append(iterators[<span class="hljs-number">1</span>])<br>            test_data_iterator.append(iterators[<span class="hljs-number">2</span>])<br>    <span class="hljs-keyword">else</span>:<br>        train_data_iterator, valid_data_iterator, test_data_iterator = (<br>            build_train_valid_test_data_iterators(train_valid_test_dataset_provider)<br>        )<br>    ...<br><br></code></pre></td></tr></table></figure><ol><li><p><code>build_train_valid_test_data_iterators</code>中首先会通过<code>build_train_valid_test_data_loaders</code>获取 train_dataloader, valid_dataloaders, test_dataloader</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_train_valid_test_data_iterators</span>(<span class="hljs-params">build_train_valid_test_datasets_provider</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build pretraining data iterators.&quot;&quot;&quot;</span><br><br>    args = get_args()<br><br>    <span class="hljs-comment"># Build loaders.</span><br>    train_dataloader, valid_dataloaders, test_dataloader = build_train_valid_test_data_loaders(<br>        build_train_valid_test_datasets_provider<br>    )<br><br>    <span class="hljs-comment"># Build iterators.</span><br>    dl_type = args.dataloader_type<br>    <span class="hljs-keyword">assert</span> dl_type <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;single&#x27;</span>, <span class="hljs-string">&#x27;cyclic&#x27;</span>, <span class="hljs-string">&#x27;external&#x27;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_iterator</span>(<span class="hljs-params">dataloader_type, dataloader</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Return dataset iterator.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> dataloader_type == <span class="hljs-string">&quot;single&quot;</span>:<br>            <span class="hljs-keyword">return</span> RerunDataIterator(<span class="hljs-built_in">iter</span>(dataloader))<br>        <span class="hljs-keyword">elif</span> dataloader_type == <span class="hljs-string">&quot;cyclic&quot;</span>:<br>            <span class="hljs-keyword">return</span> RerunDataIterator(<span class="hljs-built_in">iter</span>(cyclic_iter(dataloader)))<br>        <span class="hljs-keyword">elif</span> dataloader_type == <span class="hljs-string">&quot;external&quot;</span>:<br>            <span class="hljs-comment"># External dataloader is passed through. User is expected to define how to iterate.</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(dataloader, <span class="hljs-built_in">list</span>):<br>                <span class="hljs-keyword">return</span> [RerunDataIterator(d) <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> dataloader]<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">return</span> RerunDataIterator(dataloader)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&quot;unexpected dataloader type&quot;</span>)<br><br>    <span class="hljs-keyword">if</span> train_dataloader <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        train_data_iterator = _get_iterator(dl_type, train_dataloader)<br>    <span class="hljs-keyword">else</span>:<br>        train_data_iterator = <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># when using full validation, we need to override eval iters with the correct</span><br>    <span class="hljs-comment"># number of iterations on tp rank 0 so that it can be distributed to the other </span><br>    <span class="hljs-comment"># ranks later</span><br>    <span class="hljs-keyword">if</span> args.full_validation:<br>        <span class="hljs-keyword">if</span> args.multiple_validation_sets:<br>            <span class="hljs-keyword">if</span> valid_dataloaders[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                args.eval_iters = [<span class="hljs-literal">None</span>]*<span class="hljs-built_in">len</span>(valid_dataloaders)<br>            <span class="hljs-keyword">else</span>:<br>                args.eval_iters = [<span class="hljs-built_in">len</span>(dl) <span class="hljs-keyword">for</span> dl <span class="hljs-keyword">in</span> valid_dataloaders]<br>        <span class="hljs-keyword">else</span>:<br>            args.eval_iters = <span class="hljs-built_in">len</span>(valid_dataloaders[<span class="hljs-number">0</span>])<br><br>    <span class="hljs-keyword">if</span> args.multiple_validation_sets:<br>        <span class="hljs-keyword">if</span> valid_dataloaders[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            valid_data_iterators = [<span class="hljs-literal">None</span>] * <span class="hljs-built_in">len</span>(valid_dataloaders)<br>        <span class="hljs-keyword">else</span>:<br>            valid_dl_type = <span class="hljs-string">&quot;cyclic&quot;</span> <span class="hljs-keyword">if</span> args.full_validation <span class="hljs-keyword">else</span> dl_type<br>            <span class="hljs-built_in">print</span>(<br>                <span class="hljs-string">f&quot;[VALID DATA LOADER LENGTHS] &quot;</span><br>                <span class="hljs-string">&quot;, &quot;</span>.join(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;idx&#125;</span>: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(dl)&#125;</span>&quot;</span> <span class="hljs-keyword">for</span> idx, dl <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(valid_dataloaders))<br>            )<br>            valid_data_iterators = [<br>                _get_iterator(valid_dl_type, dl) <span class="hljs-keyword">for</span> dl <span class="hljs-keyword">in</span> valid_dataloaders<br>            ]<br>    <span class="hljs-keyword">elif</span> valid_dataloaders[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        valid_data_iterators = _get_iterator(dl_type, valid_dataloaders[<span class="hljs-number">0</span>])<br>    <span class="hljs-keyword">else</span>:<br>        valid_data_iterators = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">if</span> test_dataloader <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        test_data_iterator = _get_iterator(dl_type, test_dataloader)<br>    <span class="hljs-keyword">else</span>:<br>        test_data_iterator = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">return</span> train_data_iterator, valid_data_iterators, test_data_iterator<br><br></code></pre></td></tr></table></figure><ol><li><code>build_train_valid_test_data_loaders</code>函数如下所示：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_train_valid_test_data_loaders</span>(<span class="hljs-params">build_train_valid_test_datasets_provider</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build pretraining data loaders.&quot;&quot;&quot;</span><br><br>    args = get_args()<br><br>    (train_dataloader, valid_dataloaders, test_dataloader) = (<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>)<br><br>    print_rank_0(<span class="hljs-string">&#x27;&gt; building train, validation, and test datasets ...&#x27;</span>)<br><br>    <span class="hljs-comment"># Backward compatibility, assume fixed batch size.</span><br>    <span class="hljs-keyword">if</span> args.iteration &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> args.consumed_train_samples == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">assert</span> (<br>            args.train_samples <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>        ), <span class="hljs-string">&#x27;Only backward compatiblity support for iteration-based training&#x27;</span><br>        args.consumed_train_samples = args.iteration * args.global_batch_size<br>    <span class="hljs-keyword">if</span> args.iteration &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> args.consumed_valid_samples == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">if</span> args.train_samples <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            args.consumed_valid_samples = (<br>                (args.iteration // args.eval_interval) * args.eval_iters * args.global_batch_size<br>            )<br><br>    <span class="hljs-comment"># Rely on distributed-aware core datasets, temporary</span><br>    is_distributed = <span class="hljs-built_in">getattr</span>(build_train_valid_test_datasets_provider, <span class="hljs-string">&quot;is_distributed&quot;</span>, <span class="hljs-literal">False</span>)<br><br>    <span class="hljs-comment"># Construct the data pipeline</span><br>    <span class="hljs-keyword">if</span> is_distributed <span class="hljs-keyword">or</span> mpu.get_tensor_model_parallel_rank() == <span class="hljs-number">0</span>:<br><br>        <span class="hljs-comment"># Build datasets.</span><br>        train_ds, valid_ds, test_ds = build_train_valid_test_datasets(<br>            build_train_valid_test_datasets_provider<br>        )<br>        valid_ds = [valid_ds] <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(valid_ds, <span class="hljs-built_in">list</span>) <span class="hljs-keyword">else</span> valid_ds<br>        <br>        <span class="hljs-comment"># Build dataloders.</span><br>        train_dataloader = build_pretraining_data_loader(train_ds, args.consumed_train_samples)<br><br>        valid_dataloaders = []<br>        <span class="hljs-keyword">for</span> valid_d <span class="hljs-keyword">in</span> valid_ds:<br>            <span class="hljs-keyword">if</span> args.skip_train <span class="hljs-keyword">or</span> args.full_validation:<br>                valid_dataloaders.append(build_pretraining_data_loader(valid_d, <span class="hljs-number">0</span>))<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">if</span> args.multiple_validation_sets:<br>                    <span class="hljs-comment"># TODO(bnorick): for multiple validation sets without full validation, args.consumed_valid_samples is not</span><br>                    <span class="hljs-comment"># correct and needs to be calculated/set per validation set</span><br>                    <span class="hljs-keyword">raise</span> NotImplementedError(<span class="hljs-string">&quot;--multiple-validation-sets currently requires --full-validation&quot;</span>)<br>                valid_dataloaders.append(build_pretraining_data_loader(valid_d, args.consumed_valid_samples))<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> args.multiple_validation_sets:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(valid_dataloaders) == <span class="hljs-number">1</span><br>        test_dataloader = build_pretraining_data_loader(test_ds, <span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># Flags to know if we need to do training/validation/testing.</span><br>        do_train = train_dataloader <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> args.train_iters &gt; <span class="hljs-number">0</span><br>        do_valid = valid_dataloaders <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> (args.full_validation <span class="hljs-keyword">or</span> args.eval_iters &gt; <span class="hljs-number">0</span>)<br>        do_test = test_dataloader <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> (args.full_validation <span class="hljs-keyword">or</span> args.eval_iters &gt; <span class="hljs-number">0</span>)<br>        flags = torch.tensor(<br>            [<span class="hljs-built_in">int</span>(do_train), <span class="hljs-built_in">int</span>(do_valid), <span class="hljs-built_in">int</span>(do_test)], dtype=torch.long, device=<span class="hljs-string">&#x27;cuda&#x27;</span><br>        )<br>    <span class="hljs-keyword">else</span>:<br>        flags = torch.tensor([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], dtype=torch.long, device=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br><br>    torch.distributed.broadcast(flags, <span class="hljs-number">0</span>)<br><br>    args.do_train = <span class="hljs-built_in">getattr</span>(args, <span class="hljs-string">&quot;do_train&quot;</span>, <span class="hljs-literal">False</span>) <span class="hljs-keyword">or</span> flags[<span class="hljs-number">0</span>].item()<br>    args.do_valid = <span class="hljs-built_in">getattr</span>(args, <span class="hljs-string">&quot;do_valid&quot;</span>, <span class="hljs-literal">False</span>) <span class="hljs-keyword">or</span> flags[<span class="hljs-number">1</span>].item()<br>    args.do_test = <span class="hljs-built_in">getattr</span>(args, <span class="hljs-string">&quot;do_test&quot;</span>, <span class="hljs-literal">False</span>) <span class="hljs-keyword">or</span> flags[<span class="hljs-number">2</span>].item()<br><br>    <span class="hljs-keyword">return</span> train_dataloader, valid_dataloaders, test_dataloader<br><br></code></pre></td></tr></table></figure><ul><li><p>其首先会补全出当前已消耗的train、valid样本数量，以避免断点重训后还使用同样的数据。</p></li><li><p>然后获取<code>build_train_valid_test_datasets_provider</code>的<code>is_distributed</code>参数，如果为true才认为需要构建数据集。</p></li><li><p>然后其调用<code>build_train_valid_test_datasets</code>，通过简单计算train、valid、test需要的样本数量借助用户自定义的<code>train_valid_test_datasets_provider</code>获得train_ds, valid_ds, test_ds</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_train_valid_test_datasets</span>(<span class="hljs-params">build_train_valid_test_datasets_provider</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build pretraining datasets.&quot;&quot;&quot;</span><br>    train_valid_test_num_samples = get_train_valid_test_num_samples()<br>    print_rank_0(<span class="hljs-string">&#x27; &gt; datasets target sizes (minimum size):&#x27;</span>)<br>    print_rank_0(<span class="hljs-string">&#x27;    train:      &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(train_valid_test_num_samples[<span class="hljs-number">0</span>]))<br>    print_rank_0(<span class="hljs-string">&#x27;    validation: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(train_valid_test_num_samples[<span class="hljs-number">1</span>]))<br>    print_rank_0(<span class="hljs-string">&#x27;    test:       &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(train_valid_test_num_samples[<span class="hljs-number">2</span>]))<br>    <span class="hljs-keyword">return</span> build_train_valid_test_datasets_provider(train_valid_test_num_samples)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_train_valid_test_num_samples</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;Train/valid/test num samples.&quot;&quot;&quot;</span><br><br>    args = get_args()<br><br>    <span class="hljs-comment"># Number of train/valid/test samples.</span><br>    <span class="hljs-keyword">if</span> args.train_samples:<br>        train_samples = args.train_samples<br>    <span class="hljs-keyword">else</span>:<br>        train_samples = args.train_iters * args.global_batch_size<br>    <span class="hljs-keyword">if</span> args.full_validation:<br>        eval_samples = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">else</span>:<br>        eval_iters = (args.train_iters // args.eval_interval + <span class="hljs-number">1</span>) * args.eval_iters<br>        eval_samples = eval_iters * args.global_batch_size<br>    test_iters = args.eval_iters<br><br>    <span class="hljs-keyword">return</span> (train_samples, eval_samples, test_iters * args.global_batch_size)<br></code></pre></td></tr></table></figure><ul><li><p>然后再将train_ds, valid_ds, test_ds与当前已消耗的train、valid样本数量结合利用<code>build_pretraining_data_loader</code>构造出train_dataloader、valid_dataloaders、test_dataloader ，如果对应的dataloader不为空就设置对应args.do_train、args.do_valid、args.do_test。</p><ol><li><p><code>build_pretraining_data_loader</code>代码如下所示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_pretraining_data_loader</span>(<span class="hljs-params">dataset, consumed_samples</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build dataloader given an input dataset.&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">if</span> dataset <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>    args = get_args()<br>    <br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(dataset,<span class="hljs-string">&#x27;split&#x27;</span>):<br>        split = dataset.split<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">hasattr</span>(dataset,<span class="hljs-string">&#x27;index_split&#x27;</span>):<br>        split = dataset.index_split<br>    <span class="hljs-keyword">else</span>:<br>        split = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">if</span> split == Split.valid <span class="hljs-keyword">and</span> args.full_validation:<br>        batch_sampler = MegatronPretrainingSampler(<br>            total_samples=<span class="hljs-built_in">len</span>(dataset),<br>            consumed_samples=<span class="hljs-number">0</span>,<br>            micro_batch_size=args.micro_batch_size,<br>            data_parallel_rank=mpu.get_data_parallel_rank(),<br>            data_parallel_size=mpu.get_data_parallel_world_size())<br>    <span class="hljs-keyword">elif</span> args.dataloader_type == <span class="hljs-string">&#x27;single&#x27;</span>:<br>        <span class="hljs-comment"># Megatron sampler</span><br>        batch_sampler = MegatronPretrainingSampler(<br>            total_samples=<span class="hljs-built_in">len</span>(dataset),<br>            consumed_samples=consumed_samples,<br>            micro_batch_size=args.micro_batch_size,<br>            data_parallel_rank=mpu.get_data_parallel_rank(),<br>            data_parallel_size=mpu.get_data_parallel_world_size())<br>    <span class="hljs-keyword">elif</span> args.dataloader_type == <span class="hljs-string">&#x27;cyclic&#x27;</span>:<br>        batch_sampler = MegatronPretrainingRandomSampler(<br>            dataset,<br>            total_samples=<span class="hljs-built_in">len</span>(dataset),<br>            consumed_samples=consumed_samples,<br>            micro_batch_size=args.micro_batch_size,<br>            data_parallel_rank=mpu.get_data_parallel_rank(),<br>            data_parallel_size=mpu.get_data_parallel_world_size(),<br>            data_sharding=args.data_sharding)<br>    <span class="hljs-keyword">elif</span> args.dataloader_type == <span class="hljs-string">&quot;external&quot;</span>:<br>        <span class="hljs-comment"># External dataloaders are passed through. User is expected to provide a</span><br>        <span class="hljs-comment"># torch-compatible dataloader and define samplers, if needed.</span><br>        <span class="hljs-keyword">return</span> dataset<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">&#x27;&#123;&#125; dataloader type is not supported.&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                args.dataloader_type))<br><br>    <span class="hljs-comment"># Torch dataloader.</span><br>    <span class="hljs-keyword">return</span> torch.utils.data.DataLoader(dataset,<br>                                       batch_sampler=batch_sampler,<br>                                       num_workers=args.num_workers,<br>                                       pin_memory=<span class="hljs-literal">True</span>,<br>                                       persistent_workers=<span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> args.num_workers &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span>,<br>                                       )<br></code></pre></td></tr></table></figure></li><li><p>这里有Smapler，其中MegatronPretrainingSampler是按序读取、可恢复的Sampler，MegatronPretrainingRandomSampler是随机采样（基于当前epoch做随机数）、可无限循环的Smapler。</p></li><li><p>最后基于Samplerydataset返回了标准的<code>torch.utils.data.DataLoader</code></p></li><li><p>注意这里对于valid数据集且需要full_validation，即数据全跑一遍的情况，构建了consumed_samples&#x3D;0的MegatronPretrainingSampler。此外也支持通过<code>args.dataloader_type == &quot;external&quot;</code>自定义Dataloader</p></li></ol></li></ul></li><li><p>如果dataloader非空就通过<code>_get_iterator</code>依据<code>dataloader_type</code>对齐进行包装，包装成<code>RerunDataIterator</code>以支持rerun容错重跑</p></li><li><p>注意对于valid_dataloaders，如果参数配置了<code>full_validation</code>需要更新<code>eval_iters</code>为全部的iters</p></li></ol></li><li><p>train_data_iterator与valid_data_iterator会被传入到<code>train</code>函数中进行训练，test_data_iterator会在训练完后如果配置了<code>args.do_test</code>就最最终的测试。</p></li></ul><h2 id="数据集构造关键类介绍"><a href="#数据集构造关键类介绍" class="headerlink" title="数据集构造关键类介绍"></a>数据集构造关键类介绍</h2><h3 id="BlendedMegatronDatasetBuilder"><a href="#BlendedMegatronDatasetBuilder" class="headerlink" title="BlendedMegatronDatasetBuilder"></a><code>BlendedMegatronDatasetBuilder</code></h3><p><code>BlendedMegatronDatasetBuilder</code>主要是支持数据集混合功能，例如将常识数据集与代码数据集混合，其代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BlendedMegatronDatasetBuilder</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Builder class for the BlendedDataset and MegatronDataset classes</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        cls (Type[MegatronDataset]): The class to instantiate, must inherit from MegatronDataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        sizes (List[Optional[int]]): The minimum total number of samples to draw, or None, per split</span><br><span class="hljs-string"></span><br><span class="hljs-string">        is_built_on_rank (Callable): A callable which returns True if the dataset should be built on</span><br><span class="hljs-string">            the current rank and False otherwise. It should be Megatron Core parallelism aware i.e.</span><br><span class="hljs-string">            global rank, local group rank, and virtual rank may inform its return value.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        config (BlendedMegatronDatasetConfig): The config object which informs dataset creation</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        cls: <span class="hljs-type">Type</span>[MidLevelDataset],</span><br><span class="hljs-params">        sizes: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>],</span><br><span class="hljs-params">        is_built_on_rank: <span class="hljs-type">Callable</span>,</span><br><span class="hljs-params">        config: BlendedMegatronDatasetConfig,</span><br><span class="hljs-params">    </span>):<br>        self.cls = cls<br>        self.sizes = sizes<br>        self.is_built_on_rank = is_built_on_rank<br>        self.config = config<br><br>        log_single_rank(<br>            logger,<br>            logging.INFO,<br>            <span class="hljs-string">f&quot;Building <span class="hljs-subst">&#123;cls.__name__&#125;</span> splits with sizes=<span class="hljs-subst">&#123;self.sizes&#125;</span> and config=<span class="hljs-subst">&#123;self.config&#125;</span>&quot;</span>,<br>        )<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.config.mock:<br>            <span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> Split:<br>                size_is_none = self.sizes[split.value] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">if</span> self.config.blend_per_split <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                    weights_are_none = self.config.blend[<span class="hljs-number">1</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">if</span> self.config.blend_per_split[split.value] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                        <span class="hljs-keyword">continue</span><br>                    weights_are_none = self.config.blend_per_split[split.value][<span class="hljs-number">1</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">if</span> size_is_none:<br>                    <span class="hljs-keyword">assert</span> weights_are_none, <span class="hljs-string">f&quot;&quot;&quot;size_is_none =&gt; weights_are_none fails </span><br><span class="hljs-string">                    for <span class="hljs-subst">&#123;split.name&#125;</span> split</span><br><span class="hljs-string">                    This can occur with multiple validation sets if datasets have weights&quot;&quot;&quot;</span><br><br>        <span class="hljs-keyword">if</span> torch.distributed.is_initialized():<br>            gb_rank = torch.distributed.get_rank()<br>            <span class="hljs-keyword">if</span> gb_rank == <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">assert</span> (<br>                    self.is_built_on_rank()<br>                ), <span class="hljs-string">&quot;is_built_on_rank must return True when global rank = 0&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">Optional</span>[TopLevelDataset]]:<br>        <span class="hljs-string">&quot;&quot;&quot;Build all dataset splits according to the provided blend(s)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This method is distributed-aware and must be called on all ranks.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        The dataset splits returned can vary according to the config. Supply config.blend and</span><br><span class="hljs-string">        config.split to build BlendedDataset and/or MegatronDataset splits from the same</span><br><span class="hljs-string">        distribution. Supply config.blend_per_split to build BlendedDataset and/or MegatronDataset</span><br><span class="hljs-string">        splits from separate distributions. In either case, for each split, handle the following</span><br><span class="hljs-string">        cases:</span><br><span class="hljs-string"></span><br><span class="hljs-string">        (1) The split is None</span><br><span class="hljs-string">            - do nothing</span><br><span class="hljs-string"></span><br><span class="hljs-string">        (2) The split has one contributing dataset, and...</span><br><span class="hljs-string"></span><br><span class="hljs-string">            (a) &#x27;size&#x27; is not None</span><br><span class="hljs-string">                - Build a mid-level dataset with low-level dataset sampling in proportion to the</span><br><span class="hljs-string">                size</span><br><span class="hljs-string"></span><br><span class="hljs-string">            (b) &#x27;size&#x27; is None</span><br><span class="hljs-string">                - Build mid-level datasets with no excess low-level dataset sampling</span><br><span class="hljs-string"></span><br><span class="hljs-string">        (3) The split has multiple contributing datasets, and...</span><br><span class="hljs-string"></span><br><span class="hljs-string">            (a) &#x27;weights&#x27; is not None and &#x27;size&#x27; is not None</span><br><span class="hljs-string">                - Build mid-level datasets with low-level dataset sampling in proportion to their</span><br><span class="hljs-string">                weights and the size</span><br><span class="hljs-string">                - Build a top-level dataset of length marginally greater than &#x27;size&#x27; with mid-level</span><br><span class="hljs-string">                dataset sampling in proportion to their weights and the size</span><br><span class="hljs-string"></span><br><span class="hljs-string">            (b) &#x27;weights&#x27; is not None and &#x27;size&#x27; is None</span><br><span class="hljs-string">                - Error</span><br><span class="hljs-string"></span><br><span class="hljs-string">            (c) &#x27;weights&#x27; is None and &#x27;size&#x27; is not None</span><br><span class="hljs-string">                - Build mid-level datasets with no excess low-level dataset sampling</span><br><span class="hljs-string">                - Build a top-level dataset of length &#x27;size&#x27; (capped at the sum of the mid-level</span><br><span class="hljs-string">                dataset lengths) with mid-level dataset sampling in proportion to their lengths</span><br><span class="hljs-string">                and the size</span><br><span class="hljs-string"></span><br><span class="hljs-string">            (d) &#x27;weights&#x27; is None and &#x27;size&#x27; is None</span><br><span class="hljs-string">                - Build mid-level datasets with no excess low-level dataset sampling</span><br><span class="hljs-string">                - Build a top-level dataset with no excess mid-level dataset sampling</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            List[Optional[TopLevelDataset]]: A list containing a dataset instance (or None) per</span><br><span class="hljs-string">                split</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        datasets = self._build_blended_dataset_splits()<br><br>        <span class="hljs-keyword">for</span> dataset <span class="hljs-keyword">in</span> datasets:<br>            <span class="hljs-keyword">if</span> dataset <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(dataset) &gt; <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(dataset, BlendedDataset):<br>                    <span class="hljs-keyword">assert</span> dataset.size <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> dataset.size == <span class="hljs-built_in">len</span>(dataset)<br>                <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(dataset, MegatronDataset):<br>                    <span class="hljs-keyword">assert</span> dataset.num_samples <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> dataset.num_samples &lt;= <span class="hljs-built_in">len</span>(dataset)<br><br>        <span class="hljs-keyword">return</span> datasets<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_blended_dataset_splits</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">Optional</span>[TopLevelDataset]]:<br>        <span class="hljs-string">&quot;&quot;&quot;Build all dataset splits according to the provided blend(s)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        See the BlendedMegatronDatasetBuilder.build alias for more information.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            List[Optional[TopLevelDataset]]: A list containing a dataset instance (or None) per</span><br><span class="hljs-string">                split</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment">##</span><br>        <span class="hljs-comment"># Return fake &quot;mock&quot; datasets</span><br>        <span class="hljs-comment">##</span><br>        <span class="hljs-keyword">if</span> self.config.mock:<br>            split = self.config.split_matrix<br>            <span class="hljs-keyword">try</span>:<br>                <span class="hljs-keyword">return</span> self._build_megatron_dataset_splits(<span class="hljs-literal">None</span>, split, self.sizes)<br>            <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> error:<br>                <span class="hljs-keyword">raise</span> Exception(<br>                    <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;self.cls.__name__&#125;</span> failed to build as a mock data generator&quot;</span><br>                ) <span class="hljs-keyword">from</span> error<br><br>        <span class="hljs-comment">##</span><br>        <span class="hljs-comment"># All splits come from the same distribution</span><br>        <span class="hljs-comment">##</span><br>        <span class="hljs-keyword">elif</span> self.config.blend:<br>            prefixes, weights = self.config.blend<br>            <span class="hljs-keyword">if</span> weights <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                weights = normalize(weights)<br><br>            split = self.config.split_matrix<br><br>            <span class="hljs-comment"># Blend consists of a single prefix</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(prefixes) == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> weights <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">return</span> self._build_megatron_dataset_splits(prefixes[<span class="hljs-number">0</span>], split, self.sizes)<br><br>            <span class="hljs-comment"># Build the mid-level datasets</span><br>            <span class="hljs-keyword">if</span> weights <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-comment"># Build only one &quot;epoch&quot;</span><br>                sizes_per_dataset_buffer = [[<span class="hljs-literal">None</span> <span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> Split] <span class="hljs-keyword">for</span> prefix <span class="hljs-keyword">in</span> prefixes]<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># The number of samples we plan to use per dataset</span><br>                sizes_per_dataset_target = _get_size_per_split_per_dataset(weights, self.sizes)<br>                <span class="hljs-comment"># The number of samples we plan to build per dataset</span><br>                sizes_per_dataset_buffer = _get_size_per_split_per_dataset(<br>                    weights, self.sizes, surplus=self.config.mid_level_dataset_surplus<br>                )<br><br>            <span class="hljs-comment"># Build each dataset in parallel</span><br>            megatron_datasets = self._build_megatron_datasets_parallel(<br>                prefixes, split, sizes_per_dataset_buffer<br>            )<br><br>            <span class="hljs-comment"># Build the top-level datasets</span><br>            blended_datasets = [<span class="hljs-literal">None</span>] * <span class="hljs-built_in">len</span>(Split)<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(Split)):<br>                <span class="hljs-keyword">if</span> split[i] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    weights_i = weights<br>                    <span class="hljs-keyword">if</span> weights_i <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> self.sizes[i] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                        <span class="hljs-comment"># Blend according to client-specified weights and client-specified size</span><br>                        size_per_dataset = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(*sizes_per_dataset_target))[i]<br>                        size_i = <span class="hljs-built_in">sum</span>(size_per_dataset)<br>                    <span class="hljs-keyword">elif</span> weights_i <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                        <span class="hljs-comment"># Blend according to dataset sizes as-is and (maybe) client-specified size</span><br>                        <span class="hljs-keyword">try</span>:<br>                            weights_i = [<br>                                <span class="hljs-built_in">len</span>(megatron_dataset) <span class="hljs-keyword">for</span> megatron_dataset <span class="hljs-keyword">in</span> megatron_datasets[i]<br>                            ]<br>                        <span class="hljs-keyword">except</span> TypeError:<br>                            weights_i = [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> prefixes]<br>                        <span class="hljs-keyword">if</span> self.sizes[i] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                            size_i = <span class="hljs-built_in">min</span>(self.sizes[i], <span class="hljs-built_in">sum</span>(weights_i))<br>                        <span class="hljs-keyword">else</span>:<br>                            <span class="hljs-comment"># Build exhaustive indices</span><br>                            size_i = <span class="hljs-literal">None</span><br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-keyword">raise</span> ValueError(<br>                            <span class="hljs-string">&quot;Using client-specified weights requires client-specified size&quot;</span><br>                        )<br>                    blended_datasets[i] = self.build_generic_dataset(<br>                        BlendedDataset,<br>                        self.is_built_on_rank,<br>                        <span class="hljs-literal">True</span>,  <span class="hljs-comment"># synchronize_ranks, default behavior to build on rank-0 first</span><br>                        megatron_datasets[i],<br>                        weights_i,<br>                        size_i,<br>                        self.config,<br>                    )<br><br>            <span class="hljs-keyword">return</span> blended_datasets<br><br>        <span class="hljs-comment">##</span><br>        <span class="hljs-comment"># Each split comes from a separate distribution</span><br>        <span class="hljs-comment">##</span><br>        <span class="hljs-keyword">else</span>:<br>            blended_datasets = [<span class="hljs-literal">None</span>] * <span class="hljs-built_in">len</span>(Split)<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(Split)):<br>                split_spoof = [<span class="hljs-literal">None</span>] * <span class="hljs-built_in">len</span>(Split)<br>                split_spoof[i] = (<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>)<br>                sizes_spoof = [<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(Split)<br>                sizes_spoof[i] = self.sizes[i]<br><br>                <span class="hljs-comment"># Blend is provided for the split</span><br>                blend = self.config.blend_per_split[i]<br>                <span class="hljs-keyword">if</span> blend <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    prefixes, weights = blend<br>                    <span class="hljs-keyword">if</span> weights <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                        weights = normalize(weights)<br><br>                    <span class="hljs-comment"># Blend consists of a sigle prefix</span><br>                    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(prefixes) == <span class="hljs-number">1</span>:<br>                        blended_datasets[i] = self._build_megatron_dataset_splits(<br>                            prefixes[<span class="hljs-number">0</span>], split_spoof, sizes_spoof<br>                        )[i]<br>                        <span class="hljs-keyword">continue</span><br>                    <span class="hljs-keyword">elif</span> self.config.multiple_validation_sets <span class="hljs-keyword">and</span> i == Split.valid.value:<br>                        <span class="hljs-comment"># handle multiple validation sets</span><br>                        validation_datasets = []<br>                        <span class="hljs-keyword">if</span> self.config.full_validation:<br>                            <span class="hljs-comment"># verify that size is None, which causes a single epoch dataset</span><br>                            <span class="hljs-comment"># to be built</span><br>                            <span class="hljs-keyword">assert</span> sizes_spoof[i] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>                        <span class="hljs-keyword">for</span> prefix <span class="hljs-keyword">in</span> prefixes:<br>                            ds = self._build_megatron_dataset_splits(<br>                                prefix, split_spoof, sizes_spoof<br>                            )[i]<br>                            validation_datasets.append(ds)<br>                        blended_datasets[i] = validation_datasets<br>                        <span class="hljs-keyword">continue</span><br><br>                    <span class="hljs-comment"># Build mid-level datasets</span><br>                    <span class="hljs-keyword">if</span> weights <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                        sizes_per_dataset_buffer = [<br>                            [<span class="hljs-literal">None</span> <span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> Split] <span class="hljs-keyword">for</span> prefix <span class="hljs-keyword">in</span> prefixes<br>                        ]<br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-comment"># The number of samples we plan to use per dataset</span><br>                        sizes_per_dataset_target = _get_size_per_split_per_dataset(<br>                            weights, sizes_spoof<br>                        )<br>                        <span class="hljs-comment"># The number of samples we plan to build per dataset</span><br>                        sizes_per_dataset_buffer = _get_size_per_split_per_dataset(<br>                            weights, sizes_spoof, surplus=self.config.mid_level_dataset_surplus<br>                        )<br><br>                    <span class="hljs-comment"># Build each dataset in parallel</span><br>                    megatron_datasets = self._build_megatron_datasets_parallel(<br>                        prefixes, split_spoof, sizes_per_dataset_buffer<br>                    )[i]<br><br>                    <span class="hljs-comment"># Build top-level dataset</span><br>                    <span class="hljs-keyword">if</span> weights <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> self.sizes[i] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                        <span class="hljs-comment"># Blend according to client-specified weights and client-specified size</span><br>                        size_per_dataset = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(*sizes_per_dataset_target))[i]<br>                        size = <span class="hljs-built_in">sum</span>(size_per_dataset)<br>                    <span class="hljs-keyword">elif</span> weights <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                        <span class="hljs-comment"># Blend according to dataset sizes as-is and (maybe) client-specified size</span><br>                        <span class="hljs-keyword">try</span>:<br>                            weights = [<br>                                <span class="hljs-built_in">len</span>(megatron_dataset) <span class="hljs-keyword">for</span> megatron_dataset <span class="hljs-keyword">in</span> megatron_datasets<br>                            ]<br>                        <span class="hljs-keyword">except</span> TypeError:<br>                            weights = [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> prefixes]<br>                        <span class="hljs-keyword">if</span> self.sizes[i] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                            size = <span class="hljs-built_in">min</span>(self.sizes[i], <span class="hljs-built_in">sum</span>(weights))<br>                        <span class="hljs-keyword">else</span>:<br>                            <span class="hljs-comment"># Build exhaustive indices</span><br>                            size = <span class="hljs-literal">None</span><br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-keyword">raise</span> RuntimeError<br>                    blended_datasets[i] = self.build_generic_dataset(<br>                        BlendedDataset,<br>                        self.is_built_on_rank,<br>                        <span class="hljs-literal">True</span>,  <span class="hljs-comment"># synchronize_ranks, default behavior to build on rank-0 first</span><br>                        megatron_datasets,<br>                        weights,<br>                        size,<br>                        self.config,<br>                    )<br><br>            <span class="hljs-keyword">return</span> blended_datasets<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_megatron_datasets_parallel</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, prefixes: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>], split: <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>], sizes_per_dataset: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-type">Optional</span>[MegatronDataset]]]:<br>        <span class="hljs-string">&quot;&quot;&quot;Build the megatron datasets for a list of prefixes in parallel</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            prefixes (List[str]): The list of prefix strings</span><br><span class="hljs-string"></span><br><span class="hljs-string">            split (List[float]): The dataset split ratios (must sum to 1.00)</span><br><span class="hljs-string"></span><br><span class="hljs-string">            sizes_per_dataset (List[List[int]]): The number of samples to request</span><br><span class="hljs-string">            per MegatronDataset per spilt</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            List[List[Optional[MegatronDataset]]]: For each split, have a list of</span><br><span class="hljs-string">            MegatronDataset per prefix</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># Helper function to wrap the threading logic</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">_threading_helper</span>(<span class="hljs-params"></span><br><span class="hljs-params">            megatron_datasets: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-type">Optional</span>[MegatronDataset]]],</span><br><span class="hljs-params">            num_workers: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">            prefixes: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>],</span><br><span class="hljs-params">            split: <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>],</span><br><span class="hljs-params">            sizes_per_dataset: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]],</span><br><span class="hljs-params">        </span>) -&gt; <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">with</span> ThreadPoolExecutor(max_workers=num_workers) <span class="hljs-keyword">as</span> executor:<br>                all_futures = []<br>                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(prefixes)):<br>                    all_futures.append(<br>                        executor.submit(<br>                            self._build_megatron_dataset_splits,<br>                            prefixes[i],<br>                            split,<br>                            sizes_per_dataset[i],<br>                            <span class="hljs-literal">False</span>,  <span class="hljs-comment"># synchronize_ranks, barrier is called in this function</span><br>                        )<br>                    )<br>                <span class="hljs-keyword">for</span> future <span class="hljs-keyword">in</span> all_futures:<br>                    <span class="hljs-keyword">try</span>:<br>                        megatron_datasets_split = future.result()<br>                        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(megatron_datasets_split)):<br>                            megatron_datasets[j].append(megatron_datasets_split[j])<br>                    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> err:<br>                        <span class="hljs-keyword">raise</span> err<br><br>        megatron_datasets = [[] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(Split))]<br>        num_dataset_builder_threads = self.config.num_dataset_builder_threads<br><br>        <span class="hljs-keyword">if</span> torch.distributed.is_initialized():<br>            rank = torch.distributed.get_rank()<br>            <span class="hljs-comment"># First, build on rank 0</span><br>            <span class="hljs-keyword">if</span> rank == <span class="hljs-number">0</span>:<br>                num_workers = num_dataset_builder_threads<br>                <span class="hljs-keyword">if</span> num_workers &gt; <span class="hljs-number">1</span>:<br>                    <span class="hljs-comment"># since only rank 0 is running, scale up the thread count</span><br>                    <span class="hljs-comment"># but not too much to avoid overloading storage on miss path.</span><br>                    <span class="hljs-comment"># if user set num_dataset_builder_threads to 1,</span><br>                    <span class="hljs-comment"># i.e. meant for serial build, do not scale up.</span><br>                    num_workers *= <span class="hljs-built_in">min</span>(<span class="hljs-number">2</span>, <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, torch.cuda.device_count()))<br>                _threading_helper(<br>                    megatron_datasets, num_workers, prefixes, split, sizes_per_dataset<br>                )<br><br>            torch.distributed.barrier()<br><br>            <span class="hljs-comment"># Then, build on other ranks; guaranteed to be data_cache hit</span><br>            <span class="hljs-keyword">if</span> rank != <span class="hljs-number">0</span>:<br>                _threading_helper(<br>                    megatron_datasets,<br>                    num_dataset_builder_threads,<br>                    prefixes,<br>                    split,<br>                    sizes_per_dataset,<br>                )<br>        <span class="hljs-keyword">else</span>:<br>            _threading_helper(<br>                megatron_datasets, num_dataset_builder_threads, prefixes, split, sizes_per_dataset<br>            )<br><br>        <span class="hljs-keyword">return</span> megatron_datasets<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_megatron_dataset_splits</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        dataset_path: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>],</span><br><span class="hljs-params">        split: <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>],</span><br><span class="hljs-params">        sizes: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>],</span><br><span class="hljs-params">        synchronize_ranks: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">Optional</span>[MidLevelDataset]]:<br>        <span class="hljs-string">&quot;&quot;&quot;Build each MidLevelDataset split from a single LowLevelDataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            dataset_path (Optional[str]): The path on disk which defines the underlying</span><br><span class="hljs-string">                LowLevelDataset, or None for mock dataset classes</span><br><span class="hljs-string"></span><br><span class="hljs-string">            split (List[Tuple[float, float]]): The dataset split matrix</span><br><span class="hljs-string"></span><br><span class="hljs-string">            sizes (List[int]): The number of total samples to draw from each split</span><br><span class="hljs-string"></span><br><span class="hljs-string">            synchronize_ranks (bool): Whether to call barrier for rank-0 / barrier / other-ranks</span><br><span class="hljs-string">                behavior. Set to False when we enforce this behavior at higher level.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            List[Optional[MidLevelDataset]]: The MidLevelDataset (or None) per split</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># short-cut if we are not building on this rank</span><br>        <span class="hljs-keyword">if</span> torch.distributed.is_initialized() <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.is_built_on_rank():<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(Split)):<br>                <span class="hljs-keyword">if</span> split[i] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> synchronize_ranks:<br>                    torch.distributed.barrier()<br>            <span class="hljs-keyword">return</span> [<span class="hljs-literal">None</span>] * <span class="hljs-built_in">len</span>(Split)<br><br>        <span class="hljs-comment"># Build the low level dataset</span><br>        low_level_dataset = self.cls.build_low_level_dataset(dataset_path, self.config)<br><br>        <span class="hljs-comment"># Build the split indices for the low level dataset</span><br>        num_elements = self.cls.numel_low_level_dataset(low_level_dataset)<br>        split_indices = []<br>        <span class="hljs-keyword">for</span> i, _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(Split):<br>            <span class="hljs-keyword">if</span> split[i] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                beg = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">round</span>(split[i][<span class="hljs-number">0</span>] * <span class="hljs-built_in">float</span>(num_elements)))<br>                end = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">round</span>(split[i][<span class="hljs-number">1</span>] * <span class="hljs-built_in">float</span>(num_elements)))<br>                split_indices.append(numpy.arange(start=beg, stop=end, step=<span class="hljs-number">1</span>, dtype=numpy.int32))<br>            <span class="hljs-keyword">else</span>:<br>                split_indices.append(<span class="hljs-literal">None</span>)<br><br>        <span class="hljs-comment"># Build the mid level dataset</span><br>        mid_level_datasets = []<br>        <span class="hljs-keyword">for</span> i, _split <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(Split):<br>            <span class="hljs-keyword">if</span> split[i] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                mid_level_datasets.append(<span class="hljs-literal">None</span>)<br>            <span class="hljs-keyword">else</span>:<br>                mid_level_datasets.append(<br>                    self.build_generic_dataset(<br>                        self.cls,<br>                        self.is_built_on_rank,<br>                        synchronize_ranks,<br>                        low_level_dataset,<br>                        dataset_path,<br>                        split_indices[i],<br>                        sizes[i],<br>                        _split,<br>                        self.config,<br>                    )<br>                )<br><br>        <span class="hljs-keyword">return</span> mid_level_datasets<br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build_generic_dataset</span>(<span class="hljs-params"></span><br><span class="hljs-params">        cls: <span class="hljs-type">Union</span>[<span class="hljs-type">Type</span>[DistributedDataset], <span class="hljs-type">Callable</span>],</span><br><span class="hljs-params">        is_built_on_rank: <span class="hljs-type">Callable</span>,</span><br><span class="hljs-params">        synchronize_ranks: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        *args: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">Optional</span>[<span class="hljs-type">Union</span>[DistributedDataset, Iterable]]:<br>        <span class="hljs-string">&quot;&quot;&quot;Build the DistributedDataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Return None if and only if the underlying dataset class is not built on the current rank</span><br><span class="hljs-string">        and torch.distributed is initialized.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            cls (Union[Type[DistributedDataset], Callable]): The DistributedDataset class to be</span><br><span class="hljs-string">                built. In special cases, e.g. when we are building the low level dataset for a</span><br><span class="hljs-string">                RawMegatronDataset instance, we can accept a Callable which returns an Iterable.</span><br><span class="hljs-string"></span><br><span class="hljs-string">            synchronize_ranks (bool): Whether to call barrier for rank-0 / barrier / other-ranks</span><br><span class="hljs-string">                behavior. Set to False when we enforce this behavior at higher level.</span><br><span class="hljs-string"></span><br><span class="hljs-string">            args (Tuple[Any]): The positional arguments used to build the provided</span><br><span class="hljs-string">                DistributedDataset class</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Raises:</span><br><span class="hljs-string">            Exception: When the dataset constructor raises an OSError</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Optional[Union[DistributedDataset, Iterable]]: The DistributedDataset instantion, the</span><br><span class="hljs-string">                Iterable instantiation, or None</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> torch.distributed.is_initialized():<br>            rank = torch.distributed.get_rank()<br><br>            dataset = <span class="hljs-literal">None</span><br><br>            <span class="hljs-comment"># First, build on rank 0</span><br>            <span class="hljs-keyword">if</span> rank == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> is_built_on_rank():<br>                <span class="hljs-keyword">try</span>:<br>                    dataset = cls(*args)<br>                <span class="hljs-keyword">except</span> OSError <span class="hljs-keyword">as</span> err:<br>                    log = (<br>                        <span class="hljs-string">f&quot;Failed to write dataset materials to the data cache directory. Please &quot;</span><br>                        <span class="hljs-string">f&quot;supply a directory to which you have write access via the path_to_cache &quot;</span><br>                        <span class="hljs-string">f&quot;attribute in BlendedMegatronDatasetConfig and retry. Refer to the &quot;</span><br>                        <span class="hljs-string">f&quot;preserved traceback above for more information.&quot;</span><br>                    )<br>                    <span class="hljs-keyword">raise</span> Exception(log) <span class="hljs-keyword">from</span> err<br><br>            <span class="hljs-keyword">if</span> synchronize_ranks:<br>                torch.distributed.barrier()<br><br>            <span class="hljs-comment"># After, build on other ranks</span><br>            <span class="hljs-keyword">if</span> rank != <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> is_built_on_rank():<br>                dataset = cls(*args)<br><br>            <span class="hljs-keyword">return</span> dataset<br><br>        <span class="hljs-keyword">return</span> cls(*args)<br><br></code></pre></td></tr></table></figure><p>不过这里我们暂时不考虑数据集混合的情况，而是先看单一数据集下如何处理的。</p><p>单一数据集下会走进<code>_build_blended_dataset_splits</code>的如下代码：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">Blend consists of a single prefix</span><br>if len(prefixes) == 1 and weights is None:<br>    return self._build_megatron_dataset_splits(prefixes[0], split, self.sizes)<br><br></code></pre></td></tr></table></figure><p>在<code>_build_megatron_dataset_splits</code>中的处理流程如下：</p><ol><li>如果当前是不需要创建数据集的rank（<code>is_dataset_built_on_rank</code>）就进行同步等待，即只有最前和最后的pp并行的rank以及tp的第一位才需要构建，从而避免资源浪费。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">def is_dataset_built_on_rank():<br>    return (<br>        parallel_state.is_pipeline_first_stage(ignore_virtual=True)<br>        or parallel_state.is_pipeline_last_stage(ignore_virtual=True)<br>    ) and parallel_state.get_tensor_model_parallel_rank() == 0<br><br></code></pre></td></tr></table></figure><ul><li><p>然后通过<code>self.cls.build_low_level_dataset(dataset_path, self.config)</code>构建 low-level dataset，这里我们查看的是<code>GPTDataset</code>，如下所示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-meta">@staticmethod</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_low_level_dataset</span>(<span class="hljs-params">dataset_path: <span class="hljs-built_in">str</span>, config: GPTDatasetConfig</span>) -&gt; IndexedDataset:<br>    <span class="hljs-string">&quot;&quot;&quot;Abstract method implementation</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        dataset_path (str): The real path prefix to the IndexedDataset .bin and .idx files</span><br><span class="hljs-string"></span><br><span class="hljs-string">        config (GPTDatasetConfig): The config</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        IndexedDataset: The underlying IndexedDataset</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> is_object_storage_path(dataset_path):<br>        <span class="hljs-keyword">assert</span> config.object_storage_cache_path <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">return</span> IndexedDataset(<br>            dataset_path,<br>            multimodal=<span class="hljs-literal">False</span>,<br>            mmap=config.mmap_bin_files,<br>            object_storage_config=ObjectStorageConfig(<br>                path_to_idx_cache=config.object_storage_cache_path<br>            ),<br>        )<br>    <span class="hljs-keyword">return</span> IndexedDataset(dataset_path, multimodal=<span class="hljs-literal">False</span>, mmap=config.mmap_bin_files)<br><br></code></pre></td></tr></table></figure><ol><li><p>从代码可以看到其本质是构建了一个<code>IndexedDataset</code>，代码如下所示</p></li><li><p>它使用的数据是一对文件：</p><ul><li><p>path_prefix.idx：索引文件，记录每条样本（sequence）的长度、在 <code>.bin</code> 里的字节偏移（pointer）、以及（可选）multimodal 的 mode；还记录文档边界（document_indices）。</p></li><li><p>path_prefix.bin：真实 token 数据（连续存储的定长 dtype 数组）。</p></li></ul></li><li><p>其核心能力是：</p><ul><li><p><strong>提供O(1) 级别的随机访问能力：</strong>&#x901A;过 <code>.idx</code> 找到第 i 条样本在 <code>.bin</code> 中的 offset 和 length，然后从 <code>.bin</code> 读取对应 token 序列。</p></li><li><p><strong>支持切片读取</strong>：<code>dataset[start:stop]</code> 允许一次读连续多条样本（step 必须为 1），避免逐条调用导致频繁 IO。</p></li><li><p><strong>部分读取</strong>：<code>get(idx, offset, length)</code> 可以只取某条样本的一段 token（用于截断、窗口等场景）。</p></li><li><p><strong>高效读取策略可选</strong>：通过 mmap&#x3D;True&#x2F;False 选择用内存映射（_MMapBinReader）或文件读（_FileBinReader）；如果数据在对象存储（S3&#x2F;MSC），则用对应 reader 分块拉取，并把 <code>.idx</code> 缓存到本地。</p></li><li><p>**exists(path_prefix)**：检查 <code>.idx/.bin</code> 是否存在（本地或对象存储）。</p></li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">IndexedDataset</span>(torch.utils.data.Dataset):<br>    <span class="hljs-string">&quot;&quot;&quot;The low-level interface dataset class</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        path_prefix (str): The index (.idx) and data (.bin) prefix</span><br><span class="hljs-string"></span><br><span class="hljs-string">        multimodal (bool): Whether the dataset is multimodal. Defaults to False.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        mmap (bool): Whether to mmap the .bin files. Defaults to True.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        object_storage_config (Optional[ObjectStorageConfig]): Supplied only for data stored on S3</span><br><span class="hljs-string">            or MSC. IndexedDataset downloads the index (.idx) file to</span><br><span class="hljs-string">            `object_storage_config.path_to_idx_cache` and streams data from the data (.bin) file</span><br><span class="hljs-string">            in `object_storage_config.bin_chunk_nbytes` blocks. Note that `mmap` must be disabled</span><br><span class="hljs-string">            for S3 data loading. Defaults to None.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        path_prefix: <span class="hljs-built_in">str</span>,</span><br><span class="hljs-params">        multimodal: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        mmap: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        object_storage_config: <span class="hljs-type">Optional</span>[ObjectStorageConfig] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        s3_config: <span class="hljs-type">Optional</span>[S3Config] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.path_prefix: <span class="hljs-built_in">str</span><br>        self.multimodal: <span class="hljs-built_in">bool</span><br>        self.mmap: <span class="hljs-built_in">bool</span><br>        self.object_storage_config: <span class="hljs-type">Optional</span>[ObjectStorageConfig]<br><br>        self.bin_reader: _BinReader<br>        self.index: _IndexReader<br><br>        <span class="hljs-comment"># Deprecated: s3_config is deprecated, use object_storage_config instead</span><br>        object_storage_config = object_storage_config <span class="hljs-keyword">or</span> s3_config<br><br>        <span class="hljs-comment"># Cache the index file if it is stored on object storage</span><br>        <span class="hljs-keyword">if</span> is_object_storage_path(path_prefix) <span class="hljs-keyword">and</span> object_storage_config <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            idx_path = get_idx_path(path_prefix)<br>            cache_idx_path = get_index_cache_path(idx_path, object_storage_config)<br>            cache_index_file(idx_path, cache_idx_path)<br><br>        self.initialize(path_prefix, multimodal, mmap, object_storage_config)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        path_prefix: <span class="hljs-built_in">str</span>,</span><br><span class="hljs-params">        multimodal: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        mmap: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        object_storage_config: <span class="hljs-type">Optional</span>[ObjectStorageConfig],</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize the dataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This method is called by IndexedDataset.__init__ during object creation and by</span><br><span class="hljs-string">        IndexedDataset.__setstate__ during un-pickling</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            path_prefix (str): The index (.idx) and data (.bin) prefix</span><br><span class="hljs-string"></span><br><span class="hljs-string">            multimodal (bool): Whether the dataset is multimodal</span><br><span class="hljs-string"></span><br><span class="hljs-string">            mmap (bool): Whether to mmap the .bin file</span><br><span class="hljs-string"></span><br><span class="hljs-string">            object_storage_config (Optional[ObjectStorageConfig]): See IndexedDataset docstring</span><br><span class="hljs-string">                for details.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        idx_path = get_idx_path(path_prefix)<br>        bin_path = get_bin_path(path_prefix)<br>        <span class="hljs-keyword">if</span> object_storage_config <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">assert</span> os.path.exists(idx_path) <span class="hljs-keyword">and</span> os.path.exists(<br>                bin_path<br>            ), <span class="hljs-string">&quot;One or both of the .idx and .bin files cannot be found at the &quot;</span><br>            <span class="hljs-string">f&quot;path prefix <span class="hljs-subst">&#123;path_prefix&#125;</span>&quot;</span><br>        self.path_prefix = path_prefix<br>        self.multimodal = multimodal<br>        self.mmap = mmap<br>        self.object_storage_config = object_storage_config<br>        <span class="hljs-keyword">if</span> mmap:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> object_storage_config<br>            self.bin_reader = _MMapBinReader(bin_path)<br>        <span class="hljs-keyword">elif</span> object_storage_config:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> mmap<br>            self.bin_reader = OBJECT_STORAGE_BIN_READERS[get_object_storage_access(path_prefix)](<br>                bin_path, object_storage_config<br>            )<br>            idx_path = get_index_cache_path(get_idx_path(path_prefix), object_storage_config)<br>        <span class="hljs-keyword">else</span>:<br>            self.bin_reader = _FileBinReader(bin_path)<br>        self.index = _IndexReader(idx_path, self.multimodal)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getstate__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">bool</span>, <span class="hljs-built_in">bool</span>, <span class="hljs-type">Optional</span>[ObjectStorageConfig]]:<br>        <span class="hljs-string">&quot;&quot;&quot;Get the state during pickling</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Tuple[str, bool, bool, Optional[ObjectStorageConfig]]: The state tuple</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> self.path_prefix, self.multimodal, self.mmap, self.object_storage_config<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__setstate__</span>(<span class="hljs-params">self, state: <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">bool</span>, <span class="hljs-built_in">bool</span>, <span class="hljs-type">Optional</span>[ObjectStorageConfig]]</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Set the state during un-pickling</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            state (Tuple[str, bool, bool, Optional[ObjectStorageConfig]]): The state tuple</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        path_prefix, multimodal, mmap, object_storage_config = state<br>        self.initialize(path_prefix, multimodal, mmap, object_storage_config)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__del__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Clean up the object&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">del</span> self.bin_reader<br>        <span class="hljs-keyword">del</span> self.index<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Return the length of the dataset i.e. the number of sequences in the index</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            int: The length of the dataset</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.index)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, idx: <span class="hljs-type">Union</span>[<span class="hljs-built_in">int</span>, numpy.integer, <span class="hljs-built_in">slice</span>]</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">Union</span>[<br>        numpy.ndarray,<br>        <span class="hljs-type">Tuple</span>[numpy.ndarray, numpy.number],<br>        <span class="hljs-type">List</span>[numpy.ndarray],<br>        <span class="hljs-type">Tuple</span>[<span class="hljs-type">List</span>[numpy.ndarray], numpy.ndarray],<br>    ]:<br>        <span class="hljs-string">&quot;&quot;&quot;Return from the dataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            idx (Union[int, numpy.integer, slice]): The index or index slice into the dataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Raises:</span><br><span class="hljs-string">            ValueError: When the index slice is non-contiguous</span><br><span class="hljs-string"></span><br><span class="hljs-string">            TypeError: When the index is of an unexpected type</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Union[</span><br><span class="hljs-string">                numpy.ndarray,</span><br><span class="hljs-string">                Tuple[numpy.ndarray, numpy.number],</span><br><span class="hljs-string">                List[numpy.ndarray],</span><br><span class="hljs-string">                Tuple[List[numpy.ndarray], numpy.ndarray],</span><br><span class="hljs-string">            ]: The sequence tokens and modes at the index or index slice</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(idx, (<span class="hljs-built_in">int</span>, numpy.integer)):<br>            sequence_pointer, sequence_length, sequence_mode = self.index[idx]<br>            sequence = self.bin_reader.read(<br>                dtype=self.index.dtype, count=sequence_length, offset=sequence_pointer<br>            )<br>            <span class="hljs-keyword">return</span> (sequence, sequence_mode) <span class="hljs-keyword">if</span> sequence_mode <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> sequence<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(idx, <span class="hljs-built_in">slice</span>):<br>            start, stop, step = idx.indices(<span class="hljs-built_in">len</span>(self))<br>            <span class="hljs-keyword">if</span> step != <span class="hljs-number">1</span>:<br>                <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Slices into indexed_dataset must be contiguous&quot;</span>)<br>            sequence_lengths = self.index.sequence_lengths[idx]<br>            sequence_modes = (<br>                self.index.sequence_modes[idx] <span class="hljs-keyword">if</span> self.multimodal <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>  <span class="hljs-comment"># type: ignore[index]</span><br>            )<br>            sequence_offsets = <span class="hljs-built_in">list</span>(accumulate(sequence_lengths))<br>            sequences = numpy.split(<br>                self.bin_reader.read(<br>                    dtype=self.index.dtype,<br>                    count=<span class="hljs-built_in">sum</span>(sequence_lengths),<br>                    offset=self.index.sequence_pointers[start],<br>                ),<br>                sequence_offsets[:-<span class="hljs-number">1</span>],<br>            )<br>            <span class="hljs-keyword">return</span> (sequences, sequence_modes) <span class="hljs-keyword">if</span> sequence_modes <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> sequences<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> TypeError(<span class="hljs-string">&quot;Unexpected type received for idx: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">type</span>(idx)))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, idx: <span class="hljs-built_in">int</span>, offset: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span>, length: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span></span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">Union</span>[numpy.ndarray, <span class="hljs-type">Tuple</span>[numpy.ndarray, numpy.number]]:<br>        <span class="hljs-string">&quot;&quot;&quot;Retrieve a single item from the dataset with the option to only</span><br><span class="hljs-string">        return a portion of the item.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        get(idx) is the same as [idx] but get() does not support slicing.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            idx (Union[int, numpy.integer]): The index into the dataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">            offset (int): The integer token offset in the sequence</span><br><span class="hljs-string"></span><br><span class="hljs-string">            length (int): The number of tokens to grab from the sequence</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Union[numpy.ndarray, Tuple[numpy.ndarray, numpy.number]]: The sequence tokens and mode</span><br><span class="hljs-string">                at the index</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        sequence_pointer, sequence_length, sequence_mode = self.index[idx]<br>        <span class="hljs-keyword">if</span> length <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            length = sequence_length - offset<br>        sequence_pointer += offset * DType.size(self.index.dtype)<br>        sequence = self.bin_reader.read(<br>            dtype=self.index.dtype, count=length, offset=sequence_pointer<br>        )<br>        <span class="hljs-keyword">return</span> (sequence, sequence_mode) <span class="hljs-keyword">if</span> sequence_mode <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> sequence<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sequence_lengths</span>(<span class="hljs-params">self</span>) -&gt; numpy.ndarray:<br>        <span class="hljs-string">&quot;&quot;&quot;Get the sequence lengths</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            numpy.ndarray: The sequence lengths</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> self.index.sequence_lengths<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">document_indices</span>(<span class="hljs-params">self</span>) -&gt; numpy.ndarray:<br>        <span class="hljs-string">&quot;&quot;&quot;Get the document indices</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            numpy.ndarray: The document indices</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> self.index.document_indices<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_document_indices</span>(<span class="hljs-params">self</span>) -&gt; numpy.ndarray:<br>        <span class="hljs-string">&quot;&quot;&quot;Get the document indices</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This method is slated for deprecation.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            numpy.ndarray: The document indices</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> self.index.document_indices<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">set_document_indices</span>(<span class="hljs-params">self, document_indices: numpy.ndarray</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Set the document indices</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This method is slated for deprecation.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            document_indices (numpy.ndarray): The document indices</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.index.document_indices = document_indices<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sequence_modes</span>(<span class="hljs-params">self</span>) -&gt; numpy.ndarray:<br>        <span class="hljs-string">&quot;&quot;&quot;Get the sequence modes</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            numpy.ndarray: The sequence modes</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">assert</span> self.index.sequence_modes<br>        <span class="hljs-keyword">return</span> self.index.sequence_modes<br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">exists</span>(<span class="hljs-params">path_prefix: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Return whether the IndexedDataset exists on disk at the prefix</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            path_prefix (str): The prefix to the index (.idx) and data (.bin) files</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            bool: Whether the IndexedDataset exists on disk at the prefix</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> is_object_storage_path(path_prefix):<br>            <span class="hljs-keyword">return</span> dataset_exists(path_prefix, get_idx_path(path_prefix), get_bin_path(path_prefix))<br><br>        <span class="hljs-keyword">return</span> os.path.exists(get_idx_path(path_prefix)) <span class="hljs-keyword">and</span> os.path.exists(<br>            get_bin_path(path_prefix)<br>        )<br><br></code></pre></td></tr></table></figure></li><li><p>读取这个low-level dataset中一共有多少样本，然后依据split来计算按比例分割下的各实际范围</p></li><li><p>然后调用<code>build_generic_dataset</code>为各范围构建mid-level dataset并返回，这也就是我们得到的 train_ds, valid_ds, test_ds</p><ol><li><p>其首先是让rank 0构建mid-level dataset也就是实际的<code>GPTDataset</code>，然后让其他rank都等待</p></li><li><p>再让rank不为 0并且<code>is_built_on_rank</code>的rank构建<code>GPTDataset</code>，并对应返回该<code>GPTDataset</code>。</p></li></ol><blockquote><p><strong>rank 0 先构建</strong>是为了把“构建 dataset 时可能产生的共享缓存写入”变成“单进程写入 + 多进程读取”，从而避免竞态、提升缓存命中、保证 barrier 同步与流程一致性。</p></blockquote><ul><li><p><code>GPTDataset</code>的作用是：在底层 IndexedDataset（存放 <code>.bin/.idx</code> 的 token 序列）之上，构建<strong>可直接用于 GPT 自回归训练</strong>的 PyTorch <code>Dataset</code>。其主要作用有：</p><ul><li><p><strong>把原始序列拼接&#x2F;切片成固定长度样本</strong>：通过构建 document_index &#x2F; sample_index &#x2F; shuffle_index，把很多条变长序列按文档顺序拼接，然后切成长度为 sequence_length（可带 1 个 extra token）的训练样本。</p></li><li><p><strong>提供训练所需张量</strong>：getitem 返回 tokens&#x2F;labels，并生成（或复用缓存的）<code>attention_mask / loss_mask / position_ids</code>，满足左到右（causal）语言模型训练。</p></li><li><p><strong>支持可复现的 shuffle 与多 epoch 采样</strong>：用 shuffle_index 控制样本随机顺序；当 num_samples 大于一个 epoch 的可用样本时，会计算需要重复多少个 epoch。</p></li><li><p><strong>支持索引缓存</strong>：会把构建出来的 document_index.npy &#x2F; sample_index.npy &#x2F; shuffle_index.npy 写到 path_to_cache，下次启动直接加载，避免重复计算。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GPTDataset</span>(<span class="hljs-title class_ inherited__">MegatronDataset</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;The base GPT dataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        indexed_dataset (IndexedDataset): The IndexedDataset around which to build the GPTDataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        dataset_path (Optional[str]): The real path on disk to the dataset, for bookkeeping</span><br><span class="hljs-string"></span><br><span class="hljs-string">        indexed_indices (numpy.ndarray): The set of the documents indices to expose</span><br><span class="hljs-string"></span><br><span class="hljs-string">        num_samples (Optional[int]): The number of samples to draw from the indexed dataset. When</span><br><span class="hljs-string">            None, build as many samples as correspond to one epoch.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        index_split (Split): The indexed_indices Split</span><br><span class="hljs-string"></span><br><span class="hljs-string">        config (GPTDatasetConfig): The config</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        indexed_dataset: IndexedDataset,</span><br><span class="hljs-params">        dataset_path: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>],</span><br><span class="hljs-params">        indexed_indices: numpy.ndarray,</span><br><span class="hljs-params">        num_samples: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>],</span><br><span class="hljs-params">        index_split: Split,</span><br><span class="hljs-params">        config: GPTDatasetConfig,</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__(<br>            indexed_dataset, dataset_path, indexed_indices, num_samples, index_split, config<br>        )<br>        self.masks_and_position_ids_are_cacheable = <span class="hljs-keyword">not</span> <span class="hljs-built_in">any</span>(<br>            [<br>                self.config.reset_position_ids,<br>                self.config.reset_attention_mask,<br>                self.config.eod_mask_loss,<br>            ]<br>        )<br>        self.masks_and_position_ids_are_cached = <span class="hljs-literal">False</span><br>        self.cached_attention_mask = <span class="hljs-literal">None</span><br>        self.cached_loss_mask = <span class="hljs-literal">None</span><br>        self.cached_position_ids = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">try</span>:<br>            self._pad_token_id = self.config.tokenizer.pad<br>        <span class="hljs-keyword">except</span> Exception:<br>            self._pad_token_id = _PAD_TOKEN_ID<br><br>        (self.document_index, self.sample_index, self.shuffle_index) = (<br>            self._build_document_sample_shuffle_indices()<br>        )<br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">numel_low_level_dataset</span>(<span class="hljs-params">low_level_dataset: IndexedDataset</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Abstract method implementation</span><br><span class="hljs-string"></span><br><span class="hljs-string">        For GPT, the underlying IndexedDataset should be split by sequence, as opposed to, say,</span><br><span class="hljs-string">        BERT, which should be split by document</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            low_level_dataset (IndexedDataset): The underlying IndexedDataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            int: The number of unique elements in the underlying IndexedDataset</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> low_level_dataset.sequence_lengths.shape[<span class="hljs-number">0</span>]<br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build_low_level_dataset</span>(<span class="hljs-params">dataset_path: <span class="hljs-built_in">str</span>, config: GPTDatasetConfig</span>) -&gt; IndexedDataset:<br>        <span class="hljs-string">&quot;&quot;&quot;Abstract method implementation</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            dataset_path (str): The real path prefix to the IndexedDataset .bin and .idx files</span><br><span class="hljs-string"></span><br><span class="hljs-string">            config (GPTDatasetConfig): The config</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            IndexedDataset: The underlying IndexedDataset</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> is_object_storage_path(dataset_path):<br>            <span class="hljs-keyword">assert</span> config.object_storage_cache_path <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">return</span> IndexedDataset(<br>                dataset_path,<br>                multimodal=<span class="hljs-literal">False</span>,<br>                mmap=config.mmap_bin_files,<br>                object_storage_config=ObjectStorageConfig(<br>                    path_to_idx_cache=config.object_storage_cache_path<br>                ),<br>            )<br>        <span class="hljs-keyword">return</span> IndexedDataset(dataset_path, multimodal=<span class="hljs-literal">False</span>, mmap=config.mmap_bin_files)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Abstract method implementation</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            int: The length of the dataset</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> self.sample_index.shape[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, torch.Tensor]:<br>        <span class="hljs-string">&quot;&quot;&quot;Abstract method implementation</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            idx (Optioal[int]): The index into the dataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Dict[str, torch.Tensor]: The sample information wrapped in a dictionary</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> idx <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># Batch padding sequence so the index does not matter</span><br>            text, _ = self._query_document_sample_shuffle_indices(<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">else</span>:<br>            text, _ = self._query_document_sample_shuffle_indices(idx)<br><br>        text = torch.from_numpy(text).long()<br>        <span class="hljs-keyword">if</span> self.config.add_extra_token_to_sequence:<br>            tokens = text[:-<span class="hljs-number">1</span>].contiguous()<br>            labels = text[<span class="hljs-number">1</span>:].contiguous()<br>        <span class="hljs-keyword">else</span>:<br>            tokens = text<br>            labels = torch.roll(text, shifts=-<span class="hljs-number">1</span>, dims=<span class="hljs-number">0</span>)<br>            labels[-<span class="hljs-number">1</span>] = self._pad_token_id<br><br>        <span class="hljs-keyword">if</span> (<br>            <span class="hljs-keyword">not</span> self.masks_and_position_ids_are_cacheable<br>            <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> self.masks_and_position_ids_are_cached<br>        ):<br>            attention_mask, loss_mask, position_ids = _get_ltor_masks_and_position_ids(<br>                tokens,<br>                self.config.tokenizer.eod,<br>                self.config.reset_position_ids,<br>                self.config.reset_attention_mask,<br>                self.config.eod_mask_loss,<br>                self.config.create_attention_mask,<br>            )<br>            <span class="hljs-keyword">if</span> self.masks_and_position_ids_are_cacheable:<br>                self.cached_attention_mask = attention_mask<br>                self.cached_loss_mask = loss_mask<br>                self.cached_position_ids = position_ids<br>                self.masks_and_position_ids_are_cached = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">else</span>:<br>            attention_mask = self.cached_attention_mask<br>            loss_mask = self.cached_loss_mask<br>            position_ids = self.cached_position_ids<br><br>        <span class="hljs-comment"># For padded sequences, mask the loss</span><br>        loss_mask[labels == self._pad_token_id] = <span class="hljs-number">0.0</span><br><br>        <span class="hljs-comment"># For padded sequences, ensure the embedding layer can map the token ID</span><br>        tokens[tokens == self._pad_token_id] = <span class="hljs-number">0</span><br>        labels[labels == self._pad_token_id] = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># Batch padding sequence so we mask the loss</span><br>        <span class="hljs-keyword">if</span> idx <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            loss_mask = torch.zeros_like(loss_mask)<br><br>        <span class="hljs-keyword">if</span> self.config.create_attention_mask:<br>            <span class="hljs-keyword">return</span> &#123;<br>                <span class="hljs-string">&quot;tokens&quot;</span>: tokens,<br>                <span class="hljs-string">&quot;labels&quot;</span>: labels,<br>                <span class="hljs-string">&quot;attention_mask&quot;</span>: attention_mask,<br>                <span class="hljs-string">&quot;loss_mask&quot;</span>: loss_mask,<br>                <span class="hljs-string">&quot;position_ids&quot;</span>: position_ids,<br>            &#125;<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> &#123;<br>                <span class="hljs-string">&quot;tokens&quot;</span>: tokens,<br>                <span class="hljs-string">&quot;labels&quot;</span>: labels,<br>                <span class="hljs-string">&quot;loss_mask&quot;</span>: loss_mask,<br>                <span class="hljs-string">&quot;position_ids&quot;</span>: position_ids,<br>            &#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_query_document_sample_shuffle_indices</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, idx: <span class="hljs-built_in">int</span></span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">Tuple</span>[numpy.ndarray, numpy.ndarray]:<br>        <span class="hljs-string">&quot;&quot;&quot;Get the text (token ids) and document ids for a given index</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            idx (int): The index into the dataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Tuple[numpy.ndarray, numpy.ndarray]: The text ids and document ids</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Do the shuffle mapping</span><br>        idx = self.shuffle_index[idx]<br><br>        <span class="hljs-comment"># Get the beginning and end documents and offsets</span><br>        doc_index_beg, doc_index_beg_offset = self.sample_index[idx]<br>        doc_index_end, doc_index_end_offset = self.sample_index[idx + <span class="hljs-number">1</span>]<br><br>        document_ids = []<br>        sample_parts = []<br><br>        <span class="hljs-comment"># Sample spans a single document</span><br>        <span class="hljs-keyword">if</span> doc_index_beg == doc_index_end:<br>            <span class="hljs-comment"># Add the document id</span><br>            document_ids.append(self.document_index[doc_index_beg])<br><br>            <span class="hljs-comment"># Add the entire sample</span><br>            sample_parts.append(<br>                self.dataset.get(<br>                    self.document_index[doc_index_beg],<br>                    offset=doc_index_beg_offset,<br>                    length=doc_index_end_offset<br>                    - doc_index_beg_offset<br>                    + self.config.add_extra_token_to_sequence,<br>                )<br>            )<br><br>        <span class="hljs-comment"># Sample spans multiple documents</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(doc_index_beg, doc_index_end + <span class="hljs-number">1</span>):<br>                <span class="hljs-comment"># Add the document id</span><br>                document_ids.append(self.document_index[i])<br><br>                <span class="hljs-comment"># Add the sample part</span><br>                offset = <span class="hljs-number">0</span> <span class="hljs-keyword">if</span> i &gt; doc_index_beg <span class="hljs-keyword">else</span> doc_index_beg_offset<br>                length = (<br>                    <span class="hljs-literal">None</span><br>                    <span class="hljs-keyword">if</span> i &lt; doc_index_end<br>                    <span class="hljs-keyword">else</span> doc_index_end_offset + self.config.add_extra_token_to_sequence<br>                )<br>                sample_parts.append(<br>                    self.dataset.get(self.document_index[i], offset=offset, length=length)<br>                )<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(document_ids) == <span class="hljs-built_in">len</span>(<br>            sample_parts<br>        ), <span class="hljs-string">f&quot;len(document_ids) (<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(document_ids)&#125;</span>) != len(sample_parts) (<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(sample_parts)&#125;</span>)&quot;</span><br><br>        length = <span class="hljs-built_in">sum</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">len</span>, sample_parts))<br><br>        <span class="hljs-comment"># Pad the sample if necessary</span><br>        <span class="hljs-keyword">if</span> length &lt; (self.config.sequence_length + self.config.add_extra_token_to_sequence):<br>            sample_parts.append(<br>                [self._pad_token_id]<br>                * (self.config.sequence_length + self.config.add_extra_token_to_sequence - length)<br>            )<br><br>        <span class="hljs-keyword">return</span> (<br>            numpy.concatenate(sample_parts, dtype=numpy.int64),<br>            numpy.array(document_ids, dtype=numpy.int64),<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_document_sample_shuffle_indices</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">Tuple</span>[numpy.ndarray, numpy.ndarray, numpy.ndarray]:<br>        <span class="hljs-string">&quot;&quot;&quot;Build the document index, the sample index, and the shuffle index</span><br><span class="hljs-string"></span><br><span class="hljs-string">        The document index:</span><br><span class="hljs-string">            -- 1-D</span><br><span class="hljs-string">            -- An ordered array of document ids</span><br><span class="hljs-string"></span><br><span class="hljs-string">        The sample index:</span><br><span class="hljs-string">            -- 2-D</span><br><span class="hljs-string">            -- The document indices and offsets which mark the start of every sample</span><br><span class="hljs-string"></span><br><span class="hljs-string">        The shuffle index:</span><br><span class="hljs-string">            -- 1-D</span><br><span class="hljs-string">            -- A random permutation of index range of the sample index</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]: The document index, the sample</span><br><span class="hljs-string">            index, and the shuffle index</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        path_to_cache = self.config.path_to_cache<br>        <span class="hljs-keyword">if</span> path_to_cache <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.config.mock:<br>            path_to_cache = os.path.join(<br>                self.dataset.path_prefix, <span class="hljs-string">&quot;cache&quot;</span>, <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(self).__name__&#125;</span>_indices&quot;</span><br>            )<br><br>        <span class="hljs-keyword">if</span> path_to_cache:<br>            base = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;self.unique_description_hash&#125;</span>-<span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(self).__name__&#125;</span>-<span class="hljs-subst">&#123;self.index_split.name&#125;</span>&quot;</span><br>            get_path_to = <span class="hljs-keyword">lambda</span> affix: os.path.join(path_to_cache, <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;base&#125;</span>-<span class="hljs-subst">&#123;affix&#125;</span>&quot;</span>)<br>            path_to_description = get_path_to(<span class="hljs-string">&quot;description.txt&quot;</span>)<br>            path_to_document_index = get_path_to(<span class="hljs-string">&quot;document_index.npy&quot;</span>)<br>            path_to_sample_index = get_path_to(<span class="hljs-string">&quot;sample_index.npy&quot;</span>)<br>            path_to_shuffle_index = get_path_to(<span class="hljs-string">&quot;shuffle_index.npy&quot;</span>)<br>            cache_hit = <span class="hljs-built_in">all</span>(<br>                <span class="hljs-built_in">map</span>(<br>                    os.path.isfile,<br>                    [<br>                        path_to_description,<br>                        path_to_document_index,<br>                        path_to_sample_index,<br>                        path_to_shuffle_index,<br>                    ],<br>                )<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            cache_hit = <span class="hljs-literal">False</span><br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> path_to_cache <span class="hljs-keyword">or</span> (<br>            <span class="hljs-keyword">not</span> cache_hit<br>            <span class="hljs-keyword">and</span> (<span class="hljs-keyword">not</span> torch.distributed.is_initialized() <span class="hljs-keyword">or</span> torch.distributed.get_rank() == <span class="hljs-number">0</span>)<br>        ):<br>            log_single_rank(<br>                logger,<br>                logging.INFO,<br>                <span class="hljs-string">f&quot;Build and save the <span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(self).__name__&#125;</span> <span class="hljs-subst">&#123;self.index_split.name&#125;</span> indices&quot;</span>,<br>            )<br>            t_beg = time.time()<br><br>            sequence_length = self.config.sequence_length<br>            num_tokens_per_epoch = self._get_num_tokens_per_epoch()<br>            num_epochs = self._get_num_epochs(num_tokens_per_epoch)<br><br>            <span class="hljs-keyword">if</span> num_epochs == <span class="hljs-number">1</span>:<br>                separate_final_epoch = <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># Get the number of samples for the last epoch</span><br>                num_samples_sans_final_epoch = (<br>                    (num_epochs - <span class="hljs-number">1</span>) * num_tokens_per_epoch<br>                    - self.config.add_extra_token_to_sequence<br>                ) // sequence_length<br>                num_samples_from_final_epoch = self.num_samples - num_samples_sans_final_epoch<br>                num_samples_per_epoch = (<br>                    num_tokens_per_epoch - self.config.add_extra_token_to_sequence<br>                ) // sequence_length<br><br>                <span class="hljs-comment"># num_samples_from_final_epoch should be non-negative</span><br>                <span class="hljs-keyword">assert</span> num_samples_from_final_epoch &gt;= <span class="hljs-number">0</span><br><br>                <span class="hljs-comment"># num_samples_from_final_epoch should not exceed max value</span><br>                <span class="hljs-keyword">assert</span> num_samples_from_final_epoch &lt;= num_samples_per_epoch + <span class="hljs-number">1</span><br><br>                <span class="hljs-comment"># Separate the final epoch if it falls below the threshold</span><br>                threshold = <span class="hljs-number">0.80</span><br>                separate_final_epoch = num_samples_from_final_epoch &lt; <span class="hljs-built_in">int</span>(<br>                    threshold * num_samples_per_epoch<br>                )<br><br>                log_single_rank(<br>                    logger,<br>                    logging.DEBUG,<br>                    <span class="hljs-string">f&quot;&gt; num_samples_from_final_epoch: <span class="hljs-subst">&#123;num_samples_from_final_epoch&#125;</span>&quot;</span>,<br>                )<br>                log_single_rank(logger, logging.DEBUG, <span class="hljs-string">f&quot;&gt; threshold: <span class="hljs-subst">&#123;threshold&#125;</span>&quot;</span>)<br>                log_single_rank(<br>                    logger, logging.DEBUG, <span class="hljs-string">f&quot;&gt; num_samples_per_epoch: <span class="hljs-subst">&#123;num_samples_per_epoch&#125;</span>&quot;</span><br>                )<br><br>            log_single_rank(<br>                logger, logging.DEBUG, <span class="hljs-string">f&quot;&gt; separate_final_epoch: <span class="hljs-subst">&#123;separate_final_epoch&#125;</span>&quot;</span><br>            )<br><br>            numpy_random_state = numpy.random.RandomState(self.config.random_seed)<br><br>            <span class="hljs-comment"># Build the document index</span><br>            document_index = _build_document_index(<br>                self.indices, num_epochs, numpy_random_state, separate_final_epoch<br>            )<br><br>            <span class="hljs-comment"># Build the sample index</span><br>            <span class="hljs-keyword">from</span> megatron.core.datasets <span class="hljs-keyword">import</span> helpers<br><br>            <span class="hljs-keyword">if</span> self.index_split == Split.valid:<br>                drop_last_partial_sequence = self.config.drop_last_partial_validation_sequence<br>            <span class="hljs-keyword">else</span>:<br>                drop_last_partial_sequence = <span class="hljs-literal">True</span><br><br>            <span class="hljs-keyword">assert</span> document_index.dtype == numpy.int32<br>            <span class="hljs-keyword">assert</span> self.dataset.sequence_lengths.dtype == numpy.int32<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(document_index) * <span class="hljs-number">2</span> &gt; <span class="hljs-built_in">len</span>(self.dataset.sequence_lengths):<br>                <span class="hljs-comment"># If &quot;access density&quot; of sequence_lengths is high, force load the mmap-ed array</span><br>                <span class="hljs-comment"># into memory by making a copy.</span><br>                <span class="hljs-comment">#</span><br>                <span class="hljs-comment"># System performance benefits come from two aspects:</span><br>                <span class="hljs-comment">#   1. We sequentially pre-load the whole file, most of which we expect to read</span><br>                <span class="hljs-comment">#   2. The GIL is held when entering the c++ program, improving the speed of which</span><br>                <span class="hljs-comment">#      improves parallelism</span><br>                sequence_lengths_for_cpp = self.dataset.sequence_lengths.copy()<br>            <span class="hljs-keyword">else</span>:<br>                sequence_lengths_for_cpp = self.dataset.sequence_lengths<br>            sample_index = helpers.build_sample_idx(<br>                sequence_lengths_for_cpp,<br>                document_index,<br>                sequence_length,<br>                num_epochs,<br>                num_tokens_per_epoch,<br>                drop_last_partial_sequence,<br>                self.config.add_extra_token_to_sequence,<br>            )<br><br>            <span class="hljs-comment"># Build the shuffle index</span><br>            <span class="hljs-keyword">if</span> separate_final_epoch:<br>                shuffle_index = _build_shuffle_index(<br>                    num_samples_sans_final_epoch, sample_index.shape[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>, numpy_random_state<br>                )<br>            <span class="hljs-keyword">else</span>:<br>                shuffle_index = _build_shuffle_index(<br>                    sample_index.shape[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>, sample_index.shape[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>, numpy_random_state<br>                )<br><br>            <span class="hljs-keyword">if</span> path_to_cache:<br>                os.makedirs(path_to_cache, exist_ok=<span class="hljs-literal">True</span>)<br>                <span class="hljs-comment"># Write the description</span><br>                <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(path_to_description, <span class="hljs-string">&quot;wt&quot;</span>) <span class="hljs-keyword">as</span> writer:<br>                    writer.write(self.unique_description)<br>                numpy.save(path_to_document_index, document_index, allow_pickle=<span class="hljs-literal">True</span>)<br>                numpy.save(path_to_sample_index, sample_index, allow_pickle=<span class="hljs-literal">True</span>)<br>                numpy.save(path_to_shuffle_index, shuffle_index, allow_pickle=<span class="hljs-literal">True</span>)<br>            <span class="hljs-keyword">else</span>:<br>                log_single_rank(<br>                    logger,<br>                    logging.WARNING,<br>                    <span class="hljs-string">f&quot;Unable to save <span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(self).__name__&#125;</span> indexes because path_to_cache is None&quot;</span>,<br>                )<br><br>            t_end = time.time()<br>            log_single_rank(logger, logging.DEBUG, <span class="hljs-string">f&quot;\t&gt; time elapsed: <span class="hljs-subst">&#123;t_end - t_beg:4f&#125;</span> seconds&quot;</span>)<br><br>            log_single_rank(<br>                logger, logging.INFO, <span class="hljs-string">f&quot;&gt; total number of samples: <span class="hljs-subst">&#123;sample_index.shape[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>&#125;</span>&quot;</span><br>            )<br>            log_single_rank(logger, logging.INFO, <span class="hljs-string">f&quot;&gt; total number of epochs: <span class="hljs-subst">&#123;num_epochs&#125;</span>&quot;</span>)<br><br>            <span class="hljs-keyword">return</span> document_index, sample_index, shuffle_index<br><br>        log_single_rank(<br>            logger, logging.INFO, <span class="hljs-string">f&quot;Load the <span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(self).__name__&#125;</span> <span class="hljs-subst">&#123;self.index_split.name&#125;</span> indices&quot;</span><br>        )<br><br>        log_single_rank(<br>            logger,<br>            logging.INFO,<br>            <span class="hljs-string">f&quot;\tLoad the document index from <span class="hljs-subst">&#123;os.path.basename(path_to_document_index)&#125;</span>&quot;</span>,<br>        )<br>        t_beg = time.time()<br>        document_index = numpy.load(path_to_document_index, allow_pickle=<span class="hljs-literal">True</span>, mmap_mode=<span class="hljs-string">&quot;r&quot;</span>)<br>        t_end = time.time()<br>        log_single_rank(logger, logging.DEBUG, <span class="hljs-string">f&quot;\t&gt; time elapsed: <span class="hljs-subst">&#123;t_end - t_beg:4f&#125;</span> seconds&quot;</span>)<br><br>        log_single_rank(<br>            logger,<br>            logging.INFO,<br>            <span class="hljs-string">f&quot;\tLoad the sample index from <span class="hljs-subst">&#123;os.path.basename(path_to_sample_index)&#125;</span>&quot;</span>,<br>        )<br>        t_beg = time.time()<br>        sample_index = numpy.load(path_to_sample_index, allow_pickle=<span class="hljs-literal">True</span>, mmap_mode=<span class="hljs-string">&quot;r&quot;</span>)<br>        t_end = time.time()<br>        log_single_rank(logger, logging.DEBUG, <span class="hljs-string">f&quot;\t&gt; time elapsed: <span class="hljs-subst">&#123;t_end - t_beg:4f&#125;</span> seconds&quot;</span>)<br><br>        log_single_rank(<br>            logger,<br>            logging.INFO,<br>            <span class="hljs-string">f&quot;\tLoad the shuffle index from <span class="hljs-subst">&#123;os.path.basename(path_to_shuffle_index)&#125;</span>&quot;</span>,<br>        )<br>        t_beg = time.time()<br>        shuffle_index = numpy.load(path_to_shuffle_index, allow_pickle=<span class="hljs-literal">True</span>, mmap_mode=<span class="hljs-string">&quot;r&quot;</span>)<br>        t_end = time.time()<br>        log_single_rank(logger, logging.DEBUG, <span class="hljs-string">f&quot;\t&gt; time elapsed: <span class="hljs-subst">&#123;t_end - t_beg:4f&#125;</span> seconds&quot;</span>)<br><br>        log_single_rank(<br>            logger, logging.INFO, <span class="hljs-string">f&quot;&gt; total number of samples: <span class="hljs-subst">&#123;sample_index.shape[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>&#125;</span>&quot;</span><br>        )<br><br>        <span class="hljs-keyword">return</span> document_index, sample_index, shuffle_index<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_num_tokens_per_epoch</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Calculate the number of tokens in a single epoch</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            int: The number of tokens in a single epoch</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">int</span>(numpy.<span class="hljs-built_in">sum</span>(self.dataset.sequence_lengths[self.indices]))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_num_epochs</span>(<span class="hljs-params">self, num_tokens_per_epoch: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Calculate the number of epochs</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            num_tokens_per_epoch (int): The number of tokens in a single epoch</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            int: The number of epochs</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_epochs = <span class="hljs-number">1</span><br>        num_tokens = num_tokens_per_epoch<br>        <span class="hljs-keyword">if</span> self.num_samples <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span> num_epochs<br>        <span class="hljs-keyword">else</span>:<br>            num_tokens_requested = (<br>                self.num_samples * self.config.sequence_length<br>            ) + self.config.add_extra_token_to_sequence<br>            <span class="hljs-keyword">while</span> num_tokens &lt; num_tokens_requested:<br>                num_epochs += <span class="hljs-number">1</span><br>                num_tokens += num_tokens_per_epoch<br>        <span class="hljs-keyword">return</span> num_epochs<br><br></code></pre></td></tr></table></figure></li></ul></li></ul><h3 id="MegatronPretrainingSampler"><a href="#MegatronPretrainingSampler" class="headerlink" title="MegatronPretrainingSampler"></a><code>MegatronPretrainingSampler</code></h3><p>在得到可以获取单个GPT训练条目的mid-level dataset(train_ds, valid_ds, test_ds)后，<code>MegatronPretrainingSampler</code>会对其进行包裹，<code>MegatronPretrainingSampler</code>作为一个<strong>批采样器（batch sampler）</strong>：它不返回单个样本索引，而是每次 <code>yield</code> 一组索引（一个 micro-batch），供 PyTorch DataLoader(…, batch_sampler&#x3D;…) 直接使用。它的核心目标是：</p><ul><li><p>在 <strong>Data Parallel</strong> 训练中，把“全局 batch”（&#x3D; micro_batch_size * data_parallel_size）按 <strong>DP rank</strong> 切分成每个 rank 自己的 micro-batch 索引。</p></li><li><p>支持从某个 consumed_samples 开始继续取样（用于 resume &#x2F; 断点续训 &#x2F; 跳过已训练样本）。</p></li></ul><p>代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MegatronPretrainingSampler</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, total_samples, consumed_samples, micro_batch_size,</span><br><span class="hljs-params">                 data_parallel_rank, data_parallel_size, drop_last=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-comment"># Keep a copy of input params for later use.</span><br>        self.total_samples = total_samples<br>        self.consumed_samples = consumed_samples<br>        self.micro_batch_size = micro_batch_size<br>        self.data_parallel_rank = data_parallel_rank<br>        self.micro_batch_times_data_parallel_size = \<br>            self.micro_batch_size * data_parallel_size<br>        self.drop_last = drop_last<br><br>        <span class="hljs-comment"># Sanity checks.</span><br>        <span class="hljs-keyword">assert</span> self.total_samples &gt; <span class="hljs-number">0</span>, \<br>            <span class="hljs-string">&#x27;no sample to consume: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.total_samples)<br>        <span class="hljs-keyword">assert</span> self.consumed_samples &lt; self.total_samples, \<br>            <span class="hljs-string">&#x27;no samples left to consume: &#123;&#125;, &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.consumed_samples,<br>                                                        self.total_samples)<br>        <span class="hljs-keyword">assert</span> self.micro_batch_size &gt; <span class="hljs-number">0</span><br>        <span class="hljs-keyword">assert</span> data_parallel_size &gt; <span class="hljs-number">0</span><br>        <span class="hljs-keyword">assert</span> self.data_parallel_rank &lt; data_parallel_size, \<br>            <span class="hljs-string">&#x27;data_parallel_rank should be smaller than data size: &#123;&#125;, &#x27;</span> \<br>            <span class="hljs-string">&#x27;&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.data_parallel_rank, data_parallel_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.total_samples<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_start_end_idx</span>(<span class="hljs-params">self</span>):<br>        start_idx = self.data_parallel_rank * self.micro_batch_size<br>        end_idx = start_idx + self.micro_batch_size<br>        <span class="hljs-keyword">return</span> start_idx, end_idx<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__iter__</span>(<span class="hljs-params">self</span>):<br>        batch = []<br>        <span class="hljs-comment"># Last batch will be dropped if drop_last is not set False</span><br>        <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.consumed_samples, self.total_samples):<br>            batch.append(idx)<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(batch) == self.micro_batch_times_data_parallel_size:<br>                start_idx, end_idx = self.get_start_end_idx()<br>                <span class="hljs-keyword">yield</span> batch[start_idx:end_idx]<br>                batch = []<br><br>        <span class="hljs-comment"># Check the last partial batch and see drop_last is set</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(batch) &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.drop_last:<br>            start_idx, end_idx = self.get_start_end_idx()<br>            <span class="hljs-keyword">yield</span> batch[start_idx:end_idx]<br><br></code></pre></td></tr></table></figure><ul><li><p>在初始化阶段，其记录了当前所属的dp rank，并且通过<code>self.micro_batch_times_data_parallel_size =  self.micro_batch_size * data_parallel_size</code>计算了在数据并行下一次需要读取的实际数量</p></li><li><p>其获取迭代数据时，遍历范围为(self.consumed_samples, self.total_samples)，它会在其中获取<code>micro_batch_times_data_parallel_size</code>个训练样本的idx，然后依据当前所属的dp rank计算得到本rank实际需要的miro_batch个idx</p></li></ul><h3 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a><code>DataLoader</code></h3><p>在<code>build_pretraining_data_loader</code>中最终会结合mid-level dataset（<code>GPTDataset</code>）以及<code>MegatronPretrainingSampler</code>得到<code>torch.utils.data.DataLoader</code>。从而支持通过该DataLoader获取一个mocro_batch大小的训练样本。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">return torch.utils.data.DataLoader(dataset,<br>                                   batch_sampler=batch_sampler,<br>                                   num_workers=args.num_workers,<br>                                   pin_memory=True,<br>                                   persistent_workers=True if args.num_workers &gt; 0 else False,<br>                                   )<br></code></pre></td></tr></table></figure><h2 id="数据集使用流程"><a href="#数据集使用流程" class="headerlink" title="数据集使用流程"></a>数据集使用流程</h2><ol><li>在用户提供的forward_step函数中，就会传入类别为<code>torch.utils.data.DataLoader</code>的<code>data_iterator</code>，如下所示：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_step</span>(<span class="hljs-params">data_iterator, model: GPTModel, return_schedule_plan: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Forward training step.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        data_iterator : Input data iterator</span><br><span class="hljs-string">        model (GPTModel): The GPT Model</span><br><span class="hljs-string">        return_schedule_plan (bool): Whether to return the schedule plan instead of the output tensor</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    args = get_args()<br>    timers = get_timers()<br><br>    <span class="hljs-comment"># Get the batch.</span><br>    timers(<span class="hljs-string">&#x27;batch-generator&#x27;</span>, log_level=<span class="hljs-number">2</span>).start()<br>    <span class="hljs-keyword">global</span> stimer<br>    <span class="hljs-keyword">with</span> stimer(bdata=<span class="hljs-literal">True</span>):<br>        tokens, labels, loss_mask, attention_mask, position_ids = get_batch(data_iterator)<br>    timers(<span class="hljs-string">&#x27;batch-generator&#x27;</span>).stop()<br><br>    <span class="hljs-keyword">with</span> stimer:<br>        <span class="hljs-keyword">if</span> args.use_legacy_models:<br>            output_tensor = model(tokens, position_ids, attention_mask, labels=labels)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> return_schedule_plan:<br>                <span class="hljs-keyword">assert</span> args.overlap_moe_expert_parallel_comm, \<br>                    <span class="hljs-string">&quot;overlap_moe_expert_parallel_comm must be enabled to return the schedule plan&quot;</span><br>                schedule_plan = model.build_schedule_plan(<br>                    tokens, position_ids, attention_mask, labels=labels, loss_mask=loss_mask<br>                )<br>                <span class="hljs-keyword">return</span> schedule_plan, partial(loss_func, loss_mask, model=model)<br>            <span class="hljs-keyword">else</span>:<br>                output_tensor = model(<br>                    tokens, position_ids, attention_mask, labels=labels, loss_mask=loss_mask<br>                )<br><br>    <span class="hljs-comment"># [ModelOpt]: model is needed to access ModelOpt distillation losses</span><br>    <span class="hljs-keyword">return</span> output_tensor, partial(loss_func, loss_mask, model=model)<br>  <br></code></pre></td></tr></table></figure><ul><li>然后在获取训练数据的get_batch函数中会查看是否是pp并行的第一个或最后一个，如此才会去获取训练数据，获取数据依赖的是get_batch_on_this_tp_rank和get_batch_on_this_cp_rank，如下所示。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_batch</span>(<span class="hljs-params">data_iterator</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Generate a batch.&quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> this is pretty hacky, find a better way</span><br>    <span class="hljs-keyword">if</span> (<span class="hljs-keyword">not</span> parallel_state.is_pipeline_first_stage(ignore_virtual=<span class="hljs-literal">True</span>)) <span class="hljs-keyword">and</span> (<br>        <span class="hljs-keyword">not</span> parallel_state.is_pipeline_last_stage(ignore_virtual=<span class="hljs-literal">True</span>)<br>    ):<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># get batches based on the TP rank you are on</span><br>    batch = get_batch_on_this_tp_rank(data_iterator)<br><br>    <span class="hljs-comment"># slice batch along sequence dimension for context parallelism</span><br>    batch = get_batch_on_this_cp_rank(batch)<br><br>    <span class="hljs-keyword">return</span> batch.values()<br><br></code></pre></td></tr></table></figure><ul><li>首先是<code>get_batch_on_this_tp_rank</code>，由于前述在构造数据集的时候只会在tp rank&#x3D;0的时候构造，所以这里会查看当前rank，如果是tp rank&#x3D;0的worker就通过<code>next(data_iterator)</code>获取一批micro_batch数据，然后与其他tp rank≠0的worker进行broadcast，传播各数据。代码如下所示：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_batch_on_this_tp_rank</span>(<span class="hljs-params">data_iterator</span>):<br><br>    args = get_args()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_broadcast</span>(<span class="hljs-params">item</span>):<br>        <span class="hljs-keyword">if</span> item <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            torch.distributed.broadcast(<br>                item,<br>                mpu.get_tensor_model_parallel_src_rank(),<br>                group=mpu.get_tensor_model_parallel_group(),<br>            )<br><br>    <span class="hljs-keyword">if</span> mpu.get_tensor_model_parallel_rank() == <span class="hljs-number">0</span>:<br><br>        <span class="hljs-keyword">if</span> data_iterator <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            data = <span class="hljs-built_in">next</span>(data_iterator)<br>        <span class="hljs-keyword">else</span>:<br>            data = <span class="hljs-literal">None</span><br><br>        batch = &#123;<br>            <span class="hljs-string">&#x27;tokens&#x27;</span>: data[<span class="hljs-string">&quot;tokens&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>            <span class="hljs-string">&#x27;labels&#x27;</span>: data[<span class="hljs-string">&quot;labels&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>            <span class="hljs-string">&#x27;loss_mask&#x27;</span>: data[<span class="hljs-string">&quot;loss_mask&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>            <span class="hljs-string">&#x27;attention_mask&#x27;</span>: (<br>                <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;attention_mask&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> data<br>                <span class="hljs-keyword">else</span> data[<span class="hljs-string">&quot;attention_mask&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>)<br>            ),<br>            <span class="hljs-string">&#x27;position_ids&#x27;</span>: data[<span class="hljs-string">&quot;position_ids&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>        &#125;<br><br>        <span class="hljs-keyword">if</span> args.pipeline_model_parallel_size == <span class="hljs-number">1</span>:<br>            _broadcast(batch[<span class="hljs-string">&#x27;tokens&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;labels&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;loss_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;position_ids&#x27;</span>])<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_first_stage():<br>            _broadcast(batch[<span class="hljs-string">&#x27;tokens&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;position_ids&#x27;</span>])<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_last_stage():<br>            <span class="hljs-comment"># Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.</span><br>            <span class="hljs-comment"># Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need</span><br>            <span class="hljs-comment"># to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.</span><br>            <span class="hljs-keyword">if</span> args.mtp_num_layers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                _broadcast(batch[<span class="hljs-string">&#x27;tokens&#x27;</span>])<br>                _broadcast(batch[<span class="hljs-string">&#x27;position_ids&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;labels&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;loss_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])<br><br>    <span class="hljs-keyword">else</span>:<br><br>        tokens = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.int64,<br>            device=torch.cuda.current_device(),<br>        )<br>        labels = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.int64,<br>            device=torch.cuda.current_device(),<br>        )<br>        loss_mask = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.float32,<br>            device=torch.cuda.current_device(),<br>        )<br>        <span class="hljs-keyword">if</span> args.create_attention_mask_in_dataloader:<br>            attention_mask = torch.empty(<br>                (args.micro_batch_size, <span class="hljs-number">1</span>, args.seq_length, args.seq_length),<br>                dtype=torch.<span class="hljs-built_in">bool</span>,<br>                device=torch.cuda.current_device(),<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            attention_mask = <span class="hljs-literal">None</span><br>        position_ids = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.int64,<br>            device=torch.cuda.current_device(),<br>        )<br><br>        <span class="hljs-keyword">if</span> args.pipeline_model_parallel_size == <span class="hljs-number">1</span>:<br>            _broadcast(tokens)<br>            _broadcast(labels)<br>            _broadcast(loss_mask)<br>            _broadcast(attention_mask)<br>            _broadcast(position_ids)<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_first_stage():<br>            labels = <span class="hljs-literal">None</span><br>            loss_mask = <span class="hljs-literal">None</span><br><br>            _broadcast(tokens)<br>            _broadcast(attention_mask)<br>            _broadcast(position_ids)<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_last_stage():<br>            <span class="hljs-comment"># Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.</span><br>            <span class="hljs-comment"># Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need</span><br>            <span class="hljs-comment"># to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.</span><br>            <span class="hljs-keyword">if</span> args.mtp_num_layers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                _broadcast(tokens)<br>                _broadcast(position_ids)<br>            <span class="hljs-keyword">else</span>:<br>                tokens = <span class="hljs-literal">None</span><br>                position_ids = <span class="hljs-literal">None</span><br><br>            _broadcast(labels)<br>            _broadcast(loss_mask)<br>            _broadcast(attention_mask)<br><br>        batch = &#123;<br>            <span class="hljs-string">&#x27;tokens&#x27;</span>: tokens,<br>            <span class="hljs-string">&#x27;labels&#x27;</span>: labels,<br>            <span class="hljs-string">&#x27;loss_mask&#x27;</span>: loss_mask,<br>            <span class="hljs-string">&#x27;attention_mask&#x27;</span>: attention_mask,<br>            <span class="hljs-string">&#x27;position_ids&#x27;</span>: position_ids,<br>        &#125;<br><br>    <span class="hljs-keyword">return</span> batch<br><br></code></pre></td></tr></table></figure><ul><li>然后是执行<code>get_batch_on_this_cp_rank</code>，在上下文并行中，为了GPU负载均衡，我们往往采用的是之字型计算划分，如下所示。所以这里进行CP维度切分的核心思想是将序列切分为2*CP份，然后每第i个cp rank拿走前面的第i份以及倒数第i份，从而平衡计算负载。代码如下</li></ul><p><img src="/2025/12/28/megatron-lm-ddp/image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_batch_on_this_cp_rank</span>(<span class="hljs-params">batch: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Slice batch input along sequence dimension into multiple chunks,</span><br><span class="hljs-string">    which are parallelized across GPUs in a context parallel group.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># With causal masking, each token only attends to its prior tokens. Simply split</span><br>    <span class="hljs-comment"># sequence into CP chunks can result in severe load imbalance. That&#x27;s to say, chunks</span><br>    <span class="hljs-comment"># at the end of sequence have bigger workload than others. To address this issue,</span><br>    <span class="hljs-comment"># we split sequence into 2*CP ranks. Assuming CP=2, we then get 4 chunks, chunk_0</span><br>    <span class="hljs-comment"># and chunk_3 are assigned to GPU0, chunk_1 and chunk_2 are assigned to GPU1, so</span><br>    <span class="hljs-comment"># that we can get balanced workload among GPUs in a context parallel group.</span><br>    cp_size = parallel_state.get_context_parallel_world_size()<br>    <span class="hljs-keyword">if</span> cp_size &gt; <span class="hljs-number">1</span>:<br>        cp_rank = parallel_state.get_context_parallel_rank()<br>        <span class="hljs-keyword">for</span> key, val <span class="hljs-keyword">in</span> batch.items():<br>            <span class="hljs-keyword">if</span> val <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                seq_dim = <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> key != <span class="hljs-string">&quot;attention_mask&quot;</span> <span class="hljs-keyword">else</span> <span class="hljs-number">2</span><br>                val = val.view(<br>                    *val.shape[<span class="hljs-number">0</span>:seq_dim],<br>                    <span class="hljs-number">2</span> * cp_size,<br>                    val.shape[seq_dim] // (<span class="hljs-number">2</span> * cp_size),<br>                    *val.shape[(seq_dim + <span class="hljs-number">1</span>) :],<br>                )<br>                index = torch.zeros(<span class="hljs-number">2</span>, dtype=torch.int64, device=val.device)<br>                index[<span class="hljs-number">0</span>].fill_(cp_rank)<br>                index[<span class="hljs-number">1</span>].fill_(<span class="hljs-number">2</span> * cp_size - cp_rank - <span class="hljs-number">1</span>)<br>                val = val.index_select(seq_dim, index)<br>                val = val.view(*val.shape[<span class="hljs-number">0</span>:seq_dim], -<span class="hljs-number">1</span>, *val.shape[(seq_dim + <span class="hljs-number">2</span>) :])<br>                batch[key] = val<br><br>    <span class="hljs-keyword">return</span> batch<br><br></code></pre></td></tr></table></figure><ul><li>得到batch中的tokens, labels, loss_mask, attention_mask, position_ids数据后，就可以调用model进行前向传播计算了。最终的结果就是每个dp会使用不同的micro_batch数据，同一个dp中只有pp并行中的第一位和最后一位获取了数据集，这些pp中的各tp获取的都是同一份数据，如果有cp会进一步对这数据进行切分。</li></ul><h1 id="数据并行训练"><a href="#数据并行训练" class="headerlink" title="数据并行训练"></a>数据并行训练</h1><p>上述已经分析了数据并行中数据集是如何划分给各个worker的以及在一个训练step中是如何获取数据集的，现在看在训练过程中是如何进行dp间梯度同步的。</p><p>在一次训练迭代中，其<code>train_step</code>会完成一次完整的step，如下所示，其会调用<code>forward_backward_func</code>完成一个step的训练：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">losses_reduced = forward_backward_func(<br>    forward_step_func=forward_step_func,<br>    data_iterator=data_iterator,<br>    model=model,<br>    num_microbatches=get_num_microbatches(),<br>    seq_length=args.seq_length,<br>    micro_batch_size=args.micro_batch_size,<br>    decoder_seq_length=args.decoder_seq_length,<br>    forward_only=False,<br>    adjust_tensor_shapes_fn=adjust_tensor_shapes_fn,<br>)<br></code></pre></td></tr></table></figure><p><code>forward_backward_func</code>依据pp并行有多种，我们这里查看没有pp并行的<code>forward_backward_no_pipelining</code>，其代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_backward_no_pipelining</span>(<span class="hljs-params"></span><br><span class="hljs-params">    *,</span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    data_iterator: <span class="hljs-type">Union</span>[Iterator, <span class="hljs-type">List</span>[Iterator]],</span><br><span class="hljs-params">    model: <span class="hljs-type">Union</span>[torch.nn.Module, <span class="hljs-type">List</span>[torch.nn.Module]],</span><br><span class="hljs-params">    num_microbatches: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    seq_length: <span class="hljs-built_in">int</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    micro_batch_size: <span class="hljs-built_in">int</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    decoder_seq_length: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    forward_only: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    collect_non_loss_data: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    first_val_step: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    adjust_tensor_shapes_fn: <span class="hljs-type">Optional</span>[<span class="hljs-type">Callable</span>] = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    grad_finalize_pgs: <span class="hljs-type">Optional</span>[GradFinalizeProcessGroups] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Run forward and backward passes with no pipeline parallelism&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">if</span> grad_finalize_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        tp_group = parallel_state.get_tensor_model_parallel_group()<br>        cp_group = parallel_state.get_context_parallel_group()<br>        embd_group = parallel_state.get_embedding_group(check_initialized=<span class="hljs-literal">False</span>)<br>        pp_group = parallel_state.get_pipeline_model_parallel_group()<br>        pos_emb_group = parallel_state.get_position_embedding_group(check_initialized=<span class="hljs-literal">False</span>)<br>        grad_finalize_pgs = GradFinalizeProcessGroups()<br>        grad_finalize_pgs.tp = tp_group<br>        grad_finalize_pgs.cp = cp_group<br>        grad_finalize_pgs.embd = embd_group<br>        grad_finalize_pgs.pos_embd = pos_emb_group<br>        grad_finalize_pgs.pp = pp_group<br>        grad_finalize_pgs.dp_cp = parallel_state.get_data_parallel_group(<br>            with_context_parallel=<span class="hljs-literal">True</span>, partial_data_parallel=<span class="hljs-literal">False</span><br>        )<br><br>    <span class="hljs-keyword">elif</span> grad_finalize_pgs <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;tp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;cp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;embd&#x27;</span>), (<br>            <span class="hljs-string">&quot;grad_finalize_pgs must have a embd. In previous version, it is used default &quot;</span><br>            <span class="hljs-string">&quot;`parallel_state.default_embedding_ranks` to create the process group. If you are &quot;</span><br>            <span class="hljs-string">&quot;using the default process group, please use `parallel_state.get_embedding_group()` &quot;</span><br>            <span class="hljs-string">&quot;to get the process group. If you don&#x27;t need explicitly set it to None.&quot;</span><br>        )<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;pos_embd&#x27;</span>), (<br>            <span class="hljs-string">&quot;grad_finalize_pgs must have a pos_embd. In previous version, it is used default &quot;</span><br>            <span class="hljs-string">&quot;`parallel_state.default_position_embedding_ranks` to create the process group. &quot;</span><br>            <span class="hljs-string">&quot;If you are using the default process group, &quot;</span><br>            <span class="hljs-string">&quot;please use `parallel_state.get_position_embedding_group()` &quot;</span><br>            <span class="hljs-string">&quot;to get the process group. If you don&#x27;t need explicitly set it to None.&quot;</span><br>        )<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;pp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;dp_cp&#x27;</span>)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(model, <span class="hljs-built_in">list</span>):<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(model) == <span class="hljs-number">1</span>, <span class="hljs-string">&quot;non-pipeline-parallel schedule does not support model chunking&quot;</span><br>        model = model[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(data_iterator, <span class="hljs-built_in">list</span>):<br>        <span class="hljs-keyword">assert</span> (<br>            <span class="hljs-built_in">len</span>(data_iterator) == <span class="hljs-number">1</span><br>        ), <span class="hljs-string">&quot;non-pipeline-parallel schedule does not support model chunking&quot;</span><br>        data_iterator = data_iterator[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">assert</span> (<br>        adjust_tensor_shapes_fn <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;adjust_tensor_shapes_fn is not supported for non-pipeline-parallel schedule&quot;</span><br><br>    config = get_model_config(model)<br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;forward-backward&#x27;</span>, log_level=<span class="hljs-number">1</span>).start(barrier=config.barrier_with_L1_time)<br><br>    no_sync_func = config.no_sync_func<br>    <span class="hljs-keyword">if</span> no_sync_func <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        no_sync_func = contextlib.nullcontext<br><br>    model_type = get_model_type(model)<br><br>    forward_data_store = []<br>    input_tensor, output_tensor_grad = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    total_num_tokens = torch.zeros([], dtype=torch.<span class="hljs-built_in">int</span>, device=<span class="hljs-string">&quot;cuda&quot;</span>)<br><br>    <span class="hljs-keyword">if</span> config.overlap_moe_expert_parallel_comm <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> forward_only:<br>        forward_data_store, total_num_tokens = combined_1f1b_schedule_for_no_pipelining(<br>            forward_step_func,<br>            data_iterator,<br>            model,<br>            num_microbatches,<br>            input_tensor,<br>            output_tensor_grad,<br>            forward_data_store,<br>            config,<br>            collect_non_loss_data,<br>            first_val_step,<br>            forward_only,<br>            no_sync_func,<br>            total_num_tokens,<br>            partial(check_first_val_step, first_val_step, forward_only),<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">with</span> no_sync_func():<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_microbatches - <span class="hljs-number">1</span>):<br>                output_tensor, num_tokens = forward_step(<br>                    forward_step_func,<br>                    data_iterator,<br>                    model,<br>                    num_microbatches,<br>                    input_tensor,<br>                    forward_data_store,<br>                    config,<br>                    grad_finalize_pgs.cp.size(),<br>                    collect_non_loss_data,<br>                    is_first_microbatch=check_first_val_step(first_val_step, forward_only, i == <span class="hljs-number">0</span>),<br>                    current_microbatch=i,<br>                )<br>                total_num_tokens += num_tokens<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> forward_only:<br>                    backward_step(<br>                        input_tensor, output_tensor, output_tensor_grad, model_type, config<br>                    )<br>        <span class="hljs-comment"># Run computation for last microbatch out of context handler (want to</span><br>        <span class="hljs-comment"># synchronize gradients).</span><br>        output_tensor, num_tokens = forward_step(<br>            forward_step_func,<br>            data_iterator,<br>            model,<br>            num_microbatches,<br>            input_tensor,<br>            forward_data_store,<br>            config,<br>            grad_finalize_pgs.cp.size(),<br>            collect_non_loss_data,<br>            is_first_microbatch=check_first_val_step(<br>                first_val_step, forward_only, num_microbatches == <span class="hljs-number">1</span><br>            ),<br>            current_microbatch=num_microbatches - <span class="hljs-number">1</span>,<br>        )<br><br>        total_num_tokens += num_tokens<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> forward_only:<br>            backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)<br><br>    <span class="hljs-keyword">if</span> config.finalize_model_grads_func <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> forward_only:<br>        <span class="hljs-comment"># Finalize model grads (perform full grad all-reduce / reduce-scatter for</span><br>        <span class="hljs-comment"># data parallelism and layernorm all-reduce for sequence parallelism).</span><br>        config.finalize_model_grads_func(<br>            [model],<br>            total_num_tokens <span class="hljs-keyword">if</span> config.calculate_per_token_loss <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>            grad_finalize_pgs=grad_finalize_pgs,<br>        )<br><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;forward-backward&#x27;</span>).stop()<br><br>    <span class="hljs-keyword">if</span> (<br>        <span class="hljs-built_in">hasattr</span>(config, <span class="hljs-string">&#x27;enable_cuda_graph&#x27;</span>)<br>        <span class="hljs-keyword">and</span> config.enable_cuda_graph<br>        <span class="hljs-keyword">and</span> config.cuda_graph_scope != <span class="hljs-string">&quot;full_iteration&quot;</span><br>    ):<br>        create_cudagraphs()<br><br>    <span class="hljs-keyword">return</span> forward_data_store<br><br></code></pre></td></tr></table></figure><ol><li>在前microbatch-1次训练中，会套上<code>with no_sync_func()</code>,并且只要不是<code>forward_only</code>模式就会在backward_step中，这里其实主要都是pp的逻辑，关键的是会使用<code>torch.autograd.backward</code>来传播梯度，在梯度传播中会对各个micro_batch产生的梯度进行累加。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward_step</span>(<span class="hljs-params"></span><br><span class="hljs-params">    input_tensor,</span><br><span class="hljs-params">    output_tensor,</span><br><span class="hljs-params">    output_tensor_grad,</span><br><span class="hljs-params">    model_type,</span><br><span class="hljs-params">    config,</span><br><span class="hljs-params">    pipeline_model_parallel_size=<span class="hljs-number">1</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Backward step through passed-in output tensor.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    If last stage, output_tensor_grad is None, otherwise gradient of loss</span><br><span class="hljs-string">    with respect to stage&#x27;s output tensor.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns gradient of loss with respect to input tensor (None if first</span><br><span class="hljs-string">    stage).&quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> This code currently can handle at most one skip connection. It</span><br>    <span class="hljs-comment"># needs to be modified slightly to support arbitrary numbers of skip</span><br>    <span class="hljs-comment"># connections.</span><br><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;backward-compute&#x27;</span>, log_level=<span class="hljs-number">2</span>).start()<br><br>    <span class="hljs-comment"># Retain the grad on the input_tensor.</span><br>    unwrap_input_tensor_grad = <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(input_tensor, <span class="hljs-built_in">list</span>):<br>        input_tensor = [input_tensor]<br>        unwrap_input_tensor_grad = <span class="hljs-literal">True</span><br>    <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> input_tensor:<br>        <span class="hljs-keyword">if</span> x <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            x.retain_grad()<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(output_tensor, <span class="hljs-built_in">list</span>):<br>        output_tensor = [output_tensor]<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(output_tensor_grad, <span class="hljs-built_in">list</span>):<br>        output_tensor_grad = [output_tensor_grad]<br><br>    <span class="hljs-comment"># Backward pass.</span><br>    <span class="hljs-keyword">if</span> output_tensor_grad[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> config.grad_scale_func <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        output_tensor[<span class="hljs-number">0</span>] = config.grad_scale_func(output_tensor[<span class="hljs-number">0</span>])<br><br>    <span class="hljs-comment"># In multi-modal models like VLM, some batches may not have images.</span><br>    <span class="hljs-comment"># When no image is present, the vision encoder (as a separate pipeline stage)</span><br>    <span class="hljs-comment"># will not participate in the computation.</span><br>    <span class="hljs-comment"># This results in a tensor that does not require gradients.</span><br>    <span class="hljs-comment"># In such cases, we intentionally skip the backward pass while preserving zero gradients.</span><br>    <span class="hljs-keyword">if</span> output_tensor[<span class="hljs-number">0</span>].requires_grad:<br>        <span class="hljs-keyword">if</span> config.deallocate_pipeline_outputs:<br>            custom_backward(output_tensor[<span class="hljs-number">0</span>], output_tensor_grad[<span class="hljs-number">0</span>])<br>        <span class="hljs-keyword">else</span>:<br>            torch.autograd.backward(output_tensor[<span class="hljs-number">0</span>], grad_tensors=output_tensor_grad[<span class="hljs-number">0</span>])<br><br>    <span class="hljs-comment"># Collect the grad of the input_tensor.</span><br>    input_tensor_grad = [<span class="hljs-literal">None</span>]<br>    <span class="hljs-keyword">if</span> input_tensor <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        input_tensor_grad = []<br>        <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> input_tensor:<br>            <span class="hljs-keyword">if</span> x <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                input_tensor_grad.append(<span class="hljs-literal">None</span>)<br>            <span class="hljs-keyword">else</span>:<br>                input_tensor_grad.append(x.grad)<br><br>    <span class="hljs-keyword">if</span> unwrap_input_tensor_grad:<br>        input_tensor_grad = input_tensor_grad[<span class="hljs-number">0</span>]<br><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;backward-compute&#x27;</span>).stop()<br><br>    <span class="hljs-keyword">return</span> input_tensor_grad<br><br></code></pre></td></tr></table></figure><ul><li><p>在最后第microbatch次进行训练时没有套上<code>with no_sync_func()</code>从而允许一些同步操作</p></li><li><p>而数据并行的梯度间同步其实是在后续的config.finalize_model_grads_func中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> config.finalize_model_grads_func <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> forward_only:<br>    <span class="hljs-comment"># Finalize model grads (perform full grad all-reduce / reduce-scatter for</span><br>    <span class="hljs-comment"># data parallelism and layernorm all-reduce for sequence parallelism).</span><br>    config.finalize_model_grads_func(<br>        [model],<br>        total_num_tokens <span class="hljs-keyword">if</span> config.calculate_per_token_loss <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>        grad_finalize_pgs=grad_finalize_pgs,<br>    )<br></code></pre></td></tr></table></figure><ol><li><p>config来自get_model_config(model)中，而<code>config.finalize_model_grads_func</code>是在<code>train</code>函数中添加的功能：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model_config</span>(<span class="hljs-params">model</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Returns the config attribute, allowed to return None&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> get_attr_wrapped_model(model, <span class="hljs-string">&quot;config&quot;</span>, allow_none=<span class="hljs-literal">False</span>)<br><br></code></pre></td></tr></table></figure><ol><li>添加的<code>finalize_model_grads</code>代码如下</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">finalize_model_grads</span>(<span class="hljs-params"></span><br><span class="hljs-params">    model: <span class="hljs-type">List</span>[torch.nn.Module],</span><br><span class="hljs-params">    num_tokens: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    grad_finalize_pgs: <span class="hljs-type">Optional</span>[GradFinalizeProcessGroups] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    All-reduce all model grads across DP replicas, layernorm grads for sequence parallelism,</span><br><span class="hljs-string">    embedding grads across first and last pipeline stages (if not tied),</span><br><span class="hljs-string">    scale gradients by `num_tokens`.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    config = get_model_config(model[<span class="hljs-number">0</span>])<br>    <span class="hljs-keyword">if</span> grad_finalize_pgs <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;tp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;pp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;embd&#x27;</span>), (<br>            <span class="hljs-string">&quot;grad_finalize_pgs must have a embd. In previous version, it is used default &quot;</span><br>            <span class="hljs-string">&quot;`parallel_state.default_embedding_ranks` to create the process group.&quot;</span><br>            <span class="hljs-string">&quot; If you are using the default process group, please use&quot;</span><br>            <span class="hljs-string">&quot; `parallel_state.get_embedding_group()` &quot;</span><br>            <span class="hljs-string">&quot;If you don&#x27;t need embd_group, you need to explicitly set it to None.&quot;</span><br>        )<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;pos_embd&#x27;</span>), (<br>            <span class="hljs-string">&quot;grad_finalize_pgs must have a pos_embd. In previous version, it is used default &quot;</span><br>            <span class="hljs-string">&quot;`parallel_state.default_position_embedding_ranks` to create the process group.&quot;</span><br>            <span class="hljs-string">&quot; If you are using the default process group, please use &quot;</span><br>            <span class="hljs-string">&quot; `parallel_state.get_position_embedding_group()` &quot;</span><br>            <span class="hljs-string">&quot;If you don&#x27;t need pos_embd_group, you need to explicitly set it to None.&quot;</span><br>        )<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;dp_cp&#x27;</span>)<br>        tp_group = grad_finalize_pgs.tp<br>        pp_group = grad_finalize_pgs.pp<br>        embd_group = grad_finalize_pgs.embd<br>        pos_emb_group = grad_finalize_pgs.pos_embd<br>        dp_cp_group = grad_finalize_pgs.dp_cp<br>    <span class="hljs-keyword">else</span>:<br>        tp_group = parallel_state.get_tensor_model_parallel_group()<br>        pp_group = parallel_state.get_pipeline_model_parallel_group()<br>        embd_group = parallel_state.get_embedding_group(check_initialized=<span class="hljs-literal">False</span>)<br>        pos_emb_group = parallel_state.get_position_embedding_group(check_initialized=<span class="hljs-literal">False</span>)<br>        dp_cp_group = parallel_state.get_data_parallel_group(with_context_parallel=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-comment"># All-reduce / reduce-scatter across DP replicas.</span><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;all-grads-sync&#x27;</span>, log_level=<span class="hljs-number">1</span>).start(barrier=config.barrier_with_L1_time)<br>    <span class="hljs-keyword">for</span> model_chunk <span class="hljs-keyword">in</span> model:<br>        model_chunk.finish_grad_sync()<br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;all-grads-sync&#x27;</span>).stop()<br><br>    <span class="hljs-comment"># All-reduce t_embedder grads (for pp &amp; vpp of DiT).</span><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;conditional-embedder-grads-all-reduce&#x27;</span>, log_level=<span class="hljs-number">1</span>).start(<br>            barrier=config.barrier_with_L1_time<br>        )<br>    _allreduce_conditional_embedding_grads(model, config, pp_group)<br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;conditional-embedder-grads-all-reduce&#x27;</span>).stop()<br><br>    <span class="hljs-comment"># All-reduce layer-norm grads (for sequence parallelism) and non-tensor parallel modules.</span><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;non-tensor-parallel-grads-all-reduce&#x27;</span>, log_level=<span class="hljs-number">1</span>).start(<br>            barrier=config.barrier_with_L1_time<br>        )<br>    _allreduce_non_tensor_model_parallel_grads(model, config, tp_group)<br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;non-tensor-parallel-grads-all-reduce&#x27;</span>).stop()<br><br>    <span class="hljs-comment"># All-reduce embedding grads (for pipeline parallelism).</span><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;embedding-grads-all-reduce&#x27;</span>, log_level=<span class="hljs-number">1</span>).start(<br>            barrier=config.barrier_with_L1_time<br>        )<br>    _allreduce_word_embedding_grads(model, config, embd_group, pp_group)<br>    _allreduce_position_embedding_grads(model, config, pos_emb_group, pp_group)<br><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;embedding-grads-all-reduce&#x27;</span>).stop()<br><br>    <span class="hljs-keyword">if</span> config.moe_router_enable_expert_bias:<br>        _update_router_expert_bias(model, config)<br><br>    <span class="hljs-comment"># normalize gradients for per-token loss normalization.</span><br>    <span class="hljs-comment"># if we are using by the number of tokens, then we use that as a divisor. this number</span><br>    <span class="hljs-comment"># will be the total number of non-padded tokens in the global batch.</span><br>    <span class="hljs-keyword">if</span> num_tokens <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br><br>        <span class="hljs-comment"># the number of tokens is only present on the last stage, so broadcast it</span><br>        <span class="hljs-comment"># to the other ranks in the pipeline parallel group.</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(pp_group, <span class="hljs-built_in">list</span>)<br>        last_rank = get_pp_last_rank(pp_group)<br>        torch.distributed.broadcast(num_tokens, src=last_rank, group=pp_group)<br><br>        <span class="hljs-comment"># all-reduce across DP ranks.</span><br>        torch.distributed.all_reduce(num_tokens, group=dp_cp_group)<br>        <span class="hljs-keyword">for</span> model_chunk <span class="hljs-keyword">in</span> model:<br>            <span class="hljs-keyword">if</span> num_tokens &gt; <span class="hljs-number">0</span>:<br>                scaling = <span class="hljs-number">1.0</span> / num_tokens<br>                model_chunk.scale_gradients(scaling)<br><br></code></pre></td></tr></table></figure><ul><li>其中最关键的是如下触发各个model_chunk的<code>finish_grad_sync</code>梯度同步的代码：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># All-reduce / reduce-scatter across DP replicas.</span><br><span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    config.timers(<span class="hljs-string">&#x27;all-grads-sync&#x27;</span>, log_level=<span class="hljs-number">1</span>).start(barrier=config.barrier_with_L1_time)<br><span class="hljs-keyword">for</span> model_chunk <span class="hljs-keyword">in</span> model:<br>    model_chunk.finish_grad_sync()<br><span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    config.timers(<span class="hljs-string">&#x27;all-grads-sync&#x27;</span>).stop()<br></code></pre></td></tr></table></figure><ul><li>而这函数的执行就与model的类型有关了，model的类型由<code>get_model</code>函数决定，如下所示，这里我们以最简单的DDP（<code>DistributedDataParallel</code>）进行举例分析</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs shell">if wrap_with_ddp:<br>    if args.use_torch_fsdp2:<br>        assert HAVE_FSDP2, &quot;Torch FSDP2 requires torch&gt;=2.4.0&quot;<br>        DP = torch_FSDP<br>    elif args.use_megatron_fsdp:<br>        DP = megatron_FSDP<br>    else:<br>        DP = DDP<br>        <br>    ...<br>    <br>    with torch.cuda.stream(torch.cuda.Stream()):<br>        model = [<br>            DP(<br>                config=config,<br>                ddp_config=ddp_config,<br>                module=model_chunk,<br>                # Turn off bucketing for model_chunk 2 onwards, since communication for these<br>                # model chunks is overlapped with compute anyway.<br>                disable_bucketing=(model_chunk_idx &gt; 0)<br>                or args.overlap_param_gather_with_optimizer_step,<br>            )<br>            for (model_chunk_idx, model_chunk) in enumerate(model)<br>        ]<br></code></pre></td></tr></table></figure><ul><li><code>DistributedDataParallel</code>中的<code>finish_grad_sync</code>函数如下所示，其对每个bucket_group 调用了<code>finish_grad_sync</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">finish_grad_sync</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Finishes grad sync (all-reduce or reduce-scatter) communication operations</span><br><span class="hljs-string">    for all model gradients.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    When overlap_grad_reduce is set to True, waits for asynchronous communication</span><br><span class="hljs-string">    calls to complete. When overlap_grad_reduce is set to False, calls synchronous</span><br><span class="hljs-string">    communication ops.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">for</span> bucket_group <span class="hljs-keyword">in</span> self.bucket_groups + self.expert_parallel_bucket_groups:<br>        bucket_group.finish_grad_sync()<br></code></pre></td></tr></table></figure><ul><li>bucket_group 是<code>_ParamAndGradBucketGroup</code>类，其<code>finish_grad_sync</code>函数如下所示，这里看一般情况，也就是<code>self.ddp_config.overlap_grad_reduce</code>为False，即直接调用<code>start_grad_sync</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">finish_grad_sync</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Finishes grad sync (all-reduce or reduce-scatter) communication operations</span><br><span class="hljs-string">    for all buckets in the bucket group.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    When ddp_config.overlap_grad_reduce is set to True, waits for asynchronous</span><br><span class="hljs-string">    communication call to complete. When ddp_config.overlap_grad_reduce is set to False,</span><br><span class="hljs-string">    makes synchronous call.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    self.param_gather_dispatched = <span class="hljs-literal">False</span><br>    <span class="hljs-comment"># If overlap_grad_reduce is False, start (and finish) synchronous communication call here.</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.ddp_config.overlap_grad_reduce:<br>        self.start_grad_sync()<br>        <span class="hljs-keyword">return</span><br>    <span class="hljs-comment"># When using multiple DistOpt instances, we don&#x27;t need to sync here as we launch</span><br>    <span class="hljs-comment"># communications on a separate communication stream.</span><br>    <span class="hljs-keyword">if</span> self.ddp_config.num_distributed_optimizer_instances &gt; <span class="hljs-number">1</span>:<br>        torch.cuda.default_stream().wait_stream(self.communication_stream)<br>        <span class="hljs-keyword">return</span><br>    <span class="hljs-keyword">assert</span> self.grad_reduce_handle <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>, (<br>        <span class="hljs-string">f&quot;Communication call has not been issued for this bucket &quot;</span><br>        <span class="hljs-string">f&quot;(<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(self.params_with_grad)&#125;</span>/<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(self.params)&#125;</span> params have grad available)&quot;</span><br>    )<br>    self.grad_reduce_handle.wait()<br>    self.grad_reduce_handle = <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure><ul><li><code>start_grad_sync</code>函数如下所示，其核心是<strong>把一组 bucket（连续 grad buffer 的若干切片）触发数据并行梯度同步</strong>，并支持“是否与反向计算重叠（overlap）”、“是否用分布式优化器（reduce-scatter）”、“是否多 DistOpt 实例（两级通信）”、“是否做梯度检查&#x2F;缩放”等。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">start_grad_sync</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Initiates grad sync (all-reduce or reduce-scatter) communication operations</span><br><span class="hljs-string">    for all buckets in the bucket group.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    When ddp_config.overlap_grad_reduce is set to True, dispatches an asynchronous</span><br><span class="hljs-string">    communication call. When ddp_config.overlap_grad_reduce is set to False, makes</span><br><span class="hljs-string">    synchronous call.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">assert</span> (<br>        self.grad_reduce_handle <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;Should not have multiple communication calls outstanding at once&quot;</span><br><br>    <span class="hljs-keyword">if</span> self.ddp_config.check_for_nan_in_grad <span class="hljs-keyword">or</span> self.ddp_config.check_for_large_grads:<br>        self.check_grads(<br>            check_for_nan_or_inf=self.ddp_config.check_for_nan_in_grad,<br>            check_for_large=self.ddp_config.check_for_large_grads,<br>        )<br><br>    <span class="hljs-comment"># gradient_scaling_factor already takes into account whether we are computing</span><br>    <span class="hljs-comment"># an average or sum in the data-parallel collective.</span><br>    <span class="hljs-keyword">for</span> bucket <span class="hljs-keyword">in</span> self.buckets:<br>        <span class="hljs-keyword">if</span> bucket.gradient_scaling_factor != <span class="hljs-number">1.0</span>:<br>            bucket.grad_data *= bucket.gradient_scaling_factor<br><br>    <span class="hljs-comment"># Decide reduce_op.</span><br>    reduce_op = torch.distributed.ReduceOp.SUM<br>    <span class="hljs-keyword">if</span> self.ddp_config.average_in_collective:<br>        reduce_op = torch.distributed.ReduceOp.AVG<br><br>    <span class="hljs-comment"># We use the following stream synchronization for the gradient reduction</span><br>    <span class="hljs-comment"># within and across DistOpt instances.</span><br><br>    <span class="hljs-comment"># Compute Stream: -------------Gradient compute-------------------</span><br>    <span class="hljs-comment"># Comm. Stream:   ------(wait for NCCL)-----(wait for NCCL)-------</span><br>    <span class="hljs-comment"># NCCL Stream:          -------RS------     -------AR------</span><br><br>    <span class="hljs-comment"># Use async communications only when overlap_grad_reduce is True.</span><br>    async_op = (<br>        self.ddp_config.overlap_grad_reduce<br>        <span class="hljs-keyword">and</span> self.ddp_config.num_distributed_optimizer_instances == <span class="hljs-number">1</span><br>    )<br>    <span class="hljs-keyword">if</span> (<br>        self.ddp_config.num_distributed_optimizer_instances &gt; <span class="hljs-number">1</span><br>        <span class="hljs-keyword">and</span> self.ddp_config.overlap_grad_reduce<br>    ):<br>        <span class="hljs-comment"># Assign a communication stream if we have multiple DistOpt instances and we</span><br>        <span class="hljs-comment"># need to overlap communication.</span><br>        stream_context = torch.cuda.stream(self.communication_stream)<br><br>        <span class="hljs-comment"># The RS/AR communication stream needs to wait for the default stream</span><br>        <span class="hljs-comment"># to complete its gradient computation before launching the next</span><br>        <span class="hljs-comment"># gradient reduction collective.</span><br>        self.communication_stream.wait_stream(torch.cuda.default_stream())<br>    <span class="hljs-keyword">else</span>:<br>        stream_context = nullcontext()<br><br>    <span class="hljs-keyword">if</span> self.ddp_config.use_distributed_optimizer:<br>        communication_group = self.intra_distributed_optimizer_instance_group<br>    <span class="hljs-keyword">else</span>:<br>        communication_group = self.data_parallel_group<br><br>    <span class="hljs-comment"># Coalesce communication kernels across buckets in the bucket group.</span><br>    <span class="hljs-keyword">with</span> stream_context, _coalescing_manager(communication_group, async_ops=async_op) <span class="hljs-keyword">as</span> cm:<br>        <span class="hljs-keyword">for</span> idx, bucket <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.buckets):<br>            <span class="hljs-keyword">if</span> self.ddp_config.use_distributed_optimizer:<br>                <span class="hljs-keyword">if</span> self.cached_grad_buffer_shard_list[idx] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                    self.cached_grad_buffer_shard_list[idx] = shard_buffer(<br>                        bucket.grad_data, self.intra_distributed_optimizer_instance_size<br>                    )<br>                local_data_view = self.cached_grad_buffer_shard_list[idx][<br>                    self.intra_distributed_optimizer_instance_rank<br>                ]<br>                dist_reduce_scatter_func(<br>                    local_data_view,<br>                    bucket.grad_data,<br>                    op=reduce_op,<br>                    group=communication_group,<br>                    async_op=async_op,<br>                )<br>            <span class="hljs-keyword">else</span>:<br>                torch.distributed.all_reduce(<br>                    bucket.grad_data, op=reduce_op, group=communication_group, async_op=async_op<br>                )<br><br>    <span class="hljs-comment"># With multiple DistOpt instances, we need to all-reduce across instances.</span><br>    <span class="hljs-keyword">if</span> (<br>        self.ddp_config.use_distributed_optimizer<br>        <span class="hljs-keyword">and</span> self.ddp_config.num_distributed_optimizer_instances &gt; <span class="hljs-number">1</span><br>    ):<br>        <span class="hljs-keyword">assert</span> self.inter_distributed_optimizer_instance_group <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        <span class="hljs-comment"># Create a new coalescing manager for the inter-instance all-reduce.</span><br>        <span class="hljs-keyword">with</span> (<br>            stream_context,<br>            _coalescing_manager(<br>                self.inter_distributed_optimizer_instance_group, async_ops=async_op<br>            ) <span class="hljs-keyword">as</span> cm,<br>        ):<br>            <span class="hljs-keyword">for</span> idx, bucket <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.buckets):<br>                <span class="hljs-keyword">if</span> self.cached_grad_buffer_shard_list[idx] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                    self.cached_grad_buffer_shard_list[idx] = shard_buffer(<br>                        bucket.grad_data, self.intra_distributed_optimizer_instance_size<br>                    )<br>                local_data_view = self.cached_grad_buffer_shard_list[idx][<br>                    self.intra_distributed_optimizer_instance_rank<br>                ]<br><br>                torch.distributed.all_reduce(<br>                    local_data_view,<br>                    op=reduce_op,<br>                    group=self.inter_distributed_optimizer_instance_group,<br>                    async_op=async_op,<br>                )<br><br>    <span class="hljs-keyword">if</span> async_op:<br>        self.grad_reduce_handle = cm<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># When using `_coalescing_manager`, even if a synchronous op (async_op=False) is used,</span><br>        <span class="hljs-comment"># `cm` is not None, which is different from when `_coalescing_manager` is not used in</span><br>        <span class="hljs-comment"># which case the torch.distributed._reduce_scatter_base() will return None. In order to</span><br>        <span class="hljs-comment"># maintain consistency with prior code, we need to manually set communication handle to</span><br>        <span class="hljs-comment"># None.</span><br>        self.grad_reduce_handle = <span class="hljs-literal">None</span><br><br></code></pre></td></tr></table></figure></li></ol></li></ul><p>看DDP数据并行的torch profile结果也能证实这一系列链路的正确性：</p><p><img src="/2025/12/28/megatron-lm-ddp/image-1.png"></p><p>后续还可以再看看FSDP下的数据并行实现。</p>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>Megatron-LM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Megatron-LM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Megatron-LM源码分析（三）】-性能分析</title>
    <link href="/2025/12/26/megatron-lm-profiler/"/>
    <url>/2025/12/26/megatron-lm-profiler/</url>
    
    <content type="html"><![CDATA[<p>在算力利用率方面，Megatron-LM支持通过Pytorch Profiler和Nsys进行分析，注意这两者在Megatron-LM中是互斥的。</p><ul><li><p>PyTorch Profiler：框架原生工具，更高层，侧重于 Python&#x2F;PyTorch 算子层级，可以看到代码级的调用链，适合识别 Python 端的慢算子、内存泄漏、调度开销。</p></li><li><p>Nsys：系统级追踪工具，更底层，侧重于 CUDA 和硬件性能层级，适合分析 CUDA Kernel 执行、PCIe 带宽利用率、GPU 内存传输、多 GPU 通信（NCCL）等</p></li></ul><p>在显存占用方面，Megatron-LM支持通过Pytorch自带的snapshot的功能来记录显存分配情况。</p><p>下面就如何开启这些分析方法以及示例做介绍。</p><h1 id="PyTorch-Profiler性能分析"><a href="#PyTorch-Profiler性能分析" class="headerlink" title="PyTorch Profiler性能分析"></a>PyTorch Profiler性能分析</h1><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><p>一般使用pytorch Profile的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.profiler<br><span class="hljs-keyword">import</span> os<br><br>logdir = <span class="hljs-string">&quot;tb_profiler_test&quot;</span><br><br><span class="hljs-keyword">with</span> torch.profiler.profile(<br>    activities=[<br>        torch.profiler.ProfilerActivity.CPU,<br>        torch.profiler.ProfilerActivity.CUDA,<br>    ],<br>    schedule=torch.profiler.schedule(<br>        wait=<span class="hljs-number">1</span>,<br>        warmup=<span class="hljs-number">1</span>,<br>        active=<span class="hljs-number">2</span>,<br>        repeat=<span class="hljs-number">1</span>,<br>    ),<br>    on_trace_ready=torch.profiler.tensorboard_trace_handler(<br>        logdir<br>    ),<br>    record_shapes=<span class="hljs-literal">True</span>,<br>    with_stack=<span class="hljs-literal">True</span>,<br>) <span class="hljs-keyword">as</span> prof:<br>    <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">6</span>):<br>        x = torch.randn(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>, device=<span class="hljs-string">&quot;cuda&quot;</span>)<br>        y = x @ x<br>        torch.cuda.synchronize()<br>        prof.step()<br><br></code></pre></td></tr></table></figure><p>首先需要定义<code>torch.profiler.schedule</code>，然后通过prof.step来更新当前步数。最后的结果可以通过Tensboard或者Chrome的<code>chrome://tracing/</code>来查看。</p><h2 id="相关代码"><a href="#相关代码" class="headerlink" title="相关代码"></a>相关代码</h2><p>相关参数的介绍如下，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">group.add_argument(<span class="hljs-string">&#x27;--profile&#x27;</span>, action=<span class="hljs-string">&#x27;store_true&#x27;</span>,<br>                   <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;Enable nsys profiling. When using this option, nsys &#x27;</span><br>                   <span class="hljs-string">&#x27;options should be specified in commandline. An example &#x27;</span><br>                   <span class="hljs-string">&#x27;nsys commandline is `nsys profile -s none -t nvtx,cuda &#x27;</span><br>                   <span class="hljs-string">&#x27;-o &lt;path/to/output_file&gt; --force-overwrite true &#x27;</span><br>                   <span class="hljs-string">&#x27;--capture-range=cudaProfilerApi &#x27;</span><br>                   <span class="hljs-string">&#x27;--capture-range-end=stop`.&#x27;</span>)<br>group.add_argument(<span class="hljs-string">&#x27;--profile-step-start&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">10</span>,<br>                   <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;Global step to start profiling.&#x27;</span>)<br>group.add_argument(<span class="hljs-string">&#x27;--profile-step-end&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">12</span>,<br>                   <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;Global step to stop profiling.&#x27;</span>)<br>group.add_argument(<span class="hljs-string">&#x27;--use-pytorch-profiler&#x27;</span>, action=<span class="hljs-string">&#x27;store_true&#x27;</span>,<br>                   <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;Use the built-in pytorch profiler. &#x27;</span><br>                   <span class="hljs-string">&#x27;Useful if you wish to view profiles in tensorboard.&#x27;</span>,<br>                   dest=<span class="hljs-string">&#x27;use_pytorch_profiler&#x27;</span>)<br></code></pre></td></tr></table></figure><p>在<code>megatron/training/training.py</code>的<code>training</code>函数中有如下的代码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs shell">    if (<br>        args.profile<br>        and torch.distributed.get_rank() in args.profile_ranks<br>        and args.use_pytorch_profiler<br>    ):<br>        prof = torch.profiler.profile(<br>            schedule=torch.profiler.schedule(<br>                wait=max(args.profile_step_start - 1, 0),<br>                warmup=1 if args.profile_step_start &gt; 0 else 0,<br>                active=args.profile_step_end - args.profile_step_start,<br>                repeat=1,<br>            ),<br>            on_trace_ready=torch.profiler.tensorboard_trace_handler(args.tensorboard_dir),<br>            record_shapes=True,<br>            with_stack=True,<br>        )<br>        prof.start()<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">       ...</span><br><br>    # Run training iterations till done.<br>    while iteration &lt; args.train_iters:<br>        if args.profile and torch.distributed.get_rank() in args.profile_ranks:<br>            if args.use_pytorch_profiler:<br>                prof.step()<br>            elif iteration == args.profile_step_start:<br>                torch.cuda.cudart().cudaProfilerStart()<br>                torch.autograd.profiler.emit_nvtx(record_shapes=True).__enter__()<br></code></pre></td></tr></table></figure><p>这说明了只有开启profile、use_pytorch_profiler并且当前进程的rank在profile_ranks时才会开启prof。然后也在定义torch.profiler.profile时指明了需要跳过profile_step_start - 1步，然后只要profile_step_start&gt;1就预热一轮否则不预热，然后采集<code>args.profile_step_end - args.profile_step_start</code>步。</p><p>在每次batch的迭代中会调用prof.step()来更新采集步数。</p><p>在<code>megatron/training/training.py</code>中的<code>post_training_step_callbacks</code>有如下的代码负责在step达到profile_step_end后停止prof：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">Profiling.</span><br>if (<br>    args.profile<br>    and iteration == args.profile_step_end<br>    and torch.distributed.get_rank() in args.profile_ranks<br>):<br>    if args.use_pytorch_profiler:<br>        assert prof is not None<br>        prof.stop()<br>    else:<br>        torch.cuda.cudart().cudaProfilerStop()<br></code></pre></td></tr></table></figure><blockquote><p>注意这里说的step是指实际这一次训练运行了多少步，不与从checkpoint中恢复的当前的iter有关。</p></blockquote><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>运行脚本如下，关键是<code>PROFILER_ARGS</code>中添加的对应参数，其指示会采集110、111步。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><br><span class="hljs-comment"># Runs the &quot;857m&quot; parameter model</span><br><br><span class="hljs-built_in">export</span> CUDA_DEVICE_MAX_CONNECTIONS=1<br><br>GPUS_PER_NODE=4<br><span class="hljs-comment"># Change for multinode config</span><br>MASTER_ADDR=localhost<br>MASTER_PORT=6000<br>NUM_NODES=1<br>NODE_RANK=0<br>WORLD_SIZE=$((<span class="hljs-variable">$GPUS_PER_NODE</span>*<span class="hljs-variable">$NUM_NODES</span>))<br><br>CHECKPOINT_PATH=<span class="hljs-variable">$1</span> <span class="hljs-comment">#&lt;Specify path&gt;</span><br>TENSORBOARD_LOGS_PATH=<span class="hljs-variable">$2</span> <span class="hljs-comment">#&lt;Specify path&gt;</span><br>VOCAB_FILE=<span class="hljs-variable">$3</span> <span class="hljs-comment">#&lt;Specify path to file&gt;/gpt2-vocab.json</span><br>MERGE_FILE=<span class="hljs-variable">$4</span> <span class="hljs-comment">#&lt;Specify path to file&gt;/gpt2-merges.txt</span><br>DATA_PATH=<span class="hljs-variable">$5</span> <span class="hljs-comment">#&lt;Specify path and file prefix&gt;_text_document</span><br>USE_NSYS=0<br><span class="hljs-keyword">if</span> [[ <span class="hljs-variable">$&#123;6:-&#125;</span> == <span class="hljs-string">&quot;--nsys&quot;</span> ]]; <span class="hljs-keyword">then</span><br>  USE_NSYS=1<br><span class="hljs-keyword">fi</span><br><br>DISTRIBUTED_ARGS=(<br>    --nproc_per_node <span class="hljs-variable">$GPUS_PER_NODE</span> <br>    --nnodes <span class="hljs-variable">$NUM_NODES</span> <br>    --master_addr <span class="hljs-variable">$MASTER_ADDR</span> <br>    --master_port <span class="hljs-variable">$MASTER_PORT</span><br>)<br><br>GPT_MODEL_ARGS=(<br>    --num-layers 24 <br>    --hidden-size 1024 <br>    --num-attention-heads 16 <br>    --seq-length 2048 <br>    --max-position-embeddings 2048 <br>    --attention-backend auto <span class="hljs-comment"># Can use (flash/fused/unfused/local)</span><br>)<br><br>TRAINING_ARGS=(<br>    --micro-batch-size 4 <br>    --global-batch-size 16 <br>    <span class="hljs-comment"># --rampup-batch-size 16 16 5859375 </span><br>    --train-iters 20000 <br>    --weight-decay 0.1 <br>    --adam-beta1 0.9 <br>    --adam-beta2 0.95 <br>    --init-method-std 0.006 <br>    --clip-grad 1.0 <br>    --fp16<br>    --lr 6.0e-5 <br>    --lr-decay-style cosine <br>    --min-lr 6.0e-6<br>    --lr-warmup-fraction .001 <br>    --lr-decay-iters 20000 <br>)<br><br>MODEL_PARALLEL_ARGS=(<br>  --tensor-model-parallel-size 1 <br>  --pipeline-model-parallel-size 1 <br>)<br><br>DATA_ARGS=(<br>    --data-path <span class="hljs-variable">$DATA_PATH</span> <br>    --vocab-file <span class="hljs-variable">$VOCAB_FILE</span> <br>    --merge-file <span class="hljs-variable">$MERGE_FILE</span> <br>    --<span class="hljs-built_in">split</span> 949,50,1<br>)<br><br>EVAL_AND_LOGGING_ARGS=(<br>    --log-interval 200<br>    --save-interval 10000 <br>    --eval-interval 1000 <br>    --save <span class="hljs-variable">$CHECKPOINT_PATH</span> <br>    --load <span class="hljs-variable">$CHECKPOINT_PATH</span> <br>    --eval-iters 10<br>    --tensorboard-dir <span class="hljs-variable">$TENSORBOARD_LOGS_PATH</span> <br>)<br><br>PROFILER_ARGS=(<br>    --profile<br>    --use-pytorch-profiler<br>    --profile-step-start 110<br>    --profile-step-end 112<br>    --profile-ranks 0<br>)<br><br><span class="hljs-comment"># Build command as an array (no string concatenation)</span><br>CMD=(<br>  torchrun<br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;DISTRIBUTED_ARGS[@]&#125;</span>&quot;</span><br>  pretrain_gpt.py<br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;GPT_MODEL_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;TRAINING_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;MODEL_PARALLEL_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;DATA_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;EVAL_AND_LOGGING_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;PROFILER_ARGS[@]&#125;</span>&quot;</span><br>)<br><br><span class="hljs-keyword">if</span> [[ <span class="hljs-string">&quot;<span class="hljs-variable">$USE_NSYS</span>&quot;</span> -eq 1 ]]; <span class="hljs-keyword">then</span><br>  NSIGHT_PREFIX=<span class="hljs-string">&quot;./nsight_profile/gpt3_857m&quot;</span><br>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Running with Nsight profiling, output prefix: <span class="hljs-variable">$&#123;NSIGHT_PREFIX&#125;</span>&quot;</span><br>  <span class="hljs-built_in">exec</span> nsys profile \<br>    -s none -t nvtx,cuda \<br>    --cudabacktrace=all \<br>    --cuda-graph-trace=node \<br>    --python-backtrace=cuda \<br>    --<span class="hljs-built_in">wait</span> all \<br>    -o <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;NSIGHT_PREFIX&#125;</span>&quot;</span> \<br>    --force-overwrite <span class="hljs-literal">true</span> \<br>    --capture-range=cudaProfilerApi \<br>    --capture-range-end=stop \<br>    <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;CMD[@]&#125;</span>&quot;</span><br><span class="hljs-keyword">else</span><br>  <span class="hljs-built_in">exec</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;CMD[@]&#125;</span>&quot;</span><br><span class="hljs-keyword">fi</span><br></code></pre></td></tr></table></figure><p>运行的指令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash examples/gpt3/train_gpt3_857m_distributed.sh     /workspace/megatron-lm/model_ckpt/gpt3_857m_2     /workspace/megatron-lm/tb_logs/gpt3_857m_profiler     /workspace/megatron-lm/data/tokenizer/gpt2-vocab.json     /workspace/megatron-lm/data/tokenizer/gpt2-merges.txt     /workspace/megatron-lm/data/TinyStoriesV2-GPT4-train_text_document      &gt; gpt3_857m2.log 2&gt;&amp;1 &amp;<br></code></pre></td></tr></table></figure><p>运行后会在<code>tensorboard-dir</code>下获取对应的<code>pt.trace.json</code>文件，例如本次运行获得的是<code>tb_logs/gpt3_857m_profiler/6dacc15685cd_821091.1766741105666343018.pt.trace.json</code>文件</p><p>可以用tensor board或者是Chrome查看该文件，如下是访问Chrome的<code>chrome://tracing/</code>查看的结果：</p><p>CPU层面的Python代码分析的结果如下，可以看到整个调用链还是很清楚的：</p><p><img src="/2025/12/26/megatron-lm-profiler/image.png"></p><p>GPU层面的分析结果如下，由于这里使用的是简单的数据并行，所以每一步后都有一次all reduce进行参数收集，整体逻辑看的还是很清楚的。</p><p><img src="/2025/12/26/megatron-lm-profiler/image-1.png"></p><h1 id="Nsys性能分析"><a href="#Nsys性能分析" class="headerlink" title="Nsys性能分析"></a>Nsys性能分析</h1><h2 id="使用方法-1"><a href="#使用方法-1" class="headerlink" title="使用方法"></a>使用方法</h2><p>一般使用Nsys的代码如下，其中range_push(“xxx”)与range_pop()为一段运行的代码区间标注了区间名</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.cuda.nvtx <span class="hljs-keyword">as</span> nvtx<br><span class="hljs-keyword">import</span> time<br><br>device = <span class="hljs-string">&quot;cuda&quot;</span><br><br><span class="hljs-comment"># warmup</span><br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>):<br>    x = torch.randn(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>, device=device)<br>    y = x @ x<br>    torch.cuda.synchronize()<br><br><span class="hljs-comment"># profile 区间</span><br>nvtx.range_push(<span class="hljs-string">&quot;matmul_step&quot;</span>)<br><br>x = torch.randn(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>, device=device)<br>y = x @ x<br>torch.cuda.synchronize()<br><br>nvtx.range_pop()<br><br>time.sleep(<span class="hljs-number">0.2</span>)  <span class="hljs-comment"># 让 CPU timeline 更明显</span><br><br></code></pre></td></tr></table></figure><p>需要用如下的nsys开头的命令运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">nsys profile \<br>  --trace=cuda,nvtx,osrt \<br>  -o simple_matmul \<br>  python example.py<br></code></pre></td></tr></table></figure><p>运行后会生成<code>simple_matmul.nsys-rep</code>，然后可以下载<a href="https://developer.nvidia.com/nsight-systems">Nsight Systems</a>对其进行查看。</p><h2 id="相关代码-1"><a href="#相关代码-1" class="headerlink" title="相关代码"></a>相关代码</h2><p>相关参数的介绍其实与Pytorch Profiler类似，不同之处需要关闭use_pytorch_profiler。</p><p>Megatron-LM也在多个地方专门手动标注了profile 区间以便于查看。</p><p><img src="/2025/12/26/megatron-lm-profiler/image-2.png"></p><p><img src="/2025/12/26/megatron-lm-profiler/image-3.png"></p><p>整体与Pytorch Profile类似，其通过step步数来控制达到args.profile_step_start步数时才开启profile，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">Run training iterations till <span class="hljs-keyword">done</span>.</span><br>while iteration &lt; args.train_iters:<br>    if args.profile and torch.distributed.get_rank() in args.profile_ranks:<br>        if args.use_pytorch_profiler:<br>            prof.step()<br>        elif iteration == args.profile_step_start:<br>            torch.cuda.cudart().cudaProfilerStart()<br>            torch.autograd.profiler.emit_nvtx(record_shapes=True).__enter__()<br></code></pre></td></tr></table></figure><p>然后当步数达到args.profile_step_end步数时才关闭profile</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">Profiling.</span><br>if (<br>    args.profile<br>    and iteration == args.profile_step_end<br>    and torch.distributed.get_rank() in args.profile_ranks<br>):<br>    if args.use_pytorch_profiler:<br>        assert prof is not None<br>        prof.stop()<br>    else:<br>        torch.cuda.cudart().cudaProfilerStop()<br></code></pre></td></tr></table></figure><h2 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h2><p>运行的脚本如下，注意这里相比Pytorch Profile删掉了<code>--use-pytorch-profiler</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><br><span class="hljs-comment"># Runs the &quot;857m&quot; parameter model</span><br><br><span class="hljs-built_in">export</span> CUDA_DEVICE_MAX_CONNECTIONS=1<br><br>GPUS_PER_NODE=4<br><span class="hljs-comment"># Change for multinode config</span><br>MASTER_ADDR=localhost<br>MASTER_PORT=6000<br>NUM_NODES=1<br>NODE_RANK=0<br>WORLD_SIZE=$((<span class="hljs-variable">$GPUS_PER_NODE</span>*<span class="hljs-variable">$NUM_NODES</span>))<br><br>CHECKPOINT_PATH=<span class="hljs-variable">$1</span> <span class="hljs-comment">#&lt;Specify path&gt;</span><br>TENSORBOARD_LOGS_PATH=<span class="hljs-variable">$2</span> <span class="hljs-comment">#&lt;Specify path&gt;</span><br>VOCAB_FILE=<span class="hljs-variable">$3</span> <span class="hljs-comment">#&lt;Specify path to file&gt;/gpt2-vocab.json</span><br>MERGE_FILE=<span class="hljs-variable">$4</span> <span class="hljs-comment">#&lt;Specify path to file&gt;/gpt2-merges.txt</span><br>DATA_PATH=<span class="hljs-variable">$5</span> <span class="hljs-comment">#&lt;Specify path and file prefix&gt;_text_document</span><br>USE_NSYS=0<br><span class="hljs-keyword">if</span> [[ <span class="hljs-variable">$&#123;6:-&#125;</span> == <span class="hljs-string">&quot;--nsys&quot;</span> ]]; <span class="hljs-keyword">then</span><br>  USE_NSYS=1<br><span class="hljs-keyword">fi</span><br><br>DISTRIBUTED_ARGS=(<br>    --nproc_per_node <span class="hljs-variable">$GPUS_PER_NODE</span> <br>    --nnodes <span class="hljs-variable">$NUM_NODES</span> <br>    --master_addr <span class="hljs-variable">$MASTER_ADDR</span> <br>    --master_port <span class="hljs-variable">$MASTER_PORT</span><br>)<br><br>GPT_MODEL_ARGS=(<br>    --num-layers 24 <br>    --hidden-size 1024 <br>    --num-attention-heads 16 <br>    --seq-length 2048 <br>    --max-position-embeddings 2048 <br>    --attention-backend auto <span class="hljs-comment"># Can use (flash/fused/unfused/local)</span><br>)<br><br>TRAINING_ARGS=(<br>    --micro-batch-size 4 <br>    --global-batch-size 16 <br>    <span class="hljs-comment"># --rampup-batch-size 16 16 5859375 </span><br>    --train-iters 20000 <br>    --weight-decay 0.1 <br>    --adam-beta1 0.9 <br>    --adam-beta2 0.95 <br>    --init-method-std 0.006 <br>    --clip-grad 1.0 <br>    --fp16<br>    --lr 6.0e-5 <br>    --lr-decay-style cosine <br>    --min-lr 6.0e-6<br>    --lr-warmup-fraction .001 <br>    --lr-decay-iters 20000 <br>)<br><br>MODEL_PARALLEL_ARGS=(<br>  --tensor-model-parallel-size 1 <br>  --pipeline-model-parallel-size 1 <br>)<br><br>DATA_ARGS=(<br>    --data-path <span class="hljs-variable">$DATA_PATH</span> <br>    --vocab-file <span class="hljs-variable">$VOCAB_FILE</span> <br>    --merge-file <span class="hljs-variable">$MERGE_FILE</span> <br>    --<span class="hljs-built_in">split</span> 949,50,1<br>)<br><br>EVAL_AND_LOGGING_ARGS=(<br>    --log-interval 200<br>    --save-interval 10000 <br>    --eval-interval 1000 <br>    --save <span class="hljs-variable">$CHECKPOINT_PATH</span> <br>    --load <span class="hljs-variable">$CHECKPOINT_PATH</span> <br>    --eval-iters 10<br>    --tensorboard-dir <span class="hljs-variable">$TENSORBOARD_LOGS_PATH</span> <br>)<br><br>PROFILER_ARGS=(<br>    --profile<br>    --profile-step-start 110<br>    --profile-step-end 112<br>    --profile-ranks 0<br>)<br><br><span class="hljs-comment"># Build command as an array (no string concatenation)</span><br>CMD=(<br>  torchrun<br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;DISTRIBUTED_ARGS[@]&#125;</span>&quot;</span><br>  pretrain_gpt.py<br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;GPT_MODEL_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;TRAINING_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;MODEL_PARALLEL_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;DATA_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;EVAL_AND_LOGGING_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;PROFILER_ARGS[@]&#125;</span>&quot;</span><br>)<br><br><span class="hljs-keyword">if</span> [[ <span class="hljs-string">&quot;<span class="hljs-variable">$USE_NSYS</span>&quot;</span> -eq 1 ]]; <span class="hljs-keyword">then</span><br>  NSIGHT_PREFIX=<span class="hljs-string">&quot;./nsight_profile/gpt3_857m&quot;</span><br>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Running with Nsight profiling, output prefix: <span class="hljs-variable">$&#123;NSIGHT_PREFIX&#125;</span>&quot;</span><br>  <span class="hljs-built_in">exec</span> nsys profile \<br>    -s none -t nvtx,cuda \<br>    --cudabacktrace=all \<br>    --cuda-graph-trace=node \<br>    --python-backtrace=cuda \<br>    --<span class="hljs-built_in">wait</span> all \<br>    -o <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;NSIGHT_PREFIX&#125;</span>&quot;</span> \<br>    --force-overwrite <span class="hljs-literal">true</span> \<br>    --capture-range=cudaProfilerApi \<br>    --capture-range-end=stop \<br>    <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;CMD[@]&#125;</span>&quot;</span><br><span class="hljs-keyword">else</span><br>  <span class="hljs-built_in">exec</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;CMD[@]&#125;</span>&quot;</span><br><span class="hljs-keyword">fi</span><br></code></pre></td></tr></table></figure><p>运行的指令如下，注意这里添加了<code>--nsys</code>来在脚本中用nsys启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash examples/gpt3/train_gpt3_857m_distributed.sh     /workspace/megatron-lm/model_ckpt/gpt3_857m_2     /workspace/megatron-lm/tb_logs/gpt3_857m_profiler     /workspace/megatron-lm/data/tokenizer/gpt2-vocab.json     /workspace/megatron-lm/data/tokenizer/gpt2-merges.txt     /workspace/megatron-lm/data/TinyStoriesV2-GPT4-train_text_document     --nsys      &gt; gpt3_857m2.log 2&gt;&amp;1 &amp;<br></code></pre></td></tr></table></figure><p>最后会得到<code>nsight_profile/gpt3_857m.nsys-rep</code>，将其放入Nsight Systems中查看结果如下：</p><p><img src="/2025/12/26/megatron-lm-profiler/image-4.png"></p><p>确实看下来是更底层了些，cuda相关的分析更加全面了。</p><h1 id="Memory-Snap显存分析"><a href="#Memory-Snap显存分析" class="headerlink" title="Memory Snap显存分析"></a>Memory Snap显存分析</h1><h2 id="使用方法-2"><a href="#使用方法-2" class="headerlink" title="使用方法"></a>使用方法</h2><p>Pytorch的Memory snap的整体使用方法如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">torch.cuda.memory._record_memory_history()               <span class="hljs-comment"># 开始记录</span><br>run_your_code()                                          <span class="hljs-comment"># 训练或推理代码</span><br>torch.cuda.memory._dump_snapshot(<span class="hljs-string">&quot;my_snapshot.pickle&quot;</span>)   <span class="hljs-comment"># 保存文件</span><br>torch.cuda.memory._record_memory_history(enabled=None)   <span class="hljs-comment"># 终止记录</span><br></code></pre></td></tr></table></figure><p>运行后得到<code>my_snapshot.pickle</code>，然后可以到<a href="https://docs.pytorch.org/memory/_viz%E4%B8%AD%E8%BF%9B%E8%A1%8C%E6%9F%A5%E7%9C%8B%E3%80%82">https://docs.pytorch.org/memory\_viz中进行查看。</a></p><h2 id="相关代码-2"><a href="#相关代码-2" class="headerlink" title="相关代码"></a>相关代码</h2><p>其相关参数的介绍如下，有<code>--record-memory-history</code>与<code>--memory-snapshot-path</code>两个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">group.add_argument(<span class="hljs-string">&#x27;--record-memory-history&#x27;</span>, action=<span class="hljs-string">&quot;store_true&quot;</span>, default=<span class="hljs-literal">False</span>,<br>                   <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;Record memory history in last rank.&#x27;</span>)<br>group.add_argument(<span class="hljs-string">&#x27;--memory-snapshot-path&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=<span class="hljs-string">&quot;snapshot.pickle&quot;</span>,<br>                   <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;Specifies where to dump the memory history pickle.&#x27;</span>)<br></code></pre></td></tr></table></figure><p>在<code>pretrain_gpt.py</code>中的<code>model_provider</code>函数中，有如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> args.record_memory_history:<br>    torch.cuda.memory._record_memory_history(<br>        <span class="hljs-literal">True</span>,<br>        <span class="hljs-comment"># keep 100,000 alloc/free events from before the snapshot</span><br>        trace_alloc_max_entries=<span class="hljs-number">100000</span>,<br>        <span class="hljs-comment"># record stack information for the trace events</span><br>        trace_alloc_record_context=<span class="hljs-literal">True</span>,<br>    )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">oom_observer</span>(<span class="hljs-params">device, alloc, device_alloc, device_free</span>):<br>        <span class="hljs-comment"># snapshot right after an OOM happened</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;saving allocated state during OOM&#x27;</span>)<br>        snapshot = torch.cuda.memory._snapshot()<br>        <span class="hljs-keyword">from</span> pickle <span class="hljs-keyword">import</span> dump<br><br>        dump(<br>            snapshot,<br>            <span class="hljs-built_in">open</span>(<span class="hljs-string">f&quot;oom_rank-<span class="hljs-subst">&#123;torch.distributed.get_rank()&#125;</span>_<span class="hljs-subst">&#123;args.memory_snapshot_path&#125;</span>&quot;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>),<br>        )<br><br>    torch._C._cuda_attach_out_of_memory_observer(oom_observer)<br></code></pre></td></tr></table></figure><p>可以看到启动snap的条件是配置上<code>--record-memory-history</code>（对应 args.record_memory_history&#x3D;True）,然后snap的配置也写的比较死，就是直接写定最多保留 10 万条 alloc&#x2F;free 事件，并为每条 alloc&#x2F;free 记录调用栈&#x2F;上下文信息。然后这里还定义了一个<code>oom_observer</code>,主要作用是在oom的时候调用该函数，然后将当前显存调用情况保存下来。</p><p>然后在<code>megatron/training/training.py</code>的<code>training_log</code>函数中，存在如下的代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">if</span> iteration % args.log_interval == 0:<br>    <span class="hljs-keyword">if</span> args.record_memory_history and is_last_rank():<br>        snapshot = torch.cuda.memory._snapshot()<br>        from pickle import dump<br><br>        with open(args.memory_snapshot_path, <span class="hljs-string">&#x27;wb&#x27;</span>) as f:<br>            dump(snapshot, f)<br></code></pre></td></tr></table></figure><p>即如果当前迭代次数是<code>log_interval</code>的整数倍，并且标记了要记录Memory情况并且是最后一个rank，那么就将当前的snapshot保存下来。</p><h2 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h2><p>运行的脚本如下，关键是在<code>PROFILER_ARGS</code>参数中添加了<code>--record-memory-history</code>以及<code>--memory-snapshot-path &#39;./snapshot/snapshot.pickle&#39;</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><br><span class="hljs-comment"># Runs the &quot;857m&quot; parameter model</span><br><br><span class="hljs-built_in">export</span> CUDA_DEVICE_MAX_CONNECTIONS=1<br><br>GPUS_PER_NODE=4<br><span class="hljs-comment"># Change for multinode config</span><br>MASTER_ADDR=localhost<br>MASTER_PORT=6000<br>NUM_NODES=1<br>NODE_RANK=0<br>WORLD_SIZE=$((<span class="hljs-variable">$GPUS_PER_NODE</span>*<span class="hljs-variable">$NUM_NODES</span>))<br><br>CHECKPOINT_PATH=<span class="hljs-variable">$1</span> <span class="hljs-comment">#&lt;Specify path&gt;</span><br>TENSORBOARD_LOGS_PATH=<span class="hljs-variable">$2</span> <span class="hljs-comment">#&lt;Specify path&gt;</span><br>VOCAB_FILE=<span class="hljs-variable">$3</span> <span class="hljs-comment">#&lt;Specify path to file&gt;/gpt2-vocab.json</span><br>MERGE_FILE=<span class="hljs-variable">$4</span> <span class="hljs-comment">#&lt;Specify path to file&gt;/gpt2-merges.txt</span><br>DATA_PATH=<span class="hljs-variable">$5</span> <span class="hljs-comment">#&lt;Specify path and file prefix&gt;_text_document</span><br>USE_NSYS=0<br><span class="hljs-keyword">if</span> [[ <span class="hljs-variable">$&#123;6:-&#125;</span> == <span class="hljs-string">&quot;--nsys&quot;</span> ]]; <span class="hljs-keyword">then</span><br>  USE_NSYS=1<br><span class="hljs-keyword">fi</span><br><br>DISTRIBUTED_ARGS=(<br>    --nproc_per_node <span class="hljs-variable">$GPUS_PER_NODE</span> <br>    --nnodes <span class="hljs-variable">$NUM_NODES</span> <br>    --master_addr <span class="hljs-variable">$MASTER_ADDR</span> <br>    --master_port <span class="hljs-variable">$MASTER_PORT</span><br>)<br><br>GPT_MODEL_ARGS=(<br>    --num-layers 24 <br>    --hidden-size 1024 <br>    --num-attention-heads 16 <br>    --seq-length 2048 <br>    --max-position-embeddings 2048 <br>    --attention-backend auto <span class="hljs-comment"># Can use (flash/fused/unfused/local)</span><br>)<br><br>TRAINING_ARGS=(<br>    --micro-batch-size 4 <br>    --global-batch-size 16 <br>    <span class="hljs-comment"># --rampup-batch-size 16 16 5859375 </span><br>    --train-iters 20000 <br>    --weight-decay 0.1 <br>    --adam-beta1 0.9 <br>    --adam-beta2 0.95 <br>    --init-method-std 0.006 <br>    --clip-grad 1.0 <br>    --fp16<br>    --lr 6.0e-5 <br>    --lr-decay-style cosine <br>    --min-lr 6.0e-6<br>    --lr-warmup-fraction .001 <br>    --lr-decay-iters 20000 <br>)<br><br>MODEL_PARALLEL_ARGS=(<br>  --tensor-model-parallel-size 1 <br>  --pipeline-model-parallel-size 1 <br>)<br><br>DATA_ARGS=(<br>    --data-path <span class="hljs-variable">$DATA_PATH</span> <br>    --vocab-file <span class="hljs-variable">$VOCAB_FILE</span> <br>    --merge-file <span class="hljs-variable">$MERGE_FILE</span> <br>    --<span class="hljs-built_in">split</span> 949,50,1<br>)<br><br>EVAL_AND_LOGGING_ARGS=(<br>    --log-interval 200<br>    --save-interval 10000 <br>    --eval-interval 1000 <br>    --save <span class="hljs-variable">$CHECKPOINT_PATH</span> <br>    --load <span class="hljs-variable">$CHECKPOINT_PATH</span> <br>    --eval-iters 10<br>    --tensorboard-dir <span class="hljs-variable">$TENSORBOARD_LOGS_PATH</span> <br>)<br><br>PROFILER_ARGS=(<br>    --profile<br>    --record-memory-history<br>    --profile-step-start 110<br>    --profile-step-end 112<br>    --profile-ranks 0<br>)<br><br><span class="hljs-comment"># Build command as an array (no string concatenation)</span><br>CMD=(<br>  torchrun<br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;DISTRIBUTED_ARGS[@]&#125;</span>&quot;</span><br>  pretrain_gpt.py<br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;GPT_MODEL_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;TRAINING_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;MODEL_PARALLEL_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;DATA_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;EVAL_AND_LOGGING_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;PROFILER_ARGS[@]&#125;</span>&quot;</span><br>)<br><br><span class="hljs-keyword">if</span> [[ <span class="hljs-string">&quot;<span class="hljs-variable">$USE_NSYS</span>&quot;</span> -eq 1 ]]; <span class="hljs-keyword">then</span><br>  NSIGHT_PREFIX=<span class="hljs-string">&quot;./nsight_profile/gpt3_857m&quot;</span><br>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Running with Nsight profiling, output prefix: <span class="hljs-variable">$&#123;NSIGHT_PREFIX&#125;</span>&quot;</span><br>  <span class="hljs-built_in">exec</span> nsys profile \<br>    -s none -t nvtx,cuda \<br>    --cudabacktrace=all \<br>    --cuda-graph-trace=node \<br>    --python-backtrace=cuda \<br>    --<span class="hljs-built_in">wait</span> all \<br>    -o <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;NSIGHT_PREFIX&#125;</span>&quot;</span> \<br>    --force-overwrite <span class="hljs-literal">true</span> \<br>    --capture-range=cudaProfilerApi \<br>    --capture-range-end=stop \<br>    <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;CMD[@]&#125;</span>&quot;</span><br><span class="hljs-keyword">else</span><br>  <span class="hljs-built_in">exec</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;CMD[@]&#125;</span>&quot;</span><br><span class="hljs-keyword">fi</span><br></code></pre></td></tr></table></figure><p>运行指令为：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">bash examples/gpt3/train_gpt3_857m_distributed.sh     /workspace/megatron-lm/model_ckpt/gpt3_857m_2     /workspace/megatron-lm/tb_logs/gpt3_857m_profiler     /workspace/megatron-lm/data/tokenizer/gpt2-vocab.json     /workspace/megatron-lm/data/tokenizer/gpt2-merges.txt     /workspace/megatron-lm/data/TinyStoriesV2-GPT4-train_text_document      &gt; gpt3_857m2.log 2&gt;&amp;1 &amp;<br></code></pre></td></tr></table></figure><p>运行后会得到<code>snapshot/snapshot.pickle</code>，将其放入到<a href="https://docs.pytorch.org/memory/_viz%E4%B8%AD%E8%BF%9B%E8%A1%8C%E6%9F%A5%E7%9C%8B%EF%BC%8C%E7%BB%93%E6%9E%9C%E5%A6%82%E4%B8%8B%EF%BC%9A">https://docs.pytorch.org/memory\_viz中进行查看，结果如下：</a></p><p><img src="/2025/12/26/megatron-lm-profiler/image-5.png"></p><p>其最底层的就是基础的模型、优化器的显存占用，上面的动态激活显存可以看到呈现明显的周期性，其显存占用最高的时候就是通过cross_entropy计算loss的时候，可以达到约15GB。这是因为这时前向传播的激活全部都计算完毕，后续反向传播的时候激活依次释放。</p>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>Megatron-LM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Megatron-LM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读】ScheMoE:An Extensible Mixture-of-Experts  Distributed Training System with Tasks Scheduling</title>
    <link href="/2025/12/22/ScheMoE-paper-note/"/>
    <url>/2025/12/22/ScheMoE-paper-note/</url>
    
    <content type="html"><![CDATA[<ul><li><p><strong>链接：</strong><a href="https://dl.acm.org/doi/10.1145/3627703.3650083">https://dl.acm.org/doi/10.1145/3627703.3650083</a></p></li><li><p><strong>发表会议：EuroSys ‘24(CCF-A)&#x20;</strong></p></li><li><p><strong>团队：哈工大、香港科技、华为</strong></p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul><li>Moe架构的大模型愈发流行，而Moe流程中存在将数据通过通信转发给对应GPU上专家的行为，如下图是数据并行与专家并行的示例，这种通信行为会导致系统训练效率降低。</li></ul><p><img src="/2025/12/22/ScheMoE-paper-note/image-4.png"></p><ul><li><p>现有的优化Moe训练效率的方法有正交的3类：</p><ol><li><p>设计负载平衡的路由函数，使GPU计算更加平衡</p></li><li><p>设计高效的通信方法，如节点间和节点内分层的All2All算法、数据压缩算法</p></li><li><p>通过将大矩阵转化为小矩阵来允许设计任务调度算法流水线并行通信任务和计算任务</p></li></ol><p><img src="/2025/12/22/ScheMoE-paper-note/image-3.png"></p></li><li><p>在系统优化方面，上述的b、c更加重要，而现有相关的Moe优化系统的主要不足在于：</p><ol><li><p>可拓展性不足，难以有效支持新的通信优化算法</p></li><li><p>All2All优化算法在利用集群内的节点间与节点内带宽有不足</p></li><li><p>调度算法在并行通信任务和计算任务方面最次优</p></li></ol></li><li><p>故本文提出了一个具有最优任务调度的可扩展且高效的MoE系统ScheMoE，该系统支持对关键操作的高效模块化拓展，并提出了自适应最优调度算法来流水线通信和计算任务，提出了一种All2All通信优化算法Pipe-A2A。</p></li></ul><h2 id="方法思路"><a href="#方法思路" class="headerlink" title="方法思路"></a>方法思路</h2><h3 id="ScheMoE系统"><a href="#ScheMoE系统" class="headerlink" title="ScheMoE系统"></a>ScheMoE系统</h3><ul><li><p>对于一个经典的MoE系统，其一个模块主要包含7个事件，如下图所示：</p><ol><li><p>压缩</p></li><li><p>A2A</p></li><li><p>解压</p></li><li><p>专家计算</p></li><li><p>压缩</p></li><li><p>A2A</p></li><li><p>解压</p></li></ol></li></ul><p><img src="/2025/12/22/ScheMoE-paper-note/image-1.png"></p><ul><li><p>系统在设计时对这些流程中的计算任务（数据压缩、数据解压和专家计算）和通信任务（A2A通信）都做了良好的抽象，用户可以通过拓展基础类继续实现自己的方法</p></li><li><p>系统还有自己的任务队列、分析器与任务调度器（方便拓展自定义调度算法）</p></li></ul><h3 id="调度优化"><a href="#调度优化" class="headerlink" title="调度优化"></a>调度优化</h3><ul><li><p>其依旧使用将大矩阵转化为小矩阵的流水线并行的方法，并且限制只有计算与通信可以折叠</p></li><li><p>通过理论分析得出在这种多套7个事件并行重叠中最优结果的一些规律，使得最终最优重叠如下图(c)所示</p></li></ul><p><img src="/2025/12/22/ScheMoE-paper-note/image-2.png"></p><h3 id="通信优化"><a href="#通信优化" class="headerlink" title="通信优化"></a>通信优化</h3><ul><li>其引入了两个异步通信流，分别是节点内通信流和节点间通信流，从而允许这两个通信流可以并行执行，如下图所示，这样就使得相比顺序执行速度更快</li></ul><p><img src="/2025/12/22/ScheMoE-paper-note/image.png"></p><h2 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h2><ul><li>实验在具有8个节点、32GPU的集群上开展，实验验证了各个子优化算法的有效性，并且实验验证了ScheMoE平均比现有最先进系统快1.09×-1.3倍。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><p>对MoE系统的讲解还是很受益的</p></li><li><p>本文的重点还是在提出系统方面，算法方面的创新感觉并不强，对于这类就7个流程下的计算与通信的重叠可能能继续深挖的点确实也不多，此外这个通信优化还是有点疑虑，应该是一个很明显的优化点才对，之前的工作肯定也有尝试过，可能还是存在某些情况使得两个异步通信流不是很受欢迎。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>LLM</tag>
      
      <tag>MoE</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Megatron-LM源码分析（二）】-GPT模型pretrain流程</title>
    <link href="/2025/12/22/megatron-lm-pre-train-process/"/>
    <url>/2025/12/22/megatron-lm-pre-train-process/</url>
    
    <content type="html"><![CDATA[<p>本次查看Megatron-LM的版本是<a href="https://github.com/NVIDIA/Megatron-LM/tree/core_r0.14.0"><code>core_r0.14.0</code></a>，查看的GPT训练文件是<code>pretrain_gpt.py</code></p><h1 id="入口函数"><a href="#入口函数" class="headerlink" title="入口函数"></a>入口函数</h1><p>main入口函数代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br><br>    <span class="hljs-comment"># Temporary for transition to core datasets</span><br>    train_valid_test_datasets_provider.is_distributed = <span class="hljs-literal">True</span><br><br>    <span class="hljs-comment"># Optionally enable inprocess restart on pretrain</span><br>    pretrain, store = inprocess_restart.maybe_wrap_for_inprocess_restart(pretrain)<br><br>    pretrain(<br>        train_valid_test_datasets_provider,<br>        model_provider,<br>        ModelType.encoder_or_decoder,<br>        forward_step,<br>        args_defaults=&#123;<span class="hljs-string">&#x27;tokenizer_type&#x27;</span>: <span class="hljs-string">&#x27;GPT2BPETokenizer&#x27;</span>&#125;,<br>        extra_args_provider=add_modelopt_args <span class="hljs-keyword">if</span> has_nvidia_modelopt <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>        store=store,<br>    )<br><br></code></pre></td></tr></table></figure><p>其功能主要为：</p><ol><li><p>临时函数，告诉数据集构建器这是一个分布式训练环境，需要在多个进程间协调数据集构建</p></li><li><p>可选地启用进程内重启功能，为训练函数添加故障恢复能力，允许在 GPU 故障时自动重启而不中断整个作业</p></li><li><p>调用核心pretrain函数并传入自定义相关函数作为参数进行训练</p></li></ol><h1 id="进程重启功能"><a href="#进程重启功能" class="headerlink" title="进程重启功能"></a>进程重启功能</h1><p>其调用的是<code>maybe_wrap_for_inprocess_restart</code>函数，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">maybe_wrap_for_inprocess_restart</span>(<span class="hljs-params">pretrain</span>):<br><br>    args = arguments.parse_args(ignore_unknown_args=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">if</span> args.inprocess_restart:<br>        pretrain = inprocess_restart(pretrain, args)<br><br>        torch.distributed.TCPStore(<br>            host_name=os.environ[<span class="hljs-string">&#x27;MASTER_ADDR&#x27;</span>],      <span class="hljs-comment"># 主节点 IP 地址</span><br>            port=<span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&#x27;MASTER_PORT&#x27;</span>])+<span class="hljs-number">1</span>,    <span class="hljs-comment"># 端口 (避免与主通信冲突)</span><br>            world_size=<span class="hljs-built_in">int</span>(os.getenv(<span class="hljs-string">&#x27;WORLD_SIZE&#x27;</span>, <span class="hljs-string">&#x27;1&#x27;</span>)),  <span class="hljs-comment"># 总进程数</span><br>            is_master=(<span class="hljs-built_in">int</span>(os.getenv(<span class="hljs-string">&#x27;RANK&#x27;</span>, <span class="hljs-string">&#x27;0&#x27;</span>)) == <span class="hljs-number">0</span>),  <span class="hljs-comment"># 是否为主进程</span><br>            timeout=timedelta(seconds=<span class="hljs-number">300</span>),           <span class="hljs-comment"># 连接超时 (5分钟)</span><br>            wait_for_workers=<span class="hljs-literal">True</span>,                    <span class="hljs-comment"># 等待所有 worker 连接</span><br>            use_libuv=<span class="hljs-literal">True</span>,                          <span class="hljs-comment"># 使用 libuv 提高性能</span><br>        )<br>    <span class="hljs-keyword">else</span>:<br>        store = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">return</span> pretrain, store<br></code></pre></td></tr></table></figure><p>其主要功能是查看是否带有<code>inprocess_restart</code>启动参数，如果没有就不操作，如果有就继续操作，包括：</p><ul><li><p>调用inprocess_restart对pretrain关键函数进行包装</p></li><li><p>创建TCPStore，TCPStore类似于是一个分布式KV存储系统，充当控制面。它作用有：</p><ul><li><p>底层采用TCP协议，所以如果NCCL或训练的通信组出错也不会受影响。注意使用的是<code>int(os.environ[&#39;MASTER_PORT&#39;]) + 1</code>端口，以避免端口冲突</p></li><li><p><code>wait_for_workers=True</code>参数确保等待所有worker都正常运行</p></li><li><p>用以控制保证所有的worker都进入了新一轮的训练</p></li><li><p>其容错的运行流程类似如下</p></li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">1. 训练开始 → 所有进程连接到 TCPStore<br>2. 进程 A 检测到 GPU 故障 → 向 TCPStore 报告<br>3. TCPStore 广播故障状态 → 所有进程暂停<br>4. 协调重启决策 → 隔离故障进程<br>5. 启动新进程 → 通过 TCPStore 同步状态<br>6. 所有进程确认 → 恢复训练<br></code></pre></td></tr></table></figure><blockquote><p>不过注意的是如果一个节点确实损坏了，它无法找到新的节点来替代，只能不断地重启了，除非有足够的热备节点</p></blockquote><p>调用<code>inprocess_restart</code>对<code>pretrain</code>关键函数进行包装的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">inprocess_restart</span>(<span class="hljs-params">train, args</span>):<br>    <span class="hljs-keyword">if</span> inprocess <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        warnings.warn(<span class="hljs-string">&#x27;In-process restart is not available&#x27;</span>)<br>        <span class="hljs-keyword">return</span> train<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;TORCH_CPP_LOG_LEVEL&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> os.environ <span class="hljs-keyword">or</span> os.environ[<span class="hljs-string">&#x27;TORCH_CPP_LOG_LEVEL&#x27;</span>] <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> (<br>        <span class="hljs-string">&#x27;error&#x27;</span>,<br>        <span class="hljs-string">&#x27;fatal&#x27;</span>,<br>    ):<br>        warnings.warn(<br>            <span class="hljs-string">&#x27;Set TORCH_CPP_LOG_LEVEL=error to suppress c10d waitForInput timeout warning messages&#x27;</span><br>        )<br><br>    <span class="hljs-comment"># Layers represents a configuration for a layer of branches at a certain</span><br>    <span class="hljs-comment"># depth in a topology tree constructed by inprocess.rank_assignment.Tree.</span><br>    <span class="hljs-comment"># First layer contains all ranks and it&#x27;s the root of the topology tree,</span><br>    <span class="hljs-comment"># the second optional layer groups ranks by nodes.</span><br>    layers = [<br>        inprocess.rank_assignment.Layer(<br>            min_ranks=args.inprocess_active_world_size,<br>            max_ranks=args.inprocess_active_world_size,<br>            flag=inprocess.rank_assignment.LayerFlag.RESERVE,<br>        )<br>    ]<br>    <span class="hljs-keyword">if</span> args.inprocess_granularity == <span class="hljs-string">&#x27;node&#x27;</span>:<br>        device_count = torch.cuda.device_count()<br><br>        layers.append(<br>            inprocess.rank_assignment.Layer(<br>                min_ranks=device_count,<br>                max_ranks=device_count,<br>                key_or_fn=<span class="hljs-keyword">lambda</span> _: socket.gethostname(),<br>                flag=inprocess.rank_assignment.LayerFlag.RESERVE,<br>            )<br>        )<br><br>    finalize = [<br>        inprocess.finalize.ThreadedFinalize(timeout=timedelta(seconds=<span class="hljs-number">10</span>), fn=destroy_state)<br>    ]<br><br>    <span class="hljs-keyword">if</span> args.inprocess_empty_cuda_cache:<br>        finalize.append(<br>            inprocess.finalize.ThreadedFinalize(<br>                timeout=timedelta(seconds=<span class="hljs-number">10</span>), fn=torch.cuda.empty_cache<br>            )<br>        )<br><br>    initialize = inprocess.Compose(<br>        inprocess.initialize.RetryController(min_world_size=args.inprocess_active_world_size),<br>        inprocess.nested_restarter.NestedRestarterHandlingCompleted(),<br>    )<br>    abort = inprocess.Compose(<br>        inprocess.abort.AbortTransformerEngine(),<br>        inprocess.abort.AbortTorchDistributed(),<br>        inprocess.nested_restarter.NestedRestarterHandlingStarting(),<br>    )<br>    completion = inprocess.nested_restarter.NestedRestarterFinalized()<br>    terminate = inprocess.nested_restarter.NestedRestarterAborted()<br><br>    train = inprocess.Wrapper(<br>        store_kwargs=&#123;<br>            <span class="hljs-string">&#x27;timeout&#x27;</span>: timedelta(seconds=<span class="hljs-number">300</span>),<br>            <span class="hljs-string">&#x27;port&#x27;</span>: <span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&#x27;MASTER_PORT&#x27;</span>]) + <span class="hljs-number">2</span>,<br>        &#125;,<br>        initialize=initialize,<br>        abort=abort,<br>        completion=completion,<br>        terminate=terminate,<br>        health_check=inprocess.health_check.CudaHealthCheck(timeout=timedelta(seconds=<span class="hljs-number">10</span>)),<br>        rank_assignment=inprocess.rank_assignment.Tree(layers=layers),<br>        finalize=inprocess.Compose(*finalize),<br>        heartbeat_interval=timedelta(seconds=args.inprocess_heartbeat_interval),<br>        heartbeat_timeout=timedelta(seconds=args.inprocess_heartbeat_timeout),<br>        barrier_timeout=timedelta(seconds=args.inprocess_barrier_timeout),<br>        completion_timeout=timedelta(seconds=args.inprocess_completion_timeout),<br>        monitor_process_interval=timedelta(seconds=args.inprocess_monitor_process_interval),<br>        monitor_thread_interval=timedelta(seconds=args.inprocess_monitor_thread_interval),<br>        last_call_wait=timedelta(seconds=args.inprocess_last_call_wait),<br>        soft_timeout=timedelta(seconds=args.inprocess_soft_timeout),<br>        hard_timeout=timedelta(seconds=args.inprocess_hard_timeout),<br>        termination_grace_time=timedelta(seconds=args.inprocess_termination_grace_time),<br>        enabled=<span class="hljs-literal">True</span>,<br>    )(train)<br><br>    <span class="hljs-keyword">return</span> train<br></code></pre></td></tr></table></figure><p>其主要功能为：</p><ol><li><p>查看是否成功从<code>import nvidia_resiliency_ext.inprocess as inprocess</code>引入<code>inprocess</code>，如果没有就直接返回</p></li><li><p>提醒设置日志级别</p></li><li><p>构建Layers：（这里的Layers有啥作用没咋看懂）</p><ol><li><p>设置最小 &#x2F; 最大存活 rank以及是否采用RESERVE模式</p></li><li><p>如果是node粒度还需要再构建node层级的layers，以做到node级别的移除</p></li></ol></li><li><p>构建 abort 之后 &#x2F; restart 之前 执行的清理逻辑finalize，包含的处理逻辑有</p><ol><li><p><code>destroy_state</code>：</p><ul><li><p>destroy process group</p></li><li><p>释放 NCCL communicator</p></li><li><p>清理 Megatron 内部全局状态</p></li></ul></li><li><p><code>empty_cache</code>（可选）：</p><ul><li><p>清除从cache</p></li><li><p>在OOM场景下很有用</p></li></ul></li></ol></li><li><p>再就是构建状态机中Initialize &#x2F; Abort &#x2F; Completion &#x2F; Terminate这四个状态：</p><ol><li><p>initialize：等待至少 <code>min_world_size</code> 个 rank 可用</p></li><li><p>abort（失败时触发）：负责停 Transformer Engine，abort torch.distributed，通知 nested restarter开始重启</p></li><li><p>completion（正常结束）：标记这一轮执行完成，不触发 restart</p></li><li><p>terminate（彻底失败）： 直接终止，不再尝试恢复</p></li></ol></li><li><p>包装训练函数：</p><ol><li><p>设置了上述的状态机</p></li><li><p>设置了很多timeout</p></li><li><p>将端口设置为<code>int(os.environ[&#39;MASTER_PORT&#39;]) + 2</code>以避免端口冲突</p></li></ol></li></ol><h1 id="pretrain参数"><a href="#pretrain参数" class="headerlink" title="pretrain参数"></a>pretrain参数</h1><p>pretrain是训练的核心入口，它更加类似于一个训练流程的驱动，用户负责通过参数提供数据、模型、loss计算方法等，它负责对其进行组装然后将分布式训练策略、checkpoint、log等方法进行执行。</p><p>其函数定义如下，下面对其进行分组介绍：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pretrain</span>(<span class="hljs-params"></span><br><span class="hljs-params">    train_valid_test_dataset_provider,</span><br><span class="hljs-params">    model_provider,</span><br><span class="hljs-params">    model_type,</span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    process_non_loss_data_func=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    extra_args_provider=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    args_defaults=&#123;&#125;,</span><br><span class="hljs-params">    get_embedding_ranks=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    get_position_embedding_ranks=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    non_loss_data_func=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    store=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    inprocess_call_wrapper: <span class="hljs-type">Optional</span>[CallWrapper] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br></code></pre></td></tr></table></figure><h2 id="数据相关参数"><a href="#数据相关参数" class="headerlink" title="数据相关参数"></a>数据相关参数</h2><ul><li><p><code>train_valid_test_dataset_provider</code>负责告诉Megatron如何划分出train_ds、valid_ds、test_ds这3类数据集。</p><ul><li>本示例中传入的函数如下：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_valid_test_datasets_provider</span>(<span class="hljs-params">train_val_test_num_samples</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build the train test and validation datasets.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        train_val_test_num_samples : A list containing the number of samples in train test and validation.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    args = get_args()<br><br>    config = core_gpt_dataset_config_from_args(args)<br><br>    <span class="hljs-keyword">if</span> args.sft:<br>        dataset_type = SFTDataset<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">if</span> args.mock_data:<br>            dataset_type = MockGPTDataset<br>        <span class="hljs-keyword">else</span>:<br>            dataset_type = GPTDataset<br><br>    print_rank_0(<span class="hljs-string">&quot;&gt; building train, validation, and test datasets for GPT ...&quot;</span>)<br><br>    train_ds, valid_ds, test_ds = BlendedMegatronDatasetBuilder(<br>        dataset_type, train_val_test_num_samples, is_dataset_built_on_rank, config<br>    ).build()<br><br>    print_rank_0(<span class="hljs-string">&quot;&gt; finished creating GPT datasets ...&quot;</span>)<br><br>    <span class="hljs-keyword">return</span> train_ds, valid_ds, test_ds<br></code></pre></td></tr></table></figure><ul><li><p>其输入参数为<code>train_val_test_num_samples</code>，如注释所言，其代表的train、val、test对应的sample的数量</p></li><li><p>在函数里其根据参数得到<code>dataset_type</code></p></li><li><p>然后还有函数<code>is_dataset_built_on_rank</code>作为参数，该函数用于决定是否在当前进程构建数据集，其函数如下，实际效果是只在PP并行组的第一个和最后一个中的TP组的第一个rank构建数据集。</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">def is_dataset_built_on_rank():<br>    <span class="hljs-built_in">return</span> (<br>        parallel_state.is_pipeline_first_stage(ignore_virtual=True)<br>        or parallel_state.is_pipeline_last_stage(ignore_virtual=True)<br>    ) and parallel_state.get_tensor_model_parallel_rank() == 0<br></code></pre></td></tr></table></figure><ul><li><p>然后其构建了<code>BlendedMegatronDatasetBuilder</code>这个数据集处理类，并调用其build()函数得到了train_ds, valid_ds, test_ds。<code>BlendedMegatronDatasetBuilder</code>的功能主要如下，后面会再找机会详细介绍</p><ul><li><p>负责从多个数据源构建混合数据集</p></li><li><p>支持分布式训练环境下的数据集协调</p></li><li><p>提供高效的数据集缓存和并行构建机制</p></li></ul></li></ul></li></ul><h2 id="模型相关参数"><a href="#模型相关参数" class="headerlink" title="模型相关参数"></a>模型相关参数</h2><ul><li><p><code>model_provider</code>参数负责提供一个原始模型，即提供一个在CPU上的没有进行fp16转换没有ddp切割的原始模型。</p><ul><li>本示例中传入的函数如下：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_provider</span>(<span class="hljs-params"></span><br><span class="hljs-params">    pre_process=<span class="hljs-literal">True</span>, post_process=<span class="hljs-literal">True</span>, vp_stage: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span></span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Union</span>[GPTModel, megatron.legacy.model.GPTModel]:<br>    <span class="hljs-string">&quot;&quot;&quot;Builds the model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    If you set the use_legacy_models to True, it will return the legacy GPT model and if not the mcore GPT model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        pre_process (bool, optional): Set to true if you need to compute embedings. Defaults to True.</span><br><span class="hljs-string">        post_process (bool, optional): Set to true if you need to want to compute output logits/loss. Defaults to True.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        Union[GPTModel, megatron.legacy.model.GPTModel]: The returned model</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    args = get_args()<br><br>    <span class="hljs-keyword">if</span> has_nvidia_modelopt <span class="hljs-keyword">and</span> modelopt_args_enabled(args):  <span class="hljs-comment"># [ModelOpt]</span><br>        <span class="hljs-keyword">return</span> model_provider_modelopt(pre_process, post_process)<br><br>    use_te = args.transformer_impl == <span class="hljs-string">&quot;transformer_engine&quot;</span><br><br>    <span class="hljs-keyword">if</span> args.record_memory_history:<br>        torch.cuda.memory._record_memory_history(<br>            <span class="hljs-literal">True</span>,<br>            <span class="hljs-comment"># keep 100,000 alloc/free events from before the snapshot</span><br>            trace_alloc_max_entries=<span class="hljs-number">100000</span>,<br>            <span class="hljs-comment"># record stack information for the trace events</span><br>            trace_alloc_record_context=<span class="hljs-literal">True</span>,<br>        )<br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">oom_observer</span>(<span class="hljs-params">device, alloc, device_alloc, device_free</span>):<br>            <span class="hljs-comment"># snapshot right after an OOM happened</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;saving allocated state during OOM&#x27;</span>)<br>            snapshot = torch.cuda.memory._snapshot()<br>            <span class="hljs-keyword">from</span> pickle <span class="hljs-keyword">import</span> dump<br><br>            dump(<br>                snapshot,<br>                <span class="hljs-built_in">open</span>(<span class="hljs-string">f&quot;oom_rank-<span class="hljs-subst">&#123;torch.distributed.get_rank()&#125;</span>_<span class="hljs-subst">&#123;args.memory_snapshot_path&#125;</span>&quot;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>),<br>            )<br><br>        torch._C._cuda_attach_out_of_memory_observer(oom_observer)<br><br>    print_rank_0(<span class="hljs-string">&#x27;building GPT model ...&#x27;</span>)<br>    <span class="hljs-comment"># Experimental loading arguments from yaml</span><br>    <span class="hljs-keyword">if</span> args.yaml_cfg <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config = core_transformer_config_from_yaml(args, <span class="hljs-string">&quot;language_model&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        config = core_transformer_config_from_args(args)<br><br>    <span class="hljs-keyword">if</span> args.use_legacy_models:<br>        model = megatron.legacy.model.GPTModel(<br>            config,<br>            num_tokentypes=<span class="hljs-number">0</span>,<br>            parallel_output=<span class="hljs-literal">True</span>,<br>            pre_process=pre_process,<br>            post_process=post_process,<br>        )<br>    <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># using core models</span><br>        <span class="hljs-keyword">if</span> args.spec <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            transformer_layer_spec = import_module(args.spec)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> args.num_experts:<br>                <span class="hljs-comment"># Define the decoder block spec</span><br>                transformer_layer_spec = get_gpt_decoder_block_spec(<br>                    config, use_transformer_engine=use_te, normalization=args.normalization, qk_l2_norm=args.qk_l2_norm, vp_stage=vp_stage<br>                )<br>            <span class="hljs-keyword">elif</span> args.heterogeneous_layers_config_path <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                transformer_layer_spec = get_gpt_heterogeneous_layer_spec(config, use_te)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># Define the decoder layer spec</span><br>                transformer_layer_spec = _get_transformer_layer_spec(use_te, config)<br>        mtp_block_spec = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> args.mtp_num_layers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(transformer_layer_spec, <span class="hljs-string">&#x27;layer_specs&#x27;</span>) <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(transformer_layer_spec.layer_specs) == <span class="hljs-number">0</span>:<br>                <span class="hljs-comment"># Get the decoder layer spec explicitly if no decoder layer in the last stage,</span><br>                <span class="hljs-comment"># Only happens with block spec (TransformerBlockSubmodules) when using MoE.</span><br>                transformer_layer_spec_for_mtp = _get_transformer_layer_spec(use_te, config)<br>            <span class="hljs-keyword">else</span>:<br>                transformer_layer_spec_for_mtp = transformer_layer_spec<br>            mtp_block_spec = get_gpt_mtp_block_spec(<br>                config, transformer_layer_spec_for_mtp, use_transformer_engine=use_te, vp_stage=vp_stage<br>            )<br><br>        model = GPTModel(<br>            config=config,<br>            transformer_layer_spec=transformer_layer_spec,<br>            vocab_size=args.padded_vocab_size,<br>            max_sequence_length=args.max_position_embeddings,<br>            pre_process=pre_process,<br>            post_process=post_process,<br>            fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,<br>            parallel_output=<span class="hljs-literal">True</span>,<br>            share_embeddings_and_output_weights=<span class="hljs-keyword">not</span> args.untie_embeddings_and_output_weights,<br>            position_embedding_type=args.position_embedding_type,<br>            rotary_percent=args.rotary_percent,<br>            rotary_base=args.rotary_base,<br>            rope_scaling=args.use_rope_scaling,<br>            mtp_block_spec=mtp_block_spec,<br>            vp_stage=vp_stage,<br>        )<br><br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure><ul><li><p>该函数的参数为：</p><ul><li><p><code>pre_process</code>: 是否计算嵌入层（输入处理）</p></li><li><p><code>post_process</code>: 是否计算输出 logits&#x2F;损失</p></li><li><p><code>vp_stage</code>: 虚拟 pipeline stage（用于梯度累积优化）</p></li></ul></li><li><p>获取训练参数，如果启用 NVIDIA ModelOpt，委托给专门的 model provider</p></li><li><p>依据参数决定是否使用 Transformer Engine（NVIDIA 的高性能 transformer 实现）</p></li><li><p>依据参数决定是否启用内存历史记录用于调试 OOM 问题，以及是否自动保存内存快照到文件</p></li><li><p>依据yaml文件或者是输入参数args获取config</p></li><li><p>然后进入核心模型分支，依据参数获取transformer_layer_spec和mtp_block_spec</p></li><li><p>最后根据各参数构建出GPTModel</p></li></ul></li><li><p><code>model_type</code>：用于告诉 Megatron 模型的“拓扑语义”，其有这三类：</p><ul><li><p>encoder_or_decoder &#x3D; 1：传统的编码器-解码器模型，或仅包含解码器的自回归模型</p></li><li><p>retro_encoder &#x3D; 2：Retrieval-Enhanced Transformer (RETRO) 模型中的编码器组件，RETRO 是一种特殊的 transformer 架构，使用外部知识库进行检索增强</p></li><li><p>retro_decoder &#x3D; 3：RETRO 模型中的解码器组件，负责最终的文本生成</p></li></ul></li><li><p><code>get_embedding_ranks</code>：指定哪些 rank 持有 word embedding</p></li><li><p><code>get_position_embedding_ranks</code>：指定哪些 rank 持有 position embedding</p></li></ul><h2 id="Forward执行参数"><a href="#Forward执行参数" class="headerlink" title="Forward执行参数"></a>Forward执行参数</h2><ul><li><p>forward_step_func：最核心的训练函数，其定义了一次 iteration 的“前向 + loss 计算”逻辑，</p><ul><li><p>其主要负责：</p><ol><li><p>从 <code>data_iterator</code> 里取 batch</p></li><li><p>调用 <code>model(...)</code></p></li><li><p>计算 loss</p></li><li><p>返回：</p><ul><li><p>loss（标量 tensor）</p></li><li><p>dict（用于 logging 的指标）</p></li></ul></li></ol></li><li><p>示例中的代码如下所示，基本与上述功能一样，但是返回不一样了，合理怀疑是现在代码改了但是注释没改，返回的是计算结果以及计算loss的组合，此外还多了一些指标采集和专门的分支处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_step</span>(<span class="hljs-params">data_iterator, model: GPTModel, return_schedule_plan: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Forward training step.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        data_iterator : Input data iterator</span><br><span class="hljs-string">        model (GPTModel): The GPT Model</span><br><span class="hljs-string">        return_schedule_plan (bool): Whether to return the schedule plan instead of the output tensor</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    args = get_args()<br>    timers = get_timers()<br><br>    <span class="hljs-comment"># Get the batch.</span><br>    timers(<span class="hljs-string">&#x27;batch-generator&#x27;</span>, log_level=<span class="hljs-number">2</span>).start()<br>    <span class="hljs-keyword">global</span> stimer<br>    <span class="hljs-keyword">with</span> stimer(bdata=<span class="hljs-literal">True</span>):<br>        tokens, labels, loss_mask, attention_mask, position_ids = get_batch(data_iterator)<br>    timers(<span class="hljs-string">&#x27;batch-generator&#x27;</span>).stop()<br><br>    <span class="hljs-keyword">with</span> stimer:<br>        <span class="hljs-keyword">if</span> args.use_legacy_models:<br>            output_tensor = model(tokens, position_ids, attention_mask, labels=labels)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> return_schedule_plan:<br>                <span class="hljs-comment"># MoE 专家并行重叠通信模式</span><br>                <span class="hljs-keyword">assert</span> args.overlap_moe_expert_parallel_comm, \<br>                    <span class="hljs-string">&quot;overlap_moe_expert_parallel_comm must be enabled to return the schedule plan&quot;</span><br>                schedule_plan = model.build_schedule_plan(<br>                    tokens, position_ids, attention_mask, labels=labels, loss_mask=loss_mask<br>                )<br>                <span class="hljs-keyword">return</span> schedule_plan, partial(loss_func, loss_mask, model=model)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># 标准前向传播</span><br>                output_tensor = model(<br>                    tokens, position_ids, attention_mask, labels=labels, loss_mask=loss_mask<br>                )<br><br>    <span class="hljs-comment"># [ModelOpt]: model is needed to access ModelOpt distillation losses</span><br>    <span class="hljs-keyword">return</span> output_tensor, partial(loss_func, loss_mask, model=model)<br><br></code></pre></td></tr></table></figure><ul><li>其中的get_bach也是自定义的，如下，依据rank所属的并行组来获取数据。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_batch</span>(<span class="hljs-params">data_iterator</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Generate a batch.&quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> this is pretty hacky, find a better way</span><br>    <span class="hljs-keyword">if</span> (<span class="hljs-keyword">not</span> parallel_state.is_pipeline_first_stage(ignore_virtual=<span class="hljs-literal">True</span>)) <span class="hljs-keyword">and</span> (<br>        <span class="hljs-keyword">not</span> parallel_state.is_pipeline_last_stage(ignore_virtual=<span class="hljs-literal">True</span>)<br>    ):<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># get batches based on the TP rank you are on</span><br>    batch = get_batch_on_this_tp_rank(data_iterator)<br><br>    <span class="hljs-comment"># slice batch along sequence dimension for context parallelism</span><br>    batch = get_batch_on_this_cp_rank(batch)<br><br>    <span class="hljs-keyword">return</span> batch.values()<br></code></pre></td></tr></table></figure></li></ul></li><li><p><code>process_non_loss_data_func</code>：可选参数，用来处理<strong>不参与反向传播的数据</strong>，例如专门把一些数据dump到TensorBoard</p></li><li><p><code>non_loss_data_func</code>：在 evaluation 阶段执行自定义逻辑</p></li></ul><h2 id="参数与配置扩展"><a href="#参数与配置扩展" class="headerlink" title="参数与配置扩展"></a>参数与配置扩展</h2><ul><li><p><code>extra_args_provider</code>：允许“业务代码”向 Megatron 的 argparse 注入自定义参数</p></li><li><p><code>args_defaults</code>：覆盖 &#x2F; 预设 Megatron 参数默认值</p></li></ul><h2 id="分布式-容错相关参数"><a href="#分布式-容错相关参数" class="headerlink" title="分布式 &#x2F; 容错相关参数"></a>分布式 &#x2F; 容错相关参数</h2><ul><li><p><code>store</code>：提供一个外部的控制接口，如前述进程重启功能所示，在实例中就将控制面TCPStore传递给了该参数</p></li><li><p><code>inprocess_call_wrapper</code>： in-process restart 自动注入的“调用包装器”，负责捕获：Python exception、CUDA error，然后上报给 inprocess controller，决定是retry还是abort或是terminate等，普通用户不用传，开启 inprocess_restart 时自动生效</p></li></ul><h1 id="pretrain流程概览"><a href="#pretrain流程概览" class="headerlink" title="pretrain流程概览"></a>pretrain流程概览</h1><ul><li>pretrain的代码如下所示，上面已经对pretrain的传入参数进行了解析，下面先对其整体流程做进一步梳理</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pretrain</span>(<span class="hljs-params"></span><br><span class="hljs-params">    train_valid_test_dataset_provider,</span><br><span class="hljs-params">    model_provider,</span><br><span class="hljs-params">    model_type,</span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    process_non_loss_data_func=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    extra_args_provider=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    args_defaults=&#123;&#125;,</span><br><span class="hljs-params">    get_embedding_ranks=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    get_position_embedding_ranks=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    non_loss_data_func=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    store=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    inprocess_call_wrapper: <span class="hljs-type">Optional</span>[CallWrapper] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Main training program.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    This function will run the followings in the order provided:</span><br><span class="hljs-string">        1) initialize Megatron.</span><br><span class="hljs-string">        2) setup model, optimizer and lr schedule using the model_provider.</span><br><span class="hljs-string">        3) call train_val_test_data_provider to get train/val/test datasets.</span><br><span class="hljs-string">        4) train the model using the forward_step_func.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        train_valid_test_dataset_provider: a function that takes the size of</span><br><span class="hljs-string">            train/valid/test dataset and returns `train, valid, test` datasets.</span><br><span class="hljs-string">        model_provider: a function that returns a vanilla version of the</span><br><span class="hljs-string">            model. By vanilla we mean a simple model on cpu with no fp16 or ddp.</span><br><span class="hljs-string">        model_type: an enum that specifies the type of model being trained.</span><br><span class="hljs-string">        forward_step_func: a function that takes a `data iterator` and `model`,</span><br><span class="hljs-string">            and returns a `loss` scalar with a dictionary with key:values being</span><br><span class="hljs-string">            the info we would like to monitor during training, for example</span><br><span class="hljs-string">            `lm-loss: value`. We also require that this function add</span><br><span class="hljs-string">            `batch generator` to the timers class.</span><br><span class="hljs-string">        process_non_loss_data_func: a function to post process outputs of the</span><br><span class="hljs-string">            network. It can be used for dumping output tensors (e.g images) to</span><br><span class="hljs-string">            tensorboard. It takes `collected data`(list of tensors),</span><br><span class="hljs-string">            `current iteration index` and `tensorboard writer` as arguments.</span><br><span class="hljs-string">        extra_args_provider: a function that takes a parser and adds arguments</span><br><span class="hljs-string">            to it. It is used for programs to add their own arguments.</span><br><span class="hljs-string">        args_defaults: a dictionary from argument-name to argument-value. It</span><br><span class="hljs-string">            to set already parse arguments.</span><br><span class="hljs-string">        get_embedding_ranks (TODO):</span><br><span class="hljs-string">        get_position_embedding_ranks (TODO):</span><br><span class="hljs-string">        non_loss_data_func (callable): A custom function to call during evaluation.</span><br><span class="hljs-string">            It can run e.g. benchmarks.</span><br><span class="hljs-string">        store: an optional instance of torch.distributed.Store, to be used by</span><br><span class="hljs-string">            torch.distributed.init_process_group</span><br><span class="hljs-string">        inprocess_call_wrapper: an optional instance of inprocess.CallWrapper,</span><br><span class="hljs-string">            it is automatically injected when in-process restart is in use</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">if</span> inprocess_call_wrapper <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        iteration = inprocess_call_wrapper.iteration<br>        store = torch.distributed.PrefixStore(<span class="hljs-built_in">str</span>(iteration), store)<br><br>    <span class="hljs-comment"># Initalize and get arguments, timers, and Tensorboard writer.</span><br>    initialize_megatron(<br>        extra_args_provider=extra_args_provider,<br>        args_defaults=args_defaults,<br>        get_embedding_ranks=get_embedding_ranks,<br>        get_position_embedding_ranks=get_position_embedding_ranks,<br>        store=store,<br>    )<br><br>    args = get_args()<br>    timers = get_timers()<br><br>    <span class="hljs-keyword">if</span> args.log_progress:<br>        append_to_progress_log(<span class="hljs-string">&quot;Starting job&quot;</span>)<br><br>    <span class="hljs-comment"># Initialize fault tolerance</span><br>    <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> ft_integration functions other than `setup` are no-op if the FT is not initialized</span><br>    <span class="hljs-keyword">if</span> args.enable_ft_package:<br>        ft_integration.setup(args)<br>        ft_integration.maybe_setup_simulated_fault()<br><br>    <span class="hljs-comment"># Set pytorch JIT layer fusion options and warmup JIT functions.</span><br>    set_jit_fusion_options()<br><br>    <span class="hljs-comment"># Adjust the startup time so it reflects the largest value.</span><br>    <span class="hljs-comment"># This will be closer to what scheduler will see (outside of</span><br>    <span class="hljs-comment"># image ... launches.</span><br>    <span class="hljs-keyword">global</span> _TRAIN_START_TIME<br>    start_time_tensor = torch.tensor([_TRAIN_START_TIME], dtype=torch.double, device=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>    torch.distributed.all_reduce(start_time_tensor, op=torch.distributed.ReduceOp.MIN)<br>    _TRAIN_START_TIME = start_time_tensor.item()<br><br>    app_metrics = &#123;&#125;<br>    app_metrics[<span class="hljs-string">&#x27;app_start_time&#x27;</span>] = <span class="hljs-built_in">round</span>(_TRAIN_START_TIME * <span class="hljs-number">1000.0</span>)<br>    app_metrics[<span class="hljs-string">&#x27;app_model_init_start_time&#x27;</span>] = <span class="hljs-built_in">round</span>(_TRAIN_START_TIME * <span class="hljs-number">1000.0</span>)<br><br>    print_rank_0(<br>        <span class="hljs-string">&#x27;time to initialize megatron (seconds): &#123;:.3f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(time.time() - _TRAIN_START_TIME)<br>    )<br>    print_datetime(<span class="hljs-string">&#x27;after megatron is initialized&#x27;</span>)<br>    app_metrics[<span class="hljs-string">&#x27;app_model_init_finish_time&#x27;</span>] = one_logger_utils.get_timestamp_in_ms()<br><br>    <span class="hljs-comment"># Track E2E metrics on pretrain start</span><br>    one_logger_utils.on_pretrain_start()<br><br>    <span class="hljs-comment"># Context used for persisting some state between checkpoint saves.</span><br>    <span class="hljs-keyword">if</span> args.non_persistent_ckpt_type == <span class="hljs-string">&#x27;local&#x27;</span>:<br>        <span class="hljs-keyword">try</span>:<br>            <span class="hljs-keyword">from</span> nvidia_resiliency_ext.checkpointing.local.ckpt_managers.local_manager <span class="hljs-keyword">import</span> (<br>                LocalCheckpointManager,<br>            )<br>            <span class="hljs-keyword">from</span> nvidia_resiliency_ext.checkpointing.local.replication.group_utils <span class="hljs-keyword">import</span> (<br>                parse_group_sequence,<br>                GroupWrapper,<br>            )<br>            <span class="hljs-keyword">from</span> nvidia_resiliency_ext.checkpointing.local.replication.strategies <span class="hljs-keyword">import</span> (<br>                CliqueReplicationStrategy,<br>            )<br>        <span class="hljs-keyword">except</span> ModuleNotFoundError:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<br>                <span class="hljs-string">&quot;The &#x27;nvidia_resiliency_ext&#x27; module is required for local &quot;</span><br>                <span class="hljs-string">&quot;checkpointing but was not found. Please ensure it is installed.&quot;</span><br>            )<br><br>        <span class="hljs-keyword">if</span> args.replication:<br>            repl_strategy = CliqueReplicationStrategy.from_replication_params(<br>                args.replication_jump, args.replication_factor<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            repl_strategy = <span class="hljs-literal">None</span><br><br>        checkpointing_context = &#123;<br>            <span class="hljs-string">&#x27;local_checkpoint_manager&#x27;</span>: LocalCheckpointManager(<br>                args.non_persistent_local_ckpt_dir, repl_strategy=repl_strategy<br>            )<br>        &#125;<br>    <span class="hljs-keyword">else</span>:<br>        checkpointing_context = &#123;&#125;<br><br>    <span class="hljs-comment"># Model, optimizer, and learning rate.</span><br>    timers(<span class="hljs-string">&#x27;model-and-optimizer-setup&#x27;</span>, log_level=<span class="hljs-number">0</span>).start(barrier=<span class="hljs-literal">True</span>)<br>    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(<br>        model_provider, model_type, checkpointing_context=checkpointing_context<br>    )<br><br>    timers(<span class="hljs-string">&#x27;model-and-optimizer-setup&#x27;</span>).stop()<br>    print_datetime(<span class="hljs-string">&#x27;after model, optimizer, and learning rate &#x27;</span> <span class="hljs-string">&#x27;scheduler are built&#x27;</span>)<br>    config = get_model_config(model[<span class="hljs-number">0</span>])<br><br>    <span class="hljs-comment"># Data stuff.</span><br>    app_metrics[<span class="hljs-string">&#x27;app_build_dataiters_start_time&#x27;</span>] = one_logger_utils.get_timestamp_in_ms()<br>    timers(<span class="hljs-string">&#x27;train/valid/test-data-iterators-setup&#x27;</span>, log_level=<span class="hljs-number">0</span>).start(barrier=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">if</span> args.virtual_pipeline_model_parallel_size <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        train_data_iterator = []<br>        valid_data_iterator = []<br>        test_data_iterator = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(model)):<br>            iterators = build_train_valid_test_data_iterators(train_valid_test_dataset_provider)<br>            train_data_iterator.append(iterators[<span class="hljs-number">0</span>])<br>            valid_data_iterator.append(iterators[<span class="hljs-number">1</span>])<br>            test_data_iterator.append(iterators[<span class="hljs-number">2</span>])<br>    <span class="hljs-keyword">else</span>:<br>        train_data_iterator, valid_data_iterator, test_data_iterator = (<br>            build_train_valid_test_data_iterators(train_valid_test_dataset_provider)<br>        )<br>    timers(<span class="hljs-string">&#x27;train/valid/test-data-iterators-setup&#x27;</span>).stop()<br>    print_datetime(<span class="hljs-string">&#x27;after dataloaders are built&#x27;</span>)<br>    app_metrics[<span class="hljs-string">&#x27;app_build_dataiters_finish_time&#x27;</span>] = one_logger_utils.get_timestamp_in_ms()<br><br>    <span class="hljs-comment"># Track if training is enabled. Can only be done once args.do_train is assigned after dataloader is built.</span><br>    one_logger_utils.track_config_flags(<br>        args.train_iters,<br>        args.skip_train,<br>        args.do_train,<br>        args.do_valid,<br>        args.do_test,<br>        args.dataloader_type,<br>        args.retro_project_dir,<br>        args.retro_cyclic_train_iters,<br>    )<br><br>    <span class="hljs-comment"># Print setup timing.</span><br>    print_rank_0(<span class="hljs-string">&#x27;done with setup ...&#x27;</span>)<br>    timers.log([<span class="hljs-string">&#x27;model-and-optimizer-setup&#x27;</span>, <span class="hljs-string">&#x27;train/valid/test-data-iterators-setup&#x27;</span>], barrier=<span class="hljs-literal">True</span>)<br><br>    one_logger = get_one_logger()<br>    one_logger <span class="hljs-keyword">and</span> one_logger.log_metrics(app_metrics)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> args.skip_train:<br>        print_rank_0(<span class="hljs-string">&#x27;training ...&#x27;</span>)<br><br>        <span class="hljs-keyword">if</span> args.dataloader_type == <span class="hljs-string">&#x27;cyclic&#x27;</span> <span class="hljs-keyword">and</span> args.retro_project_dir:<br>            <span class="hljs-keyword">assert</span> args.retro_cyclic_train_iters <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>            args.train_iters = args.retro_cyclic_train_iters<br>            print_rank_0(<span class="hljs-string">&quot;retro cyclic train iters : %d&quot;</span> % args.train_iters)<br><br>        iteration = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> args.do_train <span class="hljs-keyword">and</span> args.train_iters &gt; <span class="hljs-number">0</span>:<br>            iteration, num_floating_point_operations_so_far = train(<br>                forward_step_func,<br>                model,<br>                optimizer,<br>                opt_param_scheduler,<br>                train_data_iterator,<br>                valid_data_iterator,<br>                process_non_loss_data_func,<br>                config,<br>                checkpointing_context,<br>                non_loss_data_func,<br>            )<br><br>        print_datetime(<span class="hljs-string">&#x27;after training is done&#x27;</span>)<br><br>        <span class="hljs-keyword">if</span> args.save <span class="hljs-keyword">and</span> iteration != <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> iteration % args.save_interval != <span class="hljs-number">0</span>:<br>            save_checkpoint(<br>                iteration,<br>                model,<br>                optimizer,<br>                opt_param_scheduler,<br>                num_floating_point_operations_so_far,<br>                checkpointing_context,<br>                train_data_iterator=train_data_iterator,<br>                preprocess_common_state_dict_fn=preprocess_common_state_dict,<br>            )<br><br>        one_logger <span class="hljs-keyword">and</span> one_logger.log_metrics(<br>            &#123;<span class="hljs-string">&#x27;app_train_loop_finish_time&#x27;</span>: one_logger_utils.get_timestamp_in_ms()&#125;<br>        )<br><br>    <span class="hljs-keyword">else</span>:<br>        print_rank_0(<span class="hljs-string">&#x27;skipping training (--skip-train is on) ...&#x27;</span>)<br><br>        iteration = args.iteration<br><br>    <span class="hljs-keyword">if</span> args.do_valid:<br>        prefix = <span class="hljs-string">f&#x27;iteration <span class="hljs-subst">&#123;iteration&#125;</span> on validation set&#x27;</span><br>        evaluate_and_print_results(<br>            prefix,<br>            forward_step_func,<br>            valid_data_iterator,<br>            model,<br>            iteration,<br>            process_non_loss_data_func,<br>            config,<br>            verbose=<span class="hljs-literal">True</span>,<br>            write_to_tensorboard=<span class="hljs-keyword">not</span> args.skip_train,<br>            non_loss_data_func=non_loss_data_func,<br>        )<br><br>    <span class="hljs-keyword">if</span> args.do_test:<br>        prefix = <span class="hljs-string">f&#x27;iteration <span class="hljs-subst">&#123;iteration&#125;</span> on test set&#x27;</span><br>        evaluate_and_print_results(<br>            prefix,<br>            forward_step_func,<br>            test_data_iterator,<br>            model,<br>            iteration,<br>            process_non_loss_data_func,<br>            config,<br>            verbose=<span class="hljs-literal">True</span>,<br>            write_to_tensorboard=<span class="hljs-keyword">not</span> args.skip_train,<br>            non_loss_data_func=non_loss_data_func,<br>        )<br><br>    wandb_writer = get_wandb_writer()<br>    <span class="hljs-keyword">if</span> wandb_writer:<br>        wandb_writer.finish()<br><br>    ft_integration.on_checkpointing_start()<br>    maybe_finalize_async_save(blocking=<span class="hljs-literal">True</span>, terminate=<span class="hljs-literal">True</span>)<br>    ft_integration.on_checkpointing_end(is_async_finalization=<span class="hljs-literal">True</span>)<br><br>    one_logger <span class="hljs-keyword">and</span> one_logger.log_metrics(<br>        &#123;<span class="hljs-string">&#x27;app_finish_time&#x27;</span>: one_logger_utils.get_timestamp_in_ms()&#125;<br>    )<br><br>    ft_integration.shutdown()<br>    one_logger_utils.finish()<br><br></code></pre></td></tr></table></figure><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><ol><li><p>如果参数<code>inprocess_call_wrapper</code>不为空，说明需要容错，那么再次进入pretrain的时候，为了避免还接入到原本的控制面，需要调用<code>inprocess_call_wrapper.iteration</code>进行命名空间更新，来接入新的store。</p></li><li><p>初始化megatron-lm的通信组、并行设置、关键参数等等，下面会具体介绍</p></li><li><p>获取全局参数，megatron-lm是单例设计，获取参数都是通过<code>get_*</code>来通过获取全局变量获得</p></li><li><p>&#x20;FT（Fault Tolerance）初始化，FT更偏向于是利用checkpoint进行容错，inprocess是对进程运行时的容错</p></li><li><p>设置PyTorch JIT fusion进行算子融合，如果有必要还会对其进行预热</p></li><li><p>通过min操作的all reduce来获取最小的训练开始时间，已记录相关日志</p></li><li><p>如果参数控制需要不落盘的内存级的checkpoint，就引入相关的包并设置对应的上下文</p></li><li><p>根据并行化策略等得到model、optimizer、opt_param_scheduler，下面会具体介绍</p></li><li><p>构建数据迭代器，如果采用了Virtual Pipeline并行，那么每个 pipeline stage都会有自己专门的 data iterator</p></li></ol><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><ul><li>如果参数指示跳过train，那么就不执行train，继续执行后面的，如果没有，那么就执行<code>iteration, num_floating_point_operations_so_far = train(...)</code>进行训练，下面会具体介绍。并且如果迭代次数不是保存checkpoint的倍数那么就会专门对最后的模型进行保存</li></ul><h2 id="收尾"><a href="#收尾" class="headerlink" title="收尾"></a>收尾</h2><ul><li><p>如果参数指示需要valid，那么就调用<code>evaluate_and_print_results</code>使用<code>valid_data_iterator</code>获取数据执行一次valid，下面会具体介绍</p></li><li><p>如果参数指示需要test，那么就调用<code>evaluate_and_print_results</code>使用<code>test_data_iterator</code>获取数据执行一次test</p></li><li><p>得到wandb的句柄并关闭</p></li><li><p>确保async checkpoint 完成，并将所有 IO 收尾</p></li><li><p>关闭FT和Logger</p></li></ul><h1 id="pretrain核心流程解析"><a href="#pretrain核心流程解析" class="headerlink" title="pretrain核心流程解析"></a>pretrain核心流程解析</h1><h2 id="initialize-megatron"><a href="#initialize-megatron" class="headerlink" title="initialize_megatron"></a>initialize_megatron</h2><p><code>initialize_megatron</code>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_megatron</span>(<span class="hljs-params"></span><br><span class="hljs-params">    extra_args_provider=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    args_defaults=&#123;&#125;,</span><br><span class="hljs-params">    ignore_unknown_args=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    allow_no_cuda=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    skip_mpu_initialization=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    get_embedding_ranks=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    get_position_embedding_ranks=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    parsed_args=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    store=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Set global variables, initialize distributed, and</span><br><span class="hljs-string">    set autoresume and random seeds.</span><br><span class="hljs-string">    `allow_no_cuda` should not be set unless using megatron for cpu only</span><br><span class="hljs-string">    data processing. In general this arg should not be set unless you know</span><br><span class="hljs-string">    what you are doing.</span><br><span class="hljs-string">    Returns a function to finalize distributed env initialization</span><br><span class="hljs-string">    (optionally, only when args.lazy_mpu_init == True)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> allow_no_cuda:<br>        <span class="hljs-comment"># Make sure cuda is available.</span><br>        <span class="hljs-keyword">assert</span> torch.cuda.is_available(), <span class="hljs-string">&quot;Megatron requires CUDA.&quot;</span><br><br>    <span class="hljs-comment"># Parse arguments</span><br>    <span class="hljs-keyword">if</span> parsed_args <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        args = parse_args(extra_args_provider, ignore_unknown_args)<br>    <span class="hljs-keyword">else</span>:<br>        args = parsed_args<br><br>    <span class="hljs-comment"># Prep for checkpoint conversion.</span><br>    <span class="hljs-keyword">if</span> args.ckpt_convert_format <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">assert</span> args.ckpt_convert_save <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">assert</span> args.load <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        args.exit_on_missing_checkpoint = <span class="hljs-literal">True</span><br><br>    <span class="hljs-keyword">if</span> args.use_checkpoint_args <span class="hljs-keyword">or</span> args_defaults.get(<span class="hljs-string">&quot;use_checkpoint_args&quot;</span>, <span class="hljs-literal">False</span>):<br>        <span class="hljs-keyword">assert</span> args.load <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;--use-checkpoint-args requires --load argument&quot;</span><br>        <span class="hljs-keyword">assert</span> args.non_persistent_ckpt_type != <span class="hljs-string">&quot;local&quot;</span>, (<br>            <span class="hljs-string">&quot;--use-checkpoint-args is not supported with --non_persistent_ckpt_type=local. &quot;</span><br>            <span class="hljs-string">&quot;Two-stage checkpoint loading is not implemented, and all arguments must be defined &quot;</span><br>            <span class="hljs-string">&quot;before initializing LocalCheckpointManager.&quot;</span><br>        )<br>        load_args_from_checkpoint(args)<br><br>    <span class="hljs-keyword">if</span> args.async_save <span class="hljs-keyword">and</span> args.use_persistent_ckpt_worker:<br>        init_persistent_async_worker()<br><br>    <span class="hljs-keyword">if</span> args.yaml_cfg <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        args = validate_yaml(args, args_defaults)<br>    <span class="hljs-keyword">else</span>:<br>        validate_args(args, args_defaults)<br><br>    <span class="hljs-comment"># set global args, build tokenizer, and set adlr-autoresume,</span><br>    <span class="hljs-comment"># tensorboard-writer, and timers.</span><br>    set_global_variables(args)<br><br>    <span class="hljs-comment"># set logging level</span><br>    setup_logging()<br><br>    <span class="hljs-comment"># init rerun state</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">state_save_func</span>():<br>        <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&#x27;rng_tracker_states&#x27;</span>: tensor_parallel.get_cuda_rng_tracker().get_states()&#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">state_restore_func</span>(<span class="hljs-params">state_dict</span>):<br>        <span class="hljs-keyword">if</span> state_dict[<span class="hljs-string">&#x27;rng_tracker_states&#x27;</span>]:<br>            tensor_parallel.get_cuda_rng_tracker().set_states(state_dict[<span class="hljs-string">&#x27;rng_tracker_states&#x27;</span>])<br><br>    args = get_args()<br>    initialize_rerun_state_machine(<br>        state_save_func=state_save_func,<br>        state_restore_func=state_restore_func,<br>        mode=RerunMode(args.rerun_mode),<br>        error_injector=RerunErrorInjector(<br>            error_injection_rate=args.error_injection_rate,<br>            error_injection_type=RerunDiagnostic(args.error_injection_type),<br>        ),<br>        result_rejected_tracker_filename=args.result_rejected_tracker_filename,<br>    )<br><br>    <span class="hljs-comment"># torch.distributed initialization</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">finish_mpu_init</span>():<br>        args = get_args()<br>        <span class="hljs-comment"># Pytorch distributed.</span><br>        _initialize_distributed(get_embedding_ranks, get_position_embedding_ranks, store)<br><br>        <span class="hljs-comment"># Random seeds for reproducibility.</span><br>        <span class="hljs-keyword">if</span> args.rank == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; setting random seeds to &#123;&#125; ...&quot;</span>.<span class="hljs-built_in">format</span>(args.seed))<br>        _set_random_seed(<br>            args.seed,<br>            args.data_parallel_random_init,<br>            args.te_rng_tracker,<br>            args.inference_rng_tracker,<br>            use_cudagraphable_rng=args.enable_cuda_graph <span class="hljs-keyword">or</span> args.external_cuda_graph,<br>        )<br><br>        <span class="hljs-comment"># Setup MoE aux loss scale value.</span><br>        <span class="hljs-keyword">if</span> args.num_experts <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">from</span> megatron.core.transformer.moe.router <span class="hljs-keyword">import</span> MoEAuxLossAutoScaler<br><br>            MoEAuxLossAutoScaler.set_loss_scale(torch.ones(<span class="hljs-number">1</span>, device=torch.cuda.current_device()))<br><br>    <span class="hljs-keyword">if</span> skip_mpu_initialization:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    args = get_args()<br>    <span class="hljs-keyword">if</span> args.lazy_mpu_init:<br>        <span class="hljs-comment"># TODO is this still a necessary option?</span><br>        args.use_cpu_initialization = <span class="hljs-literal">True</span><br>        <span class="hljs-comment"># delayed initialization of DDP-related stuff</span><br>        <span class="hljs-comment"># We only set basic DDP globals</span><br>        mpu.set_tensor_model_parallel_world_size(args.tensor_model_parallel_size)<br>        <span class="hljs-comment"># and return function for external DDP manager</span><br>        <span class="hljs-comment"># to call when it has DDP initialized</span><br>        mpu.set_tensor_model_parallel_rank(args.rank)<br>        <span class="hljs-keyword">return</span> finish_mpu_init<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># Megatron&#x27;s MPU is the master. Complete initialization right away.</span><br>        finish_mpu_init()<br><br>        <span class="hljs-comment"># Autoresume.</span><br>        _init_autoresume()<br><br>        <span class="hljs-comment"># Compile dependencies.</span><br>        _compile_dependencies()<br><br>        <span class="hljs-keyword">if</span> args.tp_comm_overlap:<br>            <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> Should this be activated with just decoder-tp-comm-overlap too?</span><br>            _initialize_tp_communicators()<br><br>        <span class="hljs-comment"># No continuation function</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br></code></pre></td></tr></table></figure><p><code>initialize_megatron</code>流程如下：</p><ol><li><p>检查是否包含cuda</p></li><li><p>解析参数，注意这里还使用了<code>pretrain</code>函数传递进来的<code>extra_args_provider</code></p></li><li><p>对checkpoint做格式转换并考虑从checkpoint中获取训练参数，如果使用异步checkpoint还负责启动保存checkpoint的IO worker</p></li><li><p>校验参数，设置全局参数</p></li><li><p>初始化日志</p></li><li><p>初始化容错的rerun状态机</p></li><li><p>如果使用<code>lazy_mpu_init</code>，就先设置一些模型并行参数，返回<code>finish_mpu_init</code>，等待外部调用其初始化</p></li><li><p>如果不使用<code>lazy_mpu_init</code>，就先调用<code>finish_mpu_init</code>初始化，再自动从 checkpoint 恢复，再提前编译依赖，再做 TP 通信重叠初始化。</p></li></ol><h3 id="finish-mpu-init"><a href="#finish-mpu-init" class="headerlink" title="finish_mpu_init"></a><code>finish_mpu_init</code></h3><p><code>finish_mpu_init</code>是初始化的核心模块，其代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># torch.distributed initialization</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">finish_mpu_init</span>():<br>    args = get_args()<br>    <span class="hljs-comment"># Pytorch distributed.</span><br>    _initialize_distributed(get_embedding_ranks, get_position_embedding_ranks, store)<br><br>    <span class="hljs-comment"># Random seeds for reproducibility.</span><br>    <span class="hljs-keyword">if</span> args.rank == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; setting random seeds to &#123;&#125; ...&quot;</span>.<span class="hljs-built_in">format</span>(args.seed))<br>    _set_random_seed(<br>        args.seed,<br>        args.data_parallel_random_init,<br>        args.te_rng_tracker,<br>        args.inference_rng_tracker,<br>        use_cudagraphable_rng=args.enable_cuda_graph <span class="hljs-keyword">or</span> args.external_cuda_graph,<br>    )<br><br>    <span class="hljs-comment"># Setup MoE aux loss scale value.</span><br>    <span class="hljs-keyword">if</span> args.num_experts <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">from</span> megatron.core.transformer.moe.router <span class="hljs-keyword">import</span> MoEAuxLossAutoScaler<br><br>        MoEAuxLossAutoScaler.set_loss_scale(torch.ones(<span class="hljs-number">1</span>, device=torch.cuda.current_device()))<br><br></code></pre></td></tr></table></figure><p><code>finish_mpu_init</code>流程如下：</p><ol><li><p>调用<code>_initialize_distributed</code>初始化通信组</p></li><li><p>设置随机随机种子，Megatron 的 RNG 体系是：DP 可以不同，TP &#x2F; PP 必须一致</p></li><li><p>如果是专家并行，还需要设置MoE 辅助损失缩放</p></li></ol><h3 id="initialize-distributed"><a href="#initialize-distributed" class="headerlink" title="_initialize_distributed"></a><code>_initialize_distributed</code></h3><p>对于关键的<code>_initialize_distributed</code>，其代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_initialize_distributed</span>(<span class="hljs-params">get_embedding_ranks, get_position_embedding_ranks, store</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Initialize torch.distributed and core model parallel.&quot;&quot;&quot;</span><br>    args = get_args()<br><br>    device_count = torch.cuda.device_count()<br>    <span class="hljs-keyword">if</span> torch.distributed.is_initialized():<br><br>        <span class="hljs-keyword">if</span> args.rank == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<br>                <span class="hljs-string">&quot;torch distributed is already initialized, &quot;</span> <span class="hljs-string">&quot;skipping initialization ...&quot;</span>,<br>                flush=<span class="hljs-literal">True</span>,<br>            )<br>        args.rank = torch.distributed.get_rank()<br>        args.world_size = torch.distributed.get_world_size()<br><br>    <span class="hljs-keyword">else</span>:<br><br>        <span class="hljs-keyword">if</span> args.rank == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; initializing torch distributed ...&quot;</span>, flush=<span class="hljs-literal">True</span>)<br>        <span class="hljs-comment"># Manually set the device ids.</span><br>        <span class="hljs-keyword">if</span> device_count &gt; <span class="hljs-number">0</span>:<br>            torch.cuda.set_device(args.local_rank)<br>            device_id = torch.device(<span class="hljs-string">f&#x27;cuda:<span class="hljs-subst">&#123;args.local_rank&#125;</span>&#x27;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            device_id = <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># Set to non-default stream for cudagraph capturing.</span><br>        <span class="hljs-keyword">if</span> args.external_cuda_graph:<br>            torch.cuda.set_stream(torch.cuda.Stream())<br><br>        <span class="hljs-comment"># Call the init process</span><br>        init_process_group_kwargs = &#123;<br>            <span class="hljs-string">&#x27;backend&#x27;</span>: args.distributed_backend,<br>            <span class="hljs-string">&#x27;store&#x27;</span>: store,<br>            <span class="hljs-string">&#x27;world_size&#x27;</span>: args.world_size,<br>            <span class="hljs-string">&#x27;rank&#x27;</span>: args.rank,<br>            <span class="hljs-string">&#x27;timeout&#x27;</span>: timedelta(minutes=args.distributed_timeout_minutes),<br>        &#125;<br><br>        torch.distributed.init_process_group(**init_process_group_kwargs)<br>        inprocess_restart.maybe_force_nccl_backend_init(device_id)<br><br>    <span class="hljs-comment"># Set the tensor model-parallel, pipeline model-parallel, and</span><br>    <span class="hljs-comment"># data-parallel communicators.</span><br>    <span class="hljs-keyword">if</span> device_count &gt; <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">if</span> mpu.model_parallel_is_initialized():<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;model parallel is already initialized&quot;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            mpu.initialize_model_parallel(<br>                args.tensor_model_parallel_size,<br>                args.pipeline_model_parallel_size,<br>                args.virtual_pipeline_model_parallel_size,<br>                pipeline_model_parallel_comm_backend=args.pipeline_model_parallel_comm_backend,<br>                use_sharp=args.use_sharp,<br>                context_parallel_size=args.context_parallel_size,<br>                hierarchical_context_parallel_sizes=args.hierarchical_context_parallel_sizes,<br>                expert_model_parallel_size=args.expert_model_parallel_size,<br>                num_distributed_optimizer_instances=args.num_distributed_optimizer_instances,<br>                expert_tensor_parallel_size=args.expert_tensor_parallel_size,<br>                distributed_timeout_minutes=args.distributed_timeout_minutes,<br>                nccl_communicator_config_path=args.nccl_communicator_config_path,<br>                order=<span class="hljs-string">&#x27;tp-cp-ep-dp-pp&#x27;</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> args.use_tp_pp_dp_mapping <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;tp-cp-ep-pp-dp&#x27;</span>,<br>                get_embedding_ranks=get_embedding_ranks,<br>                get_position_embedding_ranks=get_position_embedding_ranks,<br>                create_gloo_process_groups=args.enable_gloo_process_groups,<br>                high_priority_stream_groups=args.high_priority_stream_groups,<br>                sharp_enabled_group=args.sharp_enabled_group,<br>            )<br>            <span class="hljs-keyword">if</span> args.rank == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<br>                    <span class="hljs-string">f&quot;&gt; initialized tensor model parallel with size &quot;</span><br>                    <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;mpu.get_tensor_model_parallel_world_size()&#125;</span>&quot;</span><br>                )<br>                <span class="hljs-built_in">print</span>(<br>                    <span class="hljs-string">f&quot;&gt; initialized pipeline model parallel with size &quot;</span><br>                    <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;mpu.get_pipeline_model_parallel_world_size()&#125;</span>&quot;</span><br>                )<br></code></pre></td></tr></table></figure><p>流程如下：</p><ol><li><p>通过<code>torch.distributed.is_initialized()</code>检查是否初始化<code>torch.distributed</code>，如果没有就调用<code>torch.distributed.init_process_group(**init_process_group_kwargs)</code>初始化。注意这里使用了<code>pretrain</code>传入的TCPStore。然后为了防止 NCCL communicator 因进程重启而失效，还强制触发一次 NCCL 初始化。</p></li><li><p>检查设备数是否大于0，如果是就检查是否已经进行模型并行初始化，如果没有就调用<code>mpu.initialize_model_parallel</code>进行初始化。</p></li></ol><p><code>mpu.initialize_model_parallel</code>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pylint: disable=C0301</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_model_parallel</span>(<span class="hljs-params"></span><br><span class="hljs-params">    tensor_model_parallel_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">    pipeline_model_parallel_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">    virtual_pipeline_model_parallel_size: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    pipeline_model_parallel_comm_backend: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    use_sharp: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    context_parallel_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">    hierarchical_context_parallel_sizes: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    expert_model_parallel_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">    num_distributed_optimizer_instances: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">    expert_tensor_parallel_size: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    nccl_communicator_config_path: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    distributed_timeout_minutes: <span class="hljs-built_in">int</span> = <span class="hljs-number">30</span>,</span><br><span class="hljs-params">    order: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;tp-cp-ep-dp-pp&quot;</span>,</span><br><span class="hljs-params">    get_embedding_ranks: <span class="hljs-type">Optional</span>[<span class="hljs-type">Callable</span>[[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>]], <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    get_position_embedding_ranks: <span class="hljs-type">Optional</span>[<span class="hljs-type">Callable</span>[[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>]], <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    create_gloo_process_groups: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params">    high_priority_stream_groups: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    sharp_enabled_group: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Initialize model data parallel groups.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        tensor_model_parallel_size (int, default = 1):</span><br><span class="hljs-string">            The number of GPUs to split individual tensors across.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        pipeline_model_parallel_size (int, default = 1):</span><br><span class="hljs-string">            The number of tensor parallel GPU groups to split the</span><br><span class="hljs-string">            Transformer layers across. For example, if</span><br><span class="hljs-string">            tensor_model_parallel_size is 4 and</span><br><span class="hljs-string">            pipeline_model_parallel_size is 2, the model will be split</span><br><span class="hljs-string">            into 2 groups of 4 GPUs.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        virtual_pipeline_model_parallel_size (int, optional):</span><br><span class="hljs-string">            The number of stages that each pipeline group will have,</span><br><span class="hljs-string">            interleaving as necessary. If None, no interleaving is</span><br><span class="hljs-string">            performed. For example, if tensor_model_parallel_size is 1,</span><br><span class="hljs-string">            pipeline_model_parallel_size is 4,</span><br><span class="hljs-string">            virtual_pipeline_model_parallel_size is 2, and there are</span><br><span class="hljs-string">            16 transformer layers in the model, the model will be</span><br><span class="hljs-string">            split into 8 stages with two layers each and each GPU</span><br><span class="hljs-string">            would get 2 stages as such (layer number starting with 1):</span><br><span class="hljs-string"></span><br><span class="hljs-string">            GPU 0: [1, 2] [9, 10]</span><br><span class="hljs-string">            GPU 1: [3, 4] [11, 12]</span><br><span class="hljs-string">            GPU 2: [5, 6] [13, 14]</span><br><span class="hljs-string">            GPU 3: [7, 8] [15, 16]</span><br><span class="hljs-string"></span><br><span class="hljs-string">        pipeline_model_parallel_comm_backend (str, optional):</span><br><span class="hljs-string">            The backend to use for pipeline parallel communication.</span><br><span class="hljs-string">            If None, the default backend will be used.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        use_sharp (bool, default = False):</span><br><span class="hljs-string">            Set the use of SHARP for the collective communications of</span><br><span class="hljs-string">            data-parallel process groups. When `True`, run barrier</span><br><span class="hljs-string">            within each data-parallel process group, which specifies</span><br><span class="hljs-string">            the SHARP application target groups.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        context_parallel_size (int, default = 1):</span><br><span class="hljs-string">            The number of tensor parallel GPU groups to split the</span><br><span class="hljs-string">            network input sequence length across. Compute of attention</span><br><span class="hljs-string">            module requires tokens of full sequence length, so GPUs</span><br><span class="hljs-string">            in a context parallel group need to communicate with each</span><br><span class="hljs-string">            other to exchange information of other sequence chunks.</span><br><span class="hljs-string">            Each GPU and its counterparts in other tensor parallel</span><br><span class="hljs-string">            groups compose a context parallel group.</span><br><span class="hljs-string"></span><br><span class="hljs-string">            For example, assume we have 8 GPUs, if tensor model parallel</span><br><span class="hljs-string">            size is 4 and context parallel size is 2, the network input</span><br><span class="hljs-string">            will be split into two sequence chunks, which are processed</span><br><span class="hljs-string">            by 2 different groups of 4 GPUs. One chunk is processed by</span><br><span class="hljs-string">            GPU0-3, the other chunk is processed by GPU4-7. Four groups</span><br><span class="hljs-string">            are build to do context parallel communications: [GPU0, GPU4],</span><br><span class="hljs-string">            [GPU1, GPU5], [GPU2, GPU6], and [GPU3, GPU7].</span><br><span class="hljs-string"></span><br><span class="hljs-string">            Context parallelism partitions sequence length, so it has no</span><br><span class="hljs-string">            impact on weights, which means weights are duplicated among</span><br><span class="hljs-string">            GPUs in a context parallel group. Hence, weight gradients</span><br><span class="hljs-string">            all-reduce is required in backward. For simplicity, we piggyback</span><br><span class="hljs-string">            GPUs of context parallelism on data parallel group for</span><br><span class="hljs-string">            weight gradient all-reduce.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        expert_model_parallel_size (int, default = 1):</span><br><span class="hljs-string">            The number of Mixture of Experts parallel GPUs in each expert</span><br><span class="hljs-string">            parallel group.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        num_distributed_optimizer_instances (int, default = 1):</span><br><span class="hljs-string">            The number of distributed optimizer replicas across the data-</span><br><span class="hljs-string">            parallel domain.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        expert_tensor_parallel_size (int, default = tp_size):</span><br><span class="hljs-string">            The number of GPUs to split individual tensors of expert.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        nccl_communicator_config_path (str, default = None):</span><br><span class="hljs-string">            Path to the yaml file of NCCL communicator configurations.</span><br><span class="hljs-string">            `min_ctas`, `max_ctas`, and `cga_cluster_size` can be set</span><br><span class="hljs-string">            for each communicator.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        distributed_timeout_minutes (int, default = 30): Timeout, in</span><br><span class="hljs-string">            minutes,for operations executed against distributed</span><br><span class="hljs-string">            process groups. See PyTorch documentation at</span><br><span class="hljs-string">            https://pytorch.org/docs/stable/distributed.html for</span><br><span class="hljs-string">            caveats.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        order (str, default=tp-dp-pp):</span><br><span class="hljs-string">            The rank initialization order of parallelism. Now we support</span><br><span class="hljs-string">            tp-dp-pp and tp-pp-dp orders.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        get_embedding_ranks (Callable[[List[int], Optional[int]], List[int]], optional, default=None):</span><br><span class="hljs-string">            A function that takes in a list of ranks for a pipeline group and returns</span><br><span class="hljs-string">            those ranks that should have embeddings.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        get_position_embedding_ranks (Callable[[List[int], Optional[int]], List[int]], optional, default=None):</span><br><span class="hljs-string">            A function that takes in a list of ranks for a pipeline group, and returns</span><br><span class="hljs-string">            those ranks that should have position embeddings.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        create_gloo_process_groups (bool, default = True):</span><br><span class="hljs-string">            Create Gloo process groups if set to True. If set to False, Gloo process groups are</span><br><span class="hljs-string">            not created and calls to get Gloo process groups will result in assertion errors.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        high_priority_stream_groups (List[str], default = None):</span><br><span class="hljs-string">            Specify which communicator groups should use high priority streams during creation.</span><br><span class="hljs-string">            Assigning high priority to communication streams ensures that communication kernels</span><br><span class="hljs-string">            are scheduled with higher priority, minimizing the exposed communication when it is</span><br><span class="hljs-string">            overlapped with other computation kernels.</span><br><span class="hljs-string">            Example: initialize_parallel_groups(..., high_priority_stream_groups=[&#x27;dp_cp&#x27;,&#x27;ep_dp&#x27;])</span><br><span class="hljs-string"></span><br><span class="hljs-string">        sharp_enabled_group (str, default = None):</span><br><span class="hljs-string">            Specify which communicator group should use SHARP communication.</span><br><span class="hljs-string">            This option is only valid when use_sharp is True.</span><br><span class="hljs-string">            By default (None), it is enabled from dp group.</span><br><span class="hljs-string">            Available options (choose one): [dp, dp_replica]</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Let&#x27;s say we have a total of 16 GPUs denoted by g0 ... g15 and we</span><br><span class="hljs-string">    use 2 GPUs to parallelize the model tensor, and 4 GPUs to parallelize</span><br><span class="hljs-string">    the model pipeline. The present function will</span><br><span class="hljs-string">    create 8 tensor model-parallel groups, 4 pipeline model-parallel groups</span><br><span class="hljs-string">    and 8 data-parallel groups as:</span><br><span class="hljs-string">        8 data_parallel groups:</span><br><span class="hljs-string">            [g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]</span><br><span class="hljs-string">        8 tensor model-parallel groups:</span><br><span class="hljs-string">            [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]</span><br><span class="hljs-string">        4 pipeline model-parallel groups:</span><br><span class="hljs-string">            [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]</span><br><span class="hljs-string">    Note that for efficiency, the caller should make sure adjacent ranks</span><br><span class="hljs-string">    are on the same DGX box. For example if we are using 2 DGX-1 boxes</span><br><span class="hljs-string">    with a total of 16 GPUs, rank 0 to 7 belong to the first box and</span><br><span class="hljs-string">    ranks 8 to 15 belong to the second box.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># NCCL restricts IB SHARP usage to a single communicator group—the first one created</span><br>    <span class="hljs-comment"># with NCCL_COLLNET_ENABLE=1. After this group is created, NCCL_COLLNET_ENABLE must be</span><br>    <span class="hljs-comment"># set to 0 for subsequent groups.</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span> <span class="hljs-keyword">in</span> os.environ:<br>        <span class="hljs-keyword">del</span> os.environ[<span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span>]<br><br>    <span class="hljs-keyword">if</span> use_sharp:<br>        <span class="hljs-keyword">if</span> sharp_enabled_group <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># By default, SHARP is enabled from dp group.</span><br>            sharp_enabled_group = <span class="hljs-string">&quot;dp&quot;</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># Currently, only dp and dp_replica groups are supported for SHARP.</span><br>            <span class="hljs-keyword">assert</span> sharp_enabled_group <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;dp&quot;</span>, <span class="hljs-string">&quot;dp_replica&quot;</span>], <span class="hljs-string">&quot;Invalid sharp_enabled_group&quot;</span><br>            <span class="hljs-keyword">if</span> sharp_enabled_group == <span class="hljs-string">&quot;dp_replica&quot;</span>:<br>                <span class="hljs-keyword">assert</span> (<br>                    num_distributed_optimizer_instances &gt; <span class="hljs-number">1</span><br>                ), <span class="hljs-string">&quot;dp_replica group requires num_distributed_optimizer_instances &gt; 1&quot;</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">assert</span> (<br>            sharp_enabled_group <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>        ), <span class="hljs-string">&quot;sharp_enabled_group is only valid when use_sharp is True&quot;</span><br><br>    <span class="hljs-keyword">if</span> get_embedding_ranks <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        get_embedding_ranks = default_embedding_ranks<br><br>    <span class="hljs-keyword">if</span> get_position_embedding_ranks <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        get_position_embedding_ranks = default_position_embedding_ranks<br><br>    <span class="hljs-comment"># Get world size and rank. Ensure some consistencies.</span><br>    <span class="hljs-keyword">assert</span> torch.distributed.is_initialized()<br>    world_size: <span class="hljs-built_in">int</span> = torch.distributed.get_world_size()<br><br>    model_size = tensor_model_parallel_size * pipeline_model_parallel_size * context_parallel_size<br><br>    <span class="hljs-keyword">if</span> world_size % model_size != <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">f&quot;world_size (<span class="hljs-subst">&#123;world_size&#125;</span>) is not divisible by <span class="hljs-subst">&#123;model_size&#125;</span>&quot;</span>)<br><br>    data_parallel_size: <span class="hljs-built_in">int</span> = world_size // model_size<br><br>    <span class="hljs-keyword">if</span> virtual_pipeline_model_parallel_size <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> pipeline_model_parallel_size &gt; <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<br>                <span class="hljs-string">&quot;pipeline-model-parallel size should be greater than 1 with interleaved schedule&quot;</span><br>            )<br>        <span class="hljs-keyword">global</span> _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK<br>        <span class="hljs-keyword">global</span> _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE<br>        _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK = <span class="hljs-number">0</span><br>        _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = virtual_pipeline_model_parallel_size<br><br>    rank = torch.distributed.get_rank()<br><br>    nccl_comm_cfgs = &#123;&#125;<br>    <span class="hljs-keyword">if</span> nccl_communicator_config_path <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">try</span>:<br>            <span class="hljs-keyword">import</span> yaml<br>        <span class="hljs-keyword">except</span> ImportError:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<br>                <span class="hljs-string">&quot;Cannot import `yaml`. Setting custom nccl communicator configs &quot;</span><br>                <span class="hljs-string">&quot;requires the yaml package.&quot;</span><br>            )<br><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(nccl_communicator_config_path, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> stream:<br>            nccl_comm_cfgs = yaml.safe_load(stream)<br><br>    <span class="hljs-comment"># Set is_high_priority_stream flag to the nccl_comm_cfgs if it is in high_priority_stream_groups</span><br>    high_priority_stream_groups = high_priority_stream_groups <span class="hljs-keyword">or</span> []<br>    <span class="hljs-keyword">for</span> pg_name <span class="hljs-keyword">in</span> high_priority_stream_groups:<br>        overwrite_nccl_comm_cfgs(nccl_comm_cfgs, pg_name, (<span class="hljs-string">&quot;is_high_priority_stream&quot;</span>, <span class="hljs-literal">True</span>))<br><br>    decoder_rank_generator = RankGenerator(<br>        tp=tensor_model_parallel_size,<br>        ep=<span class="hljs-number">1</span>,<br>        dp=data_parallel_size,<br>        pp=pipeline_model_parallel_size,<br>        cp=context_parallel_size,<br>        order=order,<br>        rank_offset=<span class="hljs-number">0</span>,<br>    )<br><br>    <span class="hljs-comment"># Build expert rank generator</span><br>    <span class="hljs-keyword">if</span> expert_tensor_parallel_size <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        expert_tensor_parallel_size = tensor_model_parallel_size<br>    expert_tensor_model_pipeline_parallel_size = (<br>        expert_tensor_parallel_size * expert_model_parallel_size * pipeline_model_parallel_size<br>    )<br>    expert_data_parallel_size = world_size // expert_tensor_model_pipeline_parallel_size<br>    <span class="hljs-keyword">if</span> world_size % expert_tensor_model_pipeline_parallel_size != <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">raise</span> RuntimeError(<br>            <span class="hljs-string">f&quot;world_size (<span class="hljs-subst">&#123;world_size&#125;</span>) is not divisible by expert_tensor_model_pipeline_parallel size (<span class="hljs-subst">&#123;expert_tensor_model_pipeline_parallel_size&#125;</span>)&quot;</span><br>        )<br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> support expert specific ordering</span><br>    expert_decoder_rank_generator = RankGenerator(<br>        tp=expert_tensor_parallel_size,<br>        ep=expert_model_parallel_size,<br>        dp=expert_data_parallel_size,<br>        pp=pipeline_model_parallel_size,<br>        cp=<span class="hljs-number">1</span>,<br>        order=order,<br>        rank_offset=<span class="hljs-number">0</span>,<br>    )<br><br>    <span class="hljs-keyword">assert</span> (<br>        order.endswith(<span class="hljs-string">&quot;pp&quot;</span>)<br>        <span class="hljs-keyword">or</span> pipeline_model_parallel_size == <span class="hljs-number">1</span><br>        <span class="hljs-keyword">or</span> expert_data_parallel_size == data_parallel_size<br>    ), <span class="hljs-string">&quot;When not using pp-last rank ordering, the data parallel size of the attention and moe layers must be the same&quot;</span><br><br>    <span class="hljs-keyword">assert</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&quot;pp&quot;</span>) == expert_decoder_rank_generator.get_ranks(<br>        <span class="hljs-string">&quot;pp&quot;</span><br>    ), <span class="hljs-string">f&quot;Pipeline parallel groups are expected to be the same for Non-Expert and Expert part, \</span><br><span class="hljs-string">    but got <span class="hljs-subst">&#123;decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;pp&#x27;</span>)&#125;</span> and <span class="hljs-subst">&#123;expert_decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;pp&#x27;</span>)&#125;</span>&quot;</span><br><br>    timeout = timedelta(minutes=distributed_timeout_minutes)<br><br>    <span class="hljs-comment"># Build the data-parallel groups.</span><br>    <span class="hljs-keyword">global</span> _DATA_PARALLEL_GROUP<br>    <span class="hljs-keyword">global</span> _DATA_PARALLEL_GROUP_GLOO<br>    <span class="hljs-keyword">global</span> _DATA_PARALLEL_GLOBAL_RANKS<br>    <span class="hljs-keyword">global</span> _DATA_PARALLEL_GROUP_WITH_CP<br>    <span class="hljs-keyword">global</span> _DATA_PARALLEL_GROUP_WITH_CP_GLOO<br>    <span class="hljs-keyword">global</span> _DATA_PARALLEL_GLOBAL_RANKS_WITH_CP<br>    <span class="hljs-keyword">global</span> _INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP<br>    <span class="hljs-keyword">global</span> _INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP_GLOO<br>    <span class="hljs-keyword">assert</span> _DATA_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;data parallel group is already initialized&quot;</span><br><br>    <span class="hljs-keyword">assert</span> (<br>        data_parallel_size * context_parallel_size<br>    ) % num_distributed_optimizer_instances == <span class="hljs-number">0</span>, (<br>        <span class="hljs-string">&quot;Data parallel size should be divisible by partial DistOpt shard factor&quot;</span><br>    )<br>    intra_partial_data_parallel_size = (<br>        data_parallel_size * context_parallel_size<br>    ) // num_distributed_optimizer_instances<br><br>    <span class="hljs-comment"># Set NCCL_COLLNET_ENABLE to 1 to enable SHARP for the dp group.</span><br>    <span class="hljs-keyword">if</span> sharp_enabled_group == <span class="hljs-string">&quot;dp&quot;</span>:<br>        os.environ[<span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span>] = <span class="hljs-string">&quot;1&quot;</span><br><br>    <span class="hljs-comment"># In case of using SHARP, the dp-cp group requires to use NCCL COLLNET feature.</span><br>    <span class="hljs-comment"># Due to the hardware limitation, only the initially created communication group</span><br>    <span class="hljs-comment"># is eligible for using the NCCL COLLNET feature.</span><br>    <span class="hljs-comment"># Therefore, dp-cp group, which potentially requires SHARP-enablement,</span><br>    <span class="hljs-comment"># need to be created before all the other groups</span><br>    <span class="hljs-keyword">for</span> ranks_with_cp <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;dp-cp&#x27;</span>):<br>        group_with_cp = create_group(<br>            ranks_with_cp,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;dp_cp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;DATA_PARALLEL_GROUP_WITH_CP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> create_gloo_process_groups:<br>            group_with_cp_gloo = create_group(<br>                ranks_with_cp,<br>                timeout=timeout,<br>                backend=<span class="hljs-string">&quot;gloo&quot;</span>,<br>                group_desc=<span class="hljs-string">&quot;DATA_PARALLEL_GROUP_WITH_CP_GLOO&quot;</span>,<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            group_with_cp_gloo = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks_with_cp:<br>            _DATA_PARALLEL_GROUP_WITH_CP = group_with_cp<br>            _DATA_PARALLEL_GROUP_WITH_CP_GLOO = group_with_cp_gloo<br>            _DATA_PARALLEL_GLOBAL_RANKS_WITH_CP = ranks_with_cp<br><br>        <span class="hljs-keyword">if</span> num_distributed_optimizer_instances &gt; <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># Create groups for intra-partial DP domain</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_distributed_optimizer_instances):<br>                intra_partial_dp_ranks_with_cp = ranks_with_cp[<br>                    (i * intra_partial_data_parallel_size) : (<br>                        (i + <span class="hljs-number">1</span>) * intra_partial_data_parallel_size<br>                    )<br>                ]<br>                intra_partial_dp_group_with_cp = create_group(<br>                    intra_partial_dp_ranks_with_cp,<br>                    timeout=timeout,<br>                    pg_options=get_nccl_options(<span class="hljs-string">&quot;intra_dp_cp&quot;</span>, nccl_comm_cfgs),<br>                    group_desc=<span class="hljs-string">&quot;INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP&quot;</span>,<br>                )<br>                <span class="hljs-keyword">if</span> create_gloo_process_groups:<br>                    intra_partial_dp_group_with_cp_gloo = create_group(<br>                        intra_partial_dp_ranks_with_cp,<br>                        timeout=timeout,<br>                        backend=<span class="hljs-string">&quot;gloo&quot;</span>,<br>                        group_desc=<span class="hljs-string">&quot;INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP_GLOO&quot;</span>,<br>                    )<br>                <span class="hljs-keyword">else</span>:<br>                    intra_partial_dp_group_with_cp_gloo = <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> intra_partial_dp_ranks_with_cp:<br>                    _INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP = intra_partial_dp_group_with_cp<br>                    _INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP_GLOO = (<br>                        intra_partial_dp_group_with_cp_gloo<br>                    )<br>        <span class="hljs-keyword">else</span>:<br>            _INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP = _DATA_PARALLEL_GROUP_WITH_CP<br>            _INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP_GLOO = _DATA_PARALLEL_GROUP_WITH_CP_GLOO<br><br>    <span class="hljs-comment"># Apply SHARP to the dp group.</span><br>    <span class="hljs-keyword">if</span> sharp_enabled_group == <span class="hljs-string">&quot;dp&quot;</span>:<br>        <span class="hljs-keyword">if</span> rank == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<br>                <span class="hljs-string">&quot;The number of process groups to use SHARP with depends on the type &quot;</span><br>                <span class="hljs-string">&quot;of the network switch. Nvidia QM1 switch supports SAHRP up to 8 &quot;</span><br>                <span class="hljs-string">&quot;process groups and QM2 supports up to 256 process groups. We apply &quot;</span><br>                <span class="hljs-string">&quot;SHARP to the communications of the data-parallel domain. If the &quot;</span><br>                <span class="hljs-string">&quot;number of data-parallel process groups is larger than the max &quot;</span><br>                <span class="hljs-string">&quot;process groups that the network switch supports, the communication &quot;</span><br>                <span class="hljs-string">&quot;will fall back to non-SHARP operators. To enable SHARP, &quot;</span><br>                <span class="hljs-string">&quot;`#SBATCH_NETWORK=sharp` should be set in the sbatch script.&quot;</span><br>            )<br>        <span class="hljs-comment"># PyTorch is performing lazy initialization of the communicator group.</span><br>        <span class="hljs-comment"># Therefore, we need to perform a nccl call to ensure that the communicator group is created.</span><br>        torch.distributed.barrier(<br>            group=get_data_parallel_group(with_context_parallel=<span class="hljs-literal">True</span>),<br>            device_ids=[torch.cuda.current_device()],<br>        )<br>        torch.cuda.synchronize()<br>        <span class="hljs-comment"># Set `NCCL_COLLNET_ENABLE=0` to restrict SHARP application to the dp group.</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span> <span class="hljs-keyword">in</span> os.environ:<br>            <span class="hljs-keyword">del</span> os.environ[<span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span>]<br><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;dp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;dp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;DATA_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> create_gloo_process_groups:<br>            group_gloo = create_group(<br>                ranks, timeout=timeout, backend=<span class="hljs-string">&quot;gloo&quot;</span>, group_desc=<span class="hljs-string">&quot;DATA_PARALLEL_GROUP_GLOO&quot;</span><br>            )<br>        <span class="hljs-keyword">else</span>:<br>            group_gloo = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _DATA_PARALLEL_GROUP = group<br>            _DATA_PARALLEL_GROUP_GLOO = group_gloo<br>            _DATA_PARALLEL_GLOBAL_RANKS = ranks<br><br>    <span class="hljs-comment"># Build the context-parallel groups.</span><br>    <span class="hljs-keyword">global</span> _CONTEXT_PARALLEL_GROUP<br>    <span class="hljs-keyword">global</span> _CONTEXT_PARALLEL_GLOBAL_RANKS<br>    <span class="hljs-keyword">assert</span> _CONTEXT_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&#x27;context parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;cp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;cp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;CONTEXT_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _CONTEXT_PARALLEL_GROUP = group<br>            _CONTEXT_PARALLEL_GLOBAL_RANKS = ranks<br>        <span class="hljs-keyword">if</span> hierarchical_context_parallel_sizes:<br>            <span class="hljs-keyword">assert</span> np.prod(hierarchical_context_parallel_sizes) == context_parallel_size<br>            <span class="hljs-keyword">global</span> _HIERARCHICAL_CONTEXT_PARALLEL_GROUPS<br>            hierarchical_groups, _ = create_hierarchical_groups(<br>                rank,<br>                ranks,<br>                hierarchical_context_parallel_sizes,<br>                create_gloo_process_groups=<span class="hljs-literal">False</span>,<br>                pg_options=get_nccl_options(<span class="hljs-string">&quot;hcp&quot;</span>, nccl_comm_cfgs),<br>                timeout=timeout,<br>                group_desc=<span class="hljs-string">&quot;CONTEXT_PARALLEL_GROUP&quot;</span>,<br>            )<br>            <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>                _HIERARCHICAL_CONTEXT_PARALLEL_GROUPS = hierarchical_groups<br><br>    <span class="hljs-comment"># Build the model-parallel groups.</span><br>    <span class="hljs-keyword">global</span> _MODEL_PARALLEL_GROUP<br>    <span class="hljs-keyword">global</span> _MODEL_PARALLEL_GLOBAL_RANKS<br>    <span class="hljs-keyword">assert</span> _MODEL_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&#x27;model parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp-pp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;mp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;MODEL_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _MODEL_PARALLEL_GROUP = group<br>            _MODEL_PARALLEL_GLOBAL_RANKS = ranks<br><br>    <span class="hljs-comment"># Build the tensor model-parallel groups.</span><br>    <span class="hljs-keyword">global</span> _TENSOR_MODEL_PARALLEL_GROUP<br>    <span class="hljs-keyword">global</span> _TENSOR_MODEL_PARALLEL_GLOBAL_RANKS<br>    <span class="hljs-keyword">assert</span> (<br>        _TENSOR_MODEL_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&#x27;tensor model parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;tp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;TENSOR_MODEL_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _TENSOR_MODEL_PARALLEL_GROUP = group<br>            _TENSOR_MODEL_PARALLEL_GLOBAL_RANKS = ranks<br><br>    <span class="hljs-comment"># Build the pipeline model-parallel groups and embedding groups</span><br>    <span class="hljs-comment"># (first and last rank in each pipeline model-parallel group).</span><br>    <span class="hljs-keyword">global</span> _PIPELINE_MODEL_PARALLEL_GROUP<br>    <span class="hljs-keyword">global</span> _PIPELINE_GLOBAL_RANKS<br>    <span class="hljs-keyword">assert</span> (<br>        _PIPELINE_MODEL_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;pipeline model parallel group is already initialized&quot;</span><br>    <span class="hljs-keyword">global</span> _EMBEDDING_GROUP<br>    <span class="hljs-keyword">global</span> _EMBEDDING_GLOBAL_RANKS<br>    <span class="hljs-keyword">assert</span> _EMBEDDING_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;embedding group is already initialized&quot;</span><br>    <span class="hljs-keyword">global</span> _POSITION_EMBEDDING_GROUP<br>    <span class="hljs-keyword">global</span> _POSITION_EMBEDDING_GLOBAL_RANKS<br>    <span class="hljs-keyword">assert</span> _POSITION_EMBEDDING_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;position embedding group is already initialized&quot;</span><br>    <span class="hljs-keyword">if</span> pipeline_model_parallel_comm_backend == <span class="hljs-string">&quot;ucc&quot;</span>:<br>        <span class="hljs-comment"># The UCC backend provides two key benefits:</span><br>        <span class="hljs-comment"># 1) Achieves better bandwidth utilization than NCCL when using InfiniBand links.</span><br>        <span class="hljs-comment"># 2) Does not use GPU SM resources (Zero-SM), mitigating performance interference</span><br>        <span class="hljs-comment">#    with overlapping compute kernels.</span><br><br>        <span class="hljs-comment"># The UCC backend is recommended in the following cases:</span><br>        <span class="hljs-comment"># 1) When the exposed pipeline-parallel (PP) communications are significant.</span><br>        <span class="hljs-comment">#    - E.g., Pipeline parallelism with very less gradient accumulation steps.</span><br>        <span class="hljs-comment">#    - It may provide better performance due to improved bandwidth utilization.</span><br>        <span class="hljs-comment"># 2) When the critical-path pipeline stage has substantial PP-communication overlap.</span><br>        <span class="hljs-comment">#    - E.g., Uneven pipeline parallelism.</span><br>        <span class="hljs-comment">#    - It may provide better performance due to zero SM resource usage.</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;CUDA_DEVICE_MAX_CONNECTIONS&quot;</span> <span class="hljs-keyword">in</span> os.environ:<br>            <span class="hljs-comment"># UCC backend requires CUDA_DEVICE_MAX_CONNECTIONS variable to be larger than 1,</span><br>            <span class="hljs-comment"># to gurantee the overlapped UCC communications. If this environment variable is set to 1,</span><br>            <span class="hljs-comment"># all the UCC communication will be serialized.</span><br>            <span class="hljs-keyword">assert</span> (<br>                os.environ[<span class="hljs-string">&quot;CUDA_DEVICE_MAX_CONNECTIONS&quot;</span>] != <span class="hljs-string">&quot;1&quot;</span><br>            ), <span class="hljs-string">&quot;UCC-backend requires CUDA_DEVICE_MAX_CONNECTIONS &gt; 1&quot;</span><br><br>        <span class="hljs-comment"># Setting up required environment variables for ucc backend</span><br>        <span class="hljs-comment">#</span><br>        <span class="hljs-comment"># &quot;TORCH_UCC_BLOCKING_WAIT=none&quot; allows non-blocking waits of the communiction handle</span><br>        <span class="hljs-comment"># &quot;UCC_EC_CUDA_STREAM_TASK_MODE&quot; controls how CUDA execution engines (EC)</span><br>        <span class="hljs-comment"># schedule tasks on CUDA streams.</span><br>        <span class="hljs-comment"># &quot;UCX_TLS&quot; controls transport layer selection</span><br>        <span class="hljs-comment"># &quot;NSYS_UCP_COMM_PARAMS=1&quot; enables capturing ucx tracing in nsys profiling</span><br>        <span class="hljs-comment"># &quot;UCX_RNDV_THRESH&quot; controls threshold threshold for switching between</span><br>        <span class="hljs-comment"># eager and rendezvous (RNDV) communication protocols.</span><br>        <span class="hljs-comment"># &quot;UCX_NET_DEVICES&quot; select which network interfaces UCX should use.</span><br>        <span class="hljs-comment"># &quot;UCC_CL_BASIC_TLS&quot; controls which Transport Layers are used by</span><br>        <span class="hljs-comment"># the Basic Collective libraray</span><br><br>        os.environ[<span class="hljs-string">&quot;TORCH_UCC_BLOCKING_WAIT&quot;</span>] = (<br>            os.environ[<span class="hljs-string">&quot;TORCH_UCC_BLOCKING_WAIT&quot;</span>]<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;TORCH_UCC_BLOCKING_WAIT&quot;</span> <span class="hljs-keyword">in</span> os.environ<br>            <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;none&quot;</span><br>        )<br>        os.environ[<span class="hljs-string">&quot;UCC_EC_CUDA_STREAM_TASK_MODE&quot;</span>] = (<br>            os.environ[<span class="hljs-string">&quot;UCC_EC_CUDA_STREAM_TASK_MODE&quot;</span>]<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;UCC_EC_CUDA_STREAM_TASK_MODE&quot;</span> <span class="hljs-keyword">in</span> os.environ<br>            <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;driver&quot;</span><br>        )<br>        os.environ[<span class="hljs-string">&quot;UCX_TLS&quot;</span>] = (<br>            os.environ[<span class="hljs-string">&quot;UCX_TLS&quot;</span>] <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;UCX_TLS&quot;</span> <span class="hljs-keyword">in</span> os.environ <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;ib,cuda_copy&quot;</span><br>        )  <span class="hljs-comment"># cuda_ipc (i.e., NVLink-enablement) will be later supported</span><br>        os.environ[<span class="hljs-string">&quot;NSYS_UCP_COMM_PARAMS&quot;</span>] = <span class="hljs-string">&quot;1&quot;</span><br>        os.environ[<span class="hljs-string">&quot;UCX_RNDV_THRESH&quot;</span>] = <span class="hljs-string">&quot;0&quot;</span><br>        os.environ[<span class="hljs-string">&quot;UCX_NET_DEVICES&quot;</span>] = <span class="hljs-string">&quot;all&quot;</span><br>        os.environ[<span class="hljs-string">&quot;UCC_CL_BASIC_TLS&quot;</span>] = <span class="hljs-string">&quot;^sharp,nccl&quot;</span><br><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;pp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            backend=pipeline_model_parallel_comm_backend,<br>            pg_options=(<br>                <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">if</span> pipeline_model_parallel_comm_backend == <span class="hljs-string">&quot;ucc&quot;</span><br>                <span class="hljs-keyword">else</span> get_nccl_options(<span class="hljs-string">&quot;pp&quot;</span>, nccl_comm_cfgs)<br>            ),<br>            group_desc=<span class="hljs-string">&quot;PIPELINE_MODEL_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">assert</span> (<br>            pipeline_model_parallel_comm_backend == <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">or</span> pipeline_model_parallel_comm_backend == <span class="hljs-string">&quot;nccl&quot;</span><br>            <span class="hljs-keyword">or</span> pipeline_model_parallel_comm_backend == <span class="hljs-string">&quot;ucc&quot;</span><br>        ), <span class="hljs-string">f&#x27;&quot;<span class="hljs-subst">&#123;pipeline_model_parallel_comm_backend&#125;</span>&quot; backend for PP communication is currently not supported&#x27;</span><br><br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            <span class="hljs-keyword">if</span> _PIPELINE_MODEL_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                _PIPELINE_MODEL_PARALLEL_GROUP = group<br>                _PIPELINE_GLOBAL_RANKS = ranks<br>            <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(_PIPELINE_GLOBAL_RANKS[<span class="hljs-number">0</span>], <span class="hljs-built_in">list</span>):<br>                _PIPELINE_MODEL_PARALLEL_GROUP.append(group)<br>                _PIPELINE_GLOBAL_RANKS.append(ranks)<br>            <span class="hljs-keyword">else</span>:<br>                _PIPELINE_MODEL_PARALLEL_GROUP = [_PIPELINE_MODEL_PARALLEL_GROUP, group]<br>                _PIPELINE_GLOBAL_RANKS = [_PIPELINE_GLOBAL_RANKS, ranks]<br><br>        embedding_ranks = get_embedding_ranks(ranks)<br>        group = create_group(<br>            embedding_ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;embd&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;EMBEDDING_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> embedding_ranks:<br>            _EMBEDDING_GROUP = group<br>            _EMBEDDING_GLOBAL_RANKS = embedding_ranks<br><br>        position_embedding_ranks = get_position_embedding_ranks(ranks)<br>        group = create_group(<br>            position_embedding_ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;pos_embd&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;POSITION_EMBEDDING_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> position_embedding_ranks:<br>            _POSITION_EMBEDDING_GROUP = group<br>            _POSITION_EMBEDDING_GLOBAL_RANKS = position_embedding_ranks<br><br>    <span class="hljs-comment"># Build the tensor + data parallel groups.</span><br>    <span class="hljs-keyword">global</span> _TENSOR_AND_DATA_PARALLEL_GROUP<br>    <span class="hljs-keyword">global</span> _TENSOR_AND_DATA_PARALLEL_GROUP_WITH_CP<br>    <span class="hljs-keyword">assert</span> (<br>        _TENSOR_AND_DATA_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&#x27;Tensor + data parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp-dp-cp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;tp_dp_cp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;TENSOR_AND_DATA_PARALLEL_GROUP_WITH_CP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _TENSOR_AND_DATA_PARALLEL_GROUP_WITH_CP = group<br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp-dp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;tp_dp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;TENSOR_AND_DATA_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _TENSOR_AND_DATA_PARALLEL_GROUP = group<br><br>    <span class="hljs-keyword">global</span> _TENSOR_AND_CONTEXT_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> (<br>        _TENSOR_AND_CONTEXT_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&#x27;Tensor + context parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp-cp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;tp_cp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;TENSOR_AND_CONTEXT_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _TENSOR_AND_CONTEXT_PARALLEL_GROUP = group<br><br>    <span class="hljs-comment">### Expert-related parallel groups initialization</span><br>    <span class="hljs-comment"># Build the expert model parallel group</span><br>    <span class="hljs-keyword">global</span> _EXPERT_MODEL_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> _EXPERT_MODEL_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&#x27;Expert parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> expert_decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;ep&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;ep&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;EXPERT_MODEL_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _EXPERT_MODEL_PARALLEL_GROUP = group<br><br>    <span class="hljs-comment"># Build the expert tensor parallel group</span><br>    <span class="hljs-keyword">global</span> _EXPERT_TENSOR_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> (<br>        _EXPERT_TENSOR_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&#x27;Expert tensor model parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> expert_decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;ep_tp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;EXPERT_TENSOR_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _EXPERT_TENSOR_PARALLEL_GROUP = group<br><br>    <span class="hljs-comment"># Build the tensor + expert parallel groups</span><br>    <span class="hljs-keyword">global</span> _EXPERT_TENSOR_AND_MODEL_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> (<br>        _EXPERT_TENSOR_AND_MODEL_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&#x27;Expert tensor + model parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> expert_decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp-ep&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;tp_ep_mp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;EXPERT_TENSOR_AND_MODEL_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _EXPERT_TENSOR_AND_MODEL_PARALLEL_GROUP = group<br><br>    <span class="hljs-comment"># Build the expert+tensor+pipeline parallel groups</span><br>    <span class="hljs-keyword">global</span> _EXPERT_TENSOR_MODEL_PIPELINE_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> (<br>        _EXPERT_TENSOR_MODEL_PIPELINE_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&#x27;The expert_tensor_model_pipeline parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> expert_decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp-ep-pp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;tp_ep_pp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;EXPERT_TENSOR_MODEL_PIPELINE_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _EXPERT_TENSOR_MODEL_PIPELINE_PARALLEL_GROUP = group<br><br>    <span class="hljs-comment"># Build the expert data parallel group</span><br>    <span class="hljs-keyword">global</span> _EXPERT_DATA_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> _EXPERT_DATA_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;Expert data group is already initialized&quot;</span><br>    <span class="hljs-keyword">global</span> _EXPERT_DATA_PARALLEL_GROUP_GLOO<br>    <span class="hljs-keyword">assert</span> _EXPERT_DATA_PARALLEL_GROUP_GLOO <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;Expert data group-gloo is already initialized&quot;</span><br>    <span class="hljs-keyword">global</span> _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> (<br>        _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;Intra partial expert data group is already initialized&quot;</span><br>    <span class="hljs-keyword">global</span> _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP_GLOO<br>    <span class="hljs-keyword">assert</span> (<br>        _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP_GLOO <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;Intra partial expert data group-gloo is already initialized&quot;</span><br>    <span class="hljs-keyword">global</span> _INTER_PARTIAL_EXPERT_DATA_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> (<br>        _INTER_PARTIAL_EXPERT_DATA_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;Inter partial expert data group is already initialized&quot;</span><br><br>    <span class="hljs-keyword">assert</span> (<br>        expert_data_parallel_size % num_distributed_optimizer_instances == <span class="hljs-number">0</span><br>    ), <span class="hljs-string">&quot;Expert data parallel size should be divisible by partial DistOpt shard factor&quot;</span><br>    intra_partial_expert_data_parallel_size = (<br>        expert_data_parallel_size // num_distributed_optimizer_instances<br>    )<br><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> expert_decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;dp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;ep_dp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;EXPERT_DATA_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> create_gloo_process_groups:<br>            group_gloo = create_group(<br>                ranks, backend=<span class="hljs-string">&quot;gloo&quot;</span>, group_desc=<span class="hljs-string">&quot;EXPERT_DATA_PARALLEL_GROUP_GLOO&quot;</span><br>            )<br>        <span class="hljs-keyword">else</span>:<br>            group_gloo = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _EXPERT_DATA_PARALLEL_GROUP = group<br>            _EXPERT_DATA_PARALLEL_GROUP_GLOO = group_gloo<br><br>        <span class="hljs-keyword">if</span> num_distributed_optimizer_instances &gt; <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># Create groups for Partial DistOpt, one for intra-partial DP domain</span><br>            <span class="hljs-comment"># Another for inter-partial DP domain</span><br><br>            <span class="hljs-comment"># Set NCCL_COLLNET_ENABLE to 1 to enable SHARP for the dp_replica group.</span><br>            <span class="hljs-keyword">if</span> sharp_enabled_group == <span class="hljs-string">&quot;dp_replica&quot;</span>:<br>                os.environ[<span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span>] = <span class="hljs-string">&quot;1&quot;</span><br>            hierarchical_groups, hierarchical_groups_gloo = create_hierarchical_groups(<br>                rank,<br>                ranks,<br>                [intra_partial_expert_data_parallel_size, num_distributed_optimizer_instances],<br>                create_gloo_process_groups=create_gloo_process_groups,<br>                pg_options=[<br>                    get_nccl_options(<span class="hljs-string">&quot;intra_ep_dp&quot;</span>, nccl_comm_cfgs),<br>                    get_nccl_options(<span class="hljs-string">&quot;inter_ep_dp&quot;</span>, nccl_comm_cfgs),<br>                ],<br>                timeout=timeout,<br>                group_desc=<span class="hljs-string">&quot;EXPERT_DATA_PARALLEL_GROUP&quot;</span>,<br>            )<br>            <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>                _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP = hierarchical_groups[<span class="hljs-number">0</span>]<br>                _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP_GLOO = hierarchical_groups_gloo[<span class="hljs-number">0</span>]<br>                _INTER_PARTIAL_EXPERT_DATA_PARALLEL_GROUP = hierarchical_groups[<span class="hljs-number">1</span>]<br><br>            <span class="hljs-keyword">if</span> sharp_enabled_group == <span class="hljs-string">&quot;dp_replica&quot;</span>:<br>                <span class="hljs-comment"># PyTorch is performing lazy initialization of the communicator group.</span><br>                <span class="hljs-comment"># Therefore, we need to perform a nccl call to ensure that the communicator group is created.</span><br>                <span class="hljs-keyword">if</span> _INTER_PARTIAL_EXPERT_DATA_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    torch.distributed.barrier(<br>                        group=_INTER_PARTIAL_EXPERT_DATA_PARALLEL_GROUP,<br>                        device_ids=[torch.cuda.current_device()],<br>                    )<br>                    torch.cuda.synchronize()<br>                <span class="hljs-comment"># Set NCCL_COLLNET_ENABLE to 0 to restrict SHARP application to the dp_replica group.</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span> <span class="hljs-keyword">in</span> os.environ:<br>                    <span class="hljs-keyword">del</span> os.environ[<span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span>]<br>        <span class="hljs-keyword">else</span>:<br>            _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP = _EXPERT_DATA_PARALLEL_GROUP<br>            _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP_GLOO = _EXPERT_DATA_PARALLEL_GROUP_GLOO<br>    <span class="hljs-comment">### End of expert related parallel groups initialization</span><br><br>    <span class="hljs-comment"># build the intra distributed optimizer instance group</span><br>    <span class="hljs-keyword">global</span> _INTRA_DISTRIBUTED_OPTIMIZER_INSTANCE_GROUP<br>    <span class="hljs-keyword">assert</span> (<br>        _INTRA_DISTRIBUTED_OPTIMIZER_INSTANCE_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;Intra distributed optimizer instance group is already initialized&quot;</span><br><br>    model_parallel_group_id = <span class="hljs-number">0</span><br>    intra_dist_opt_ranks = []<br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> expert_decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp-ep-pp&#x27;</span>):<br>        model_parallel_group_id += <span class="hljs-number">1</span><br>        intra_dist_opt_ranks.extend(ranks)<br>        <span class="hljs-keyword">if</span> model_parallel_group_id % intra_partial_expert_data_parallel_size == <span class="hljs-number">0</span>:<br>            intra_dist_opt_instance_group = create_group(<br>                intra_dist_opt_ranks,<br>                timeout=timeout,<br>                pg_options=get_nccl_options(<span class="hljs-string">&quot;intra_dist_opt_instance&quot;</span>, nccl_comm_cfgs),<br>                group_desc=<span class="hljs-string">&quot;INTRA_DISTRIBUTED_OPTIMIZER_INSTANCE_GROUP&quot;</span>,<br>            )<br>            <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> intra_dist_opt_ranks:<br>                _INTRA_DISTRIBUTED_OPTIMIZER_INSTANCE_GROUP = intra_dist_opt_instance_group<br>            intra_dist_opt_ranks = []<br><br>    <span class="hljs-comment"># Initialize global memory buffer</span><br>    <span class="hljs-comment"># This isn&#x27;t really &quot;parallel state&quot; but there isn&#x27;t another good place to</span><br>    <span class="hljs-comment"># put this. If we end up with a more generic initialization of megatron-core</span><br>    <span class="hljs-comment"># we could stick it there</span><br>    _set_global_memory_buffer()<br><br></code></pre></td></tr></table></figure><ul><li><p><code>mpu.initialize_model_parallel</code>的核心目的是依据并行策略设置创建一堆并行通信组，实现各worker的rank与并行组的映射。这包括了TP、PP、DP、Context Parallel（CP）、Expert Parallel（EP）。</p></li><li><p>其首先构建了一个<code>RankGenerator</code>，这是rank与并行组匹配的核心</p><ul><li><code>RankGenerator</code>的相关代码如下：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RankGenerator</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;A class for generating rank groups for different modes of parallelism.&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, tp: <span class="hljs-built_in">int</span>, ep: <span class="hljs-built_in">int</span>, dp: <span class="hljs-built_in">int</span>, pp: <span class="hljs-built_in">int</span>, cp: <span class="hljs-built_in">int</span>, order: <span class="hljs-built_in">str</span>, rank_offset: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span></span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">assert</span> (<br>            ep == <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> cp == <span class="hljs-number">1</span><br>        ), <span class="hljs-string">&quot;Both EP and CP &gt; 1 in not allow in one rank generator. \</span><br><span class="hljs-string">            CP is only included in default RankGenerator, and EP only in expert RankGenerator.&quot;</span><br><br>        self.tp = tp<br>        self.ep = ep<br>        self.dp = dp<br>        self.pp = pp<br>        self.cp = cp<br>        self.rank_offset = rank_offset<br>        self.world_size = tp * dp * pp * cp * ep<br><br>        self.name_to_size = &#123;<br>            <span class="hljs-string">&quot;tp&quot;</span>: self.tp,<br>            <span class="hljs-string">&quot;pp&quot;</span>: self.pp,<br>            <span class="hljs-string">&quot;dp&quot;</span>: self.dp,<br>            <span class="hljs-string">&quot;ep&quot;</span>: self.ep,<br>            <span class="hljs-string">&quot;cp&quot;</span>: self.cp,<br>        &#125;<br>        self.order = order<br>        order = order.lower()<br><br>        <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> self.name_to_size.keys():<br>            <span class="hljs-keyword">if</span> name <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> order <span class="hljs-keyword">and</span> self.name_to_size[name] != <span class="hljs-number">1</span>:<br>                <span class="hljs-keyword">raise</span> RuntimeError(<br>                    <span class="hljs-string">f&quot;The size of (<span class="hljs-subst">&#123;name&#125;</span>) is (<span class="hljs-subst">&#123;self.name_to_size[name]&#125;</span>), but you haven&#x27;t&quot;</span><br>                    <span class="hljs-string">f&quot;specified the order (<span class="hljs-subst">&#123;self.order&#125;</span>).&quot;</span><br>                )<br>            <span class="hljs-keyword">elif</span> name <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> order:<br>                order = order + <span class="hljs-string">&quot;-&quot;</span> + name<br><br>        self.order = order<br>        self.ordered_size = []<br><br>        <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> order.split(<span class="hljs-string">&quot;-&quot;</span>):<br>            self.ordered_size.append(self.name_to_size[token])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_mask</span>(<span class="hljs-params">self, order: <span class="hljs-built_in">str</span>, token: <span class="hljs-built_in">str</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Create a mask for the specified tokens based on the given order.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            order (str): The order of parallelism types (e.g., &#x27;tp-dp-pp&#x27;).</span><br><span class="hljs-string">            token (str): The specific parallelism types to include in the mask,</span><br><span class="hljs-string">                         separated by hyphens (e.g., &#x27;tp-dp&#x27;).</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        ordered_token = order.split(<span class="hljs-string">&quot;-&quot;</span>)<br>        token_list = token.split(<span class="hljs-string">&quot;-&quot;</span>)<br>        mask = [<span class="hljs-literal">False</span>] * <span class="hljs-built_in">len</span>(ordered_token)<br>        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> token_list:<br>            mask[ordered_token.index(t)] = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">return</span> mask<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_ranks</span>(<span class="hljs-params">self, token</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Get rank group by input token.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            token (str):</span><br><span class="hljs-string">                Specify the ranks type that want to get. If we want</span><br><span class="hljs-string">                to obtain multiple parallel types, we can use a hyphen</span><br><span class="hljs-string">                &#x27;-&#x27; to separate them. For example, if we want to obtain</span><br><span class="hljs-string">                the TP_DP group, the token should be &#x27;tp-dp&#x27;.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        mask = self.get_mask(self.order, token)<br>        ranks = generate_masked_orthogonal_rank_groups(self.world_size, self.ordered_size, mask)<br>        <span class="hljs-keyword">if</span> self.rank_offset &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">for</span> rank_group <span class="hljs-keyword">in</span> ranks:<br>                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rank_group)):<br>                    rank_group[i] += self.rank_offset<br>        <span class="hljs-keyword">return</span> ranks<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_masked_orthogonal_rank_groups</span>(<span class="hljs-params"></span><br><span class="hljs-params">    world_size: <span class="hljs-built_in">int</span>, parallel_size: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], mask: <span class="hljs-type">List</span>[<span class="hljs-built_in">bool</span>]</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>    <span class="hljs-string">r&quot;&quot;&quot;Generate orthogonal parallel groups based on the parallel size and mask.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">        world_size (int): world size</span><br><span class="hljs-string"></span><br><span class="hljs-string">        parallel_size (List[int]):</span><br><span class="hljs-string">            The parallel size of each orthogonal parallel type. For example, if</span><br><span class="hljs-string">            tensor_parallel_size = 2, pipeline_model_parallel_group = 3, data_parallel_size = 4,</span><br><span class="hljs-string">            and the parallel mapping order is tp-pp-dp, then the parallel_size = [2, 3, 4].</span><br><span class="hljs-string"></span><br><span class="hljs-string">        mask (List[bool]):</span><br><span class="hljs-string">            The mask controls which parallel methods the generated groups represent. If mask[i] is</span><br><span class="hljs-string">            True, it means the generated group contains the i-th parallelism method. For example,</span><br><span class="hljs-string">            if parallel_size = [tp_size, pp_size, dp_size], and mask = [True, False , True], then</span><br><span class="hljs-string">            the generated group is the `tp-dp` group, if the mask = [False, True, False], then the</span><br><span class="hljs-string">            generated group is the `pp` group.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Algorithm:</span><br><span class="hljs-string">        For orthogonal parallelism, such as tp/dp/pp/cp, the global_rank and</span><br><span class="hljs-string">        local_rank satisfy the following equation:</span><br><span class="hljs-string">            global_rank = tp_rank + dp_rank * tp_size + pp_rank * tp_size * dp_size (1)</span><br><span class="hljs-string">                tp_rank \in [0, tp_size)</span><br><span class="hljs-string">                dp_rank \in [0, dp_size)</span><br><span class="hljs-string">                pp_rank \in [0, pp_size)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        If we want to get the `dp_group` (tp_size * pp_size groups of dp_size ranks each.</span><br><span class="hljs-string">        For example,  if the gpu size is 8 and order is &#x27;tp-pp-dp&#x27;, size is &#x27;2-2-2&#x27;, and the</span><br><span class="hljs-string">        dp_group here is [[0, 4], [1, 5], [2, 6], [3, 7]].)</span><br><span class="hljs-string">        The tp_rank and pp_rank will be combined to form the `dp_group_index`.</span><br><span class="hljs-string">            dp_group_index = tp_rank + pp_rank * tp_size (2)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        So, Given that tp_rank and pp_rank satisfy equation (2), and dp_rank in</span><br><span class="hljs-string">        range(0, dp_size), the ranks in dp_group[dp_group_index] satisfies the</span><br><span class="hljs-string">        equation (1).</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This function solve this math problem.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    For example, if the parallel_size = [tp_size, dp_size, pp_size] = [2, 3, 4],</span><br><span class="hljs-string">    and the mask = [False, True, False]. Then,</span><br><span class="hljs-string">        dp_group_index(0) = tp_rank(0) + pp_rank(0) * 2</span><br><span class="hljs-string">        dp_group_index(1) = tp_rank(1) + pp_rank(0) * 2</span><br><span class="hljs-string">        ...</span><br><span class="hljs-string">        dp_group_index(7) = tp_rank(1) + pp_rank(3) * 2</span><br><span class="hljs-string"></span><br><span class="hljs-string">        dp_group[0] = 0 + range(0, 3) * 2 + 0 = [0, 2, 4]</span><br><span class="hljs-string">        dp_group[1] = 1 + range(0, 3) * 2 + 0 = [1, 3, 5]</span><br><span class="hljs-string">        ...</span><br><span class="hljs-string">        dp_group[7] = 1 + range(0, 3) * 2 + 3 * 2 * 3 = [19, 21, 23]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">prefix_product</span>(<span class="hljs-params">a: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], init=<span class="hljs-number">1</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        r = [init]<br>        <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> a:<br>            init = init * v<br>            r.append(init)<br>        <span class="hljs-keyword">return</span> r<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">inner_product</span>(<span class="hljs-params">a: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], b: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>([x * y <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(a, b)])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decompose</span>(<span class="hljs-params">index, shape, stride=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        This function solve the math problem below:</span><br><span class="hljs-string">            There is an equation:</span><br><span class="hljs-string">                index = sum(idx[i] * stride[i])</span><br><span class="hljs-string">            And given the value of index, stride.</span><br><span class="hljs-string">            Return the idx.</span><br><span class="hljs-string">        This function will be used to get the pp/dp/pp_rank</span><br><span class="hljs-string">        from group_index and rank_in_group.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> stride <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            stride = prefix_product(shape)<br>        idx = [(index // d) % s <span class="hljs-keyword">for</span> s, d <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(shape, stride)]<br>        <span class="hljs-comment"># stride is a prefix_product result. And the value of stride[-1]</span><br>        <span class="hljs-comment"># is not used.</span><br>        <span class="hljs-keyword">assert</span> (<br>            <span class="hljs-built_in">sum</span>([x * y <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(idx, stride[:-<span class="hljs-number">1</span>])]) == index<br>        ), <span class="hljs-string">&quot;idx &#123;&#125; with shape &#123;&#125; mismatch the return idx &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(index, shape, idx)<br>        <span class="hljs-keyword">return</span> idx<br><br>    masked_shape = [s <span class="hljs-keyword">for</span> s, m <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(parallel_size, mask) <span class="hljs-keyword">if</span> m]<br>    unmasked_shape = [s <span class="hljs-keyword">for</span> s, m <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(parallel_size, mask) <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> m]<br><br>    global_stride = prefix_product(parallel_size)<br>    masked_stride = [d <span class="hljs-keyword">for</span> d, m <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(global_stride, mask) <span class="hljs-keyword">if</span> m]<br>    unmasked_stride = [d <span class="hljs-keyword">for</span> d, m <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(global_stride, mask) <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> m]<br><br>    group_size = prefix_product(masked_shape)[-<span class="hljs-number">1</span>]<br>    num_of_group = world_size // group_size<br><br>    ranks = []<br>    <span class="hljs-keyword">for</span> group_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_of_group):<br>        <span class="hljs-comment"># get indices from unmaksed for group_index.</span><br>        decomposed_group_idx = decompose(group_index, unmasked_shape)<br>        rank = []<br>        <span class="hljs-keyword">for</span> rank_in_group <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(group_size):<br>            <span class="hljs-comment"># get indices from masked for rank_in_group.</span><br>            decomposed_rank_idx = decompose(rank_in_group, masked_shape)<br>            rank.append(<br>                inner_product(decomposed_rank_idx, masked_stride)<br>                + inner_product(decomposed_group_idx, unmasked_stride)<br>            )<br>        ranks.append(rank)<br>    <span class="hljs-keyword">return</span> ranks<br><br></code></pre></td></tr></table></figure><ul><li><p><code>RankGenerator</code>需要先获取到各个并行方法的并行度，此外还需要获得一个rank计数的顺序，这是一个字符串，如<code>&quot;tp-dp-pp&quot;</code>，说明先计数tp再是dp再是pp，<code>RankGenerator</code>在初始化时还会进行一定程度的补全与解析。</p></li><li><p><code>RankGenerator</code>有两个函数</p><ul><li><p>一个是<code>get_mask</code>,负责根据order和token返回mask。例如order是’tp-dp-pp’，token是’tp-dp’，那么就会返回[true, false, true]</p></li><li><p>一个是<code>get_ranks</code>,负责依据token返回对应的rank group。例如现在的order是’tp-pp-dp’，tp_size&#x3D;2,pp_size&#x3D;2,dp_size&#x3D;2，现在global rank的计算公式为tp_rank+pp_rank*tp_size+dp_rank*tp_size*pp_size，现在token是’dp’，说明想要知道tp所属rank、pp所属rank相同，但是所属dp不同的rank的集合，即tp_rank+pp_rank*tp_size+rang(dp_rank)*tp_size*pp_size，rang(dp_rank)&#x3D;{0,1}，也就是需要知道哪些rank需要进行dp间通信以共享对应相同模型参数的梯度计算结果等，在这个例子中我们得到的就是[[0,4],[1,5],[2,6],[3,7]]，如下图所示，同颜色的就是同一个dp_group内的rank。</p><p><img src="/2025/12/22/megatron-lm-pre-train-process/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E4%BB%B6.jpg"></p></li><li><p>同理对于<code>get_ranks</code>，如果现在的order是’tp-dp-pp’，tp_size&#x3D;2,dp_size&#x3D;3,pp_size&#x3D;4，现在global rank的计算公式为tp_rank+dp_rank*tp_size+pp_rank*tp_size*dp_size，如果token依旧是’dp’，那么dp group的计算公式为tp_rank+rang(dp_rank)*tp_size+pp_rank*tp_size*dp_size，即[[0,2,4],[1,3,5]…[19,21,23]]</p></li></ul></li></ul></li><li><p>然后借助<code>RankGenerator</code>，我们就可以创建各个通信组，其创建流程基本与如下代码类似，即得到不同并行策略的groups，然后遍历这些group，对每个group创建<code>torch.distributed.new_group</code>，然后查看如果本进程的rank在这个group里，那么就设置其相关全局变量为这个group。注意这里每次都创建了new_group，但是本进程接下来可能并不会保存它，这么做是为了在分布式执行中让所有worker都执行同样的new_group，以保证分布式通信的正确，防止死锁等问题。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> rank_generator.get_ranks(<span class="hljs-string">&#x27;xxx&#x27;</span>):<br>    group = create_group(ranks, backend=..., pg_options=...)<br>    <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>        GLOBAL_GROUP = group<br>        GLOBAL_RANKS = ranks<br></code></pre></td></tr></table></figure><ul><li><p>非 Expert（Decoder &#x2F; Attention）并行组对应关系</p></li><li><p>Expert（MoE）相关并行组对应关系：</p></li><li><p>Distributed Optimizer 相关组对应关系</p></li><li></li></ul></li></ul><h2 id="setup-model-and-optimizer"><a href="#setup-model-and-optimizer" class="headerlink" title="setup_model_and_optimizer"></a>setup_model_and_optimizer</h2><p>setup_model_and_optimizer的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_model_and_optimizer</span>(<span class="hljs-params"></span><br><span class="hljs-params">    model_provider_func,</span><br><span class="hljs-params">    model_type,</span><br><span class="hljs-params">    no_wd_decay_cond=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    scale_lr_cond=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    lr_mult=<span class="hljs-number">1.0</span>,</span><br><span class="hljs-params">    checkpointing_context=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Setup model and optimizer.&quot;&quot;&quot;</span><br>    args = get_args()<br>    timers = get_timers()<br>    one_logger = get_one_logger()<br><br>    model = get_model(model_provider_func, model_type)<br>    unwrapped_model = unwrap_model(model)<br><br>    one_logger <span class="hljs-keyword">and</span> one_logger.log_metrics(&#123;<span class="hljs-string">&quot;app_build_optimzer_start_time&quot;</span>: one_logger_utils.get_timestamp_in_ms()&#125;)<br>    kwargs = &#123;&#125;<br>    <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> dataclasses.fields(OptimizerConfig):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(args, f.name):<br>            kwargs[f.name] = <span class="hljs-built_in">getattr</span>(args, f.name)<br>    config = OptimizerConfig(**kwargs)<br>    config.timers = timers<br>    optimizer = get_megatron_optimizer(<br>        config,<br>        model,<br>        no_wd_decay_cond,<br>        scale_lr_cond,<br>        lr_mult,<br>        use_gloo_process_groups=args.enable_gloo_process_groups,<br>        <span class="hljs-comment"># If the user is asking for a non-zero embedding init std, skip weight decay for embeddings</span><br>        <span class="hljs-comment">#  to avoid embeddings from shrinking to zero as recommended in https://arxiv.org/abs/2312.16903</span><br>        default_skip_embedding_weight_decay=args.embedding_init_method_std <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>,<br>    )<br>    opt_param_scheduler = get_optimizer_param_scheduler(optimizer)<br>    one_logger <span class="hljs-keyword">and</span> one_logger.log_metrics(&#123;<span class="hljs-string">&quot;app_build_optimzer_finish_time&quot;</span>: one_logger_utils.get_timestamp_in_ms()&#125;)<br><br>    <span class="hljs-keyword">if</span> args.moe_use_upcycling:<br>        torch.distributed.barrier()<br>        <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> checkpoint_exists(args.save), (<br>            <span class="hljs-string">&quot;The upcycling destination directory already exists. &quot;</span><br>            <span class="hljs-string">&quot;Please check if --moe-use-upcycling is mistakenly enabled. &quot;</span><br>            <span class="hljs-string">&quot;Upcycling should only be set for the first run when converting the dense model. &quot;</span><br>            <span class="hljs-string">&quot;All subsequent runs should remove this flag. &quot;</span><br>        )<br>        <span class="hljs-comment"># before changing moe related global args, save them in local variables</span><br>        num_experts = args.num_experts<br>        expert_model_parallel_size = args.expert_model_parallel_size<br>        moe_ffn_hidden_size = args.ffn_hidden_size<br><br>        <span class="hljs-comment"># set dense model related args in to global args before getting dense model</span><br>        args.num_experts = <span class="hljs-literal">None</span><br>        args.expert_model_parallel_size = <span class="hljs-number">1</span><br>        args.ffn_hidden_size = moe_ffn_hidden_size * args.moe_upcycling_granularity <br><br>        <span class="hljs-comment"># get dense model</span><br>        dense_model_for_upcycling = get_model(model_provider_func, model_type)<br><br>        <span class="hljs-comment"># recover moe upcycling related args in global args before executing upcycling</span><br>        args.num_experts = num_experts<br>        args.expert_model_parallel_size = expert_model_parallel_size<br>        args.ffn_hidden_size = moe_ffn_hidden_size<br><br>        <span class="hljs-comment"># execute upcycling</span><br>        _, args.num_floating_point_operations_so_far = upcycling_utils.load_and_upcycle_model(<br>            load_checkpoint,<br>            unwrapped_model,<br>            dense_model_for_upcycling,<br>            load_kwargs=&#123;<br>                <span class="hljs-string">&#x27;model&#x27;</span>: dense_model_for_upcycling,<br>                <span class="hljs-string">&#x27;optimizer&#x27;</span>: <span class="hljs-literal">None</span>,<br>                <span class="hljs-string">&#x27;opt_param_scheduler&#x27;</span>: <span class="hljs-literal">None</span>,<br>            &#125;,<br>        )<br>        args.iteration = <span class="hljs-number">1</span><br>        save_checkpoint(<br>            args.iteration, model, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, args.num_floating_point_operations_so_far<br>        )<br>        torch.distributed.barrier()<br>        <span class="hljs-keyword">del</span> dense_model_for_upcycling<br>        <span class="hljs-keyword">if</span> (args.fp16 <span class="hljs-keyword">or</span> args.bf16) <span class="hljs-keyword">and</span> optimizer <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            optimizer.reload_model_params()<br>        print_rank_0(<span class="hljs-string">f&#x27;Upcycled checkpoint saved to <span class="hljs-subst">&#123;args.save&#125;</span>&#x27;</span>)<br><br>    <span class="hljs-keyword">if</span> (<br>        args.load <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> args.pretrained_checkpoint <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>    ) <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> args.moe_use_upcycling:<br>        one_logger <span class="hljs-keyword">and</span> one_logger.log_metrics(<br>            &#123;<span class="hljs-string">&#x27;load_checkpoint_start_time&#x27;</span>: one_logger_utils.get_timestamp_in_ms()&#125;<br>        )<br>        timers(<span class="hljs-string">&#x27;load-checkpoint&#x27;</span>, log_level=<span class="hljs-number">0</span>).start(barrier=<span class="hljs-literal">True</span>)<br><br>        args.iteration, args.num_floating_point_operations_so_far = load_checkpoint(<br>            model,<br>            optimizer,<br>            opt_param_scheduler,<br>            checkpointing_context=checkpointing_context,<br>            skip_load_to_model_and_opt=HAVE_FSDP2<br>            <span class="hljs-keyword">and</span> <span class="hljs-built_in">getattr</span>(args, <span class="hljs-string">&quot;use_torch_fsdp2&quot;</span>, <span class="hljs-literal">False</span>)<br>            <span class="hljs-keyword">and</span> args.ckpt_format == <span class="hljs-string">&quot;torch_dist&quot;</span>,<br>        )<br>        timers(<span class="hljs-string">&#x27;load-checkpoint&#x27;</span>).stop(barrier=<span class="hljs-literal">True</span>)<br>        timers.log([<span class="hljs-string">&#x27;load-checkpoint&#x27;</span>])<br>        one_logger <span class="hljs-keyword">and</span> one_logger.log_metrics(<br>            &#123;<br>                <span class="hljs-string">&#x27;load_checkpoint_finish_time&#x27;</span>: one_logger_utils.get_timestamp_in_ms(),<br>                <span class="hljs-string">&#x27;load_checkpoint_time&#x27;</span>: timers(<span class="hljs-string">&#x27;load-checkpoint&#x27;</span>).active_time(),<br>            &#125;<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        args.iteration = <span class="hljs-number">0</span><br>        args.num_floating_point_operations_so_far = <span class="hljs-number">0</span><br><br>    <span class="hljs-comment"># get model without FP16 and/or DDP wrappers</span><br>    <span class="hljs-keyword">if</span> (<br>        args.iteration == <span class="hljs-number">0</span><br>        <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(unwrapped_model) == <span class="hljs-number">1</span><br>        <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(unwrapped_model[<span class="hljs-number">0</span>], <span class="hljs-string">&#x27;init_state_dict_from_bert&#x27;</span>)<br>    ):<br>        print_rank_0(<span class="hljs-string">&quot;Initializing ICT from pretrained BERT model&quot;</span>)<br>        unwrapped_model[<span class="hljs-number">0</span>].init_state_dict_from_bert()<br>        <span class="hljs-keyword">if</span> args.fp16:<br>            optimizer.reload_model_params()<br><br>    <span class="hljs-comment"># Convert checkpoint format.</span><br>    <span class="hljs-keyword">if</span> args.ckpt_convert_format <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        load_ckpt_format = args.ckpt_format<br>        args.ckpt_format = args.ckpt_convert_format<br>        args.save = os.path.join(args.ckpt_convert_save, args.ckpt_convert_format)<br>        update_use_dist_ckpt(args)<br><br>        save_checkpoint(<br>            args.iteration,<br>            model,<br>            optimizer,<br>            opt_param_scheduler,<br>            args.num_floating_point_operations_so_far,<br>            preprocess_common_state_dict_fn=preprocess_common_state_dict,<br>        )<br><br>        print_rank_0(<span class="hljs-string">&quot;&gt; converted checkpoint: %s -&gt; %s.&quot;</span> % (load_ckpt_format, args.ckpt_format))<br>        torch.distributed.barrier()<br>        exit()<br><br>    <span class="hljs-keyword">return</span> model, optimizer, opt_param_scheduler<br><br></code></pre></td></tr></table></figure><p>其整体流程如下：</p><ol><li><p>通过get_model获取本worker上的模型</p></li><li><p>通过<code>unwrap_model</code>来获取DDP包装下的原始模型</p></li><li><p>通过<code>get_megatron_optimizer</code>获取optimizer</p></li><li><p>通过<code>get_optimizer_param_scheduler</code>获取optimizer学习率参数调度器</p></li><li><p>如有配置args.moe_use_upcycling，执行MoE upcycling，把 Dense FFN 模型转成 MoE 模型，然后从检查点中获取模型并保存，还会调整优化器</p></li><li><p>如果没有配置args.moe_use_upcycling并且配置了检查点，那么就从检查点中加载模型、优化器等</p></li><li><p>如果iteration &#x3D; 0 时的特殊初始化（BERT），执行optimizer.reload_model_params()</p></li><li><p>如果配置了args.ckpt_convert_format，就加载旧格式的模型检查点，然后保存为新格式的模型检查点</p></li><li><p>最终return model, optimizer, opt_param_scheduler</p></li></ol><h3 id="get-model"><a href="#get-model" class="headerlink" title="get_model"></a>get_model</h3><p>get_model代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>(<span class="hljs-params">model_provider_func, model_type=ModelType.encoder_or_decoder, wrap_with_ddp=<span class="hljs-literal">True</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build the model.&quot;&quot;&quot;</span><br>    args = get_args()<br>    args.model_type = model_type<br><br>    <span class="hljs-comment"># Build model.</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build_model</span>():<br>        <span class="hljs-keyword">if</span> (<br>            mpu.get_pipeline_model_parallel_world_size() &gt; <span class="hljs-number">1</span><br>            <span class="hljs-keyword">and</span> args.virtual_pipeline_model_parallel_size <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        ):<br>            model = []<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(args.virtual_pipeline_model_parallel_size):<br>                <span class="hljs-comment"># Set pre_process and post_process only after virtual rank is set.</span><br>                pre_process = mpu.is_pipeline_first_stage(ignore_virtual=<span class="hljs-literal">False</span>, vp_stage=i)<br>                post_process = mpu.is_pipeline_last_stage(ignore_virtual=<span class="hljs-literal">False</span>, vp_stage=i)<br>                this_model = model_provider_func(<br>                    pre_process=pre_process, post_process=post_process, vp_stage=i)<br>                this_model.model_type = model_type<br>                this_model.vp_stage = i<br>                model.append(this_model)<br>        <span class="hljs-keyword">else</span>:<br>            pre_process = mpu.is_pipeline_first_stage()<br>            post_process = mpu.is_pipeline_last_stage()<br>            model = model_provider_func(pre_process=pre_process, post_process=post_process)<br>            model.model_type = model_type<br>        <span class="hljs-keyword">return</span> model<br><br>    <span class="hljs-keyword">if</span> args.init_model_with_meta_device:<br>        <span class="hljs-keyword">with</span> torch.device(<span class="hljs-string">&#x27;meta&#x27;</span>):<br>            model = build_model()<br>    <span class="hljs-keyword">else</span>:<br>        model = build_model()<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(model, <span class="hljs-built_in">list</span>):<br>        model = [model]<br><br>    <span class="hljs-comment"># Set tensor model parallel attributes if not set.</span><br>    <span class="hljs-comment"># Only parameters that are already tensor model parallel have these</span><br>    <span class="hljs-comment"># attributes set for them. We should make sure the default attributes</span><br>    <span class="hljs-comment"># are set for all params so the optimizer can use them.</span><br>    <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model:<br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model_module.parameters():<br>            tensor_parallel.set_defaults_if_not_set_tensor_model_parallel_attributes(param)<br><br>    <span class="hljs-comment"># Print number of parameters.</span><br>    num_parameters = <span class="hljs-built_in">sum</span>(<br>        [<span class="hljs-built_in">sum</span>([p.nelement() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model_module.parameters()]) <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model]<br>    )<br>    <span class="hljs-keyword">if</span> mpu.get_data_parallel_rank() == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> mpu.get_context_parallel_rank() == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(<br>            <span class="hljs-string">&#x27; &gt; number of parameters on (tensor, pipeline) &#x27;</span><br>            <span class="hljs-string">&#x27;model parallel rank (&#123;&#125;, &#123;&#125;): &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                mpu.get_tensor_model_parallel_rank(),<br>                mpu.get_pipeline_model_parallel_rank(),<br>                num_parameters,<br>            ),<br>            flush=<span class="hljs-literal">True</span>,<br>        )<br><br>    <span class="hljs-comment"># GPU allocation.</span><br>    <span class="hljs-comment"># For FSDP2, we don&#x27;t allocate GPU memory here. We allocate GPU memory</span><br>    <span class="hljs-comment"># in the fully_shard function of FSDP2 instead.</span><br>    <span class="hljs-keyword">if</span> (<br>        <span class="hljs-keyword">not</span> (args.use_torch_fsdp2 <span class="hljs-keyword">and</span> args.use_cpu_initialization)<br>        <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> args.init_model_with_meta_device<br>    ):<br>        <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model:<br>            model_module.cuda(torch.cuda.current_device())<br><br>    <span class="hljs-comment"># Fp16 conversion.</span><br>    <span class="hljs-keyword">if</span> args.fp16 <span class="hljs-keyword">or</span> args.bf16:<br>        config = get_model_config(model[<span class="hljs-number">0</span>])<br>        model = [Float16Module(config, model_module) <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model]<br><br>    <span class="hljs-comment"># Before TE2.x: The model_module.bfloat16()/model_module.half() above will call the inplace</span><br>    <span class="hljs-comment">#               copy of TE&#x27;s Float8Tensor, which will write an unwanted value (amax calculated</span><br>    <span class="hljs-comment">#               from the current fp8 param) to its amax_history. The below function will correct</span><br>    <span class="hljs-comment">#               the amax_history back.</span><br>    <span class="hljs-comment"># After TE2.x: Below function is an empty function and does nothing.</span><br>    correct_amax_history_if_needed(model)<br><br>    <span class="hljs-keyword">if</span> wrap_with_ddp:<br>        <span class="hljs-keyword">if</span> args.use_torch_fsdp2:<br>            <span class="hljs-keyword">assert</span> HAVE_FSDP2, <span class="hljs-string">&quot;Torch FSDP2 requires torch&gt;=2.4.0&quot;</span><br>            DP = torch_FSDP<br>        <span class="hljs-keyword">elif</span> args.use_megatron_fsdp:<br>            DP = megatron_FSDP<br>        <span class="hljs-keyword">else</span>:<br>            DP = DDP<br><br>        config = get_model_config(model[<span class="hljs-number">0</span>])<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">getattr</span>(args, <span class="hljs-string">&quot;use_torch_fsdp2&quot;</span>, <span class="hljs-literal">False</span>):<br>            reshard_after_forward = <span class="hljs-built_in">getattr</span>(args, <span class="hljs-string">&quot;torch_fsdp2_reshard_after_forward&quot;</span>, <span class="hljs-literal">True</span>)<br>            ddp_config = TorchFullyShardedDataParallelConfig(reshard_after_forward=reshard_after_forward)<br>        <span class="hljs-keyword">else</span>:<br>            kwargs = &#123;&#125;<br>            <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> dataclasses.fields(DistributedDataParallelConfig):<br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(args, f.name):<br>                    kwargs[f.name] = <span class="hljs-built_in">getattr</span>(args, f.name)<br>            kwargs[<span class="hljs-string">&#x27;grad_reduce_in_fp32&#x27;</span>] = args.accumulate_allreduce_grads_in_fp32<br>            kwargs[<span class="hljs-string">&#x27;check_for_nan_in_grad&#x27;</span>] = args.check_for_nan_in_loss_and_grad<br>            kwargs[<span class="hljs-string">&#x27;check_for_large_grads&#x27;</span>] = args.check_for_large_grads<br>            <span class="hljs-keyword">if</span> args.ddp_num_buckets <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">assert</span> args.ddp_bucket_size <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, \<br>                    <span class="hljs-string">&quot;Cannot specify both --ddp-num-buckets and --ddp-bucket-size&quot;</span><br>                <span class="hljs-keyword">assert</span> args.ddp_num_buckets &gt; <span class="hljs-number">0</span>, \<br>                    <span class="hljs-string">&quot;--ddp-num-buckets must be greater than 0&quot;</span><br>                kwargs[<span class="hljs-string">&#x27;bucket_size&#x27;</span>] = num_parameters // args.ddp_num_buckets<br>            <span class="hljs-keyword">else</span>:<br>                kwargs[<span class="hljs-string">&#x27;bucket_size&#x27;</span>] = args.ddp_bucket_size<br>            kwargs[<span class="hljs-string">&#x27;pad_buckets_for_high_nccl_busbw&#x27;</span>] = args.ddp_pad_buckets_for_high_nccl_busbw<br>            kwargs[<span class="hljs-string">&#x27;average_in_collective&#x27;</span>] = args.ddp_average_in_collective<br>            <span class="hljs-keyword">if</span> args.use_megatron_fsdp <span class="hljs-keyword">and</span> args.use_precision_aware_optimizer:<br>                kwargs[<span class="hljs-string">&quot;preserve_fp32_weights&quot;</span>] = <span class="hljs-literal">False</span><br>            ddp_config = DistributedDataParallelConfig(**kwargs)<br><br>            <span class="hljs-comment"># In the Megatron FSDP and DDP use path, we need to initialize the bucket size.</span><br>            <span class="hljs-comment"># If bucket_size is not provided as an input, use sane default.</span><br>            <span class="hljs-comment"># If using very large dp_sizes, make buckets larger to ensure that chunks used in NCCL</span><br>            <span class="hljs-comment"># ring-reduce implementations are large enough to remain bandwidth-bound rather than</span><br>            <span class="hljs-comment"># latency-bound.</span><br>            <span class="hljs-keyword">if</span> ddp_config.bucket_size <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                ddp_config.bucket_size = <span class="hljs-built_in">max</span>(<br>                    <span class="hljs-number">40000000</span>, <span class="hljs-number">1000000</span> * mpu.get_data_parallel_world_size(with_context_parallel=<span class="hljs-literal">True</span>)<br>                )<br>            <span class="hljs-comment"># Set bucket_size to infinity if overlap_grad_reduce is False.</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> ddp_config.overlap_grad_reduce:<br>                ddp_config.bucket_size = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">with</span> torch.cuda.stream(torch.cuda.Stream()):<br>            model = [<br>                DP(<br>                    config=config,<br>                    ddp_config=ddp_config,<br>                    module=model_chunk,<br>                    <span class="hljs-comment"># Turn off bucketing for model_chunk 2 onwards, since communication for these</span><br>                    <span class="hljs-comment"># model chunks is overlapped with compute anyway.</span><br>                    disable_bucketing=(model_chunk_idx &gt; <span class="hljs-number">0</span>)<br>                    <span class="hljs-keyword">or</span> args.overlap_param_gather_with_optimizer_step,<br>                )<br>                <span class="hljs-keyword">for</span> (model_chunk_idx, model_chunk) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(model)<br>            ]<br><br>        <span class="hljs-comment"># Broadcast params from data parallel src rank to other data parallel ranks.</span><br>        <span class="hljs-keyword">if</span> args.data_parallel_random_init:<br>            <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model:<br>                model_module.broadcast_params()<br><br>    <span class="hljs-keyword">return</span> model<br><br></code></pre></td></tr></table></figure><p>其整体流程如下</p><ol><li><p>构建了一个<code>build_model()</code>函数，并使用其获得各个worker上应有的模型。其主要是负责根据当前是否是第一阶段、是否是最后一阶段以及当前的pp维度、vp维度构建在当前worker上的模型。例如当前layer数量是8，pp_size是4，vp_size是2，则4个worker获得的模型分别是[embedding+layer_0, layer_4], [layer_1, layer_5], [layer_2,layer_6], [layer_3,layer_7+lm head+loss]。</p></li><li><p>给各个model的parameters设置tp的默认参数：</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">_MODEL_PARALLEL_ATTRIBUTE_DEFAULTS = &#123;<br>    <span class="hljs-string">&quot;tensor_model_parallel&quot;</span>: False,<br>    <span class="hljs-string">&quot;partition_dim&quot;</span>: -1,<br>    <span class="hljs-string">&quot;partition_stride&quot;</span>: 1,<br>&#125;<br></code></pre></td></tr></table></figure><ul><li><p>如果没有使用fsdp2并且不使用cpu初始化并且没有<code>init_model_with_meta_device</code>，那么就将模型搬运到GPU显存上</p></li><li><p>如果配置了fp16、bf16，那么就对model进行低精度转换</p></li><li><p>然后如果启用了DDP就进行DDP包装：</p><ol><li><p>首先定义DP，这里支持3类DDP包装，分别是torch_FSDP、megatron_FSDP和DDP</p></li><li><p>然后构造ddp_config：</p><ol><li><p>如果使用的是fsdp2，就直接TorchFullyShardedDataParallelConfig作为ddp_config</p></li><li><p>不然就自行填写参数构造DistributedDataParallelConfig作为ddp_config</p></li></ol></li><li><p>然后对于当前rank的各个model，使用DP对其进行包装</p></li><li><p>如果使用了data_parallel_random_init，还需要在ddp内进行broadcast_params以统一参数。</p></li></ol></li></ul><h3 id="get-megatron-optimizer"><a href="#get-megatron-optimizer" class="headerlink" title="get_megatron_optimizer"></a>get_megatron_optimizer</h3><p><code>get_megatron_optimizer</code>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_megatron_optimizer</span>(<span class="hljs-params"></span><br><span class="hljs-params">    config: OptimizerConfig,</span><br><span class="hljs-params">    model_chunks: <span class="hljs-type">List</span>[MegatronModule],</span><br><span class="hljs-params">    no_weight_decay_cond: <span class="hljs-type">Optional</span>[<span class="hljs-type">Callable</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    scale_lr_cond: <span class="hljs-type">Optional</span>[<span class="hljs-type">Callable</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    lr_mult: <span class="hljs-built_in">float</span> = <span class="hljs-number">1.0</span>,</span><br><span class="hljs-params">    use_gloo_process_groups: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params">    default_skip_embedding_weight_decay: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    grad_comm_pgs: <span class="hljs-type">Optional</span>[GradCommProcessGroups] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    model_comm_pgs: <span class="hljs-type">Optional</span>[ModelCommProcessGroups] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>) -&gt; MegatronOptimizer:<br>    <span class="hljs-string">&quot;&quot;&quot;Retrieve the Megatron optimizer for model chunks.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    We use separate optimizers for expert parameters and non-expert parameters.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        config (OptimizerConfig): optimizer configuration object.</span><br><span class="hljs-string">        model_chunks (List[MegatronModule]): model chunks to get optimizer for.</span><br><span class="hljs-string">        no_weight_decay_cond (func, optional): function to determine whether a parameter</span><br><span class="hljs-string">            should not perform weight decay. Defaults to None.</span><br><span class="hljs-string">        scale_lr_cond (func, optional): function to determine whether a parameter</span><br><span class="hljs-string">            should have a scaled learning rate. Defaults to None.</span><br><span class="hljs-string">        lr_mult (float, optional): learning rate multiplier for parameters that</span><br><span class="hljs-string">            satisfy scale_lr_cond. Defaults to 1.0.</span><br><span class="hljs-string">        use_gloo_process_groups (bool): if false, disable use of Gloo process groups</span><br><span class="hljs-string">            in underlying Megatron optimizers.</span><br><span class="hljs-string">        default_skip_embedding_weight_decay (bool): whether to skip weight decay for</span><br><span class="hljs-string">            embedding parameters by default, if no_weight_decay_cond is not provided.</span><br><span class="hljs-string">            This is useful if you do not want embeddings to shrink to zero in training</span><br><span class="hljs-string">            as recommended in https://arxiv.org/abs/2312.16903</span><br><span class="hljs-string">        grad_comm_pgs (Optional[GradCommProcessGroups]): gradient communication process groups.</span><br><span class="hljs-string">            If None, uses default parallel_state groups.</span><br><span class="hljs-string">        model_comm_pgs (Optional[ModelCommProcessGroups]): model communication process groups.</span><br><span class="hljs-string">            If None, uses default parallel_state groups.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        Instance of MegatronOptimizer.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    log_single_rank(logger, logging.INFO, <span class="hljs-string">f&#x27;Setting up optimizer with config <span class="hljs-subst">&#123;config&#125;</span>&#x27;</span>)<br><br>    <span class="hljs-comment"># Separate out first model chunk if overlapping param AG with optimizer step.</span><br>    <span class="hljs-keyword">if</span> config.overlap_param_gather_with_optimizer_step:<br>        all_dense_model_chunks = [[model_chunks[<span class="hljs-number">0</span>]], model_chunks[<span class="hljs-number">1</span>:]]<br>        overlap_param_gather_with_optimizer_step_flags = [<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>]<br>    <span class="hljs-keyword">else</span>:<br>        all_dense_model_chunks = [model_chunks]<br>        overlap_param_gather_with_optimizer_step_flags = [<span class="hljs-literal">False</span>]<br><br>    <span class="hljs-keyword">if</span> grad_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> model_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># Gradient communication groups</span><br>        dp_cp_group = parallel_state.get_data_parallel_group(<br>            with_context_parallel=<span class="hljs-literal">True</span>, partial_data_parallel=<span class="hljs-literal">False</span><br>        )<br>        intra_dp_cp_group = parallel_state.get_data_parallel_group(<br>            with_context_parallel=<span class="hljs-literal">True</span>, partial_data_parallel=<span class="hljs-literal">True</span><br>        )<br><br>        intra_expt_dp_group = parallel_state.get_expert_data_parallel_group(<br>            partial_expert_data_parallel=<span class="hljs-literal">True</span><br>        )<br><br>        <span class="hljs-comment"># Gloo groups</span><br>        <span class="hljs-keyword">if</span> use_gloo_process_groups:<br>            intra_dp_cp_group_gloo = parallel_state.get_data_parallel_group_gloo(<br>                with_context_parallel=<span class="hljs-literal">True</span>, partial_data_parallel=<span class="hljs-literal">True</span><br>            )<br>            intra_expt_dp_group_gloo = parallel_state.get_expert_data_parallel_group_gloo(<br>                partial_expert_data_parallel=<span class="hljs-literal">True</span><br>            )<br>        <span class="hljs-keyword">else</span>:<br>            intra_dp_cp_group_gloo = <span class="hljs-literal">None</span><br>            intra_expt_dp_group_gloo = <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># Model communication groups</span><br>        mp_group = parallel_state.get_model_parallel_group()<br>        expt_tp_pp_group = parallel_state.get_expert_tensor_model_pipeline_parallel_group()<br>    <span class="hljs-keyword">elif</span> grad_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> model_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># 1. dp group - this is always required</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(grad_comm_pgs, <span class="hljs-string">&#x27;dp&#x27;</span>):<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;dp process group is required but not provided in grad_comm_pgs&quot;</span>)<br>        dp_group = grad_comm_pgs.dp<br><br>        <span class="hljs-comment"># 2. dp_cp group:</span><br>        <span class="hljs-comment"># - If provided in grad_comm_pgs, use it</span><br>        <span class="hljs-comment"># - Otherwise check context_parallel_size</span><br>        <span class="hljs-comment">#   - If cp_size is 1, use same as dp</span><br>        <span class="hljs-comment">#   - If cp_size &gt; 1, raise error as dp_cp is needed</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(grad_comm_pgs, <span class="hljs-string">&#x27;dp_cp&#x27;</span>):<br>            dp_cp_group = grad_comm_pgs.dp_cp<br>        <span class="hljs-keyword">else</span>:<br>            model_config = get_model_config(model_chunks[<span class="hljs-number">0</span>])<br>            cp_size = <span class="hljs-built_in">getattr</span>(model_config, <span class="hljs-string">&#x27;context_parallel_size&#x27;</span>, <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> cp_size == <span class="hljs-number">1</span>:<br>                <span class="hljs-comment"># If no context parallelism, dp_cp is same as dp</span><br>                dp_cp_group = dp_group<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">raise</span> ValueError(<br>                    <span class="hljs-string">&quot;dp_cp process group is required when context_parallel_size &gt; 1 &quot;</span><br>                    <span class="hljs-string">&quot;but not provided in grad_comm_pgs&quot;</span><br>                )<br><br>        <span class="hljs-comment"># 3. Handle expert data parallel group</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_comm_pgs, <span class="hljs-string">&#x27;expt_dp&#x27;</span>), (<br>            <span class="hljs-string">&quot;expt_dp process group is required but not provided in grad_comm_pgs&quot;</span>,<br>            <span class="hljs-string">&quot;please explicitly set it to None if you don&#x27;t need it&quot;</span>,<br>        )<br>        expt_dp_group = grad_comm_pgs.expt_dp<br><br>        <span class="hljs-comment"># 4. Handle intra_dp_cp, intra_expt_dp, and inter_dist_opt</span><br>        <span class="hljs-comment">#    based on optimizer instances:</span><br>        <span class="hljs-comment"># Get ddp_config from model chunks to determine optimizer instances</span><br>        ddp_config = model_chunks[<span class="hljs-number">0</span>].ddp_config<br>        <span class="hljs-keyword">if</span> ddp_config.num_distributed_optimizer_instances == <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># With a single optimizer instance:</span><br>            <span class="hljs-comment"># - intra_dp_cp is same as dp_cp</span><br>            <span class="hljs-comment"># - intra_expt_dp is same as expt_dp</span><br>            <span class="hljs-comment"># - inter_dist_opt is not needed (set to None)</span><br>            intra_dp_cp_group = dp_cp_group<br>            intra_expt_dp_group = expt_dp_group<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># With multiple optimizer instances, both groups must be provided</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> (<br>                <span class="hljs-built_in">hasattr</span>(grad_comm_pgs, <span class="hljs-string">&#x27;intra_dp_cp&#x27;</span>)<br>                <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(grad_comm_pgs, <span class="hljs-string">&#x27;intra_expt_dp&#x27;</span>)<br>                <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(grad_comm_pgs, <span class="hljs-string">&#x27;inter_dist_opt&#x27;</span>)<br>            ):<br>                <span class="hljs-keyword">raise</span> ValueError(<br>                    <span class="hljs-string">&quot;intra_dp_cp, intra_expt_dp, and inter_dist_opt &quot;</span><br>                    <span class="hljs-string">&quot;process groups are required when using multiple optimizer &quot;</span><br>                    <span class="hljs-string">&quot;instances (&gt;1) but not provided in grad_comm_pgs&quot;</span><br>                )<br>            intra_dp_cp_group = grad_comm_pgs.intra_dp_cp<br>            intra_expt_dp_group = grad_comm_pgs.intra_expt_dp<br><br>        <span class="hljs-comment"># 5. Model communication groups</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(model_comm_pgs, <span class="hljs-string">&#x27;mp&#x27;</span>), (<br>            <span class="hljs-string">&quot;mp process group is required but not provided in model_comm_pgs&quot;</span>,<br>            <span class="hljs-string">&quot;please explicitly set it to None if you don&#x27;t need it&quot;</span>,<br>        )<br>        mp_group = model_comm_pgs.mp<br><br>        <span class="hljs-comment"># Expert tensor-model-pipeline group for MoE</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(model_comm_pgs, <span class="hljs-string">&#x27;tp_ep_pp&#x27;</span>), (<br>            <span class="hljs-string">&quot;tp_ep_pp process group is required but not provided in model_comm_pgs&quot;</span>,<br>            <span class="hljs-string">&quot;please explicitly set it to None if you don&#x27;t need it&quot;</span>,<br>        )<br>        expt_tp_pp_group = model_comm_pgs.tp_ep_pp<br><br>        <span class="hljs-comment"># Set up gloo groups - these might not be provided in process groups config</span><br>        <span class="hljs-comment"># so we need to create them or set to None</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> use_gloo_process_groups, (<br>            <span class="hljs-string">&quot;Gloo process groups are not supported when grad_comm_pgs and model_comm_pgs are &quot;</span><br>            <span class="hljs-string">&quot;provided. Please set use_gloo_process_groups to False.&quot;</span><br>        )<br>        intra_dp_cp_group_gloo = <span class="hljs-literal">None</span><br>        intra_expt_dp_group_gloo = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Grad and model comm process groups must be provided or both must be None&quot;</span>)<br><br>    model_parallel_rank = get_pg_rank(mp_group)<br><br>    <span class="hljs-keyword">if</span> get_pg_size(dp_cp_group) &gt; get_pg_size(intra_dp_cp_group):<br>        <span class="hljs-keyword">if</span> grad_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            inter_dist_opt_group = grad_comm_pgs.inter_dist_opt<br>        <span class="hljs-keyword">else</span>:<br>            inter_dist_opt_group = parallel_state.get_inter_distributed_optimizer_instance_group()<br>        distributed_optimizer_instance_id = get_pg_rank(inter_dist_opt_group)<br>    <span class="hljs-keyword">else</span>:<br>        distributed_optimizer_instance_id = <span class="hljs-number">0</span><br><br>    optimizers = []<br>    model_chunk_offset = <span class="hljs-number">0</span><br>    ddp_config = model_chunks[<span class="hljs-number">0</span>].ddp_config  <span class="hljs-comment"># Use the first model chunk&#x27;s DDP config</span><br>    <span class="hljs-keyword">if</span> ddp_config.use_megatron_fsdp:<br>        <span class="hljs-keyword">for</span> model_chunk, overlap_param_gather_with_optimizer_step <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<br>            all_dense_model_chunks, overlap_param_gather_with_optimizer_step_flags<br>        ):<br>            param_groups, buffers = _get_param_groups_and_buffers(<br>                model_chunk,<br>                model_chunk_offset=model_chunk_offset,<br>                config=config,<br>                no_weight_decay_cond=no_weight_decay_cond,<br>                scale_lr_cond=scale_lr_cond,<br>                lr_mult=lr_mult,<br>                filter_fn=<span class="hljs-keyword">lambda</span> g: <span class="hljs-literal">True</span>,<br>                buffer_name=<span class="hljs-string">&#x27;buffers&#x27;</span>,<br>                default_skip_embedding_weight_decay=default_skip_embedding_weight_decay,<br>            )<br><br>            optimizers.append(<br>                _get_megatron_optimizer_based_on_param_groups(<br>                    config,<br>                    model_chunks=model_chunk,<br>                    param_groups=param_groups,<br>                    per_model_buffers=buffers,<br>                    model_parallel_group=mp_group,<br>                    data_parallel_group=dp_cp_group,<br>                    data_parallel_group_gloo=intra_dp_cp_group_gloo,<br>                    data_parallel_group_idx=model_parallel_rank,<br>                    distributed_optimizer_instance_id=distributed_optimizer_instance_id,<br>                )<br>            )<br>            model_chunk_offset += <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(optimizers) == <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> optimizers[<span class="hljs-number">0</span>]<br><br>        <span class="hljs-keyword">return</span> ChainedOptimizer(optimizers)<br><br>    <span class="hljs-keyword">for</span> dense_model_chunks, overlap_param_gather_with_optimizer_step <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<br>        all_dense_model_chunks, overlap_param_gather_with_optimizer_step_flags<br>    ):<br>        param_groups, buffers = _get_param_groups_and_buffers(<br>            dense_model_chunks,<br>            model_chunk_offset=model_chunk_offset,<br>            config=config,<br>            no_weight_decay_cond=no_weight_decay_cond,<br>            scale_lr_cond=scale_lr_cond,<br>            lr_mult=lr_mult,<br>            filter_fn=<span class="hljs-keyword">lambda</span> g: <span class="hljs-keyword">not</span> g[<span class="hljs-string">&#x27;is_expert_parallel&#x27;</span>],<br>            buffer_name=<span class="hljs-string">&#x27;buffers&#x27;</span>,<br>            default_skip_embedding_weight_decay=default_skip_embedding_weight_decay,<br>        )<br>        <span class="hljs-keyword">for</span> model_chunk <span class="hljs-keyword">in</span> dense_model_chunks:<br>            model_chunk.overlap_param_gather_with_optimizer_step = (<br>                overlap_param_gather_with_optimizer_step<br>            )<br><br>        <span class="hljs-comment"># Pass Gloo process groups into optimizer only if needed.</span><br>        optimizers.append(<br>            _get_megatron_optimizer_based_on_param_groups(<br>                config,<br>                model_chunks=dense_model_chunks,<br>                param_groups=param_groups,<br>                per_model_buffers=buffers,<br>                model_parallel_group=mp_group,<br>                data_parallel_group=intra_dp_cp_group,<br>                data_parallel_group_gloo=intra_dp_cp_group_gloo,<br>                data_parallel_group_idx=model_parallel_rank,<br>                distributed_optimizer_instance_id=distributed_optimizer_instance_id,<br>            )<br>        )<br>        model_chunk_offset += <span class="hljs-number">1</span><br><br>    moe_param_groups, moe_buffers = _get_param_groups_and_buffers(<br>        model_chunks,<br>        model_chunk_offset=<span class="hljs-number">0</span>,<br>        config=config,<br>        no_weight_decay_cond=no_weight_decay_cond,<br>        scale_lr_cond=scale_lr_cond,<br>        lr_mult=lr_mult,<br>        filter_fn=<span class="hljs-keyword">lambda</span> g: g[<span class="hljs-string">&#x27;is_expert_parallel&#x27;</span>],<br>        buffer_name=<span class="hljs-string">&#x27;expert_parallel_buffers&#x27;</span>,<br>        default_skip_embedding_weight_decay=default_skip_embedding_weight_decay,<br>    )<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(moe_param_groups) &gt; <span class="hljs-number">0</span>:<br>        expt_model_parallel_rank = get_pg_rank(expt_tp_pp_group)<br>        <span class="hljs-comment"># Pass Gloo process groups into optimizer only if needed.</span><br>        <span class="hljs-keyword">if</span> use_gloo_process_groups:<br>            expt_data_parallel_group_gloo = intra_expt_dp_group_gloo<br>        <span class="hljs-keyword">else</span>:<br>            expt_data_parallel_group_gloo = <span class="hljs-literal">None</span><br>        optimizers.append(<br>            _get_megatron_optimizer_based_on_param_groups(<br>                config,<br>                model_chunks=model_chunks,<br>                param_groups=moe_param_groups,<br>                per_model_buffers=moe_buffers,<br>                model_parallel_group=expt_tp_pp_group,<br>                data_parallel_group=intra_expt_dp_group,<br>                data_parallel_group_gloo=expt_data_parallel_group_gloo,<br>                data_parallel_group_idx=expt_model_parallel_rank,<br>                distributed_optimizer_instance_id=distributed_optimizer_instance_id,<br>            )<br>        )<br><br>    <span class="hljs-keyword">return</span> ChainedOptimizer(optimizers)<br><br></code></pre></td></tr></table></figure><h3 id="get-optimizer-param-scheduler"><a href="#get-optimizer-param-scheduler" class="headerlink" title="get_optimizer_param_scheduler"></a>get_optimizer_param_scheduler</h3><p><code>get_optimizer_param_scheduler</code>负责给optimizer计算每一步使用什么学习率，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_optimizer_param_scheduler</span>(<span class="hljs-params">optimizer</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build the learning rate scheduler.&quot;&quot;&quot;</span><br>    args = get_args()<br><br>    <span class="hljs-comment"># Iteration-based training.</span><br>    <span class="hljs-keyword">if</span> args.train_iters:<br>        <span class="hljs-keyword">if</span> args.lr_decay_iters <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            args.lr_decay_iters = args.train_iters<br>        lr_decay_steps = args.lr_decay_iters * args.global_batch_size<br>        wd_incr_steps = args.train_iters * args.global_batch_size<br>        wsd_decay_steps = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> args.lr_wsd_decay_iters <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            wsd_decay_steps = args.lr_wsd_decay_iters * args.global_batch_size<br>        <span class="hljs-keyword">if</span> args.lr_warmup_fraction <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            lr_warmup_steps = args.lr_warmup_fraction * lr_decay_steps<br>        <span class="hljs-keyword">else</span>:<br>            lr_warmup_steps = args.lr_warmup_iters * args.global_batch_size<br>    <span class="hljs-comment"># Sample-based training.</span><br>    <span class="hljs-keyword">elif</span> args.train_samples:<br>        <span class="hljs-comment"># We need to set training iters for later use. Technically</span><br>        <span class="hljs-comment"># we need to adjust the training samples too (due to last</span><br>        <span class="hljs-comment"># batch being incomplete) but we leave it as is for now.</span><br>        update_train_iters(args)<br>        <span class="hljs-keyword">if</span> args.lr_decay_samples <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            args.lr_decay_samples = args.train_samples<br>        lr_decay_steps = args.lr_decay_samples<br>        wd_incr_steps = args.train_samples<br>        wsd_decay_steps = args.lr_wsd_decay_samples<br>        <span class="hljs-keyword">if</span> args.lr_warmup_fraction <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            lr_warmup_steps = args.lr_warmup_fraction * lr_decay_steps<br>        <span class="hljs-keyword">else</span>:<br>            lr_warmup_steps = args.lr_warmup_samples<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">&#x27;either train-iters or train-samples should be provided.&#x27;</span>)<br><br>    opt_param_scheduler = OptimizerParamScheduler(<br>        optimizer,<br>        init_lr=args.lr_warmup_init,<br>        max_lr=args.lr,<br>        min_lr=args.min_lr,<br>        lr_warmup_steps=lr_warmup_steps,<br>        lr_decay_steps=lr_decay_steps,<br>        lr_decay_style=args.lr_decay_style,<br>        start_wd=args.start_weight_decay,<br>        end_wd=args.end_weight_decay,<br>        wd_incr_steps=wd_incr_steps,<br>        wd_incr_style=args.weight_decay_incr_style,<br>        use_checkpoint_opt_param_scheduler=args.use_checkpoint_opt_param_scheduler,<br>        override_opt_param_scheduler=args.override_opt_param_scheduler,<br>        wsd_decay_steps=wsd_decay_steps,<br>        lr_wsd_decay_style=args.lr_wsd_decay_style,<br>    )<br><br>    <span class="hljs-keyword">return</span> opt_param_scheduler<br><br></code></pre></td></tr></table></figure><p>其有两类训练描述方式：</p><ul><li><p>Iteration-based training，即设置了类似<code>--train-iters 500000</code></p><ul><li><p>因为Megatron 的 <code>OptimizerParamScheduler</code> 内部是以已处理的 sample 数作为横轴，所以1 iteration &#x3D; global_batch_size 个 samples</p></li><li><p>然后其支持设置warm up步数、Warm Start Decay步数</p></li></ul></li><li><p>Sample-based training，即设置了类似<code>--train-samples 300B</code></p><ul><li><p>首先其通过<code>update_train_iters(args)</code>来反推<code>train_iters</code>，因为可能其他地方还需要使用iters</p></li><li><p>然后可以直接得到lr_decay_steps &#x3D; args.lr_decay_samples，wd_incr_steps &#x3D; args.train_samples</p></li></ul></li><li><p>最后借助这些参数构造<code>OptimizerParamScheduler</code></p></li></ul><h2 id="train"><a href="#train" class="headerlink" title="train"></a>train</h2><p>train的代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params"></span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    model,</span><br><span class="hljs-params">    optimizer,</span><br><span class="hljs-params">    opt_param_scheduler,</span><br><span class="hljs-params">    train_data_iterator,</span><br><span class="hljs-params">    valid_data_iterator,</span><br><span class="hljs-params">    process_non_loss_data_func,</span><br><span class="hljs-params">    config,</span><br><span class="hljs-params">    checkpointing_context,</span><br><span class="hljs-params">    non_loss_data_func,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Training function: run train_step desired number of times, run validation, checkpoint.&quot;&quot;&quot;</span><br>    args = get_args()<br>    timers = get_timers()<br>    energy_monitor = get_energy_monitor()<br>    one_logger = get_one_logger()<br><br>    <span class="hljs-keyword">if</span> args.run_workload_inspector_server:<br>        <span class="hljs-keyword">try</span>:<br>            <span class="hljs-keyword">from</span> workload_inspector.utils.webserver <span class="hljs-keyword">import</span> run_server<br>            <span class="hljs-keyword">import</span> threading<br><br>            threading.Thread(<br>                target=run_server, daemon=<span class="hljs-literal">True</span>, args=(torch.distributed.get_rank(),)<br>            ).start()<br>        <span class="hljs-keyword">except</span> ModuleNotFoundError:<br>            print_rank_0(<span class="hljs-string">&quot;workload inspector module not found.&quot;</span>)<br><br>    <span class="hljs-comment"># Write args to tensorboard</span><br>    write_args_to_tensorboard()<br><br>    <span class="hljs-comment"># Turn on training mode which enables dropout.</span><br>    <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model:<br>        model_module.train()<br><br>    <span class="hljs-comment"># Tracking loss.</span><br>    total_loss_dict = &#123;&#125;<br><br>    <span class="hljs-comment"># Iterations.</span><br>    iteration = args.iteration<br>    <span class="hljs-comment"># Make sure rerun_state_machine has the right iteration loaded from checkpoint.</span><br>    rerun_state_machine = get_rerun_state_machine()<br>    <span class="hljs-keyword">if</span> rerun_state_machine.current_iteration != iteration:<br>        print_rank_0(<span class="hljs-string">f&quot;Overwriting rerun_state_machine.current_iteration from &quot;</span><br>                     <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;rerun_state_machine.current_iteration&#125;</span> to <span class="hljs-subst">&#123;iteration&#125;</span>...&quot;</span>)<br>        rerun_state_machine.current_iteration = iteration<br><br>    <span class="hljs-comment"># Track E2E metrics at the start of training.</span><br>    one_logger_utils.on_train_start(<br>        iteration=iteration,<br>        consumed_train_samples=args.consumed_train_samples,<br>        train_samples=args.train_samples,<br>        seq_length=args.seq_length,<br>        train_iters=args.train_iters,<br>        save=args.save,<br>        async_save=args.async_save,<br>        log_throughput=args.log_throughput,<br>        num_floating_point_operations_so_far=args.num_floating_point_operations_so_far,<br>    )<br><br>    num_floating_point_operations_so_far = args.num_floating_point_operations_so_far<br><br>    <span class="hljs-comment"># Setup some training config params.</span><br>    config.grad_scale_func = optimizer.scale_loss<br>    config.timers = timers<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(model[<span class="hljs-number">0</span>], (megatron_FSDP, DDP)) <span class="hljs-keyword">and</span> args.overlap_grad_reduce:<br>        <span class="hljs-keyword">assert</span> config.no_sync_func <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, (<br>            <span class="hljs-string">&#x27;When overlap_grad_reduce is True, config.no_sync_func must be None; &#x27;</span><br>            <span class="hljs-string">&#x27;a custom no_sync_func is not supported when overlapping grad-reduce&#x27;</span><br>        )<br>        config.no_sync_func = [model_chunk.no_sync <span class="hljs-keyword">for</span> model_chunk <span class="hljs-keyword">in</span> model]<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(model) == <span class="hljs-number">1</span>:<br>            config.no_sync_func = config.no_sync_func[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">if</span> args.align_grad_reduce:<br>            config.grad_sync_func = [model_chunk.start_grad_sync <span class="hljs-keyword">for</span> model_chunk <span class="hljs-keyword">in</span> model]<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(model) == <span class="hljs-number">1</span>:<br>                config.grad_sync_func = config.grad_sync_func[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">if</span> args.overlap_param_gather <span class="hljs-keyword">and</span> args.align_param_gather:<br>        config.param_sync_func = [model_chunk.start_param_sync <span class="hljs-keyword">for</span> model_chunk <span class="hljs-keyword">in</span> model]<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(model) == <span class="hljs-number">1</span>:<br>            config.param_sync_func = config.param_sync_func[<span class="hljs-number">0</span>]<br>    config.finalize_model_grads_func = finalize_model_grads<br><br>    <span class="hljs-keyword">if</span> args.log_energy:<br>        energy_monitor.setup()<br>        energy_monitor.resume()<br><br>    timers(<span class="hljs-string">&#x27;interval-time&#x27;</span>, log_level=<span class="hljs-number">0</span>).start(barrier=<span class="hljs-literal">True</span>)<br>    print_datetime(<span class="hljs-string">&#x27;before the start of training step&#x27;</span>)<br>    report_memory_flag = <span class="hljs-literal">True</span><br>    pre_hook_enabled = <span class="hljs-literal">False</span><br>    should_exit = <span class="hljs-literal">False</span><br>    exit_code = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">if</span> args.manual_gc:<br>        <span class="hljs-comment"># Disable the default garbage collector and perform the collection manually.</span><br>        <span class="hljs-comment"># This is to align the timing of garbage collection across ranks.</span><br>        <span class="hljs-keyword">assert</span> (<br>            args.manual_gc_interval &gt;= <span class="hljs-number">0</span><br>        ), <span class="hljs-string">&#x27;Manual garbage collection interval should be larger than or equal to 0&#x27;</span><br>        gc.disable()<br>        gc.collect()<br><br>    <span class="hljs-comment"># Singleton initialization of straggler detector.</span><br>    <span class="hljs-keyword">if</span> args.log_straggler:<br>        <span class="hljs-keyword">global</span> stimer<br>        world = torch.distributed.get_world_size()<br>        rank = torch.distributed.get_rank()<br>        mmcnt = args.straggler_minmax_count<br>        stimer.configure(<br>            world,<br>            rank,<br>            mmcnt=mmcnt,<br>            enabled=<span class="hljs-keyword">not</span> args.disable_straggler_on_startup,<br>            port=args.straggler_ctrlr_port,<br>        )<br>    num_floating_point_operations_since_last_log_event = <span class="hljs-number">0.0</span><br><br>    num_microbatches = get_num_microbatches()<br>    eval_duration = <span class="hljs-number">0.0</span><br>    eval_iterations = <span class="hljs-number">0</span><br>    <span class="hljs-comment"># Wrap forward_backward_func for Full iteration CUDA graph</span><br>    forward_backward_func = get_forward_backward_func()<br>    <span class="hljs-keyword">if</span> args.enable_cuda_graph <span class="hljs-keyword">and</span> args.cuda_graph_scope==<span class="hljs-string">&quot;full_iteration&quot;</span>:<br>        forward_backward_func = FullCudaGraphWrapper(forward_backward_func, cuda_graph_warmup_steps=args.cuda_graph_warmup_steps)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_e2e_base_metrics</span>():<br>        <span class="hljs-string">&quot;&quot;&quot;Get base metrics values for one-logger to calculate E2E tracking metrics.&quot;&quot;&quot;</span><br>        num_floating_point_operations_since_current_train_start = (<br>            num_floating_point_operations_so_far - args.num_floating_point_operations_so_far<br>        )<br>        <span class="hljs-keyword">return</span> &#123;<br>            <span class="hljs-string">&#x27;iteration&#x27;</span>: iteration,<br>            <span class="hljs-string">&#x27;train_duration&#x27;</span>: timers(<span class="hljs-string">&#x27;interval-time&#x27;</span>).active_time(),<br>            <span class="hljs-string">&#x27;eval_duration&#x27;</span>: eval_duration,<br>            <span class="hljs-string">&#x27;eval_iterations&#x27;</span>: eval_iterations,<br>            <span class="hljs-string">&#x27;total_flops_since_current_train_start&#x27;</span>: num_floating_point_operations_since_current_train_start,<br>            <span class="hljs-string">&#x27;num_floating_point_operations_so_far&#x27;</span>: num_floating_point_operations_so_far,<br>            <span class="hljs-string">&#x27;consumed_train_samples&#x27;</span>: args.consumed_train_samples,<br>            <span class="hljs-string">&#x27;world_size&#x27;</span>: args.world_size,<br>            <span class="hljs-string">&#x27;seq_length&#x27;</span>: args.seq_length,<br>        &#125;<br><br>    <span class="hljs-comment"># Cache into one-logger for callback.</span><br>    <span class="hljs-keyword">if</span> one_logger:<br>        <span class="hljs-keyword">with</span> one_logger.get_context_manager():<br>            one_logger.store_set(<span class="hljs-string">&#x27;get_e2e_base_metrics&#x27;</span>, get_e2e_base_metrics)<br><br>    prof = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> (<br>        args.profile<br>        <span class="hljs-keyword">and</span> torch.distributed.get_rank() <span class="hljs-keyword">in</span> args.profile_ranks<br>        <span class="hljs-keyword">and</span> args.use_pytorch_profiler<br>    ):<br>        prof = torch.profiler.profile(<br>            schedule=torch.profiler.schedule(<br>                wait=<span class="hljs-built_in">max</span>(args.profile_step_start - <span class="hljs-number">1</span>, <span class="hljs-number">0</span>),<br>                warmup=<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> args.profile_step_start &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>,<br>                active=args.profile_step_end - args.profile_step_start,<br>                repeat=<span class="hljs-number">1</span>,<br>            ),<br>            on_trace_ready=torch.profiler.tensorboard_trace_handler(args.tensorboard_dir),<br>            record_shapes=<span class="hljs-literal">True</span>,<br>            with_stack=<span class="hljs-literal">True</span>,<br>        )<br>        prof.start()<br><br>    start_iteration = iteration<br>    <span class="hljs-comment"># Disable forward pre-hook to start training to ensure that errors in checkpoint loading</span><br>    <span class="hljs-comment"># or random initialization don&#x27;t propagate to all ranks in first all-gather (which is a</span><br>    <span class="hljs-comment"># no-op if things work correctly).</span><br>    <span class="hljs-keyword">if</span> should_disable_forward_pre_hook(args):<br>        disable_forward_pre_hook(model, param_sync=<span class="hljs-literal">False</span>)<br>        <span class="hljs-comment"># Also remove param_sync_func temporarily so that sync calls made in</span><br>        <span class="hljs-comment"># `forward_backward_func` are no-ops.</span><br>        param_sync_func = config.param_sync_func<br>        config.param_sync_func = <span class="hljs-literal">None</span><br>        pre_hook_enabled = <span class="hljs-literal">False</span><br>    <span class="hljs-comment"># Also, check weight hash across DP replicas to be very pedantic.</span><br>    <span class="hljs-keyword">if</span> args.check_weight_hash_across_dp_replicas_interval <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">assert</span> check_param_hashes_across_dp_replicas(<br>            model, cross_check=<span class="hljs-literal">True</span><br>        ), <span class="hljs-string">&quot;Parameter hashes not matching across DP replicas&quot;</span><br>        torch.distributed.barrier()<br>        print_rank_0(<span class="hljs-string">f&quot;&gt;&gt;&gt; Weight hashes match after <span class="hljs-subst">&#123;iteration&#125;</span> iterations...&quot;</span>)<br><br>    <span class="hljs-comment"># Capture CUDA Graphs.</span><br>    <span class="hljs-keyword">if</span> args.external_cuda_graph:<br>        cuda_graph_helper = TECudaGraphHelper(<br>            model=model,<br>            config=config,<br>            seq_length=args.seq_length,<br>            micro_batch_size=args.micro_batch_size,<br>            optimizers=[optimizer],<br>        )<br>        cuda_graph_helper.create_cudagraphs()<br><br>    <span class="hljs-comment"># Run training iterations till done.</span><br>    <span class="hljs-keyword">while</span> iteration &lt; args.train_iters:<br>        <span class="hljs-keyword">if</span> args.profile <span class="hljs-keyword">and</span> torch.distributed.get_rank() <span class="hljs-keyword">in</span> args.profile_ranks:<br>            <span class="hljs-keyword">if</span> args.use_pytorch_profiler:<br>                prof.step()<br>            <span class="hljs-keyword">elif</span> iteration == args.profile_step_start:<br>                torch.cuda.cudart().cudaProfilerStart()<br>                torch.autograd.profiler.emit_nvtx(record_shapes=<span class="hljs-literal">True</span>).__enter__()<br><br>        ft_integration.on_checkpointing_start()<br>        maybe_finalize_async_save(blocking=<span class="hljs-literal">False</span>)<br>        ft_integration.on_checkpointing_end(is_async_finalization=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-comment"># Update number of microbatches first without consistency check to decide if a</span><br>        <span class="hljs-comment"># checkpoint should be saved. If the number of microbatches is different</span><br>        <span class="hljs-comment"># from the previous iteration, save a checkpoint. Then run consistency check</span><br>        <span class="hljs-comment"># to make sure training configuration is still valid.</span><br>        update_num_microbatches(args.consumed_train_samples, consistency_check=<span class="hljs-literal">False</span>, verbose=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">if</span> get_num_microbatches() != num_microbatches <span class="hljs-keyword">and</span> iteration != <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">assert</span> get_num_microbatches() &gt; num_microbatches, (<br>                <span class="hljs-string">f&quot;Number of microbatches should be increasing due to batch size rampup; &quot;</span><br>                <span class="hljs-string">f&quot;instead going from <span class="hljs-subst">&#123;num_microbatches&#125;</span> to <span class="hljs-subst">&#123;get_num_microbatches()&#125;</span>&quot;</span><br>            )<br>            <span class="hljs-keyword">if</span> args.save <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                save_checkpoint_and_time(<br>                    iteration,<br>                    model,<br>                    optimizer,<br>                    opt_param_scheduler,<br>                    num_floating_point_operations_so_far,<br>                    checkpointing_context,<br>                    train_data_iterator=train_data_iterator,<br>                )<br>        num_microbatches = get_num_microbatches()<br>        update_num_microbatches(args.consumed_train_samples, consistency_check=<span class="hljs-literal">True</span>, verbose=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-comment"># Completely skip iteration if needed.</span><br>        <span class="hljs-keyword">if</span> iteration <span class="hljs-keyword">in</span> args.iterations_to_skip:<br>            <span class="hljs-comment"># Dummy train_step to fast forward train_data_iterator.</span><br>            dummy_train_step(train_data_iterator)<br>            iteration += <span class="hljs-number">1</span><br>            batch_size = (<br>                mpu.get_data_parallel_world_size() * args.micro_batch_size * get_num_microbatches()<br>            )<br>            args.consumed_train_samples += batch_size<br>            args.skipped_train_samples += batch_size<br>            <span class="hljs-keyword">continue</span><br><br>        <span class="hljs-comment"># Run training step.</span><br>        args.curr_iteration = iteration<br>        ft_integration.on_training_step_start()<br>        (<br>            loss_dict,<br>            skipped_iter,<br>            should_checkpoint,<br>            should_exit,<br>            exit_code,<br>            grad_norm,<br>            num_zeros_in_grad,<br>        ) = train_step(<br>            forward_step_func, train_data_iterator, model, optimizer, opt_param_scheduler, config, forward_backward_func<br>        )<br>        ft_integration.on_training_step_end()<br>        <span class="hljs-keyword">if</span> should_checkpoint:<br>            save_checkpoint_and_time(<br>                iteration,<br>                model,<br>                optimizer,<br>                opt_param_scheduler,<br>                num_floating_point_operations_so_far,<br>                checkpointing_context,<br>                train_data_iterator=train_data_iterator,<br>            )<br>        <span class="hljs-keyword">if</span> should_exit:<br>            <span class="hljs-keyword">break</span><br><br>        <span class="hljs-comment"># Enable forward pre-hooks after first set of forward and backward passes.</span><br>        <span class="hljs-comment"># When running in fp16, skip all NaN iterations until steady-state loss scaling value</span><br>        <span class="hljs-comment"># is reached.</span><br>        <span class="hljs-keyword">if</span> iteration == start_iteration:<br>            <span class="hljs-keyword">if</span> skipped_iter:<br>                <span class="hljs-comment"># Only enable forward pre-hook after a training step has successfully run. Relevant</span><br>                <span class="hljs-comment"># for fp16 codepath where first XX iterations are skipped until steady-state loss</span><br>                <span class="hljs-comment"># scale value is reached.</span><br>                start_iteration = iteration + <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># Enable forward pre-hook after training step has successfully run. All subsequent</span><br>                <span class="hljs-comment"># forward passes will use the forward pre-hook / `param_sync_func` in</span><br>                <span class="hljs-comment"># `forward_backward_func`.</span><br>                <span class="hljs-keyword">if</span> should_disable_forward_pre_hook(args):<br>                    enable_forward_pre_hook(model)<br>                    config.param_sync_func = param_sync_func<br>                    pre_hook_enabled = <span class="hljs-literal">True</span><br>                    <span class="hljs-comment"># Set the manual hooks when CUDA Graphs are used.</span><br>                    <span class="hljs-keyword">if</span> args.external_cuda_graph:<br>                        cuda_graph_helper.cuda_graph_set_manual_hooks()<br><br>        iteration += <span class="hljs-number">1</span><br>        batch_size = (<br>            mpu.get_data_parallel_world_size() * args.micro_batch_size * get_num_microbatches()<br>        )<br>        args.consumed_train_samples += batch_size<br>        num_skipped_samples_in_batch = (<br>            get_current_global_batch_size() - get_current_running_global_batch_size()<br>        )<br>        <span class="hljs-keyword">if</span> args.decrease_batch_size_if_needed:<br>            <span class="hljs-keyword">assert</span> num_skipped_samples_in_batch &gt;= <span class="hljs-number">0</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">assert</span> num_skipped_samples_in_batch == <span class="hljs-number">0</span><br>        args.skipped_train_samples += num_skipped_samples_in_batch<br>        num_floating_point_operations_in_batch = num_floating_point_operations(args, batch_size)<br>        num_floating_point_operations_so_far += num_floating_point_operations_in_batch<br>        num_floating_point_operations_since_last_log_event += num_floating_point_operations_in_batch<br><br>        <span class="hljs-comment"># Logging.</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> optimizer.is_stub_optimizer:<br>            loss_scale = optimizer.get_loss_scale().item()<br>        <span class="hljs-keyword">else</span>:<br>            loss_scale = <span class="hljs-number">1.0</span><br>        params_norm = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> args.log_params_norm:<br>            params_norm = calc_params_l2_norm(model)<br>        learning_rate = <span class="hljs-literal">None</span><br>        decoupled_learning_rate = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">for</span> param_group <span class="hljs-keyword">in</span> optimizer.param_groups:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(param_group[<span class="hljs-string">&#x27;params&#x27;</span>]) == <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">if</span> param_group[<span class="hljs-string">&#x27;is_decoupled_lr&#x27;</span>]:<br>                decoupled_learning_rate = param_group[<span class="hljs-string">&#x27;lr&#x27;</span>]<br>            <span class="hljs-keyword">else</span>:<br>                learning_rate = param_group[<span class="hljs-string">&#x27;lr&#x27;</span>]<br>        report_memory_flag = training_log(<br>            loss_dict,<br>            total_loss_dict,<br>            learning_rate,<br>            decoupled_learning_rate,<br>            iteration,<br>            loss_scale,<br>            report_memory_flag,<br>            skipped_iter,<br>            grad_norm,<br>            params_norm,<br>            num_zeros_in_grad,<br>        )<br><br>        <span class="hljs-comment"># Evaluation.</span><br>        <span class="hljs-keyword">if</span> args.eval_interval <span class="hljs-keyword">and</span> iteration % args.eval_interval == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> args.do_valid:<br>            <span class="hljs-keyword">if</span> args.log_energy:<br>                energy_monitor.pause()<br>            timers(<span class="hljs-string">&#x27;interval-time&#x27;</span>).stop()<br>            <span class="hljs-keyword">if</span> should_disable_forward_pre_hook(args):<br>                disable_forward_pre_hook(model)<br>                pre_hook_enabled = <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">if</span> args.manual_gc <span class="hljs-keyword">and</span> args.manual_gc_eval:<br>                <span class="hljs-comment"># Collect all objects.</span><br>                gc.collect()<br>            prefix = <span class="hljs-string">f&#x27;iteration <span class="hljs-subst">&#123;iteration&#125;</span>&#x27;</span><br>            timers(<span class="hljs-string">&#x27;eval-time&#x27;</span>, log_level=<span class="hljs-number">0</span>).start(barrier=<span class="hljs-literal">True</span>)<br>            evaluate_and_print_results(<br>                prefix,<br>                forward_step_func,<br>                valid_data_iterator,<br>                model,<br>                iteration,<br>                process_non_loss_data_func,<br>                config,<br>                verbose=<span class="hljs-literal">False</span>,<br>                write_to_tensorboard=<span class="hljs-literal">True</span>,<br>                non_loss_data_func=non_loss_data_func,<br>            )<br>            eval_duration += timers(<span class="hljs-string">&#x27;eval-time&#x27;</span>).elapsed()<br>            eval_iterations += <span class="hljs-built_in">sum</span>(args.eval_iters) <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(args.eval_iters, <span class="hljs-built_in">list</span>) <span class="hljs-keyword">else</span> args.eval_iters<br>            timers(<span class="hljs-string">&#x27;eval-time&#x27;</span>).stop()<br>            one_logger_utils.track_e2e_metrics()<br><br>            <span class="hljs-keyword">if</span> args.manual_gc <span class="hljs-keyword">and</span> args.manual_gc_eval:<br>                <span class="hljs-comment"># Collect only the objects created and used in evaluation.</span><br>                gc.collect(generation=<span class="hljs-number">0</span>)<br>            <span class="hljs-keyword">if</span> should_disable_forward_pre_hook(args):<br>                enable_forward_pre_hook(model)<br>                pre_hook_enabled = <span class="hljs-literal">True</span><br>            timers(<span class="hljs-string">&#x27;interval-time&#x27;</span>, log_level=<span class="hljs-number">0</span>).start(barrier=<span class="hljs-literal">True</span>)<br>            <span class="hljs-keyword">if</span> args.log_energy:<br>                energy_monitor.resume()<br><br>        <span class="hljs-comment"># Miscellaneous post-training-step functions (e.g., FT heartbeats, GC).</span><br>        <span class="hljs-comment"># Some of these only happen at specific iterations.</span><br>        post_training_step_callbacks(<br>            model,<br>            optimizer,<br>            opt_param_scheduler,<br>            iteration,<br>            prof,<br>            num_floating_point_operations_since_last_log_event,<br>        )<br><br>        <span class="hljs-comment"># Checkpoint and decide whether to exit.</span><br>        should_exit = checkpoint_and_decide_exit(<br>            model,<br>            optimizer,<br>            opt_param_scheduler,<br>            iteration,<br>            num_floating_point_operations_so_far,<br>            checkpointing_context,<br>            train_data_iterator,<br>        )<br>        <span class="hljs-keyword">if</span> should_exit:<br>            <span class="hljs-keyword">break</span><br><br>    one_logger_utils.track_e2e_metrics()<br><br>    <span class="hljs-comment"># Flush TensorBoard, WandB writers and one-logger.</span><br>    writer = get_tensorboard_writer()<br>    <span class="hljs-keyword">if</span> writer:<br>        writer.flush()<br><br>    <span class="hljs-comment"># Close out pre-hooks if using distributed optimizer and overlapped param gather.</span><br>    <span class="hljs-keyword">if</span> pre_hook_enabled:<br>        disable_forward_pre_hook(model)<br><br>    ft_integration.on_checkpointing_start()<br>    <span class="hljs-comment"># This will finalize all unfinalized async request and terminate</span><br>    <span class="hljs-comment"># a persistent async worker if persistent ckpt worker is enabled</span><br>    maybe_finalize_async_save(blocking=<span class="hljs-literal">True</span>, terminate=<span class="hljs-literal">True</span>)<br>    ft_integration.on_checkpointing_end(is_async_finalization=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">if</span> args.enable_ft_package <span class="hljs-keyword">and</span> ft_integration.get_rank_monitor_client() <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        ft_integration.get_rank_monitor_client().shutdown_workload_monitoring()<br><br>    <span class="hljs-keyword">if</span> args.log_energy:<br>        energy_monitor.lap()<br>        total_energy = energy_monitor.get_total()<br>        print_rank_0(<span class="hljs-string">f&quot;Total training energy (GPU): <span class="hljs-subst">&#123;total_energy / <span class="hljs-number">1e6</span>&#125;</span> MJ&quot;</span>)<br>        energy_monitor.shutdown()<br><br>    <span class="hljs-comment"># If any exit conditions (signal handler, duration, iterations) have been reached, exit.</span><br>    <span class="hljs-keyword">if</span> should_exit:<br>        wandb_writer = get_wandb_writer()<br>        <span class="hljs-keyword">if</span> wandb_writer:<br>            wandb_writer.finish()<br>        ft_integration.shutdown()<br>        one_logger_utils.finish()<br>        sys.exit(exit_code)<br><br>    <span class="hljs-keyword">return</span> iteration, num_floating_point_operations_so_far<br><br></code></pre></td></tr></table></figure><p>其主要流程为：</p><ol><li><p>获取全局配置</p></li><li><p>切换到 train() 模式</p></li><li><p>将迭代次数与rerun_state_machine对齐</p></li><li><p>给 <code>config</code> 注入“分布式重叠通信&#x2F;同步”钩子，<code>config</code> 会被 <code>forward_backward_func</code> &#x2F; <code>train_step</code> 使用，用于控制 no_sync（梯度累积）、梯度同步重叠、参数 all-gather 重叠 等。</p></li><li><p>还有一些可选的配置，如控制gc、开启profiler等</p></li><li><p>进入主循环：<code>while iteration &lt; args.train_iters</code>：</p><ol><li><p>更新<code>microbatch</code>，因为存在动态batch size的场景</p></li><li><p>使用<code>train_step</code>进行一轮训练</p></li><li><p>按需保存checkpoint</p></li><li><p>第一轮成功后再启用 forward pre-hook</p></li><li><p>更新 iteration&#x2F;样本数&#x2F;FLOPs</p></li><li><p>按需进行评估</p></li></ol></li><li><p>训练收尾，flush writer（TensorBoard&#x2F;WandB&#x2F;one-logger），若 pre-hook 还开着，关闭它（避免退出时还有挂钩或后台同步），<strong>最终 finalize 异步检查点等</strong></p></li></ol><h3 id="train-step"><a href="#train-step" class="headerlink" title="train_step"></a><code>train_step</code></h3><h4 id="参数forward-backward-func"><a href="#参数forward-backward-func" class="headerlink" title="参数forward_backward_func"></a>参数<code>forward_backward_func</code></h4><p><code>train_step</code>的关键参数还包含了<code>forward_backward_func</code>，其通过如下获得：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Wrap forward_backward_func for Full iteration CUDA graph</span><br>forward_backward_func = get_forward_backward_func()<br><span class="hljs-keyword">if</span> args.enable_cuda_graph and args.cuda_graph_scope==<span class="hljs-string">&quot;full_iteration&quot;</span>:<br>    forward_backward_func = FullCudaGraphWrapper(forward_backward_func, cuda_graph_warmup_steps=args.cuda_graph_warmup_steps)<br></code></pre></td></tr></table></figure><p><code>get_forward_backward_func</code>函数如下：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><code class="hljs markdown">def get<span class="hljs-emphasis">_forward_</span>backward<span class="hljs-emphasis">_func():</span><br><span class="hljs-emphasis">    &quot;&quot;&quot;Retrieves the appropriate forward_</span>backward function given the<br><span class="hljs-code">    configuration of parallel_state.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    Returns a function that will perform all of the forward and</span><br><span class="hljs-code">    backward passes of the model given the pipeline model parallel</span><br><span class="hljs-code">    world size and virtual pipeline model parallel world size in the</span><br><span class="hljs-code">    global parallel_state.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    Note that if using sequence parallelism, the sequence length component of</span><br><span class="hljs-code">    the tensor shape is updated to original_sequence_length /</span><br><span class="hljs-code">    tensor_model_parallel_world_size.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    The function returned takes the following arguments:</span><br><span class="hljs-code"></span><br><span class="hljs-code">    forward_step_func (required): A function that takes a data</span><br><span class="hljs-code">        iterator and a model as its arguments and return the model&#x27;s</span><br><span class="hljs-code">        forward output and the loss function. The loss function should</span><br><span class="hljs-code">        take one torch.Tensor and return a torch.Tensor of loss and a</span><br><span class="hljs-code">        dictionary of string -&gt; torch.Tensor.</span><br><span class="hljs-code"></span><br><span class="hljs-code">        A third argument, checkpoint_activations_microbatch, indicates</span><br><span class="hljs-code">        that the activations for this microbatch should be</span><br><span class="hljs-code">        checkpointed. A None value for this argument indicates that</span><br><span class="hljs-code">        the default from the configuration should be used. This is</span><br><span class="hljs-code">        used when the</span><br><span class="hljs-code">        num_microbatches_with_partial_activation_checkpoints is used.</span><br><span class="hljs-code"></span><br><span class="hljs-code">        For example:</span><br><span class="hljs-code"></span><br><span class="hljs-code">        def loss_func(loss_mask, output_tensor):</span><br><span class="hljs-code">            losses = output_tensor.float()</span><br><span class="hljs-code">            loss_mask = loss_mask.view(-1).float()</span><br><span class="hljs-code">            loss = torch.sum(losses.view(-1) * loss_mask) / loss_mask.sum()</span><br><span class="hljs-code"></span><br><span class="hljs-code">            # Reduce loss for logging.</span><br><span class="hljs-code">            averaged_loss = average_losses_across_data_parallel_group([loss])</span><br><span class="hljs-code"></span><br><span class="hljs-code">            return loss, &#123;&#x27;lm loss&#x27;: averaged_loss[0]&#125;</span><br><span class="hljs-code"></span><br><span class="hljs-code">        def forward_step(data_iterator, model):</span><br><span class="hljs-code">            data, loss_mask = next(data_iterator)</span><br><span class="hljs-code">            output = model(data)</span><br><span class="hljs-code">            return output, partial(loss_func, loss_mask)</span><br><span class="hljs-code"></span><br><span class="hljs-code">        forward_backward_func(forward_step_func=forward_step, ...)</span><br><span class="hljs-code"></span><br><span class="hljs-code">    data_iterator (required): an iterator over the data, will be</span><br><span class="hljs-code">        passed as is to forward_step_func. Expected to be a list of</span><br><span class="hljs-code">        iterators in the case of interleaved pipeline parallelism.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    model (required): the actual model. Expected to be a list of modules in the case of interleaved</span><br><span class="hljs-code">        pipeline parallelism. Must be a (potentially wrapped) megatron.core.models.MegatronModule.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    num_microbatches (int, required):</span><br><span class="hljs-code">        The number of microbatches to go through</span><br><span class="hljs-code"></span><br><span class="hljs-code">    seq_length (int, required): Sequence length of the current global batch. If this is a dual-stack</span><br><span class="hljs-code">        transformer, this is the encoder&#x27;s sequence length. This is ignored if variable_seq_lengths</span><br><span class="hljs-code">        in the config is True. Otherwise, each microbatch in the current global batch size must use</span><br><span class="hljs-code">        this sequence length.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    micro_batch_size (int, required): The number of sequences in a microbatch.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    decoder_seq_length (int, optional): The sequence length for the decoder in a dual-stack</span><br><span class="hljs-code">        transformer. This is ignored for a single-stack transformer.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    forward_only (optional, default = False): Perform only the forward step</span><br><span class="hljs-code"></span><br><span class="hljs-code">    collect_non_loss_data (optional, bool, default=False): TODO</span><br><span class="hljs-code"></span><br><span class="hljs-code">    first_val_step (bool, optional): Is the first step of the validation phase. Used by</span><br><span class="hljs-code">        Transformer Engine modules to only update their fp8 weights only on the first validation</span><br><span class="hljs-code">        step.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    adjust_tensor_shapes_fn (Callable, optional): A function that adjusts the receive and send</span><br><span class="hljs-code">        tensor shapes. Only applicable in forward_backward_pipelining_without_interleaving for now.</span><br><span class="hljs-code">        Takes in a list of receive shapes and a list of send shapes and returns the adjusted</span><br><span class="hljs-code">        respective list of shapes. Thus it is not used in the other forward-backward functions</span><br><span class="hljs-code">        which have different shape handling.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    &quot;&quot;&quot;</span><br><span class="hljs-code">    pipeline_model_parallel_size = parallel_state.get_pipeline_model_parallel_world_size()</span><br><span class="hljs-code">    if pipeline_model_parallel_size &gt; 1:</span><br><span class="hljs-code">        if parallel_state.get_virtual_pipeline_model_parallel_world_size() is not None:</span><br><span class="hljs-code">            forward_backward_func = forward_backward_pipelining_with_interleaving</span><br><span class="hljs-code">        else:</span><br><span class="hljs-code">            forward_backward_func = forward_backward_pipelining_without_interleaving</span><br><span class="hljs-code">    else:</span><br><span class="hljs-code">        forward_backward_func = forward_backward_no_pipelining</span><br><span class="hljs-code">    return forward_backward_func</span><br></code></pre></td></tr></table></figure><p>其依据pipeline划分为了多个类别</p><ul><li><p><code>forward_backward_pipelining_with_interleaving</code>：开了pp和vp</p></li><li><p><code>forward_backward_pipelining_without_interleaving</code>：开了pp但是没有vp</p></li><li><p><code>forward_backward_no_pipelining</code>：没有开pp</p></li></ul><p>先只看最简单的<code>forward_backward_no_pipelining</code>，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_backward_no_pipelining</span>(<span class="hljs-params"></span><br><span class="hljs-params">    *,</span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    data_iterator: <span class="hljs-type">Union</span>[Iterator, <span class="hljs-type">List</span>[Iterator]],</span><br><span class="hljs-params">    model: <span class="hljs-type">Union</span>[torch.nn.Module, <span class="hljs-type">List</span>[torch.nn.Module]],</span><br><span class="hljs-params">    num_microbatches: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    seq_length: <span class="hljs-built_in">int</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    micro_batch_size: <span class="hljs-built_in">int</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    decoder_seq_length: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    forward_only: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    collect_non_loss_data: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    first_val_step: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    adjust_tensor_shapes_fn: <span class="hljs-type">Optional</span>[<span class="hljs-type">Callable</span>] = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    grad_finalize_pgs: <span class="hljs-type">Optional</span>[GradFinalizeProcessGroups] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Run forward and backward passes with no pipeline parallelism&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">if</span> grad_finalize_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        tp_group = parallel_state.get_tensor_model_parallel_group()<br>        cp_group = parallel_state.get_context_parallel_group()<br>        embd_group = parallel_state.get_embedding_group(check_initialized=<span class="hljs-literal">False</span>)<br>        pp_group = parallel_state.get_pipeline_model_parallel_group()<br>        pos_emb_group = parallel_state.get_position_embedding_group(check_initialized=<span class="hljs-literal">False</span>)<br>        grad_finalize_pgs = GradFinalizeProcessGroups()<br>        grad_finalize_pgs.tp = tp_group<br>        grad_finalize_pgs.cp = cp_group<br>        grad_finalize_pgs.embd = embd_group<br>        grad_finalize_pgs.pos_embd = pos_emb_group<br>        grad_finalize_pgs.pp = pp_group<br>        grad_finalize_pgs.dp_cp = parallel_state.get_data_parallel_group(<br>            with_context_parallel=<span class="hljs-literal">True</span>, partial_data_parallel=<span class="hljs-literal">False</span><br>        )<br><br>    <span class="hljs-keyword">elif</span> grad_finalize_pgs <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;tp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;cp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;embd&#x27;</span>), (<br>            <span class="hljs-string">&quot;grad_finalize_pgs must have a embd. In previous version, it is used default &quot;</span><br>            <span class="hljs-string">&quot;`parallel_state.default_embedding_ranks` to create the process group. If you are &quot;</span><br>            <span class="hljs-string">&quot;using the default process group, please use `parallel_state.get_embedding_group()` &quot;</span><br>            <span class="hljs-string">&quot;to get the process group. If you don&#x27;t need explicitly set it to None.&quot;</span><br>        )<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;pos_embd&#x27;</span>), (<br>            <span class="hljs-string">&quot;grad_finalize_pgs must have a pos_embd. In previous version, it is used default &quot;</span><br>            <span class="hljs-string">&quot;`parallel_state.default_position_embedding_ranks` to create the process group. &quot;</span><br>            <span class="hljs-string">&quot;If you are using the default process group, &quot;</span><br>            <span class="hljs-string">&quot;please use `parallel_state.get_position_embedding_group()` &quot;</span><br>            <span class="hljs-string">&quot;to get the process group. If you don&#x27;t need explicitly set it to None.&quot;</span><br>        )<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;pp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;dp_cp&#x27;</span>)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(model, <span class="hljs-built_in">list</span>):<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(model) == <span class="hljs-number">1</span>, <span class="hljs-string">&quot;non-pipeline-parallel schedule does not support model chunking&quot;</span><br>        model = model[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(data_iterator, <span class="hljs-built_in">list</span>):<br>        <span class="hljs-keyword">assert</span> (<br>            <span class="hljs-built_in">len</span>(data_iterator) == <span class="hljs-number">1</span><br>        ), <span class="hljs-string">&quot;non-pipeline-parallel schedule does not support model chunking&quot;</span><br>        data_iterator = data_iterator[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">assert</span> (<br>        adjust_tensor_shapes_fn <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;adjust_tensor_shapes_fn is not supported for non-pipeline-parallel schedule&quot;</span><br><br>    config = get_model_config(model)<br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;forward-backward&#x27;</span>, log_level=<span class="hljs-number">1</span>).start(barrier=config.barrier_with_L1_time)<br><br>    no_sync_func = config.no_sync_func<br>    <span class="hljs-keyword">if</span> no_sync_func <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        no_sync_func = contextlib.nullcontext<br><br>    model_type = get_model_type(model)<br><br>    forward_data_store = []<br>    input_tensor, output_tensor_grad = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    total_num_tokens = torch.zeros([], dtype=torch.<span class="hljs-built_in">int</span>, device=<span class="hljs-string">&quot;cuda&quot;</span>)<br><br>    <span class="hljs-keyword">if</span> config.overlap_moe_expert_parallel_comm <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> forward_only:<br>        forward_data_store, total_num_tokens = combined_1f1b_schedule_for_no_pipelining(<br>            forward_step_func,<br>            data_iterator,<br>            model,<br>            num_microbatches,<br>            input_tensor,<br>            output_tensor_grad,<br>            forward_data_store,<br>            config,<br>            collect_non_loss_data,<br>            first_val_step,<br>            forward_only,<br>            no_sync_func,<br>            total_num_tokens,<br>            partial(check_first_val_step, first_val_step, forward_only),<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">with</span> no_sync_func():<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_microbatches - <span class="hljs-number">1</span>):<br>                output_tensor, num_tokens = forward_step(<br>                    forward_step_func,<br>                    data_iterator,<br>                    model,<br>                    num_microbatches,<br>                    input_tensor,<br>                    forward_data_store,<br>                    config,<br>                    grad_finalize_pgs.cp.size(),<br>                    collect_non_loss_data,<br>                    is_first_microbatch=check_first_val_step(first_val_step, forward_only, i == <span class="hljs-number">0</span>),<br>                    current_microbatch=i,<br>                )<br>                total_num_tokens += num_tokens<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> forward_only:<br>                    backward_step(<br>                        input_tensor, output_tensor, output_tensor_grad, model_type, config<br>                    )<br>        <span class="hljs-comment"># Run computation for last microbatch out of context handler (want to</span><br>        <span class="hljs-comment"># synchronize gradients).</span><br>        output_tensor, num_tokens = forward_step(<br>            forward_step_func,<br>            data_iterator,<br>            model,<br>            num_microbatches,<br>            input_tensor,<br>            forward_data_store,<br>            config,<br>            grad_finalize_pgs.cp.size(),<br>            collect_non_loss_data,<br>            is_first_microbatch=check_first_val_step(<br>                first_val_step, forward_only, num_microbatches == <span class="hljs-number">1</span><br>            ),<br>            current_microbatch=num_microbatches - <span class="hljs-number">1</span>,<br>        )<br><br>        total_num_tokens += num_tokens<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> forward_only:<br>            backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)<br><br>    <span class="hljs-keyword">if</span> config.finalize_model_grads_func <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> forward_only:<br>        <span class="hljs-comment"># Finalize model grads (perform full grad all-reduce / reduce-scatter for</span><br>        <span class="hljs-comment"># data parallelism and layernorm all-reduce for sequence parallelism).</span><br>        config.finalize_model_grads_func(<br>            [model],<br>            total_num_tokens <span class="hljs-keyword">if</span> config.calculate_per_token_loss <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>            grad_finalize_pgs=grad_finalize_pgs,<br>        )<br><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;forward-backward&#x27;</span>).stop()<br><br>    <span class="hljs-keyword">if</span> (<br>        <span class="hljs-built_in">hasattr</span>(config, <span class="hljs-string">&#x27;enable_cuda_graph&#x27;</span>)<br>        <span class="hljs-keyword">and</span> config.enable_cuda_graph<br>        <span class="hljs-keyword">and</span> config.cuda_graph_scope != <span class="hljs-string">&quot;full_iteration&quot;</span><br>    ):<br>        create_cudagraphs()<br><br>    <span class="hljs-keyword">return</span> forward_data_store<br><br></code></pre></td></tr></table></figure><p>其流程如下：</p><ol><li><p>如果没有设置<code>grad_finalize_pgs</code>，就构造默认值，从而在做梯度规约&#x2F;归并时，需要明确“在哪些通信域里做什么规约”，例如DP&#x2F;DP×CP：参数梯度的 all-reduce 或 reduce-scatter</p></li><li><p>做一些参数检查</p></li><li><p>然后分为MoE overlap 与普通路径：</p><ol><li><p>MoE会调用<code>combined_1f1b_schedule_for_no_pipelining</code></p></li><li><p>普通路径就是执行<code>num_microbatches</code>次<code>forward_step</code>，如果不是<code>forward_only</code>就执行<code>backward_step</code>，注意后一个 micro-batch 在 no_sync 外，从而让“梯度同步&#x2F;规约”在这一轮的尾部发生（典型的梯度累积实现方式）</p></li></ol></li><li><p>最后执行<code>finalize_model_grads_func</code>，这一步通常包含：</p><ul><li><p>DP 的全梯度 all-reduce &#x2F; reduce-scatter（取决于 optimizer&#x2F;ZeRO 类策略）</p></li><li><p>sequence parallel 的 LayerNorm 等规约</p></li><li><p>embedding&#x2F;pos embedding 的特定规约</p></li></ul></li></ol><p>其中一个micro batch的<code>forward_step</code>代码如下所示，注意到这里就会调用到用户传入的<code>forward_step_func</code>函数了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_step</span>(<span class="hljs-params"></span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    data_iterator,</span><br><span class="hljs-params">    model,</span><br><span class="hljs-params">    num_microbatches,</span><br><span class="hljs-params">    input_tensor,</span><br><span class="hljs-params">    forward_data_store,</span><br><span class="hljs-params">    config,</span><br><span class="hljs-params">    cp_group_size,</span><br><span class="hljs-params">    collect_non_loss_data=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    checkpoint_activations_microbatch=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    is_first_microbatch=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    current_microbatch=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    vp_stage=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    is_last_stage=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Forward step for passed-in model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    If it is the first stage, the input tensor is obtained from the data_iterator.</span><br><span class="hljs-string">    Otherwise, the passed-in input_tensor is used.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        forward_step_func (callable):</span><br><span class="hljs-string">            The forward step function for the model that takes the</span><br><span class="hljs-string">            data iterator as the first argument, and model as the second.</span><br><span class="hljs-string">            This user&#x27;s forward step is expected to output a tuple of two elements:</span><br><span class="hljs-string"></span><br><span class="hljs-string">                1. The output object from the forward step. This output object needs to be a</span><br><span class="hljs-string">                    tensor or some kind of collection of tensors. The only hard requirement</span><br><span class="hljs-string">                    for this object is that it needs to be acceptible as input into the second</span><br><span class="hljs-string">                    function.</span><br><span class="hljs-string">                2. A function to reduce (optionally) the output from the forward step. This</span><br><span class="hljs-string">                    could be a reduction over the loss from the model, it could be a function that</span><br><span class="hljs-string">                    grabs the output from the model and reformats, it could be a function that just</span><br><span class="hljs-string">                    passes through the model output. This function must have one of the following</span><br><span class="hljs-string">                    patterns, and depending on the pattern different things happen internally:</span><br><span class="hljs-string"></span><br><span class="hljs-string">                        a. A tuple of reduced loss and some other data. Note that in this case</span><br><span class="hljs-string">                            the first argument is divided by the number of global microbatches,</span><br><span class="hljs-string">                            assuming it is a loss, so that the loss is stable as a function of</span><br><span class="hljs-string">                            the number of devices the step is split across.</span><br><span class="hljs-string">                        b. A triple of reduced loss, number of tokens, and some other data. This</span><br><span class="hljs-string">                            is similar to case (a), but the loss is further averaged across the</span><br><span class="hljs-string">                            number of tokens in the batch. If the user is not already averaging</span><br><span class="hljs-string">                            across the number of tokens, this pattern is useful to use.</span><br><span class="hljs-string">                        c. Any arbitrary data the user wants (eg a dictionary of tensors, a list</span><br><span class="hljs-string">                            of tensors, etc in the case of inference). To trigger case 3 you need</span><br><span class="hljs-string">                            to specify `collect_non_loss_data=True` and you may also want to</span><br><span class="hljs-string">                            specify `forward_only=True` in the call to the parent forward_backward</span><br><span class="hljs-string">                            function.</span><br><span class="hljs-string">        data_iterator (iterator):</span><br><span class="hljs-string">            The data iterator.</span><br><span class="hljs-string">        model (nn.Module):</span><br><span class="hljs-string">            The model to perform the forward step on.</span><br><span class="hljs-string">        num_microbatches (int):</span><br><span class="hljs-string">            The number of microbatches.</span><br><span class="hljs-string">        input_tensor (Tensor or list[Tensor]):</span><br><span class="hljs-string">            The input tensor(s) for the forward step.</span><br><span class="hljs-string">        forward_data_store (list):</span><br><span class="hljs-string">            The list to store the forward data. If you go down path 2.a or</span><br><span class="hljs-string">            2.b for the return of your forward reduction function then this will store only the</span><br><span class="hljs-string">            final dimension of the output, for example the metadata output by the loss function.</span><br><span class="hljs-string">            If you go down the path of 2.c then this will store the entire output of the forward</span><br><span class="hljs-string">            reduction function applied to the model output.</span><br><span class="hljs-string">        config (object):</span><br><span class="hljs-string">            The configuration object.</span><br><span class="hljs-string">        collect_non_loss_data (bool, optional):</span><br><span class="hljs-string">            Whether to collect non-loss data. Defaults to False.</span><br><span class="hljs-string">            This is the path to use if you want to collect arbitrary output from the model forward,</span><br><span class="hljs-string">            such as with inference use cases. Defaults to False.</span><br><span class="hljs-string">        checkpoint_activations_microbatch (int, optional):</span><br><span class="hljs-string">            The microbatch to checkpoint activations.</span><br><span class="hljs-string">            Defaults to None.</span><br><span class="hljs-string">        is_first_microbatch (bool, optional):</span><br><span class="hljs-string">            Whether it is the first microbatch. Defaults to False.</span><br><span class="hljs-string">        current_microbatch (int, optional):</span><br><span class="hljs-string">            The current microbatch. Defaults to None.</span><br><span class="hljs-string">        vp_stage (int, optional):</span><br><span class="hljs-string">            The virtual pipeline stage. Defaults to None.</span><br><span class="hljs-string">        is_last_stage (bool, optional):</span><br><span class="hljs-string">            Whether it is the last stage. Defaults to True.</span><br><span class="hljs-string">            Also considering virtual stages.</span><br><span class="hljs-string">            In case of PP/VPP, is_last_stage/is_vp_last_stage.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        Tensor or list[Tensor]: The output object(s) from the forward step.</span><br><span class="hljs-string">        Tensor: The number of tokens.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">from</span> megatron.core.transformer.multi_token_prediction <span class="hljs-keyword">import</span> MTPLossAutoScaler<br><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;forward-compute&#x27;</span>, log_level=<span class="hljs-number">2</span>).start()<br><br>    <span class="hljs-keyword">if</span> is_first_microbatch <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(model, <span class="hljs-string">&#x27;set_is_first_microbatch&#x27;</span>):<br>        model.set_is_first_microbatch()<br>    <span class="hljs-keyword">if</span> current_microbatch <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        set_current_microbatch(model, current_microbatch)<br><br>    unwrap_output_tensor = <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(input_tensor, <span class="hljs-built_in">list</span>):<br>        input_tensor = [input_tensor]<br>        unwrap_output_tensor = <span class="hljs-literal">True</span><br><br>    set_input_tensor = get_attr_wrapped_model(model, <span class="hljs-string">&quot;set_input_tensor&quot;</span>)<br>    set_input_tensor(input_tensor)<br><br>    <span class="hljs-keyword">if</span> config.enable_autocast:<br>        context_manager = torch.autocast(<span class="hljs-string">&quot;cuda&quot;</span>, dtype=config.autocast_dtype)<br>    <span class="hljs-keyword">else</span>:<br>        context_manager = contextlib.nullcontext()<br>    <span class="hljs-keyword">with</span> context_manager:<br>        <span class="hljs-keyword">if</span> checkpoint_activations_microbatch <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            output_tensor, loss_func = forward_step_func(data_iterator, model)<br>        <span class="hljs-keyword">else</span>:<br>            output_tensor, loss_func = forward_step_func(<br>                data_iterator, model, checkpoint_activations_microbatch<br>            )<br>    output_tensor, num_tokens = forward_step_calc_loss(<br>        model,<br>        output_tensor,<br>        loss_func,<br>        config,<br>        vp_stage,<br>        collect_non_loss_data,<br>        num_microbatches,<br>        forward_data_store,<br>        cp_group_size,<br>        is_last_stage,<br>    )<br><br>    <span class="hljs-keyword">if</span> unwrap_output_tensor:<br>        <span class="hljs-keyword">return</span> output_tensor, num_tokens<br>    <span class="hljs-keyword">return</span> [output_tensor], num_tokens<br><br></code></pre></td></tr></table></figure><h4 id="train-step代码-流程"><a href="#train-step代码-流程" class="headerlink" title="train_step代码&amp;流程"></a><code>train_step</code>代码&amp;流程</h4><p><code>train_step</code>的代码如下，它是训练循环里真正执行一次参数更新（或跳过更新） 的函数。它把 “梯度清零 → 前后向（可能重跑）→ optimizer.step → LR scheduler.step → 统计&#x2F;归约 loss” 串起来，并与 容错重跑机制、分布式优化器&#x2F;通信重叠、视觉预训练特殊逻辑交织在一起。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_step<br></code></pre></td></tr></table></figure><p>其流程如下：</p><ol><li><p>其计算Forward的代码外包了一层<code>while rerun_state_machine.should_run_forward_backward(data_iterator)</code>，这是为了当检测到某些 transient 错误或需要重新取 batch 时可以进行容错与重跑。具体Forward如下</p><ol><li><p>清空model和optimizer的grad</p></li><li><p>一些特殊逻辑，包含形状调整函数（仅 ModelOpt 蒸馏 + PP）和mxfp8 参数复用 grad buffer 时的参数 buffer 拷贝</p></li><li><p>调用<code>forward_backward_func</code>执行一次forward并拿到loss</p></li></ol></li><li><p>统一判断是否要 checkpoint或exit</p></li><li><p>清空cuda cache，判断是否要走视觉预训练的特殊逻辑</p></li><li><p>调用<code>optimizer.step()</code>更新模型参数</p></li><li><p>进一步调整opt_param_scheduler的学习率</p></li><li><p>如果当前是 pipeline last stage 则汇总 microbatches 的 loss，主要是用于日志数据记录等</p></li></ol><h2 id="evaluate-and-print-results"><a href="#evaluate-and-print-results" class="headerlink" title="evaluate_and_print_results"></a>evaluate_and_print_results</h2><p><code>evaluate_and_print_results</code>的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_and_print_results</span>(<span class="hljs-params"></span><br><span class="hljs-params">    prefix,</span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    data_iterator,</span><br><span class="hljs-params">    model,</span><br><span class="hljs-params">    iteration,</span><br><span class="hljs-params">    process_non_loss_data_func,</span><br><span class="hljs-params">    config,</span><br><span class="hljs-params">    verbose=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    write_to_tensorboard=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">    non_loss_data_func=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Helper function to evaluate and dump results on screen.&quot;&quot;&quot;</span><br>    args = get_args()<br>    <span class="hljs-keyword">if</span> write_to_tensorboard:<br>        writer = get_tensorboard_writer()<br>    <span class="hljs-keyword">else</span>:<br>        writer = <span class="hljs-literal">None</span><br><br>    wandb_writer = get_wandb_writer()<br><br>    data_iterators = data_iterator <span class="hljs-keyword">if</span> args.multiple_validation_sets <span class="hljs-keyword">else</span> [data_iterator]<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> args.multiple_validation_sets:<br>        eval_iters = [args.eval_iters]<br>    <span class="hljs-keyword">else</span>:<br>        eval_iters = args.eval_iters<br>        <br>    <span class="hljs-keyword">if</span> args.full_validation:<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(eval_iters) == <span class="hljs-built_in">len</span>(data_iterators)<br><br>        <span class="hljs-comment"># with full validation we need to distribute eval_iters to all ranks</span><br>        <span class="hljs-keyword">if</span> mpu.get_tensor_model_parallel_rank() == <span class="hljs-number">0</span>:<br>            eval_iters = torch.tensor(args.eval_iters, dtype=torch.long, device=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            eval_iters = torch.tensor([<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(eval_iters), dtype=torch.long, device=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>        torch.distributed.broadcast(eval_iters, <span class="hljs-number">0</span>)<br>        eval_iters = eval_iters.tolist()<br>        args.eval_iters = eval_iters[<span class="hljs-number">0</span>] <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> args.multiple_validation_sets <span class="hljs-keyword">else</span> eval_iters<br>    <br>    <span class="hljs-keyword">for</span> index, (iterator, iterations) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(data_iterators, eval_iters)):<br>        suffix = <span class="hljs-string">&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> args.multiple_validation_sets:<br>            suffix = <span class="hljs-string">f&quot;-<span class="hljs-subst">&#123;index&#125;</span>&quot;</span><br>        total_loss_dict, collected_non_loss_data, timelimit = evaluate(<br>            forward_step_func,<br>            iterator,<br>            model,<br>            process_non_loss_data_func,<br>            config,<br>            verbose,<br>            non_loss_data_func,<br>            eval_iters=iterations,<br>        )<br>        <span class="hljs-comment"># Timelimit hit during evaluation</span><br>        <span class="hljs-keyword">if</span> timelimit:<br>            <span class="hljs-keyword">return</span><br>        string = <span class="hljs-string">f&#x27; validation<span class="hljs-subst">&#123;suffix&#125;</span> loss at <span class="hljs-subst">&#123;prefix&#125;</span> | &#x27;</span><br>        <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> total_loss_dict:<br>            string += <span class="hljs-string">&#x27;&#123;&#125; value: &#123;:.6E&#125; | &#x27;</span>.<span class="hljs-built_in">format</span>(key, total_loss_dict[key].item())<br>            ppl = math.exp(<span class="hljs-built_in">min</span>(<span class="hljs-number">20</span>, total_loss_dict[key].item()))<br>            string += <span class="hljs-string">&#x27;&#123;&#125; PPL: &#123;:.6E&#125; | &#x27;</span>.<span class="hljs-built_in">format</span>(key, ppl)<br>            <span class="hljs-keyword">if</span> writer:<br>                writer.add_scalar(<span class="hljs-string">&#x27;&#123;&#125; validation&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(key, suffix), total_loss_dict[key].item(), iteration)<br>                writer.add_scalar(<br>                    <span class="hljs-string">&#x27;&#123;&#125; validation&#123;&#125; vs samples&#x27;</span>.<span class="hljs-built_in">format</span>(key, suffix),<br>                    total_loss_dict[key].item(),<br>                    args.consumed_train_samples,<br>                )<br>                <span class="hljs-keyword">if</span> args.log_validation_ppl_to_tensorboard:<br>                    writer.add_scalar(<span class="hljs-string">&#x27;&#123;&#125; validation&#123;&#125; ppl&#x27;</span>.<span class="hljs-built_in">format</span>(key, suffix), ppl, iteration)<br>                    writer.add_scalar(<br>                        <span class="hljs-string">&#x27;&#123;&#125; validation&#123;&#125; ppl vs samples&#x27;</span>.<span class="hljs-built_in">format</span>(key, suffix), ppl, args.consumed_train_samples<br>                    )<br>                <span class="hljs-keyword">if</span> wandb_writer <span class="hljs-keyword">and</span> is_last_rank():<br>                    wandb_writer.log(<br>                        &#123;<span class="hljs-string">&#x27;&#123;&#125; validation&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(key, suffix): total_loss_dict[key].item()&#125;, iteration<br>                    )<br><br>        <span class="hljs-keyword">if</span> process_non_loss_data_func <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> writer <span class="hljs-keyword">and</span> is_last_rank():<br>            process_non_loss_data_func(collected_non_loss_data, iteration, writer)<br><br>        length = <span class="hljs-built_in">len</span>(string) + <span class="hljs-number">1</span><br>        print_rank_last(<span class="hljs-string">&#x27;-&#x27;</span> * length)<br>        print_rank_last(string)<br>        print_rank_last(<span class="hljs-string">&#x27;-&#x27;</span> * length)<br><br></code></pre></td></tr></table></figure><p>其流程如下：</p><ol><li><p>进行TensorBoard 与 WandB的初始化</p></li><li><p>如果是多验证集，就获取对应的<code>eval_iters</code>列表</p></li><li><p>如果设置了<code>full_validation</code>，就把验证集发给所有rank</p></li><li><p>对每个验证集调用 evaluate(…)</p></li><li><p>打印对应的日志，写入到WandB</p></li><li><p>调用<code>process_non_loss_data_func</code>进行自定义的非 loss 数据的处理</p></li></ol><h3 id="evaluate"><a href="#evaluate" class="headerlink" title="evaluate"></a>evaluate</h3><p>evaluate的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params"></span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    data_iterator,</span><br><span class="hljs-params">    model,</span><br><span class="hljs-params">    process_non_loss_data_func,</span><br><span class="hljs-params">    config,</span><br><span class="hljs-params">    verbose=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    non_loss_data_func=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    eval_iters=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Evaluation.&quot;&quot;&quot;</span><br>    args = get_args()<br>    timers = get_timers()<br><br>    timers(<span class="hljs-string">&#x27;evaluate&#x27;</span>, log_level=<span class="hljs-number">0</span>).start(barrier=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">if</span> args.vision_pretraining <span class="hljs-keyword">and</span> args.vision_pretraining_type == <span class="hljs-string">&quot;dino&quot;</span>:<br>        <span class="hljs-keyword">from</span> megatron.legacy.model.vision.knn_monitor <span class="hljs-keyword">import</span> compute_feature_bank<br><br>        compute_feature_bank(model)<br><br>    <span class="hljs-comment"># Turn on evaluation mode which disables dropout.</span><br>    <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model:<br>        model_module.<span class="hljs-built_in">eval</span>()<br><br>    <span class="hljs-comment"># Disable result validation during evaluation</span><br>    rerun_state_machine = get_rerun_state_machine()<br>    rerun_mode = rerun_state_machine.get_mode()<br>    rerun_state_machine.set_mode(RerunMode.DISABLED)<br><br>    total_loss_dict = &#123;&#125;<br><br>    <span class="hljs-comment"># make validation batch size independent from training batch size</span><br>    eval_batch_size = args.global_batch_size<br>    eval_num_microbatches = eval_batch_size // (args.micro_batch_size * args.data_parallel_size)<br>    forward_backward_func = get_forward_backward_func()<br>    <span class="hljs-keyword">if</span> args.enable_cuda_graph <span class="hljs-keyword">and</span> args.cuda_graph_scope==<span class="hljs-string">&quot;full_iteration&quot;</span>:<br>        forward_backward_func = FullCudaGraphWrapper(forward_backward_func, cuda_graph_warmup_steps=args.cuda_graph_warmup_steps)<br><br>    <span class="hljs-keyword">if</span> eval_iters <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        eval_iters = args.eval_iters<br><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        iteration = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> verbose:<br>            print_rank_0(<span class="hljs-string">f&#x27;Evaluating on <span class="hljs-subst">&#123;eval_iters * eval_batch_size&#125;</span> samples&#x27;</span>)<br>        <span class="hljs-keyword">while</span> iteration &lt; eval_iters:<br>            iteration += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> verbose:<br>                print_rank_0(<span class="hljs-string">f&#x27;Evaluating iter <span class="hljs-subst">&#123;iteration&#125;</span>/<span class="hljs-subst">&#123;eval_iters&#125;</span>&#x27;</span>)<br><br>            <span class="hljs-comment"># Don&#x27;t care about timing during evaluation</span><br>            config.timers = <span class="hljs-literal">None</span><br>            ft_integration.on_eval_step_start()<br>            loss_dicts = forward_backward_func(<br>                forward_step_func=forward_step_func,<br>                data_iterator=data_iterator,<br>                model=model,<br>                num_microbatches=eval_num_microbatches,<br>                seq_length=args.seq_length,<br>                micro_batch_size=args.micro_batch_size,<br>                decoder_seq_length=args.decoder_seq_length,<br>                forward_only=<span class="hljs-literal">True</span>,<br>            )<br>            ft_integration.on_eval_step_end()<br>            config.timers = get_timers()<br><br>            <span class="hljs-comment"># Empty unused memory</span><br>            <span class="hljs-keyword">if</span> args.empty_unused_memory_level &gt;= <span class="hljs-number">1</span>:<br>                torch.cuda.empty_cache()<br><br>            <span class="hljs-keyword">if</span> mpu.is_pipeline_last_stage(ignore_virtual=<span class="hljs-literal">True</span>):<br>                <span class="hljs-comment"># Reduce across processes.</span><br>                <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> loss_dicts[<span class="hljs-number">0</span>].keys():<br>                    <span class="hljs-keyword">if</span> key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> total_loss_dict:<br>                        total_loss_dict[key] = torch.tensor(<br>                            [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>], dtype=torch.<span class="hljs-built_in">float</span><br>                        ).cuda()<br>                    val = [x[key].view(-<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> loss_dicts]<br><br>                    <span class="hljs-keyword">if</span> val[<span class="hljs-number">0</span>].numel() == <span class="hljs-number">2</span>:<br>                        <span class="hljs-keyword">if</span> args.sft:<br>                            <span class="hljs-comment"># normalize over micro batch instead of global</span><br>                            val = torch.vstack(val)<br>                            val = val[:, <span class="hljs-number">0</span>] / val[:, <span class="hljs-number">1</span>]<br>                            val = val.mean()<br>                            torch.distributed.all_reduce(<br>                                val,<br>                                group=mpu.get_data_parallel_group(with_context_parallel=<span class="hljs-literal">True</span>)<br>                            )<br>                            val /= torch.distributed.get_world_size(<br>                                group=mpu.get_data_parallel_group(with_context_parallel=<span class="hljs-literal">True</span>)<br>                            )<br>                            total_loss_dict[key][<span class="hljs-number">0</span>] += val<br>                            total_loss_dict[key][<span class="hljs-number">1</span>] += <span class="hljs-number">1</span><br>                        <span class="hljs-keyword">else</span> :<br>                            val = torch.vstack(val).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">0</span>)<br>                            torch.distributed.all_reduce(<br>                                val,<br>                                group=mpu.get_data_parallel_group(with_context_parallel=<span class="hljs-literal">True</span>)<br>                            )<br>                            total_loss_dict[key] += val<br>                    <span class="hljs-keyword">elif</span> val[<span class="hljs-number">0</span>].numel() == <span class="hljs-number">1</span>:<br>                        val = torch.cat(val).<span class="hljs-built_in">sum</span>()<br>                        total_loss_dict[key][<span class="hljs-number">0</span>] += val<br>                        total_loss_dict[key][<span class="hljs-number">1</span>] += <span class="hljs-built_in">len</span>(loss_dicts)<br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;Invalid value shape: <span class="hljs-subst">&#123;val[<span class="hljs-number">0</span>].shape&#125;</span> for key <span class="hljs-subst">&#123;key&#125;</span>&quot;</span>)<br><br>            args.consumed_valid_samples += eval_batch_size<br><br>            <span class="hljs-keyword">if</span> args.exit_duration_in_mins:<br>                train_time = (time.time() - _TRAIN_START_TIME) / <span class="hljs-number">60.0</span><br>                done_cuda = torch.tensor(<br>                    [train_time &gt; args.exit_duration_in_mins], dtype=torch.<span class="hljs-built_in">int</span>, device=<span class="hljs-string">&#x27;cuda&#x27;</span><br>                )<br>                torch.distributed.all_reduce(done_cuda, op=torch.distributed.ReduceOp.MAX)<br>                done = done_cuda.item()<br>                <span class="hljs-keyword">if</span> done:<br>                    rerun_state_machine.set_mode(rerun_mode)<br>                    print_rank_0(<span class="hljs-string">&#x27;Exiting during evaluation, timelimit reached&#x27;</span>)<br>                    <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">True</span><br><br>        collected_non_loss_data = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> non_loss_data_func <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            collected_non_loss_data = non_loss_data_func(model)<br>        <span class="hljs-keyword">elif</span> process_non_loss_data_func <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> is_last_rank():<br>            collected_non_loss_data = forward_backward_func(<br>                forward_step_func=forward_step_func,<br>                data_iterator=data_iterator,<br>                model=model,<br>                num_microbatches=get_num_microbatches(),<br>                seq_length=args.seq_length,<br>                micro_batch_size=args.micro_batch_size,<br>                decoder_seq_length=args.decoder_seq_length,<br>                forward_only=<span class="hljs-literal">True</span>,<br>                collect_non_loss_data=<span class="hljs-literal">True</span>,<br>            )<br><br>    <span class="hljs-comment"># Move model back to the train mode.</span><br>    <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model:<br>        model_module.train()<br><br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> total_loss_dict:<br>        numerator, denominator = total_loss_dict[key]<br>        total_loss_dict[key] = numerator / denominator<br><br>    timers(<span class="hljs-string">&#x27;evaluate&#x27;</span>).stop()<br>    timers.log([<span class="hljs-string">&#x27;evaluate&#x27;</span>])<br><br>    rerun_state_machine.set_mode(rerun_mode)<br><br>    rerun_state_machine.set_mode(rerun_mode)<br><br>    <span class="hljs-keyword">return</span> total_loss_dict, collected_non_loss_data, <span class="hljs-literal">False</span><br><br></code></pre></td></tr></table></figure><p>其整体流程如下所示：</p><ol><li><p>切换模型到 eval()，并临时禁用<code>rerun_state_machine</code></p></li><li><p>计算evaluate用的 microbatch 大小</p></li><li><p>与train类似，得到<code>forward_backward_func</code></p></li><li><p>在no_grad下依次执行各个eval_iter：</p><ol><li><p>调用<code>forward_backward_func</code>，注意调用时专门设置了forward_only&#x3D;True</p></li><li><p>在 pipeline last stage 收集与规约 loss</p></li><li><p>通过评估样本计数与 time limit 计算是否需要提前退出</p></li><li><p>执行<code>non_loss_data_func</code>与<code>forward_backward_func</code></p></li></ol></li><li><p>将model恢复训练模式，并把累计的 numerator&#x2F;denominator 转成标量</p></li><li><p>恢复<code>rerun_state_machine</code>、返回结果</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>Megatron-LM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Megatron-LM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Megatron-LM源码分析（一）】-环境配置与训练示例跑通</title>
    <link href="/2025/12/14/megatron-lm-env-and-example/"/>
    <url>/2025/12/14/megatron-lm-env-and-example/</url>
    
    <content type="html"><![CDATA[<h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><ol><li>下载代码：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/NVIDIA/Megatron-LM.git<br></code></pre></td></tr></table></figure><ul><li>切换到稳定分支：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git checkout -b core_r0.14.0 origin/core_r0.14.0<br></code></pre></td></tr></table></figure><ul><li>拉取指定docker镜像：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker pull nvcr.io/nvidia/pytorch:25.04-py3<br></code></pre></td></tr></table></figure><h1 id="最小示例"><a href="#最小示例" class="headerlink" title="最小示例"></a>最小示例</h1><h2 id="示例运行流程"><a href="#示例运行流程" class="headerlink" title="示例运行流程"></a>示例运行流程</h2><ol><li>进入Docker容器，将本地的<code>Megatron-LM</code>挂载到宿主机的<code>/workspace/megatron-lm</code>上注意这里只将GPU0和GPU1引入进容器里，并且使用的是16GB的共享内存。注意&#x5C06;<strong><code>/home/xxx/Megatron-LM</code></strong>&#x6362;成自己的本地路径。</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker run --gpus <span class="hljs-string">&#x27;&quot;device=0,1&quot;&#x27;</span> -it --<span class="hljs-built_in">rm</span> --shm-size=16G   -v /home/xxx/Megatron-LM:/workspace/megatron-lm   -e PIP_CONSTRAINT=   nvcr.io/nvidia/pytorch:25.04-py3<br></code></pre></td></tr></table></figure><ul><li>进入megatron-lm目录，运行最小示例：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> megatron-lm/<br>PYTHONPATH=<span class="hljs-variable">$PYTHONPATH</span>:./megatron torchrun --nproc-per-node 2 examples/run_simple_mcore_train_loop.py<br></code></pre></td></tr></table></figure><ul><li>查看运行结果，虽然会有一些warning，但是还是能正常运行结束的。</li></ul><p><img src="/2025/12/14/megatron-lm-env-and-example/image-2.png"></p><h2 id="最小示例解读"><a href="#最小示例解读" class="headerlink" title="最小示例解读"></a>最小示例解读</h2><p>下面解读一下这最小示例<code>examples/run_simple_mcore_train_loop.py</code>的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> Adam<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> partial<br><span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path<br><br><span class="hljs-keyword">from</span> megatron.core <span class="hljs-keyword">import</span> parallel_state<br><span class="hljs-keyword">from</span> megatron.core <span class="hljs-keyword">import</span> dist_checkpointing<br><span class="hljs-keyword">from</span> megatron.core.pipeline_parallel.schedules <span class="hljs-keyword">import</span> get_forward_backward_func<br><span class="hljs-keyword">from</span> megatron.core.tensor_parallel.random <span class="hljs-keyword">import</span> model_parallel_cuda_manual_seed<br><span class="hljs-keyword">from</span> megatron.core.transformer.transformer_config <span class="hljs-keyword">import</span> TransformerConfig<br><span class="hljs-keyword">from</span> megatron.core.models.gpt.gpt_model <span class="hljs-keyword">import</span> GPTModel<br><span class="hljs-keyword">from</span> megatron.core.models.gpt.gpt_layer_specs <span class="hljs-keyword">import</span> get_gpt_layer_local_spec<br><span class="hljs-keyword">from</span> megatron.core.datasets.utils <span class="hljs-keyword">import</span> compile_helpers <br><span class="hljs-keyword">from</span> megatron.core.datasets.blended_megatron_dataset_builder <span class="hljs-keyword">import</span> BlendedMegatronDatasetBuilder<br><span class="hljs-keyword">from</span> megatron.core.datasets.gpt_dataset <span class="hljs-keyword">import</span> GPTDatasetConfig, MockGPTDataset<br><span class="hljs-keyword">from</span> megatron.training.tokenizer.tokenizer <span class="hljs-keyword">import</span> _NullTokenizer<br><br>_SEQUENCE_LENGTH = <span class="hljs-number">64</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_distributed</span>(<span class="hljs-params">tensor_model_parallel_size=<span class="hljs-number">1</span>, pipeline_model_parallel_size=<span class="hljs-number">1</span></span>):<br>    parallel_state.destroy_model_parallel()<br><br>    <span class="hljs-comment"># Torch setup for distributed training</span><br>    rank = <span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&#x27;LOCAL_RANK&#x27;</span>])<br>    world_size = torch.cuda.device_count()<br>    torch.cuda.set_device(rank)<br>    torch.distributed.init_process_group(world_size=world_size, rank=rank)<br><br>    <span class="hljs-comment"># Megatron core distributed training initialization</span><br>    parallel_state.initialize_model_parallel(tensor_model_parallel_size, pipeline_model_parallel_size)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_provider</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;Build the model.&quot;&quot;&quot;</span><br><br>    transformer_config = TransformerConfig(<br>        num_layers=<span class="hljs-number">2</span>, <br>        hidden_size=<span class="hljs-number">12</span>, <br>        num_attention_heads=<span class="hljs-number">4</span>, <br>        use_cpu_initialization=<span class="hljs-literal">True</span>, <br>        pipeline_dtype=torch.float32,<br>    )<br><br>    gpt_model = GPTModel(<br>        config=transformer_config, <br>        transformer_layer_spec=get_gpt_layer_local_spec(), <br>        vocab_size=<span class="hljs-number">100</span>, <br>        max_sequence_length=_SEQUENCE_LENGTH,<br>    )<br><br>    <span class="hljs-keyword">return</span> gpt_model<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_train_data_iterator</span>():<br>    <span class="hljs-keyword">if</span> torch.distributed.is_available() <span class="hljs-keyword">and</span> torch.distributed.is_initialized():<br>        <span class="hljs-keyword">if</span> torch.distributed.get_rank() == <span class="hljs-number">0</span>:<br>            compile_helpers()<br>        torch.distributed.barrier()<br>    <span class="hljs-keyword">else</span>:<br>        compile_helpers()<br><br>    config = GPTDatasetConfig(<br>        random_seed=<span class="hljs-number">0</span>,<br>        sequence_length=_SEQUENCE_LENGTH,<br>        reset_position_ids=<span class="hljs-literal">False</span>,<br>        reset_attention_mask=<span class="hljs-literal">False</span>,<br>        eod_mask_loss=<span class="hljs-literal">False</span>,<br>        tokenizer=_NullTokenizer(vocab_size=_SEQUENCE_LENGTH),<br>        mid_level_dataset_surplus=<span class="hljs-number">0.005</span>,<br>    )<br><br>    datasets = BlendedMegatronDatasetBuilder(<br>        MockGPTDataset, [<span class="hljs-number">1000</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>], <span class="hljs-keyword">lambda</span>: <span class="hljs-literal">True</span>, config<br>    ).build()<br><br>    train_dataloader = DataLoader(datasets[<span class="hljs-number">0</span>], batch_size=<span class="hljs-number">8</span>, shuffle=<span class="hljs-literal">True</span>)<br><br>    train_iterator = <span class="hljs-built_in">iter</span>(train_dataloader)<br><br>    <span class="hljs-keyword">return</span> train_iterator<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_step_func</span>(<span class="hljs-params">data_iterator, model</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss_func</span>(<span class="hljs-params">loss_mask: torch.Tensor, output_tensor: torch.Tensor</span>):<br><br>        losses = output_tensor.<span class="hljs-built_in">float</span>()<br>        loss_mask = loss_mask.view(-<span class="hljs-number">1</span>).<span class="hljs-built_in">float</span>()<br>        loss = torch.<span class="hljs-built_in">sum</span>(losses.view(-<span class="hljs-number">1</span>) * loss_mask) / loss_mask.<span class="hljs-built_in">sum</span>()<br>        <span class="hljs-comment"># If you have data parallel reduce loss across data parallel groups.</span><br>        <span class="hljs-comment"># If pipeline parallel, loss computation is done only in last stage.</span><br><br>        <span class="hljs-keyword">return</span> loss, &#123;<span class="hljs-string">&#x27;lm loss&#x27;</span>: loss&#125;<br><br>    data = <span class="hljs-built_in">next</span>(data_iterator)<br>    tokens = data[<span class="hljs-string">&#x27;tokens&#x27;</span>].to(device)<br>    attention_mask = data[<span class="hljs-string">&#x27;attention_mask&#x27;</span>].to(device)<br>    position_ids = data[<span class="hljs-string">&#x27;position_ids&#x27;</span>].to(device)<br>    labels = data[<span class="hljs-string">&#x27;labels&#x27;</span>].to(device)<br>    loss_mask = data[<span class="hljs-string">&#x27;loss_mask&#x27;</span>].to(device)<br><br>    output_tensor = model(tokens, position_ids, attention_mask,<br>                          labels=labels)<br><br>    <span class="hljs-keyword">return</span> output_tensor, partial(loss_func, loss_mask)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">save_distributed_checkpoint</span>(<span class="hljs-params">checkpoint_path, gpt_model</span>):<br>    sharded_state_dict = gpt_model.sharded_state_dict(prefix=<span class="hljs-string">&#x27;&#x27;</span>)<br>    dist_checkpointing.save(sharded_state_dict=sharded_state_dict, checkpoint_dir=checkpoint_path)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_distributed_checkpoint</span>(<span class="hljs-params">checkpoint_path, gpt_model</span>):<br>    sharded_state_dict=gpt_model.sharded_state_dict(prefix=<span class="hljs-string">&#x27;&#x27;</span>)<br>    checkpoint = dist_checkpointing.load(sharded_state_dict=sharded_state_dict, checkpoint_dir=checkpoint_path)<br>    gpt_model.load_state_dict(checkpoint)<br>    <span class="hljs-keyword">return</span> gpt_model<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    initialize_distributed(tensor_model_parallel_size=<span class="hljs-number">2</span>, pipeline_model_parallel_size=<span class="hljs-number">1</span>)<br>    model_parallel_cuda_manual_seed(<span class="hljs-number">123</span>)<br><br>    gpt_model = model_provider()<br>    device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>)<br>    gpt_model.to(device)<br><br>    optim = Adam(gpt_model.parameters())<br><br>    train_iterator = get_train_data_iterator()<br><br>    forward_backward_func = get_forward_backward_func()<br><br>    <span class="hljs-comment"># Running the model for 5 iterations</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>        optim.zero_grad()<br><br>        losses_reduced = forward_backward_func(<br>            forward_step_func=forward_step_func,<br>            data_iterator=train_iterator,<br>            model=gpt_model,<br>            num_microbatches=<span class="hljs-number">1</span>,<br>            seq_length=_SEQUENCE_LENGTH,<br>            micro_batch_size=<span class="hljs-number">8</span>,<br>            decoder_seq_length=_SEQUENCE_LENGTH,<br>            forward_only=<span class="hljs-literal">False</span>)<br><br>        optim.step()<br><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;Losses reduced :  <span class="hljs-subst">&#123;losses_reduced&#125;</span>&#x27;</span>)<br><br>    <span class="hljs-comment"># Saving the model</span><br>    ckpt_path = os.getcwd() + <span class="hljs-string">&#x27;/ckpt&#x27;</span><br>    Path(ckpt_path).mkdir(exist_ok=<span class="hljs-literal">True</span>)<br>    save_distributed_checkpoint(gpt_model=gpt_model, checkpoint_path=ckpt_path)<br><br>    <span class="hljs-comment"># Loading the model</span><br>    gpt_model = load_distributed_checkpoint(gpt_model=gpt_model, checkpoint_path=ckpt_path)<br>    gpt_model.to(device)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Successfully loaded the model&#x27;</span>)<br><br></code></pre></td></tr></table></figure><h3 id="初始化阶段"><a href="#初始化阶段" class="headerlink" title="初始化阶段"></a>初始化阶段</h3><ul><li><p>销毁之前的模型并行状态</p></li><li><p>获取本地rank和world_size（GPU数量）</p></li><li><p>设置当前进程使用的CUDA设备</p></li><li><p>初始化PyTorch分布式进程组</p></li><li><p>初始化Megatron-Core的模型并行：张量并行度&#x3D;2，流水线并行度&#x3D;1，这是稍微复杂一些的地方，其主要作用为：</p><ul><li><p>依据输入的并行参数以及默认参数计算各种关键并行参数，并进行参数检查，例如检查如下的内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">model_size = tensor_model_parallel_size * pipeline_model_parallel_size * context_parallel_size<br><br>   <span class="hljs-keyword">if</span> world_size % model_size != 0:<br>       raise RuntimeError(f<span class="hljs-string">&quot;world_size (&#123;world_size&#125;) is not divisible by &#123;model_size&#125;&quot;</span>)<br></code></pre></td></tr></table></figure></li><li><p>其还创建了各种dp、tp、pp、ep、cp的通信组<code>torch.distributed.new_group</code>，并通过全局变量进行传递。（怎么这么多全局变量啊。。。）</p></li></ul></li></ul><h3 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h3><ul><li><p>构建了<code>TransformerConfig</code>，其层数为2，隐藏层维度为12，注意力头数量为4</p></li><li><p>然后依据<code>TransformerConfig</code>以及其他参数（词表大小100，最大序列长度64）构建了GPT模型</p></li></ul><h3 id="优化器构建"><a href="#优化器构建" class="headerlink" title="优化器构建"></a>优化器构建</h3><ul><li>使用<code>gpt_model.parameters()</code>来初始化了torch原生的<code>Adam</code>优化器</li></ul><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><ul><li><p><strong>创建数据集配置：</strong></p><ul><li><p>使用_NullTokenizer（虚拟分词器）</p></li><li><p>序列长度64</p></li><li><p>不重置position_ids和attention_mask</p></li></ul></li><li><p><strong>构建混合数据集：</strong></p><ul><li><p>使用MockGPTDataset（模拟数据集，用于测试）</p></li><li><p>1000个样本</p></li></ul></li><li><p><strong>创建torch原生的DataLoader，并用迭代器进行包装：</strong></p><ul><li><p>batch_size&#x3D;8</p></li><li><p>shuffle&#x3D;True</p></li></ul></li></ul><h3 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h3><ul><li><p>获取<code>forward_backward_func</code></p><ul><li>该函数提供了如何根据<code>data_iterator</code>获取数据，然后使用模型计算出<code>output_tensor</code>，并最终返回了<code>output_tensor, partial(loss_func, loss_mask)</code></li></ul></li><li><p>训练了5个迭代</p></li></ul><h3 id="模型保存与加载"><a href="#模型保存与加载" class="headerlink" title="模型保存与加载"></a>模型保存与加载</h3><ul><li><p>调用<code>gpt_model.sharded_state_dict()</code>获取分片状态字典</p></li><li><p>使用<code>dist_checkpointing.save()</code>保存</p><ul><li><p>每个GPU只保存自己的模型分片</p></li><li><p>支持张量并行、流水线并行的自动分片</p></li></ul></li></ul><h1 id="GPT训练示例"><a href="#GPT训练示例" class="headerlink" title="GPT训练示例"></a>GPT训练示例</h1><h2 id="分词器准备"><a href="#分词器准备" class="headerlink" title="分词器准备"></a>分词器准备</h2><ul><li>直接使用GPT-2 的 tokenizer，下载方式如下:</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> -p data/tokenizer<br>wget https://huggingface.co/gpt2/resolve/main/vocab.json -O data/tokenizer/gpt2-vocab.json<br>wget https://huggingface.co/gpt2/resolve/main/merges.txt -O data/tokenizer/gpt2-merges.txt<br></code></pre></td></tr></table></figure><blockquote><p>vocab是每个token对应的序号，merges表示如何两两token合并为一个大token</p></blockquote><h2 id="训练数据准备"><a href="#训练数据准备" class="headerlink" title="训练数据准备"></a>训练数据准备</h2><ul><li><p>使用的是TinyStories，hugging face链接为：<a href="https://huggingface.co/datasets/roneneldan/TinyStories/tree/main">https://huggingface.co/datasets/roneneldan/TinyStories/tree/main</a></p></li><li><p>它包含训练集和测试集，而实际上Megatron-LM也支持自动切分数据集为训练集和测试集、验证集，所以这里只下载了训练集：</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget -p data https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt?download=<span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><ul><li>由于Megatron-LM处理的数据集基本格式是jsonl，所以这里使用了一个脚本<code>txt_convert_to_jsonl.py</code>进行格式转化，如下，在容器中运行该脚本后会得到<code>data/TinyStoriesV2-GPT4-valid.jsonl</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br>in_file = <span class="hljs-string">&quot;/workspace/megatron-lm/data/TinyStoriesV2-GPT4-train.txt&quot;</span><br>out_file = <span class="hljs-string">&quot;/workspace/megatron-lm/data/TinyStoriesV2-GPT4-train.jsonl&quot;</span><br><br><span class="hljs-comment"># 先统计总行数（用于进度条）</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(in_file, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    total_lines = <span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> f)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(in_file, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> fin, \<br>     <span class="hljs-built_in">open</span>(out_file, <span class="hljs-string">&quot;w&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> fout:<br><br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tqdm(fin, total=total_lines, desc=<span class="hljs-string">&quot;Converting&quot;</span>):<br>        line = line.strip()<br>        <span class="hljs-keyword">if</span> line:<br>            fout.write(json.dumps(&#123;<span class="hljs-string">&quot;text&quot;</span>: line&#125;, ensure_ascii=<span class="hljs-literal">False</span>) + <span class="hljs-string">&quot;\n&quot;</span>)<br><br></code></pre></td></tr></table></figure><ul><li>然后还需要用Megatron-LM的脚本借助分词器进一步转化为bin+idx的格式。运行脚本如下，最终会得到<code>data/TinyStoriesV2-GPT4-train_text_document.bin</code>和<code>data/TinyStoriesV2-GPT4-train_text_document.idx</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">python tools/preprocess_data.py \<br>    --input /workspace/megatron-lm/data/TinyStoriesV2-GPT4-train.jsonl \<br>    --output-prefix /workspace/megatron-lm/data/TinyStoriesV2 \<br>    --vocab-file /workspace/megatron-lm/data/tokenizer/gpt2-vocab.json \<br>    --merge-file /workspace/megatron-lm/data/tokenizer/gpt2-merges.txt \<br>    --tokenizer-type GPT2BPETokenizer \<br>    --append-eod \<br>    --workers 8<br></code></pre></td></tr></table></figure><h2 id="857m-GPT3模型训练脚本"><a href="#857m-GPT3模型训练脚本" class="headerlink" title="857m GPT3模型训练脚本"></a>857m GPT3模型训练脚本</h2><p>在<code>examples/gpt3</code>下有175b的GPT模型训练的脚本<code>train_gpt3_175b_distributed.sh</code>，不过可惜我只有4块4090，所以我稍微修改了一下脚本，改为训练其给出的857m的模型，该训练脚本<code>train_gpt3_857m_distributed.sh</code>如下所示：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><br><span class="hljs-comment"># Runs the &quot;857m&quot; parameter model</span><br><br><span class="hljs-built_in">export</span> CUDA_DEVICE_MAX_CONNECTIONS=1<br><br>GPUS_PER_NODE=4<br><span class="hljs-comment"># Change for multinode config</span><br>MASTER_ADDR=localhost<br>MASTER_PORT=6000<br>NUM_NODES=1<br>NODE_RANK=0<br>WORLD_SIZE=$((<span class="hljs-variable">$GPUS_PER_NODE</span>*<span class="hljs-variable">$NUM_NODES</span>))<br><br>CHECKPOINT_PATH=<span class="hljs-variable">$1</span> <span class="hljs-comment">#&lt;Specify path&gt;</span><br>TENSORBOARD_LOGS_PATH=<span class="hljs-variable">$2</span> <span class="hljs-comment">#&lt;Specify path&gt;</span><br>VOCAB_FILE=<span class="hljs-variable">$3</span> <span class="hljs-comment">#&lt;Specify path to file&gt;/gpt2-vocab.json</span><br>MERGE_FILE=<span class="hljs-variable">$4</span> <span class="hljs-comment">#&lt;Specify path to file&gt;/gpt2-merges.txt</span><br>DATA_PATH=<span class="hljs-variable">$5</span> <span class="hljs-comment">#&lt;Specify path and file prefix&gt;_text_document</span><br><br>DISTRIBUTED_ARGS=(<br>    --nproc_per_node <span class="hljs-variable">$GPUS_PER_NODE</span> <br>    --nnodes <span class="hljs-variable">$NUM_NODES</span> <br>    --master_addr <span class="hljs-variable">$MASTER_ADDR</span> <br>    --master_port <span class="hljs-variable">$MASTER_PORT</span><br>)<br><br>GPT_MODEL_ARGS=(<br>    --num-layers 24 <br>    --hidden-size 1024 <br>    --num-attention-heads 16 <br>    --seq-length 2048 <br>    --max-position-embeddings 2048 <br>    --attention-backend auto <span class="hljs-comment"># Can use (flash/fused/unfused/local)</span><br>)<br><br>TRAINING_ARGS=(<br>    --micro-batch-size 1 <br>    --global-batch-size 16 <br>    <span class="hljs-comment"># --rampup-batch-size 16 16 5859375 </span><br>    --train-iters 10000 <br>    --weight-decay 0.1 <br>    --adam-beta1 0.9 <br>    --adam-beta2 0.95 <br>    --init-method-std 0.006 <br>    --clip-grad 1.0 <br>    --fp16<br>    --lr 6.0e-5 <br>    --lr-decay-style cosine <br>    --min-lr 6.0e-6<br>    --lr-warmup-fraction .001 <br>    --lr-decay-iters 430000 <br>)<br><br>MODEL_PARALLEL_ARGS=(<br>    --tensor-model-parallel-size 2 <br>    --pipeline-model-parallel-size 2 <br>)<br><br>DATA_ARGS=(<br>    --data-path <span class="hljs-variable">$DATA_PATH</span> <br>    --vocab-file <span class="hljs-variable">$VOCAB_FILE</span> <br>    --merge-file <span class="hljs-variable">$MERGE_FILE</span> <br>    --<span class="hljs-built_in">split</span> 949,50,1<br>)<br><br>EVAL_AND_LOGGING_ARGS=(<br>    --log-interval 100<br>    --save-interval 10000 <br>    --eval-interval 1000 <br>    --save <span class="hljs-variable">$CHECKPOINT_PATH</span> <br>    --load <span class="hljs-variable">$CHECKPOINT_PATH</span> <br>    --eval-iters 10<br>    --tensorboard-dir <span class="hljs-variable">$TENSORBOARD_LOGS_PATH</span> <br>)<br><br>torchrun <span class="hljs-variable">$&#123;DISTRIBUTED_ARGS[@]&#125;</span> pretrain_gpt.py \<br>    <span class="hljs-variable">$&#123;GPT_MODEL_ARGS[@]&#125;</span> \<br>    <span class="hljs-variable">$&#123;TRAINING_ARGS[@]&#125;</span> \<br>    <span class="hljs-variable">$&#123;MODEL_PARALLEL_ARGS[@]&#125;</span> \<br>    <span class="hljs-variable">$&#123;DATA_ARGS[@]&#125;</span> \<br>    <span class="hljs-variable">$&#123;EVAL_AND_LOGGING_ARGS[@]&#125;</span><br><br></code></pre></td></tr></table></figure><p>改变的内容有：</p><ul><li><p>在GPT模型结构上，修改为：</p><ul><li><p>num-layers 24&#x20;</p></li><li><p>hidden-size 1024&#x20;</p></li><li><p>num-attention-heads 16&#x20;</p></li></ul></li><li><p>为了避免显存不足，训练步数train-iters改为10000，global-batch-size改为16，取消rampup-batch-size</p></li><li><p>并行训练策略改为TP度为2，PP度也为2</p></li></ul><blockquote><p>注意这些只是为了能跑起来写的，参数没有得到很好的设计</p></blockquote><h2 id="857m-GPT3模型训练"><a href="#857m-GPT3模型训练" class="headerlink" title="857m GPT3模型训练"></a>857m GPT3模型训练</h2><ul><li>首先进入容器内，这次将本地全部4块GPU都挂载进去：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker run --gpus all -it --<span class="hljs-built_in">rm</span> --shm-size=128G   -v /home/ljw/Megatron-LM:/workspace/megatron-lm   -e PIP_CONSTRAINT=   nvcr.io/nvidia/pytorch:25.04-py3<br></code></pre></td></tr></table></figure><ul><li>创建tb_log、checkpoint需要的文件夹</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> -p /workspace/megatron-lm/model_ckpt/gpt3_857m<br><span class="hljs-built_in">mkdir</span> -p /workspace/megatron-lm/tb_logs/gpt3_857m<br></code></pre></td></tr></table></figure><ul><li>直接执行下面这个脚本训练即可：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash examples/gpt3/train_gpt3_857m_distributed.sh \<br>    /workspace/megatron-lm/model_ckpt/gpt3_857m \<br>    /workspace/megatron-lm/tb_logs/gpt3_857m \<br>    /workspace/megatron-lm/data/tokenizer/gpt2-vocab.json \<br>    /workspace/megatron-lm/data/tokenizer/gpt2-merges.txt \<br>    /workspace/megatron-lm/data/TinyStoriesV2-GPT4-train_text_document<br></code></pre></td></tr></table></figure><ul><li>运行截图如下所示：</li></ul><p><img src="/2025/12/14/megatron-lm-env-and-example/image.png"></p><ul><li>nvtop查看GPU占用情况如下图所示</li></ul><p><img src="/2025/12/14/megatron-lm-env-and-example/image-1.png"></p>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>Megatron-LM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Megatron-LM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读】The Llama 3 Herd of Models（Section 3 Pre-Training）</title>
    <link href="/2025/12/13/llama3-pre-training-paper-note/"/>
    <url>/2025/12/13/llama3-pre-training-paper-note/</url>
    
    <content type="html"><![CDATA[<ul><li><p><strong>链接：</strong> <a href="https://arxiv.org/abs/2407.21783">https://arxiv.org/abs/2407.21783</a></p></li><li><p><strong>团队：</strong> META</p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><ul><li><p>Llama 3发表于2024年7月，其是一个包含8B、70B和405B参数的多语言语言模型群，实验结果显示，Llama 3的旗舰模型在各种任务上的表现与当时领先的语言模型如GPT-4相当，接近最先进水平。并且其还开发了多模态扩展模型，支持图像识别、视频识别和语音理解功能。</p></li><li><p>Llama 3语言模型的开发主要分为语言模型预训练和语言模型后训练两个阶段，文中也对其进行了详细介绍，本博客此次主要介绍预训练方面的内容</p></li></ul><h2 id="方法思路"><a href="#方法思路" class="headerlink" title="方法思路"></a>方法思路</h2><h3 id="大规模训练语料库的整理和筛选"><a href="#大规模训练语料库的整理和筛选" class="headerlink" title="大规模训练语料库的整理和筛选"></a>大规模训练语料库的整理和筛选</h3><blockquote><p>训练数据涵盖了截至2023年底的知识。</p></blockquote><h4 id="网页数据清理"><a href="#网页数据清理" class="headerlink" title="网页数据清理"></a><strong>网页数据清理</strong></h4><ul><li><p><strong>内容获取：</strong></p><ul><li><p><strong>内容过滤：</strong>&#x5BF9;包含有害内容（包括成人内容）的域名进行筛选过滤</p></li><li><p><strong>文本提取：</strong>&#x79FB;除html中的特殊标记提取纯文本内容，并且发现md格式的标记也对模型性能有害，所以也md标记进行移除</p></li></ul></li><li><p><strong>内容过滤：</strong></p><ul><li><p><strong>数据去重：</strong></p><ul><li><p>url级：依据url去重，只保留最新的；</p></li><li><p>文档级：依据文档的MinHash移除近似内容；</p></li><li><p>行级：执行类似ccNet的高强度逐行去重。在每3000万文档的桶中，移除出现次数超过6次的行。</p></li></ul></li><li><p><strong>启发式过滤</strong>：使用启发式方法，以移除额外的低质量文档、离群值和过多重复的文档。</p></li></ul></li><li><p><strong>内容优选与特殊内容提取：</strong></p><ul><li><p><strong>基于模型的质量过滤：</strong>&#x5E94;用了各种基于模型的质量分类器来筛选高质量token。</p></li><li><p><strong>代码和推理数据</strong>：由于代码和数学的token分布与自然语言有很大不同，其构建了提取代码和数学相关网页的领域特定工具。</p></li><li><p><strong>多语言数据</strong>：将文档分类为176种语言，并执行去重和低质量文档筛选</p></li></ul></li></ul><h4 id="数据比例设计"><a href="#数据比例设计" class="headerlink" title="数据比例设计"></a>数据比例设计</h4><ul><li><p><strong>知识分类：</strong></p><ul><li>开发了一种分类器进行分类，并使用这个分类器对过多的艺术类与娱乐类内容来减少采样</li></ul></li><li><p><strong>数据比例的扩展法则：</strong></p><ul><li>其在一个数据比例上训练了几个小模型，并使用这些数据比例来预测大模型的性能，然后测试不同比例下表现。</li></ul></li><li><p>最终的数据比例包含<strong>大约50%的常识token，25%的数学和推理token，17%的代码token，以及8%的多语言token</strong>。</p></li></ul><h4 id="退火训练数据"><a href="#退火训练数据" class="headerlink" title="退火训练数据"></a>退火训练数据</h4><ul><li><p>其在经验上发现，对<strong>少量高质量的代码和数学数据进行退火训练</strong>，可以<strong>提升预训练模型在关键基准上的表现</strong>。</p><ul><li><p>例如在GSM8k和MATH数据集上评估退火的效果。他们发现退火使得一个预训练的Llama 3 8B模型在GSM8k和MATH验证集上的性能分别提高了24.0%和6.4%。</p></li><li><p>然而在405B模型上的改进微乎其微，这表明Llama 3旗舰模型具有强大的上下文学习和推理能力，不需要特定领域的训练样本来获得强大的表现。</p></li></ul></li><li><p>此外他们也发现<strong>退火训练</strong>使模型能够<strong>判断领域特定小数据集的价值</strong>。</p><ul><li>其通过在40B token上线性退火训练一个训练到50%的Llama 3 8B模型的学习率到0来衡量这些数据集的价值。</li></ul></li></ul><h3 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h3><ul><li><p><strong>模型架构</strong>上还是标准的稠密Transformer架构，<strong>只有一些小改进</strong>：</p><ul><li><p><strong>分组查询注意力</strong>：使用分组查询注意力，包含8个键值头，以提高推理速度并在解码过程中减少键值缓存的大小。</p></li><li><p><strong>注意力掩码：</strong>&#x4F7F;用了一种注意力掩码，防止同一序列内不同文档之间的自注意力。在对非常长的序列进行持续预训练时，此变化很重要。</p></li><li><p><strong>词汇表扩展：</strong>&#x4F7F;用一个包含128K token的词汇表。该词汇表结合了来自tiktoken3 tokenizer的100K个token和28K个额外token，以更好地支持非英语语言。</p></li><li><p><strong>RoPE基本频率超参数：</strong>&#x5C06;RoPE基本频率超参数增加到500,000，这使模型能够更好地支持更长的上下文。</p></li></ul></li></ul><h3 id="Scaling-Laws设计-模型规模确定"><a href="#Scaling-Laws设计-模型规模确定" class="headerlink" title="Scaling-Laws设计&amp;模型规模确定"></a>Scaling-Laws设计&amp;模型规模确定</h3><h4 id="Scaling-Laws实验"><a href="#Scaling-Laws实验" class="headerlink" title="Scaling-Laws实验"></a>Scaling-Laws实验</h4><ul><li><p>Scaling-Laws实验如下所示：</p><ul><li>首先是得到不同计算量下最优表现的Tokens数：</li></ul><p><img src="/2025/12/13/llama3-pre-training-paper-note/image.png"></p><ul><li>然后对计算量C与最优Tokens数的关系N进行拟合，拟合的幂等关系为：$N(C)&#x3D;AC^\alpha$，最终拟合的结果如下</li></ul><p><img src="/2025/12/13/llama3-pre-training-paper-note/image-1.png"></p><ul><li>将Scaling-Laws外推到3.8*10^25 FLOPs，得到应该在16.55T token上训练一个402B参数模型。考虑到随着计算预算的增加，曲线在最小值附近变得平坦，所以最终选定的是405B参数模型</li></ul></li></ul><h4 id="下游任务Scaling-Laws"><a href="#下游任务Scaling-Laws" class="headerlink" title="下游任务Scaling-Laws"></a>下游任务Scaling-Laws</h4><ul><li><p>前期的尝试使用的是小模型，而旗舰模型的规模确认需要借助Scaling-Laws，但是其还存在一些问题需要解决：</p><ul><li><p>问题：Scaling-Laws主要是预测生成下一个Token的损失，而不是在评测集上的损失</p><ul><li>解决方案：建立计算最优模型在下游任务上的负对数可能性与训练FLOPs之间的关联性，如下图左所示。</li></ul></li><li><p>问题：Scaling-Laws可能噪声大且不可靠，因为它们基于小计算预算的预训练运行开发</p><ul><li>解决方案：利用Scaling-Laws模型和使用更高计算FLOPs训练的旧模型，建立下游任务上负对数可能性与任务准确度之间的关联性。如下图右所示，其添加了Llama 2模型。</li></ul></li></ul></li></ul><p><img src="/2025/12/13/llama3-pre-training-paper-note/image-2.png"></p><p>团队发现这种两步扩展法则预测在四个数量级的范围内外推时非常准确：<strong>它仅略微低估了Llama 3旗舰模型的最终表现。</strong></p><h3 id="基础设施、扩展和效率"><a href="#基础设施、扩展和效率" class="headerlink" title="基础设施、扩展和效率"></a>基础设施、扩展和效率</h3><h4 id="基础设施"><a href="#基础设施" class="headerlink" title="基础设施"></a>基础设施</h4><ul><li><p>Llama 3的训练迁移到了Meta的<strong>生产集群</strong>，从而提高了可靠性：</p><ul><li><p><strong>计算资源：</strong>&#x4C;lama 3 405B在多达16,000个H100 GPU上进行训练，每个服务器配备8个GPU和2个CPU。在一个服务器内，8个GPU通过NVLink连接。训练任务由Meta的全球规模训练调度器MAST调度。</p></li><li><p><strong>存储资源：</strong>&#x54;ectonic——Meta的通用分布式文件系统，用于构建Llama 3预训练的存储架构。它提供240PB的存储容量，来自7500台配备SSD的服务器，并支持2TB&#x2F;s的可持续吞吐量和7TB&#x2F;s的峰值吞吐量。</p></li><li><p><strong>网络资源：</strong>&#x4C;lama 3 405B使用基于Arista 7800和Minipack2开放计算项目机架交换机的融合以太网RDMA（RoCE）结构。Llama 3家族中的较小模型使用Nvidia Quantum2 InfiniBand结构进行训练。RoCE和InfiniBand集群都利用了400Gbps的GPU间互连</p><ul><li><p>网络架构采用三层Clos网络架构，连接了24,000个GPU。在底层一个Minipack2机架顶部（ToR）交换机连接2个服务器，每个服务器包含8个GPU。在中层有192个集群交换机连接了3,072个GPU，形成了一个pod。在顶层，同一数据中心建筑内的八个这样的pod通过聚合交换机连接，形成一个24,000个GPU的集群。</p></li><li><p>此外还有一些网络负载均衡、流量拥塞控制的技术</p></li></ul></li></ul></li></ul><h4 id="并行策略及优化"><a href="#并行策略及优化" class="headerlink" title="并行策略及优化"></a>并行策略及优化</h4><ul><li><p>模型训练采用了<strong>4D并行</strong>，如下图所示，其结合了张量并行（Tensor Parallelism, TP）、流水线并行（Pipeline Parallelism, PP）、上下文并行（Context Parallelism, CP）和完全分片的数据并行（FSDP）。其还做了一些优化：</p><p><img src="/2025/12/13/llama3-pre-training-paper-note/image-3.png"></p><ul><li><p><strong>流水线并行优化：</strong></p><ul><li>预训练通常需要灵活调整批次大小，但是流水线并行限制了批次大小必须是流水线阶段数量的倍数，所以其修改了流水线调度算法，如下图所示，其使用了交错流水线，层数是4，交错度是2，但是批次选取为了5。</li></ul><p><img src="/2025/12/13/llama3-pre-training-paper-note/image-4.png"></p><ul><li>此外其也发现流水线的第一阶段由于向量和预热微批次的原因需要更多的内存，第二阶段因为需要计算输出和损失所以需要更多的计算，所以分别从第一阶段和最后一阶段减少一个Transformer层。这意味着第一阶段的第一个模型分块只有向量，而最后一阶段的最后一个模型分块只有输出投影和损失计算。</li></ul></li><li><p><strong>上下文并行优化:</strong></p><ul><li><p>其利用上下文并行（CP）在扩展Llama 3的上下文长度时提高内存效率，并实现最长可达128,000个长度的序列训练，在CP中，模型沿序列维度进行分割。</p></li><li><p>不同于现有的CP实现以环形结构重叠通信和计算，Llama 3的CP实现采用了基于全收集的方法，在这种方法中，首先全收集键（K）和值（V）张量，然后计算本地查询（Q）张量块的注意力输出。这有利于支持不同类型的注意力掩码，并且由于使用了GQA，通信的K和V张量比Q张量小得多，因此注意力计算的时间复杂度比全收集大一个数量级，使全收集开销可以忽略不计。</p></li></ul></li><li><p><strong>网络感知的并行配置：</strong></p><ul><li><p>基于网络带宽和延迟的需求，团队将并行维度按顺序排列为[TP，CP，PP，DP]。DP（即FSDP）是最外层的并行，因为它可以通过异步预取分片模型权重和减少梯度来容忍更长的网络延迟。最内存的并行需要最高的网络带宽和最低的延迟，因此通常限制在同一服务器内。最外层的并行可以跨越多跳网络，并应能够容忍较高的网络延迟。</p></li><li><p>其还开发了一个内存消耗估算器和性能投影工具，帮助他们探索各种并行配置，投影整体训练性能并有效识别内存差距。</p></li></ul></li></ul></li></ul><h4 id="可靠性优化"><a href="#可靠性优化" class="headerlink" title="可靠性优化"></a>可靠性优化</h4><ul><li><p><strong>可靠性优化：</strong></p><ul><li><p>Llama 3在支持自动化集群维护，（如固件和Linux内核升级）的同时，仍实现了超过90%的有效训练时间。在预训练的54天快照期间，团队总共经历了466次作业中断。其中：</p><ul><li><p>47次是由于自动化维护操作（如固件升级）或操作员发起的操作（如配置或数据集更新）而计划的中断</p></li><li><p>其余419次是计划外的中断，具体见下表，大约78%的非计划中断归因于确认的硬件问题，如GPU或主机组件故障，或疑似与硬件相关的问题，如隐性数据损坏和未计划的单个主机维护事件。GPU问题是最大的类别，占所有非计划问题的58.7%。尽管故障数量众多，但在此期间仅需三次人工干预，其余问题由自动化处理。</p></li></ul></li></ul><p><img src="/2025/12/13/llama3-pre-training-paper-note/image-5.png"></p></li></ul><h3 id="预训练流程"><a href="#预训练流程" class="headerlink" title="预训练流程"></a>预训练流程</h3><p>预训练Llama 3 405B的流程包括三个主要阶段：（1）初始预训练，（2）长上下文预训练，和（3）退火。</p><h4 id="初始预训练"><a href="#初始预训练" class="headerlink" title="初始预训练"></a><strong>初始预训练</strong></h4><ul><li><p>使用AdamW优化器，最大学习率为 8*10^(-5) ，前8000步采用线性预热策略，然后在120万步内按余弦学习率计划逐步衰减至 8*10^(-7)。</p></li><li><p>批次大小调整：</p><ul><li><p>训练初期采用较小的批次大小(初始批次大小为4M tokens，序列长度为4096)以提高稳定性</p></li><li><p>训练了252M tokens后，将批次大小和序列长度加倍至8M和8192 tokens以提高效率</p></li><li><p>预训练了2.87T tokens后，再次将批次大小加倍至16M</p></li></ul></li><li><p>数据配比调整：</p><ul><li><p>在训练过程中增加了非英语数据的比例，以提高Llama 3的多语言性能</p></li><li><p>还增加了数学数据的比例，以提高模型的数学推理能力</p></li><li><p>在预训练的后期阶段，增加了更近期的网络数据以更新模型的知识截止点，同时减少了质量较低的数据子集。</p></li></ul></li></ul><h4 id="长上下文预训练"><a href="#长上下文预训练" class="headerlink" title="长上下文预训练"></a><strong>长上下文预训练</strong></h4><ul><li><p>因为自注意力层的计算量随序列长度呈二次增长，所以为了节省训练成本，没有在早期阶段使用长序列进行训练</p></li><li><p>其逐步增加支持的上下文长度，直到模型成功适应增加的上下文长度。通</p><ul><li>过评估模型在短上下文任务中的表现是否完全恢复，以及模型是否能够完美解决“大海捞针”任务来判断是否成功适应</li></ul></li><li><p>其分六个阶段拓展上下文窗口，从最初的8K上下文窗口开始，最终达到128K上下文窗口</p></li><li><p>长上下文预训练阶段使用了大约800B的训练tokens。</p></li></ul><h4 id="退火"><a href="#退火" class="headerlink" title="退火"></a><strong>退火</strong></h4><ul><li><p>在预训练的最后40M tokens期间，在保持128K tokens的上下文长度下，其线性地将学习率退火至0。</p></li><li><p>在退火阶段，增加了高质量的数据源比例。</p></li><li><p>最终在退火期间计算模型检查点的平均值（Polyak平均化），以生成最终的预训练模型。</p></li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><p>非常详细的技术报告，能够学到很实际的训练流程，但是“纸上得来终觉浅，绝知此事要躬行”，希望后面有机会深度参与了。</p></li><li><p>整体看完确实也拓展了眼界，在整个预训练过程中除了需要关注并行策略外，还需要关注数据构造、模型规模设计，以及训练流程中学习率、批次配置、数据配比的变化等。</p></li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/712251536">【LLM技术报告】《The Llama 3 Herd of Models》——Llama 3.1技术报告（精华版）</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>大模型</tag>
      
      <tag>Llama3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读】Reducing Activation Recomputation in Large Transformer Models</title>
    <link href="/2025/12/08/Reducing-Activation-Paper-Note/"/>
    <url>/2025/12/08/Reducing-Activation-Paper-Note/</url>
    
    <content type="html"><![CDATA[<ul><li><p><strong>链接：</strong>: <a href="https://arxiv.org/pdf/2205.05198.pdf">https://arxiv.org/pdf/2205.05198.pdf</a></p></li><li><p><strong>发表会议：MLSYS’22(CCF-B)&#x20;</strong></p></li><li><p><strong>团队：NVIDIA</strong></p></li></ul><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在论文发表的2022年看来，一些研究在Transformer大模型训练中会采用激活重计算而不是存储的方式来减少激活值所占存储，但是其问题在于会引入很多冗余计算的开销，导致训练效率降低。故本文提出了<strong>序列并行</strong>和<strong>选择性激活重计算</strong>方法，并结合Tensor并行的方式大幅削减了激活重计算带来的开销。</p><h2 id="方法思路"><a href="#方法思路" class="headerlink" title="方法思路"></a>方法思路</h2><h3 id="理论分析激活值"><a href="#理论分析激活值" class="headerlink" title="理论分析激活值"></a>理论分析激活值</h3><p>其理论计算了主要的激活量（注意下述默认各个元素采用16位也就是2字节存储，公式表示的也都是占用的元素字节数），变量名如下：</p><p><img src="/2025/12/08/Reducing-Activation-Paper-Note/image.png"></p><p>Transformer大模型结构如下：</p><p><img src="/2025/12/08/Reducing-Activation-Paper-Note/image-1.png"></p><ul><li><p>Attention模块占用了：11sbh + 5as^2b</p></li><li><p>MLP模块占用了：19sbh</p></li><li><p>Layer norm模块：4sbh</p></li><li><p>上述3者组合得到一层Transformer占用的激活大小：sbh(34+5*as&#x2F;h)</p></li></ul><h4 id="Tensor并行"><a href="#Tensor并行" class="headerlink" title="Tensor并行"></a>Tensor并行</h4><p><img src="/2025/12/08/Reducing-Activation-Paper-Note/image-2.png"></p><ul><li>如果采用了t路Tensor并行，那么会对如上图所示的Attention+MLP模块进行并行化，将每层激活值减少为sbh(10+24&#x2F;t+5*as&#x2F;ht)</li></ul><h4 id="序列并行"><a href="#序列并行" class="headerlink" title="序列并行"></a>序列并行</h4><ul><li>Tensor并行并不能对LayerNorm和Dropout进行并行化，所以本文还引入了序列并行，以序列维度进行切分，在t维序列并行下，每层激活值减少为：sbh(10&#x2F;t+24&#x2F;t+5*as&#x2F;ht)&#x3D;sbh&#x2F;t(34+5*as&#x2F;h)</li></ul><h4 id="流水线并行"><a href="#流水线并行" class="headerlink" title="流水线并行"></a>流水线并行</h4><ul><li><p>在流水线切分下，Transformer的L层被划分为L&#x2F;p组，p是流水线并行的大小。</p></li><li><p>对于1F1B的流水线调度策略，每级流水线在第一阶段需要保留L&#x2F;p*p&#x3D;L层激活，故激活值大小为：sbhL&#x2F;t(34+5*as&#x2F;h)</p></li><li><p>注意不同调度策略下激活值大小会有所不同</p></li></ul><h4 id="总激活值"><a href="#总激活值" class="headerlink" title="总激活值"></a>总激活值</h4><ul><li>上述在考虑时没有考虑输入嵌入、最后一层层归一化和输出层所需的激活，其占用的额外激活为：</li></ul><p><img src="/2025/12/08/Reducing-Activation-Paper-Note/image-3.png"></p><ul><li>由于其相比原其他相加的值sbhL&#x2F;t(34+5*as&#x2F;h)往往比较小，所以可忽略不计</li></ul><h3 id="选择性激活"><a href="#选择性激活" class="headerlink" title="选择性激活"></a>选择性激活</h3><ul><li><p>在上述并行优化下得到的激活值大小以及很大，故还需要进一步优化</p></li><li><p>完全激活重计算（即只保留输入激活）可以将总内存需求减少到2sbhL，如果进一步在张量并行等级优化，可以将其优化到2sbhL&#x2F;t。但是这会导致过度的冗余计算，并且张量并行下会增加通信开销。</p></li><li><p>为了平衡计算与内存的开销，其核心思想是仅对占用了大量内存但是重计算方便的Transformer层进行检查点设置。</p></li><li><p>对于公式中的34其与大型矩阵计算更相关，对于5*as&#x2F;h主要指的是QK^T矩阵乘法、softmax、softmax dropout和V上的注意力，这些操作有较大的输入和激活，但是其运算量低。带入GPT 3大模型的参数，计算得到5*as&#x2F;h&#x3D;80，大于34，占更大的部分。所以其倾向于将5*as&#x2F;h相关部分的激活去掉以快速重计算，而将34相关计算的激活保留，避免缓慢的重计算。</p></li></ul><h2 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h2><ul><li>如下图所示，其验证了Sequence 并行与选择性激活区激活值的优化效果，以及两者叠加的优化效果。</li></ul><p><img src="/2025/12/08/Reducing-Activation-Paper-Note/image-4.png"></p><ul><li>如下图所示，也实际验证了其对模型训练过程中显存占用的优化情况</li></ul><p><img src="/2025/12/08/Reducing-Activation-Paper-Note/image-5.png"></p><ul><li>相比与不重计算的耗时只增加了约4%。</li></ul><p><img src="/2025/12/08/Reducing-Activation-Paper-Note/image-6.png"></p><ul><li>相比于完全重计算，吞吐也增加了30%左右。</li></ul><p><img src="/2025/12/08/Reducing-Activation-Paper-Note/image-7.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><p>对激活值理论计算的部分很精彩。</p></li><li><p>引入了Sequence并行，这在后续也成为了一大标准。</p></li><li><p>选择性激活重计算很朴素但确实很好用。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读】Megatron-LM论文阅读</title>
    <link href="/2025/12/07/Megatron-LM-paper-note/"/>
    <url>/2025/12/07/Megatron-LM-paper-note/</url>
    
    <content type="html"><![CDATA[<h1 id="Megatron-LM-Training-Multi-Billion-Parameter-Language-Models-Using-Model-Parallelism"><a href="#Megatron-LM-Training-Multi-Billion-Parameter-Language-Models-Using-Model-Parallelism" class="headerlink" title="Megatron-LM: Training Multi-Billion Parameter Language Models Using  Model Parallelism"></a>Megatron-LM: Training Multi-Billion Parameter Language Models Using  Model Parallelism</h1><p><a href="https://arxiv.org/abs/1909.08053">https://arxiv.org/abs/1909.08053</a></p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在2019年就出现了大语言模型规模越来越大，单个GPU的显存难以放下的情况，过去已有方法提出了一些关于模型并行的方法，但是其往往需要重写模型，依赖于自定义编译器和框架。</p><h2 id="方法思路"><a href="#方法思路" class="headerlink" title="方法思路"></a>方法思路</h2><p>本文提出了一种针对transfer模型的层内模型并行的方法，完全依靠原生pytorch即可实现。具体而言针对以下3个位置进行了优化。</p><p><img src="/2025/12/07/Megatron-LM-paper-note/image-9.png"></p><h3 id="MLP层模型并行优化"><a href="#MLP层模型并行优化" class="headerlink" title="MLP层模型并行优化"></a>MLP层模型并行优化</h3><p><img src="/2025/12/07/Megatron-LM-paper-note/image-10.png"></p><ul><li><p>如上图所示，MLP层往往是由Gemm+Gelu+Gemm组成，为了不在Gelu前多一次聚集操作，所以采用先进行列并行再进行行并行的方式切分，只需要最后Dropout前再All reduce一次即可。</p></li><li><p>注意反向传播会在图中f的位置进行一次All Reduce</p></li></ul><h3 id="Attention层模型优化"><a href="#Attention层模型优化" class="headerlink" title="Attention层模型优化"></a>Attention层模型优化</h3><p><img src="/2025/12/07/Megatron-LM-paper-note/image-11.png"></p><ul><li><p>多头注意力机制天然形成了列并行，一个注意力头为划分单位。</p></li><li><p>一般的多头注意力在各个头计算完后要拼接，然后再进行一次Gemm的操作，而借助原本就是列并行，所以多头注意力可以不需要拼接，直接与行并行切分的权重进行计算，然后在Gropout前再All reduce一次即可。</p></li><li><p>注意反向传播会在图中f的位置进行一次All Reduce。</p></li></ul><h3 id="Embedding层模型并行优化"><a href="#Embedding层模型并行优化" class="headerlink" title="Embedding层模型并行优化"></a>Embedding层模型并行优化</h3><ul><li><p>往往输入和输出会共用参数，而vocab size一般很大，所以有切分的必要。</p></li><li><p>对该参数采用列并行的方式，每个GPU拥有一定范围内的vocab内容：</p><ul><li><p>对于输入：</p><ul><li>如果这个token的id在GPU的范围内就给它对应列的值，否则就设0，然后对各个GPU进行All Reduce得到正确结果</li></ul></li><li><p>对于输出：</p><ul><li>正常列并行是与列并行的参数计算后再All Gather得到完整的logits，再计算交叉熵损失，如下：</li></ul><p><img src="/2025/12/07/Megatron-LM-paper-note/image.png"></p><ul><li>但是由于这里All Gather的数据量达到了Batch Size * Vocab Size * Mode Size，所以就改成每个GPU计算自己对应列的各个logits后再计算交叉熵所需要的各个logit的指数和，以及如果target在自己的范围内，就返回对应的logits，如此就把交换的信息量降低为了Batch Size * Vocab Size 级别。</li></ul></li></ul></li></ul><h2 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h2><p><img src="/2025/12/07/Megatron-LM-paper-note/image-1.png"></p><p>对于8.3B的模型在512卡上进行数据并行+模型并行，最终实现了77%的线性缩放，并且通过训练更大的模型也证明了模型规模的增大带来的模型能力的提升。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>主要是围绕了Tensor并行来展开，针对Transformer模型做了针对性的优化，形成了后面的共识，也做了一系列在当时来看的大型实验。此外开源工具Megatron-LM做的很好，现在也依然是主流的预训练框架。</p><h1 id="Efficient-Large-Scale-Language-Model-Training-on-GPU-Clusters-Using-Megatron-LM"><a href="#Efficient-Large-Scale-Language-Model-Training-on-GPU-Clusters-Using-Megatron-LM" class="headerlink" title="Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"></a>Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</h1><p><a href="https://arxiv.org/abs/2104.04473">https://arxiv.org/abs/2104.04473</a></p><h2 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h2><ul><li><p>21年时，大语言模型的发展已经使得多GPU服务器都难以适应训练大模型</p></li><li><p>由于训练数据的增多，需要更多GPU并行训练来降低训练时间，而单纯的依靠数据并行会导致：</p><ul><li><p>Batch size太小导致GPU利用率下降，需要增加通信成本</p></li><li><p>Batch size大小受到设备数量限制</p></li></ul></li><li><p>现有的一些其他方法提出了模型并行的相关技术：</p><ul><li><p>张量并行的问题：</p><ul><li><p>All Reduce若通过服务器间链路会导致通信耗时大</p></li><li><p>高度的张量并行会导致多个小矩阵相乘，降低利用率</p></li></ul></li><li><p>流水线并行的问题：</p><ul><li>为了严格保持优化器语义，每个batch结束时，需要跨服务器进行梯度同步，导致长时间的等待，即气泡，而气泡含量往往与Micro batch size&#x2F;pipeline size负相关，所以往往会设置很大的batch size以提高效率</li></ul></li></ul></li><li><p>用户可以单独使用或者组合各个并行技术进行训练，但是组合是有难度的，所以就带来了一个经典问题：</p><ul><li><p><em>How should parallelism techniques be combined to maximize the training throughput of large models given a batch size while retaining strict optimizer semantics?</em></p></li><li><p>应该如何结合并行技术来最大化给定批量大小的大型模型的训练吞吐量，同时保留严格的优化器语义？</p></li></ul></li><li><p>故本文展示了如何高效组合数据并行、张量并行与流水线并行以拓展训练大模型。</p></li></ul><h2 id="方法思路-1"><a href="#方法思路-1" class="headerlink" title="方法思路"></a>方法思路</h2><h3 id="张量并行与流水线并行结合"><a href="#张量并行与流水线并行结合" class="headerlink" title="张量并行与流水线并行结合"></a>张量并行与流水线并行结合</h3><p><img src="/2025/12/07/Megatron-LM-paper-note/image-2.png"></p><ul><li><p>张量并行如上一篇论文所述，在MHA与MLP中进行切分，注意张量并行必须在服务器内才高效</p></li><li><p>流水线并行在多个Transformer层中进行切分</p></li></ul><h3 id="流水线并行调度优化"><a href="#流水线并行调度优化" class="headerlink" title="流水线并行调度优化"></a>流水线并行调度优化</h3><ul><li><p>最简单的AFAB调度如下，其气泡含量为：(p-1)&#x2F;m，其中p是流水线条数，m是micro batch size。要想减少气泡最简单的就是增加m，但是由于需要保持m个micro batch激活，会导致显存占用过大</p><p><img src="/2025/12/07/Megatron-LM-paper-note/image-3.png"></p></li><li><p>更进一步的优化是使用1F1B，如下所示，虽然气泡含量依旧是(p-1)&#x2F;m，但是只需要保留p个micro batch的激活，可以更好地增加m</p><p><img src="/2025/12/07/Megatron-LM-paper-note/image-4.png"></p></li><li><p>再进一步其提出了交错的方式，上面都是一个device保留连续的一段流水线阶段，例如，假设总共有16层，上述方法就是Device 1保留1~4层，Device 2保留5~8层，以此类推。但是实际上可以交错一下，Device 1保留1~2层和9~10层，Device 2保留3~4层和11~12层，以此类推。这使得气泡含量降为(p-1)&#x2F;(m*v)，其中v是交错的数量，如下图所示。而注意随之而来的代价就是有v倍的通信量。</p><p><img src="/2025/12/07/Megatron-LM-paper-note/image-5.png"></p></li></ul><h3 id="实现过程中的通信优化"><a href="#实现过程中的通信优化" class="headerlink" title="实现过程中的通信优化"></a>实现过程中的通信优化</h3><p><img src="/2025/12/07/Megatron-LM-paper-note/image-6.png"></p><ul><li>流水线并行需要点对点之间交换信息量，对于Tensor并行叠加流水线并行的情况，各个pipeline之间传递的信息是有冗余的，所以可以改为先pipeline两两之间传递各自的部分，然后再在一个Tensor并行组内通过All Gather进行聚合。</li></ul><h3 id="实现过程中的计算优化"><a href="#实现过程中的计算优化" class="headerlink" title="实现过程中的计算优化"></a>实现过程中的计算优化</h3><ul><li><p>将数据布局从[B，s，a，h]更改为[s，B，a，h]，其中B，s，a和h分别为batch，sequence，attention-head， hidden-size</p></li><li><p>为element-wise算子如bias + GeLU和bias + dropout + add创建了融合算子</p></li><li><p>创建了两个自定义内核，以实现scale，mask和softmax的融合</p></li></ul><h2 id="实验效果-1"><a href="#实验效果-1" class="headerlink" title="实验效果"></a>实验效果</h2><p><img src="/2025/12/07/Megatron-LM-paper-note/image-7.png"></p><ul><li>利用3D并行实验最大在3072个GPU上训练了1000B的模型实现了最高52%的理论计算峰值</li></ul><p><img src="/2025/12/07/Megatron-LM-paper-note/image-8.png"></p><ul><li>还将3D并行与ZERO技术进行了比较，验证了其在更大规模的GPU场景上的良好的拓展性</li></ul><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><ul><li>很全面的探究了3D并行的相关事宜，不仅有理论分析也有详尽的实验，相关的许多见解现在依然是标准。</li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>Megatron-LM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>LLM</tag>
      
      <tag>Megatron-LM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【k8s APIServer 源码阅读（一）】-对象缓存</title>
    <link href="/2025/08/24/k8s%20APIServer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%90%E4%B8%80%E3%80%91-%E5%AF%B9%E8%B1%A1%E7%BC%93%E5%AD%98/"/>
    <url>/2025/08/24/k8s%20APIServer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%90%E4%B8%80%E3%80%91-%E5%AF%B9%E8%B1%A1%E7%BC%93%E5%AD%98/</url>
    
    <content type="html"><![CDATA[<p><img src="/2025/08/24/k8s%20APIServer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%90%E4%B8%80%E3%80%91-%E5%AF%B9%E8%B1%A1%E7%BC%93%E5%AD%98/image.png"></p><h1 id="Cacher结构"><a href="#Cacher结构" class="headerlink" title="Cacher结构"></a>Cacher结构</h1><p>cacher的结构如下，已将注释翻译成了中文，关键的内容有</p><ul><li><p><code>incoming chan watchCacheEvent</code>：事件分发的管道</p><ul><li>这里事件的结构如下：</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// watchCacheEvent is a single &quot;watch event&quot; that is send to users of</span><br><span class="hljs-comment">// watchCache. Additionally to a typical &quot;watch.Event&quot; it contains</span><br><span class="hljs-comment">// the previous value of the object to enable proper filtering in the</span><br><span class="hljs-comment">// upper layers.</span><br><span class="hljs-keyword">type</span> watchCacheEvent <span class="hljs-keyword">struct</span> &#123;<br>    Type            watch.EventType<br>    Object          runtime.Object<br>    ObjLabels       labels.Set<br>    ObjFields       fields.Set<br>    PrevObject      runtime.Object<br>    PrevObjLabels   labels.Set<br>    PrevObjFields   fields.Set<br>    Key             <span class="hljs-type">string</span><br>    ResourceVersion <span class="hljs-type">uint64</span><br>    RecordTime      time.Time<br>&#125;<br><br><span class="hljs-comment">// EventType defines the possible types of events.</span><br><span class="hljs-keyword">type</span> EventType <span class="hljs-type">string</span><br><br><span class="hljs-keyword">const</span> (<br>    Added    EventType = <span class="hljs-string">&quot;ADDED&quot;</span><br>    Modified EventType = <span class="hljs-string">&quot;MODIFIED&quot;</span><br>    Deleted  EventType = <span class="hljs-string">&quot;DELETED&quot;</span><br>    Bookmark EventType = <span class="hljs-string">&quot;BOOKMARK&quot;</span><br>    Error    EventType = <span class="hljs-string">&quot;ERROR&quot;</span><br>)<br></code></pre></td></tr></table></figure></li><li><p><code>resourcePrefix string</code>：存储API资源的路径前缀（如<code>/api/v1/pods</code>或<code>/apis/apps/v1/deployments</code>），确保不同资源类型的缓存相互独立</p></li><li><p><code>storage storage.Interface</code>：对存储接口的抽象，该接口由具体存储实现（如etcd3）提供，上层组件（如Cacher）通过此接口与存储交互：</p><ul><li><p>Cacher使用<code>Watch</code>方法监听底层存储变化</p></li><li><p>API服务器通过<code>Get</code>&#x2F;<code>GetList</code>满足LIST请求</p></li><li><p>控制器通过<code>GuaranteedUpdate</code>实现资源的安全更新</p></li></ul></li><li><p><code>watchCache *watchCache</code>：一个维护了当前kind的所有的资源变动事件的滑动窗口</p></li><li><p><code>reflector  *cache.Reflector</code>：回调函数，list并watch etcd 并将事件和资源存到watchCache这个滑动窗口中</p></li><li><p><code>versioner storage.Versioner</code>：处理资源的version相关信息</p></li><li><p><code>watchersBuffer []*cacheWatcher</code>：所有对该资源有watch需求的连接</p></li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Cacher负责为给定资源提供WATCH和LIST请求服务</span><br><span class="hljs-comment">// 并基于底层存储内容在后台更新其缓存。</span><br><span class="hljs-comment">// Cacher实现了storage.Interface（尽管大多数调用只是委托给底层存储）</span><br><span class="hljs-keyword">type</span> Cacher <span class="hljs-keyword">struct</span> &#123;<br>        <span class="hljs-comment">// HighWaterMarks用于性能调试</span><br>        <span class="hljs-comment">// 重要：由于HighWaterMark使用sync/atomic，它必须位于结构体顶部，以解决32位平台上的bug</span><br>        <span class="hljs-comment">// 参见：https://golang.org/pkg/sync/atomic/了解更多信息</span><br>        incomingHWM storage.HighWaterMark<br>        <span class="hljs-comment">// 应分发给观察者的传入事件</span><br>        incoming <span class="hljs-keyword">chan</span> watchCacheEvent<br><br>        resourcePrefix <span class="hljs-type">string</span><br><br>        sync.RWMutex<br><br>        <span class="hljs-comment">// 在访问缓存器的缓存之前，等待ready状态变为ok</span><br>        <span class="hljs-comment">// 这对于防止用户访问未初始化或正在重新填充的结构是必要的</span><br>        <span class="hljs-comment">// 当缓存器暂停或停止时，ready需要设置为false</span><br>        <span class="hljs-comment">// 当初始化后缓存器准备好使用时，ready需要设置为true</span><br>        ready *ready<br><br>        <span class="hljs-comment">// 底层storage.Interface</span><br>        storage storage.Interface<br><br>        <span class="hljs-comment">// 底层缓存中对象的预期类型</span><br>        objectType reflect.Type<br>        <span class="hljs-comment">// 用于区分*unstructured.Unstructured（自定义资源定义）</span><br>groupResource schema.GroupResource<br><br>        <span class="hljs-comment">// 对象最近变更的“滑动窗口”和当前状态</span><br>        watchCache *watchCache<br>        reflector *cache.Reflector<br><br>        <span class="hljs-comment">// Versioner用于处理资源版本</span><br>        versioner storage.Versioner<br><br>        <span class="hljs-comment">// newFunc是创建新空对象的函数，用于存储Type类型的对象</span><br>        newFunc <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> runtime.Object<br><br>        <span class="hljs-comment">// newListFunc是创建新空列表用于存储Type类型对象</span><br>        newListFunc <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> runtime.Object<br><br>        <span class="hljs-comment">// indexedTrigger用于优化需要处理传入事件的观察者数量</span><br>        indexedTrigger *indexedTriggerFunc<br>        <span class="hljs-comment">// watchers将观察者感兴趣的触发函数值映射到观察者</span><br>        watcherIdx <span class="hljs-type">int</span><br>        watchers indexedWatchers<br><br>        <span class="hljs-comment">// 定义了分发事件时可用于等待未就绪观察者</span><br>        <span class="hljs-comment">// 然后关闭它们的时间预算</span><br>        dispatchTimeoutBudget timeBudget<br><br>        <span class="hljs-comment">// 处理优雅终止</span><br>        stopLock sync.RWMutex<br>        stopped <span class="hljs-type">bool</span><br>        stopCh <span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;<br>        stopWg sync.WaitGroup<br><br>        clock clock.Clock<br>        <span class="hljs-comment">// timer用于避免底层观察者中不必要的分配</span><br>        timer *time.Timer<br><br>        <span class="hljs-comment">// dispatching确定当前是否有任何事件正在分发中</span><br>        dispatching <span class="hljs-type">bool</span><br>        <span class="hljs-comment">// watchersBuffer是当前可能对当前分发事件感兴趣的观察者列表</span><br>        watchersBuffer []*cacheWatcher<br>        <span class="hljs-comment">// blockedWatchers是缓冲区当前已满的观察者列表</span><br>        blockedWatchers []*cacheWatcher<br>        <span class="hljs-comment">// watchersToStop当前分发期间应该停止但推迟到</span><br>        <span class="hljs-comment">// 当前事件分发结束才停止以避免与观察者关闭通道竞争</span><br>        watchersToStop []*cacheWatcher<br>        <span class="hljs-comment">// 维护超时队列以在观察者超时前发送书签事件</span><br>        <span class="hljs-comment">// 注意：访问此字段时必须受Cacher.lock保护</span><br>        bookmarkWatchers *watcherBookmarkTimeBuckets<br>        <span class="hljs-comment">// expiredBookmarkWatchers是已过期并需要安排下一个书签事件</span><br>        expiredBookmarkWatchers []*cacheWatcher<br>&#125;<br><br></code></pre></td></tr></table></figure><h1 id="Cache初始化"><a href="#Cache初始化" class="headerlink" title="Cache初始化"></a>Cache初始化</h1><p>初始化的流程如下：</p><ol><li><p>&#x20;配置验证与前置检查</p></li><li><p>索引触发器初始化（支持资源索引功能）</p></li><li><p>初始化Cacher，注意这里可以看到incoming的事件管道的容量只有100</p></li><li><p>给Cacher创建watchCache与reflector</p></li><li><p>后台触发<code>progressRequester</code>协程定期发送watch进度更新（bookmark事件），防止连接超时，并向客户端发送最新的资源版本信息。</p></li><li><p>后台触发<code>cacher.dispatchEvents()</code>协程处理<code>incoming</code>通道中的事件并推送给WATCH客户端</p></li><li><p><code>后台触发wait.Until</code>以1秒间隔执行<code>startCaching</code>，确保缓存持续同步</p></li></ol><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// NewCacherFromConfig creates a new Cacher responsible for servicing WATCH and LIST requests from</span><br><span class="hljs-comment">// its internal cache and updating its cache in the background based on the</span><br><span class="hljs-comment">// given configuration.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">NewCacherFromConfig</span><span class="hljs-params">(config Config)</span></span> (*Cacher, <span class="hljs-type">error</span>) &#123;<br>    stopCh := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;)<br>    obj := config.NewFunc()<br>    <span class="hljs-comment">// 1. 配置验证与前置检查</span><br>    <span class="hljs-keyword">if</span> err := runtime.CheckCodec(config.Codec, obj); err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, fmt.Errorf(<span class="hljs-string">&quot;storage codec doesn&#x27;t seem to match given type: %v&quot;</span>, err)<br>    &#125;<br><br>    <span class="hljs-comment">// 2. 索引触发器初始化（支持资源索引功能）</span><br>    <span class="hljs-keyword">var</span> indexedTrigger *indexedTriggerFunc<br>    <span class="hljs-keyword">if</span> config.IndexerFuncs != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// ... 索引器函数处理逻辑 ...</span><br>    &#125;<br><br>    <span class="hljs-comment">// Cacher结构体初始化</span><br>    cacher := &amp;Cacher&#123;<br>        resourcePrefix: config.ResourcePrefix,<br>        ready:          newReady(config.Clock),<br>        storage:        config.Storage,<br>        objectType:     reflect.TypeOf(obj),<br>        groupResource:  config.GroupResource,<br>        versioner:      config.Versioner,<br>        newFunc:        config.NewFunc,<br>        newListFunc:    config.NewListFunc,<br>        indexedTrigger: indexedTrigger,<br>        watcherIdx:     <span class="hljs-number">0</span>,<br>        watchers: indexedWatchers&#123;<br>            allWatchers:   <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[namespacedName]watchersMap),<br>            valueWatchers: <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]watchersMap),<br>        &#125;,<br>        incoming:              <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> watchCacheEvent, <span class="hljs-number">100</span>),<br>        dispatchTimeoutBudget: newTimeBudget(),<br>        stopCh:           stopCh,<br>        clock:            config.Clock,<br>        timer:            time.NewTimer(time.Duration(<span class="hljs-number">0</span>)),<br>        bookmarkWatchers: newTimeBucketWatchers(config.Clock, defaultBookmarkFrequency),<br>    &#125;<br><br>    <span class="hljs-comment">// 3. 缓存与同步机制初始化</span><br>    progressRequester := progress.NewConditionalProgressRequester(config.Storage.RequestWatchProgress)<br>    watchCache := newWatchCache(<br>        config.KeyFunc, cacher.processEvent, config.GetAttrsFunc, config.Versioner, <br>        config.Indexers, config.Clock,<br>    )<br>    listerWatcher := NewListerWatcher(config.Storage, config.ResourcePrefix, config.NewListFunc)<br>    reflector := cache.NewNamedReflector(<span class="hljs-string">&quot;storage/cacher.go:&quot;</span>+config.ResourcePrefix,<br>        listerWatcher, obj, watchCache, <span class="hljs-number">0</span>)<br>    reflector.WatchListPageSize = storageWatchListPageSize<br>    reflector.MaxInternalErrorRetryDuration = time.Second * <span class="hljs-number">30</span><br><br>    <span class="hljs-comment">// 4. 后台协程启动</span><br>    <span class="hljs-keyword">go</span> cacher.dispatchEvents()<br>    <span class="hljs-keyword">go</span> progressRequester.Run(stopCh)<br><br>    cacher.stopWg.Add(<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>        <span class="hljs-keyword">defer</span> cacher.stopWg.Done()<br>        <span class="hljs-keyword">defer</span> cacher.terminateAllWatchers()<br>        wait.Until(<br>            <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123; <span class="hljs-keyword">if</span> !cacher.isStopped() &#123; cacher.startCaching(stopCh) &#125; &#125;,<br>            time.Second, stopCh,<br>        )<br>    &#125;()<br><br>    <span class="hljs-keyword">return</span> cacher, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><h1 id="watchCache"><a href="#watchCache" class="headerlink" title="watchCache"></a>watchCache</h1><p>watchCache的结构如下，这里核心是使用了一个<code>cache []*watchCacheEvent</code>这个循环数组来缓存历史事件。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// watchCache实现了Store接口</span><br><span class="hljs-comment">// 但它依赖元素实现runtime.Object接口</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// watchCache是一个&quot;滑动窗口&quot;（具有有限容量），用于存储从watch中观察到的对象</span><br><span class="hljs-keyword">type</span> watchCache <span class="hljs-keyword">struct</span> &#123;<br>    sync.RWMutex<br><br>    <span class="hljs-comment">// 列表等待足够新资源版本的条件变量</span><br>    cond *sync.Cond<br><br>    <span class="hljs-comment">// 历史窗口的最大容量</span><br>    capacity <span class="hljs-type">int</span><br><br>    <span class="hljs-comment">// 事件缓存的动态容量上限</span><br>    upperBoundCapacity <span class="hljs-type">int</span><br><br>    <span class="hljs-comment">// 事件缓存的动态容量下限</span><br>    lowerBoundCapacity <span class="hljs-type">int</span><br><br>    <span class="hljs-comment">// keyFunc用于为给定对象获取底层存储中的键</span><br>    keyFunc <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(runtime.Object)</span></span> (<span class="hljs-type">string</span>, <span class="hljs-type">error</span>)<br><br>    <span class="hljs-comment">// getAttrsFunc用于获取对象的标签和字段</span><br>    getAttrsFunc <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(runtime.Object)</span></span> (labels.Set, fields.Set, <span class="hljs-type">error</span>)<br><br>    <span class="hljs-comment">// cache用作循环缓冲区 - 其&quot;当前&quot;内容存储在[start_index%capacity, end_index%capacity)范围内</span><br>    <span class="hljs-comment">// 因此&quot;当前&quot;内容正好有end_index-start_index个项目</span><br>    cache      []*watchCacheEvent<br>    startIndex <span class="hljs-type">int</span><br>    endIndex   <span class="hljs-type">int</span><br>    <span class="hljs-comment">// removedEventSinceRelist表示自上次重新列表以来，是否有任何事件已从`cache`循环缓冲区中删除</span><br>    removedEventSinceRelist <span class="hljs-type">bool</span><br><br>    <span class="hljs-comment">// store用于支持从&quot;缓存历史末尾&quot;（即最新缓存的watch事件之后的时刻）开始的LIST操作</span><br>    <span class="hljs-comment">// 这对于允许客户端从&quot;现在&quot;开始watch是必要的</span><br>    <span class="hljs-comment">// 注意：我们假设&lt;store&gt;是线程安全的</span><br>    store storeIndexer<br><br>    <span class="hljs-comment">// watchCache传播到的资源版本</span><br>    resourceVersion <span class="hljs-type">uint64</span><br><br>    <span class="hljs-comment">// 最后一次列表结果的资源版本（通过Replace()方法填充）</span><br>    listResourceVersion <span class="hljs-type">uint64</span><br><br>    <span class="hljs-comment">// 此处理程序在每次成功的Replace()方法结束时运行</span><br>    onReplace <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span><br><br>    <span class="hljs-comment">// 此处理程序在每次Add/Update/Delete方法结束时运行</span><br>    <span class="hljs-comment">// 此外还获取对象的先前值</span><br>    eventHandler <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(*watchCacheEvent)</span></span><br><br>    <span class="hljs-comment">// 用于测试超时</span><br>    clock clock.Clock<br><br>    <span class="hljs-comment">// eventFreshDuration定义watch缓存将存储的最小watch历史时长</span><br>    eventFreshDuration time.Duration<br><br>    <span class="hljs-comment">// 底层的storage.Versioner</span><br>    versioner storage.Versioner<br><br>    <span class="hljs-comment">// cacher的组资源</span><br>    groupResource schema.GroupResource<br><br>    <span class="hljs-comment">// 用于测试缓存间隔失效</span><br>    indexValidator indexValidator<br><br>    <span class="hljs-comment">// 如果有请求等待watch变为最新，则请求进度通知</span><br>    waitingUntilFresh *progress.ConditionalProgressRequester<br><br>    <span class="hljs-comment">// 存储orderedLister的先前快照，以允许从先前的修订版本提供请求</span><br>    snapshots Snapshotter<br>&#125;<br></code></pre></td></tr></table></figure><p>此外还提供了<code>Add</code>、<code>Update</code>、<code>Delete</code>接口来给reflector暴露更改的接口，注意这里的函数只是进行了简单的包装，即将事件类型还有obj包装成了<code>watch.Event</code>，然后就使用了<code>processEvent</code>来处理。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Add takes runtime.Object as an argument.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(w *watchCache)</span></span> Add(obj <span class="hljs-keyword">interface</span>&#123;&#125;) <span class="hljs-type">error</span> &#123;<br>    object, resourceVersion, err := w.objectToVersionedRuntimeObject(obj)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>       <span class="hljs-keyword">return</span> err<br>    &#125;<br>    event := watch.Event&#123;Type: watch.Added, Object: object&#125;<br><br>    f := <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(elem *storeElement)</span></span> <span class="hljs-type">error</span> &#123; <span class="hljs-keyword">return</span> w.store.Add(elem) &#125;<br>    <span class="hljs-keyword">return</span> w.processEvent(event, resourceVersion, f)<br>&#125;<br><br><span class="hljs-comment">// Update takes runtime.Object as an argument.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(w *watchCache)</span></span> Update(obj <span class="hljs-keyword">interface</span>&#123;&#125;) <span class="hljs-type">error</span> &#123;<br>    object, resourceVersion, err := w.objectToVersionedRuntimeObject(obj)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>       <span class="hljs-keyword">return</span> err<br>    &#125;<br>    event := watch.Event&#123;Type: watch.Modified, Object: object&#125;<br><br>    f := <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(elem *storeElement)</span></span> <span class="hljs-type">error</span> &#123; <span class="hljs-keyword">return</span> w.store.Update(elem) &#125;<br>    <span class="hljs-keyword">return</span> w.processEvent(event, resourceVersion, f)<br>&#125;<br><br><span class="hljs-comment">// Delete takes runtime.Object as an argument.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(w *watchCache)</span></span> Delete(obj <span class="hljs-keyword">interface</span>&#123;&#125;) <span class="hljs-type">error</span> &#123;<br>    object, resourceVersion, err := w.objectToVersionedRuntimeObject(obj)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>       <span class="hljs-keyword">return</span> err<br>    &#125;<br>    event := watch.Event&#123;Type: watch.Deleted, Object: object&#125;<br><br>    f := <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(elem *storeElement)</span></span> <span class="hljs-type">error</span> &#123; <span class="hljs-keyword">return</span> w.store.Delete(elem) &#125;<br>    <span class="hljs-keyword">return</span> w.processEvent(event, resourceVersion, f)<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="watchCache-processEvent"><a href="#watchCache-processEvent" class="headerlink" title="watchCache.processEvent()"></a>watchCache.processEvent()</h2><p>其流程如下：</p><ul><li>首先在watch.Event的基础上添加更多的信息，诸如Fields、resourceVersion、RecordTime等，此外如果之前就有，那么也会获取到previous，最终组成watchCacheEvent。</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// processEvent is safe as long as there is at most one call to it in flight</span><br><span class="hljs-comment">// at any point in time.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(w *watchCache)</span></span> processEvent(event watch.Event, resourceVersion <span class="hljs-type">uint64</span>, updateFunc <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(*storeElement)</span></span> <span class="hljs-type">error</span>) <span class="hljs-type">error</span> &#123;<br>    metrics.EventsReceivedCounter.WithLabelValues(w.groupResource.String()).Inc()<br><br>    key, err := w.keyFunc(event.Object)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>       <span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;couldn&#x27;t compute key: %v&quot;</span>, err)<br>    &#125;<br>    elem := &amp;storeElement&#123;Key: key, Object: event.Object&#125;<br>    elem.Labels, elem.Fields, err = w.getAttrsFunc(event.Object)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>       <span class="hljs-keyword">return</span> err<br>    &#125;<br><br>    wcEvent := &amp;watchCacheEvent&#123;<br>       Type:            event.Type,<br>       Object:          elem.Object,<br>       ObjLabels:       elem.Labels,<br>       ObjFields:       elem.Fields,<br>       Key:             key,<br>       ResourceVersion: resourceVersion,<br>       RecordTime:      w.clock.Now(),<br>    &#125;<br><br>    <span class="hljs-comment">// We can call w.store.Get() outside of a critical section,</span><br>    <span class="hljs-comment">// because the w.store itself is thread-safe and the only</span><br>    <span class="hljs-comment">// place where w.store is modified is below (via updateFunc)</span><br>    <span class="hljs-comment">// and these calls are serialized because reflector is processing</span><br>    <span class="hljs-comment">// events one-by-one.</span><br>    previous, exists, err := w.store.Get(elem)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>       <span class="hljs-keyword">return</span> err<br>    &#125;<br>    <span class="hljs-keyword">if</span> exists &#123;<br>       previousElem := previous.(*storeElement)<br>       wcEvent.PrevObject = previousElem.Object<br>       wcEvent.PrevObjLabels = previousElem.Labels<br>       wcEvent.PrevObjFields = previousElem.Fields<br>    &#125;<br><br>    <span class="hljs-keyword">if</span> err := <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> <span class="hljs-type">error</span> &#123;<br>       w.Lock()<br>       <span class="hljs-keyword">defer</span> w.Unlock()<br><br>       w.updateCache(wcEvent)<br>       w.resourceVersion = resourceVersion<br>       <span class="hljs-keyword">defer</span> w.cond.Broadcast()<br><br>       err := updateFunc(elem)<br>       <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>          <span class="hljs-keyword">return</span> err<br>       &#125;<br>       <span class="hljs-keyword">if</span> w.snapshots != <span class="hljs-literal">nil</span> &#123;<br>          <span class="hljs-keyword">if</span> orderedLister, ordered := w.store.(orderedLister); ordered &#123;<br>             <span class="hljs-keyword">if</span> w.isCacheFullLocked() &#123;<br>                oldestRV := w.cache[w.startIndex%w.capacity].ResourceVersion<br>                w.snapshots.RemoveLess(oldestRV)<br>             &#125;<br>             w.snapshots.Add(w.resourceVersion, orderedLister)<br>          &#125;<br>       &#125;<br>       <span class="hljs-keyword">return</span> err<br>    &#125;(); err != <span class="hljs-literal">nil</span> &#123;<br>       <span class="hljs-keyword">return</span> err<br>    &#125;<br><br>    <span class="hljs-comment">// Avoid calling event handler under lock.</span><br>    <span class="hljs-comment">// This is safe as long as there is at most one call to Add/Update/Delete and</span><br>    <span class="hljs-comment">// UpdateResourceVersion in flight at any point in time, which is true now,</span><br>    <span class="hljs-comment">// because reflector calls them synchronously from its main thread.</span><br>    <span class="hljs-keyword">if</span> w.eventHandler != <span class="hljs-literal">nil</span> &#123;<br>       w.eventHandler(wcEvent)<br>    &#125;<br>    metrics.RecordResourceVersion(w.groupResource.String(), resourceVersion)<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><h1 id="cacher-dispatchEvents"><a href="#cacher-dispatchEvents" class="headerlink" title="cacher.dispatchEvents()"></a><code>cacher.dispatchEvents()</code></h1><p>该方法实现了事件分发的主循环，处理来自底层存储的资源变更事件，并定期生成书签事件以维持watch连接活性，是API服务器缓存系统向客户端推送实时更新的关键组件。</p><ul><li><p>bookmarkTimer每s进行一次，但是为了避免过高的并发有0.25比例的随机扰动。</p></li><li><p>事件处理的主循环处理三种事件源：</p><ul><li><p>从<code>incoming</code>通道接收存储层事件（Added、Modified、Deleted、Bookmark、Error），然后将非BOOKMARK的事件（这类事件很频繁，亚秒级）进行分发，然后更新最后处理的资源版本lastProcessedResourceVersion</p></li><li><p>从<code>bookmarkTimer</code>定时器收到的事件，给cacher.version推送一个Bookmark事件的更新并附带最后处理的资源版本lastProcessedResourceVersion，从而让cacher.version更新这个object记录的ResourceVersion。</p></li><li><p>从<code>stopCh</code>收到事件执行优雅退出</p></li></ul></li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Cacher)</span></span> dispatchEvents() &#123;<br>    <span class="hljs-comment">// Jitter to help level out any aggregate load.</span><br>    bookmarkTimer := c.clock.NewTimer(wait.Jitter(time.Second, <span class="hljs-number">0.25</span>))<br>    <span class="hljs-keyword">defer</span> bookmarkTimer.Stop()<br><br>    <span class="hljs-comment">// The internal informer populates the RV as soon as it conducts</span><br>    <span class="hljs-comment">// The first successful sync with the underlying store.</span><br>    <span class="hljs-comment">// The cache must wait until this first sync is completed to be deemed ready.</span><br>    <span class="hljs-comment">// Since we cannot send a bookmark when the lastProcessedResourceVersion is 0,</span><br>    <span class="hljs-comment">// we poll aggressively for the first list RV before entering the dispatch loop.</span><br>    lastProcessedResourceVersion := <span class="hljs-type">uint64</span>(<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">if</span> err := wait.PollUntilContextCancel(wait.ContextForChannel(c.stopCh), <span class="hljs-number">10</span>*time.Millisecond, <span class="hljs-literal">true</span>, <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(_ context.Context)</span></span> (<span class="hljs-type">bool</span>, <span class="hljs-type">error</span>) &#123;<br>       <span class="hljs-keyword">if</span> rv := c.watchCache.getListResourceVersion(); rv != <span class="hljs-number">0</span> &#123;<br>          lastProcessedResourceVersion = rv<br>          <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>, <span class="hljs-literal">nil</span><br>       &#125;<br>       <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>, <span class="hljs-literal">nil</span><br>    &#125;); err != <span class="hljs-literal">nil</span> &#123;<br>       <span class="hljs-comment">// given the function above never returns error,</span><br>       <span class="hljs-comment">// the non-empty error means that the stopCh was closed</span><br>       <span class="hljs-keyword">return</span><br>    &#125;<br>    <span class="hljs-keyword">for</span> &#123;<br>       <span class="hljs-keyword">select</span> &#123;<br>       <span class="hljs-keyword">case</span> event, ok := &lt;-c.incoming:<br>          <span class="hljs-keyword">if</span> !ok &#123;<br>             <span class="hljs-keyword">return</span><br>          &#125;<br>          <span class="hljs-comment">// Don&#x27;t dispatch bookmarks coming from the storage layer.</span><br>          <span class="hljs-comment">// They can be very frequent (even to the level of subseconds)</span><br>          <span class="hljs-comment">// to allow efficient watch resumption on kube-apiserver restarts,</span><br>          <span class="hljs-comment">// and propagating them down may overload the whole system.</span><br>          <span class="hljs-comment">//</span><br>          <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span> If at some point we decide the performance and scalability</span><br>          <span class="hljs-comment">// footprint is acceptable, this is the place to hook them in.</span><br>          <span class="hljs-comment">// However, we then need to check if this was called as a result</span><br>          <span class="hljs-comment">// of a bookmark event or regular Add/Update/Delete operation by</span><br>          <span class="hljs-comment">// checking if resourceVersion here has changed.</span><br>          <span class="hljs-keyword">if</span> event.Type != watch.Bookmark &#123;<br>             c.dispatchEvent(&amp;event)<br>          &#125;<br>          lastProcessedResourceVersion = event.ResourceVersion<br>          metrics.EventsCounter.WithLabelValues(c.groupResource.String()).Inc()<br>       <span class="hljs-keyword">case</span> &lt;-bookmarkTimer.C():<br>          bookmarkTimer.Reset(wait.Jitter(time.Second, <span class="hljs-number">0.25</span>))<br>          bookmarkEvent := &amp;watchCacheEvent&#123;<br>             Type:            watch.Bookmark,<br>             Object:          c.newFunc(),<br>             ResourceVersion: lastProcessedResourceVersion,<br>          &#125;<br>          <span class="hljs-keyword">if</span> err := c.versioner.UpdateObject(bookmarkEvent.Object, bookmarkEvent.ResourceVersion); err != <span class="hljs-literal">nil</span> &#123;<br>             klog.Errorf(<span class="hljs-string">&quot;failure to set resourceVersion to %d on bookmark event %+v&quot;</span>, bookmarkEvent.ResourceVersion, bookmarkEvent.Object)<br>             <span class="hljs-keyword">continue</span><br>          &#125;<br>          c.dispatchEvent(bookmarkEvent)<br>       <span class="hljs-keyword">case</span> &lt;-c.stopCh:<br>          <span class="hljs-keyword">return</span><br>       &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="cacher-dispatchEvent"><a href="#cacher-dispatchEvent" class="headerlink" title="cacher.dispatchEvent()"></a>cacher.dispatchEvent()</h2><ul><li><p>首先通过startDispatching构建<code>watchBuffer</code></p></li><li><p>如果分发的事件是Bookmark：</p><ul><li>给<code>watchBuffer</code>中的每个watcher通过nonblockingAdd添加，其会判断是否 event.ResourceVersion &gt;&#x3D; c.bookmarkAfterResourceVersion，只有是才会加入到cacher.input中。</li></ul></li><li><p>如果是其他普通的事件：</p><ul><li><p>对每个watcher调用<code>nonblockingAdd</code>，成功则事件入队，失败则加入<code>blockedWatchers</code>列表</p></li><li><p>如果<code>blockedWatchers</code>列表有时事件则：</p><ul><li><p>超时预算管理：从<code>dispatchTimeoutBudget</code>获取可用超时时间，避免单个事件占用过多资源</p></li><li><p>带超时的阻塞分发：调用<code>watcher.add</code>并传入计时器，允许在超时前等待watcher处理</p></li><li><p>智能计时器复用：如果计时器触发则设为nil，避免后续watcher使用已过期的计时器</p></li><li><p>资源回收：返回未使用的超时时间到预算池，实现整体资源调控</p></li></ul></li></ul></li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Cacher)</span></span> dispatchEvent(event *watchCacheEvent) &#123;<br>    c.startDispatching(event)<br>    <span class="hljs-keyword">defer</span> c.finishDispatching()<br>    <span class="hljs-comment">// Watchers stopped after startDispatching will be delayed to finishDispatching,</span><br><br>    <span class="hljs-comment">// Since add() can block, we explicitly add when cacher is unlocked.</span><br>    <span class="hljs-comment">// Dispatching event in nonblocking way first, which make faster watchers</span><br>    <span class="hljs-comment">// not be blocked by slower ones.</span><br>    <span class="hljs-comment">//</span><br>    <span class="hljs-comment">// Note: if we ever decide to cache the serialization of bookmark events,</span><br>    <span class="hljs-comment">// we will also need to modify the watchEncoder encoder</span><br>    <span class="hljs-keyword">if</span> event.Type == watch.Bookmark &#123;<br>       <span class="hljs-keyword">for</span> _, watcher := <span class="hljs-keyword">range</span> c.watchersBuffer &#123;<br>          watcher.nonblockingAdd(event)<br>       &#125;<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>       <span class="hljs-comment">// Set up caching of object serializations only for dispatching this event.</span><br>       <span class="hljs-comment">//</span><br>       <span class="hljs-comment">// Storing serializations in memory would result in increased memory usage,</span><br>       <span class="hljs-comment">// but it would help for caching encodings for watches started from old</span><br>       <span class="hljs-comment">// versions. However, we still don&#x27;t have a convincing data that the gain</span><br>       <span class="hljs-comment">// from it justifies increased memory usage, so for now we drop the cached</span><br>       <span class="hljs-comment">// serializations after dispatching this event.</span><br>       <span class="hljs-comment">//</span><br>       <span class="hljs-comment">// Given that CachingObject is just wrapping the object and not perfoming</span><br>       <span class="hljs-comment">// deep-copying (until some field is explicitly being modified), we create</span><br>       <span class="hljs-comment">// it unconditionally to ensure safety and reduce deep-copying.</span><br>       <span class="hljs-comment">//</span><br>       <span class="hljs-comment">// Make a shallow copy to allow overwriting Object and PrevObject.</span><br>       wcEvent := *event<br>       setCachingObjects(&amp;wcEvent, c.versioner)<br>       event = &amp;wcEvent<br><br>       c.blockedWatchers = c.blockedWatchers[:<span class="hljs-number">0</span>]<br>       <span class="hljs-keyword">for</span> _, watcher := <span class="hljs-keyword">range</span> c.watchersBuffer &#123;<br>          <span class="hljs-keyword">if</span> !watcher.nonblockingAdd(event) &#123;<br>             c.blockedWatchers = <span class="hljs-built_in">append</span>(c.blockedWatchers, watcher)<br>          &#125;<br>       &#125;<br><br>       <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(c.blockedWatchers) &gt; <span class="hljs-number">0</span> &#123;<br>          <span class="hljs-comment">// dispatchEvent is called very often, so arrange</span><br>          <span class="hljs-comment">// to reuse timers instead of constantly allocating.</span><br>          startTime := time.Now()<br>          timeout := c.dispatchTimeoutBudget.takeAvailable()<br>          c.timer.Reset(timeout)<br><br>          <span class="hljs-comment">// Send event to all blocked watchers. As long as timer is running,</span><br>          <span class="hljs-comment">// `add` will wait for the watcher to unblock. After timeout,</span><br>          <span class="hljs-comment">// `add` will not wait, but immediately close a still blocked watcher.</span><br>          <span class="hljs-comment">// Hence, every watcher gets the chance to unblock itself while timer</span><br>          <span class="hljs-comment">// is running, not only the first ones in the list.</span><br>          timer := c.timer<br>          <span class="hljs-keyword">for</span> _, watcher := <span class="hljs-keyword">range</span> c.blockedWatchers &#123;<br>             <span class="hljs-keyword">if</span> !watcher.add(event, timer) &#123;<br>                <span class="hljs-comment">// fired, clean the timer by set it to nil.</span><br>                timer = <span class="hljs-literal">nil</span><br>             &#125;<br>          &#125;<br><br>          <span class="hljs-comment">// Stop the timer if it is not fired</span><br>          <span class="hljs-keyword">if</span> timer != <span class="hljs-literal">nil</span> &amp;&amp; !timer.Stop() &#123;<br>             <span class="hljs-comment">// Consume triggered (but not yet received) timer event</span><br>             <span class="hljs-comment">// so that future reuse does not get a spurious timeout.</span><br>             &lt;-timer.C<br>          &#125;<br><br>          c.dispatchTimeoutBudget.returnUnused(timeout - time.Since(startTime))<br>       &#125;<br>    &#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *cacheWatcher)</span></span> nonblockingAdd(event *watchCacheEvent) <span class="hljs-type">bool</span> &#123;<br>    <span class="hljs-comment">// if the bookmarkAfterResourceVersion hasn&#x27;t been seen</span><br>    <span class="hljs-comment">// we will try to deliver a bookmark event every second.</span><br>    <span class="hljs-comment">// the following check will discard a bookmark event</span><br>    <span class="hljs-comment">// if it is &lt; than the bookmarkAfterResourceVersion</span><br>    <span class="hljs-comment">// so that we don&#x27;t pollute the input channel</span><br>    <span class="hljs-keyword">if</span> event.Type == watch.Bookmark &amp;&amp; event.ResourceVersion &lt; c.bookmarkAfterResourceVersion &#123;<br>       <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span><br>    &#125;<br>    <span class="hljs-keyword">select</span> &#123;<br>    <span class="hljs-keyword">case</span> c.input &lt;- event:<br>       c.markBookmarkAfterRvAsReceived(event)<br>       <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span><br>    <span class="hljs-keyword">default</span>:<br>       <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>k8s</category>
      
      <category>ApiServer</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【k8s kubelet 源代码阅读（二）】-节点状态上报</title>
    <link href="/2025/08/19/k8s%20kubelet%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%90%E4%BA%8C%E3%80%91-%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81%E4%B8%8A%E6%8A%A5/"/>
    <url>/2025/08/19/k8s%20kubelet%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%90%E4%BA%8C%E3%80%91-%E8%8A%82%E7%82%B9%E7%8A%B6%E6%80%81%E4%B8%8A%E6%8A%A5/</url>
    
    <content type="html"><![CDATA[<h1 id="主处理流程"><a href="#主处理流程" class="headerlink" title="主处理流程"></a>主处理流程</h1><p>在kubelet的Run中会调用相应的代码进行节点状态更新，代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">if</span> kl.kubeClient != <span class="hljs-literal">nil</span> &#123;<br>    <span class="hljs-comment">// Start two go-routines to update the status.</span><br>    <span class="hljs-comment">//</span><br>    <span class="hljs-comment">// The first will report to the apiserver every nodeStatusUpdateFrequency and is aimed to provide regular status intervals,</span><br>    <span class="hljs-comment">// while the second is used to provide a more timely status update during initialization and runs an one-shot update to the apiserver</span><br>    <span class="hljs-comment">// once the node becomes ready, then exits afterwards.</span><br>    <span class="hljs-comment">//</span><br>    <span class="hljs-comment">// Introduce some small jittering to ensure that over time the requests won&#x27;t start</span><br>    <span class="hljs-comment">// accumulating at approximately the same time from the set of nodes due to priority and</span><br>    <span class="hljs-comment">// fairness effect.</span><br>    <span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>       <span class="hljs-comment">// Call updateRuntimeUp once before syncNodeStatus to make sure kubelet had already checked runtime state</span><br>       <span class="hljs-comment">// otherwise when restart kubelet, syncNodeStatus will report node notReady in first report period</span><br>       kl.updateRuntimeUp()<br>       wait.JitterUntil(kl.syncNodeStatus, kl.nodeStatusUpdateFrequency, <span class="hljs-number">0.04</span>, <span class="hljs-literal">true</span>, wait.NeverStop)<br>    &#125;()<br><br>    <span class="hljs-keyword">go</span> kl.fastStatusUpdateOnce()<br><br>    <span class="hljs-comment">// start syncing lease</span><br>    <span class="hljs-keyword">go</span> kl.nodeLeaseController.Run(context.Background())<br><br>    <span class="hljs-comment">// Mirror pods for static pods may not be created immediately during node startup</span><br>    <span class="hljs-comment">// due to node registration or informer sync delays. They will be created eventually</span><br>    <span class="hljs-comment">//  when static pods are resynced (every 1-1.5 minutes).</span><br>    <span class="hljs-comment">// To ensure kube-scheduler is aware of static pod resource usage faster,</span><br>    <span class="hljs-comment">// mirror pods are created as soon as the node registers.</span><br>    <span class="hljs-keyword">go</span> kl.fastStaticPodsRegistration(ctx)<br>&#125;<br><span class="hljs-keyword">go</span> wait.Until(kl.updateRuntimeUp, <span class="hljs-number">5</span>*time.Second, wait.NeverStop)<br></code></pre></td></tr></table></figure><h2 id="updateRuntimeUp"><a href="#updateRuntimeUp" class="headerlink" title="updateRuntimeUp"></a>updateRuntimeUp</h2><p>它在最开始初始化时会被调用，后面也会被周期性地调用（每 5 秒一次），主要职责是：检查底层容器运行时的健康状况，并根据检查结果更新 Kubelet 内部的状态。当容器运行时首次准备就绪时，它还会触发一系列依赖于运行时的模块的初始化。其主要流程为：</p><ol><li><p>通过 CRI (Container Runtime Interface) 调用底层容器运行时（如 containerd 或 CRI-O）的 Status 接口，获取一个包含运行时当前状态信息的结构体 s。</p></li><li><p>通过s中的NetworkReady来检查网络状态是否ready</p></li><li><p>通过s中的RuntimeReady来检查运行时是否ready</p></li><li><p>当 RuntimeReady 为 true 时才会去更新内部缓存，并初始化那些依赖容器运行时的模块（只会被初始调用一次），比如 cAdvisor（用于监控）、ContainerManager（用于资源管理）、EvictionManager（驱逐管理器）、pluginManager、shutdownManager等。这个设计确保了这些模块只在容器运行时真正可用后才被启动。</p></li></ol><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// updateRuntimeUp calls the container runtime status callback, initializing</span><br><span class="hljs-comment">// the runtime dependent modules when the container runtime first comes up,</span><br><span class="hljs-comment">// and returns an error if the status check fails.  If the status check is OK,</span><br><span class="hljs-comment">// update the container runtime uptime in the kubelet runtimeState.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(kl *Kubelet)</span></span> updateRuntimeUp() &#123;<br>    kl.updateRuntimeMux.Lock()<br>    <span class="hljs-keyword">defer</span> kl.updateRuntimeMux.Unlock()<br>    ctx := context.Background()<br><br>    s, err := kl.containerRuntime.Status(ctx)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>       klog.ErrorS(err, <span class="hljs-string">&quot;Container runtime sanity check failed&quot;</span>)<br>       <span class="hljs-keyword">return</span><br>    &#125;<br>    <span class="hljs-keyword">if</span> s == <span class="hljs-literal">nil</span> &#123;<br>       klog.ErrorS(<span class="hljs-literal">nil</span>, <span class="hljs-string">&quot;Container runtime status is nil&quot;</span>)<br>       <span class="hljs-keyword">return</span><br>    &#125;<br>    <span class="hljs-comment">// Periodically log the whole runtime status for debugging.</span><br>    klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Container runtime status&quot;</span>, <span class="hljs-string">&quot;status&quot;</span>, s)<br>    klogErrorS := klog.ErrorS<br>    <span class="hljs-keyword">if</span> !kl.containerRuntimeReadyExpected &#123;<br>       klogErrorS = klog.V(<span class="hljs-number">4</span>).ErrorS<br>    &#125;<br>    networkReady := s.GetRuntimeCondition(kubecontainer.NetworkReady)<br>    <span class="hljs-keyword">if</span> networkReady == <span class="hljs-literal">nil</span> || !networkReady.Status &#123;<br>       klogErrorS(<span class="hljs-literal">nil</span>, <span class="hljs-string">&quot;Container runtime network not ready&quot;</span>, <span class="hljs-string">&quot;networkReady&quot;</span>, networkReady)<br>       kl.runtimeState.setNetworkState(fmt.Errorf(<span class="hljs-string">&quot;container runtime network not ready: %v&quot;</span>, networkReady))<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>       <span class="hljs-comment">// Set nil if the container runtime network is ready.</span><br>       kl.runtimeState.setNetworkState(<span class="hljs-literal">nil</span>)<br>    &#125;<br>    <span class="hljs-comment">// information in RuntimeReady condition will be propagated to NodeReady condition.</span><br>    runtimeReady := s.GetRuntimeCondition(kubecontainer.RuntimeReady)<br>    <span class="hljs-comment">// If RuntimeReady is not set or is false, report an error.</span><br>    <span class="hljs-keyword">if</span> runtimeReady == <span class="hljs-literal">nil</span> || !runtimeReady.Status &#123;<br>       klogErrorS(<span class="hljs-literal">nil</span>, <span class="hljs-string">&quot;Container runtime not ready&quot;</span>, <span class="hljs-string">&quot;runtimeReady&quot;</span>, runtimeReady)<br>       kl.runtimeState.setRuntimeState(fmt.Errorf(<span class="hljs-string">&quot;container runtime not ready: %v&quot;</span>, runtimeReady))<br>       <span class="hljs-keyword">return</span><br>    &#125;<br><br>    kl.runtimeState.setRuntimeState(<span class="hljs-literal">nil</span>)<br>    kl.runtimeState.setRuntimeHandlers(s.Handlers)<br>    kl.runtimeState.setRuntimeFeatures(s.Features)<br>    kl.oneTimeInitializer.Do(kl.initializeRuntimeDependentModules)<br>    kl.runtimeState.setRuntimeSync(kl.clock.Now())<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="syncNodeStatus"><a href="#syncNodeStatus" class="headerlink" title="syncNodeStatus"></a>syncNodeStatus</h2><p>其首先会去registerWithAPIServer，然后再updateNodeStatus。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// syncNodeStatus should be called periodically from a goroutine.</span><br><span class="hljs-comment">// It synchronizes node status to master if there is any change or enough time</span><br><span class="hljs-comment">// passed from the last sync, registering the kubelet first if necessary.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(kl *Kubelet)</span></span> syncNodeStatus() &#123;<br>    kl.syncNodeStatusMux.Lock()<br>    <span class="hljs-keyword">defer</span> kl.syncNodeStatusMux.Unlock()<br>    ctx := context.Background()<br><br>    <span class="hljs-keyword">if</span> kl.kubeClient == <span class="hljs-literal">nil</span> || kl.heartbeatClient == <span class="hljs-literal">nil</span> &#123;<br>       <span class="hljs-keyword">return</span><br>    &#125;<br>    <span class="hljs-keyword">if</span> kl.registerNode &#123;<br>       <span class="hljs-comment">// This will exit immediately if it doesn&#x27;t need to do anything.</span><br>       kl.registerWithAPIServer()<br>    &#125;<br>    <span class="hljs-keyword">if</span> err := kl.updateNodeStatus(ctx); err != <span class="hljs-literal">nil</span> &#123;<br>       klog.ErrorS(err, <span class="hljs-string">&quot;Unable to update node status&quot;</span>)<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="registerWithAPIServer"><a href="#registerWithAPIServer" class="headerlink" title="registerWithAPIServer"></a>registerWithAPIServer</h3><p>该函数的核心作用是：<strong>尝试将 Kubelet 所在的节点（Node）注册到 Kubernetes API Server。如果节点已经存在，则进行一次状态协调（reconcile）。</strong></p><p>函数返回一个布尔值，<code>true</code> 表示注册&#x2F;协调成功，<code>false</code> 表示失败。</p><p>下面是该函数的详细执行流程：</p><ol><li><p>尝试创建新节点</p><ul><li><p><code>kl.kubeClient.CoreV1().Nodes().Create(...)</code>: 函数首先会乐观地尝试直接创建一个新的 Node 对象。</p></li><li><p>如果创建成功 (<code>err == nil</code>)，说明这是一个全新的节点加入集群。函数会记录相关延迟指标，然后直接返回 <code>true</code>，表示注册成功。</p></li></ul></li><li><p>处理创建失败的情况</p><ul><li><p>如果创建失败，函数会通过一个 <code>switch</code> 语句来分析错误类型：</p><ul><li><p><code>apierrors.IsAlreadyExists(err)</code>: 这是最常见的情况。错误表明 API Server 中已经存在一个同名的 Node 对象。这意味着 Kubelet 可能是在重启，或者之前的注册请求因为某些原因中断了。在这种情况下，程序会继续往下执行，进入“协调”逻辑。</p></li><li><p><code>apierrors.IsForbidden(err)</code>: 这个错误表示 Kubelet 没有创建 Node 对象的权限。这通常是由于 RBAC 配置不正确导致的。如果 <code>KubeletRegistrationGetOnExistsOnly</code> 这个特性门控被启用，它会直接报错并返回 <code>false</code>。否则，它会继续尝试获取节点信息，因为节点可能已经由其他方式创建好了。</p></li><li><p><code>default</code>: 对于其他类型的错误（如网络问题），函数会记录错误日志并直接返回 <code>false</code>，表示本次尝试失败。</p></li></ul></li></ul></li><li><p>获取已存在的节点对象</p><ul><li><p>如果是因为 <code>AlreadyExists</code> 或 <code>Forbidden</code>（且特性门控未开启）进入后续流程，函数会调用 <code>kl.kubeClient.CoreV1().Nodes().Get(...)</code> 来获取 API Server 中已存在的 Node 对象。</p></li><li><p>如果获取失败或返回的 <code>existingNode</code> 为 <code>nil</code>，说明发生了异常，函数会记录错误并返回 <code>false</code>。</p></li></ul></li><li><p>协调（Reconcile）节点状态</p><ul><li><p>如果成功获取到了 <code>existingNode</code>，说明节点确实已经注册过了。这时 Kubelet 的任务就变成了确保当前 Kubelet 的配置和状态与 API Server 中的记录保持一致。</p></li><li><p><code>originalNode := existingNode.DeepCopy()</code>: 先深度拷贝一份原始的节点对象，用于后续的 <code>Patch</code> 操作，以计算出差异。</p></li><li><p>接下来会调用一系列的 <code>reconcile</code> 和 <code>update</code> 函数，来检查并更新 <code>existingNode</code> 对象中的字段，包括：</p><ul><li><p><code>reconcileCMADAnnotationWithExistingNode</code>: 协调与存储卷挂载相关的注解。</p></li><li><p><code>updateDefaultLabels</code>: 更新节点的默认标签（如操作系统、架构等）。</p></li><li><p><code>reconcileExtendedResource</code>: 协调扩展资源（如 GPU 等）。</p></li><li><p><code>reconcileHugePageResource</code>: 协调大页内存资源。</p></li></ul></li><li><p><code>requiresUpdate</code> 标志位会记录在这些检查过程中是否有任何字段需要被更新。</p></li></ul></li><li><p>发起 Patch 更新</p><ul><li><p>如果 <code>requiresUpdate</code> 为 <code>true</code>，说明节点的某些状态需要更新。</p></li><li><p><code>nodeutil.PatchNodeStatus(...)</code>: 函数会调用这个帮助函数，向 API Server 发送一个 <code>PATCH</code> 请求。<code>PATCH</code> 请求只会更新发生变化的字段，比 <code>UPDATE</code> 请求更高效。</p></li><li><p>如果 <code>PATCH</code> 操作失败，记录错误并返回 <code>false</code>。</p></li></ul></li><li><p>成功返回</p><ul><li>如果 <code>PATCH</code> 成功，或者根本不需要更新（<code>requiresUpdate</code> 为 <code>false</code>），函数最终会返回 <code>true</code>，表示 Kubelet 已经成功地将自己与 API Server 中的节点状态同步了。</li></ul></li></ol><p><strong>总结一下</strong>：</p><p><code>tryRegisterWithAPIServer</code> 函数封装了 Kubelet 节点注册的核心逻辑。它不仅处理了首次注册的场景，更重要的是处理了 Kubelet 重启后与现有节点对象进行状态同步的场景，通过一系列的协调和 PATCH 操作，确保了节点信息的准确性和一致性。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// registerWithAPIServer registers the node with the cluster master. It is safe</span><br><span class="hljs-comment">// to call multiple times, but not concurrently (kl.registrationCompleted is</span><br><span class="hljs-comment">// not locked).</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(kl *Kubelet)</span></span> registerWithAPIServer() &#123;<br>    <span class="hljs-keyword">if</span> kl.registrationCompleted &#123;<br>       <span class="hljs-keyword">return</span><br>    &#125;<br><br>    kl.nodeStartupLatencyTracker.RecordAttemptRegisterNode()<br><br>    step := <span class="hljs-number">100</span> * time.Millisecond<br><br>    <span class="hljs-keyword">for</span> &#123;<br>       time.Sleep(step)<br>       step = step * <span class="hljs-number">2</span><br>       <span class="hljs-keyword">if</span> step &gt;= <span class="hljs-number">7</span>*time.Second &#123;<br>          step = <span class="hljs-number">7</span> * time.Second<br>       &#125;<br><br>       node, err := kl.initialNode(context.TODO())<br>       <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>          klog.ErrorS(err, <span class="hljs-string">&quot;Unable to construct v1.Node object for kubelet&quot;</span>)<br>          <span class="hljs-keyword">continue</span><br>       &#125;<br><br>       klog.InfoS(<span class="hljs-string">&quot;Attempting to register node&quot;</span>, <span class="hljs-string">&quot;node&quot;</span>, klog.KObj(node))<br>       registered := kl.tryRegisterWithAPIServer(node)<br>       <span class="hljs-keyword">if</span> registered &#123;<br>          klog.InfoS(<span class="hljs-string">&quot;Successfully registered node&quot;</span>, <span class="hljs-string">&quot;node&quot;</span>, klog.KObj(node))<br>          kl.registrationCompleted = <span class="hljs-literal">true</span><br>          <span class="hljs-keyword">return</span><br>       &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="updateNodeStatus"><a href="#updateNodeStatus" class="headerlink" title="updateNodeStatus"></a>updateNodeStatus</h3><p>该函数每10s会被调用一次，但是有4%的抖动以避免多个节点同时上报状态，使得APIServer压力过大。</p><p>具体在update时会最多尝试5次的tryUpdateNodeStatus。每次update的流程如下：</p><ol><li><p>获取当前节点对象（<code>originalNode</code>）</p><ul><li><p>这个函数的设计考虑到了大规模集群下对 API Server 的性能影响。</p></li><li><p><code>if tryNumber == 0</code>: 如果是第一次尝试（<code>tryNumber</code> 为 0），它会优先从本地的 <code>nodeLister</code>（一个本地缓存）中获取 Node 对象。这样做可以极大地减少对 API Server 的 <code>GET</code> 请求，降低控制平面的负载。本地缓存的数据可能会有轻微延迟，但通常是可以接受的。</p></li><li><p><code>else</code>: 如果不是第一次尝试（意味着前一次尝试可能因为数据冲突而失败），它会直接通过 <code>heartbeatClient</code> 向 API Server 发送 <code>GET</code> 请求，以获取最新、最准确的 Node 对象数据，避免再次发生冲突。</p></li><li><p>如果获取 Node 对象失败，或者获取到的对象为 <code>nil</code>，则直接返回错误。</p></li></ul></li><li><p>计算更新后的节点状态</p><ul><li><p><code>node, changed := kl.updateNode(ctx, originalNode)</code>: 调用 <code>updateNode</code> 函数。这个函数会：</p><ul><li><p>创建一个 <code>originalNode</code> 的深拷贝（<code>DeepCopy</code>）。</p></li><li><p>基于 Kubelet 当前的内部状态（如容器运行时状态、资源容量、Pod CIDR 等）来更新这个拷贝的 <code>Status</code> 字段。</p></li><li><p>返回更新后的 <code>node</code> 对象和一个布尔值 <code>changed</code>，该值表示 <code>node.Status</code> 与 <code>originalNode.Status</code> 相比是否发生了实质性的变化。</p></li></ul></li></ul></li><li><p>判断是否需要发送更新</p><ul><li><p><code>shouldPatchNodeStatus := changed || kl.clock.Since(kl.lastStatusReportTime) &gt;= kl.nodeStatusReportFrequency</code>: 这里决定了是否真的需要向 API Server 发送更新请求。满足以下任一条件即可：</p><ul><li><p><code>changed</code>: 节点状态发生了变化（比如 <code>NodeReady</code> 条件从 <code>False</code> 变成了 <code>True</code>）。</p></li><li><p><code>kl.clock.Since(kl.lastStatusReportTime) &gt;= kl.nodeStatusReportFrequency</code>: 距离上次成功上报状态的时间已经超过了预设的 <code>nodeStatusReportFrequency</code>（例如 5 分钟）。这是一个强制上报机制，确保即使节点状态一直没有变化，API Server 也能定期收到该节点的“心跳”，确认它还活着。</p></li></ul></li></ul></li><li><p>执行更新或跳过</p><ul><li><p><code>if !shouldPatchNodeStatus</code>: 如果不需要更新，函数会调用 <code>kl.markVolumesFromNode(node)</code> 来同步一下卷（Volume）的使用状态，然后直接返回 <code>nil</code>，表示本次操作成功完成（虽然没有发送网络请求）。</p></li><li><p><code>updatedNode, err := kl.patchNodeStatus(originalNode, node)</code>: 如果需要更新，则调用 <code>patchNodeStatus</code> 函数，向 API Server 发送一个 <code>PATCH</code> 请求，只更新发生变化的字段。</p></li><li><p><code>if err == nil</code>: 如果 <code>PATCH</code> 请求成功，函数会用 API Server 返回的最新 <code>updatedNode</code> 对象来调用 <code>kl.markVolumesFromNode(updatedNode)</code>，确保 Volume Manager 的状态与刚上报到 API Server 的状态一致。</p></li><li><p>最后，返回 <code>patchNodeStatus</code> 的结果（<code>err</code>）。如果 <code>err</code> 不为 <code>nil</code>，外层的 <code>updateNodeStatus</code> 就会进行重试。</p></li></ul></li></ol><p><strong>总结一下</strong>：</p><p><code>tryUpdateNodeStatus</code> 是一个既高效又健壮的函数。它通过优先使用本地缓存来降低 API Server 负载，同时通过强制上报周期和冲突后直连 API Server 的机制来保证状态同步的最终一致性和可靠性。它是 Kubelet 节点状态管理的核心实现。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// updateNodeStatus updates node status to master with retries if there is any</span><br><span class="hljs-comment">// change or enough time passed from the last sync.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(kl *Kubelet)</span></span> updateNodeStatus(ctx context.Context) <span class="hljs-type">error</span> &#123;<br>    klog.V(<span class="hljs-number">5</span>).InfoS(<span class="hljs-string">&quot;Updating node status&quot;</span>)<br>    <span class="hljs-keyword">for</span> i := <span class="hljs-number">0</span>; i &lt; nodeStatusUpdateRetry; i++ &#123; <span class="hljs-comment">// nodeStatusUpdateRetry = 5</span><br>       <span class="hljs-keyword">if</span> err := kl.tryUpdateNodeStatus(ctx, i); err != <span class="hljs-literal">nil</span> &#123;<br>          <span class="hljs-keyword">if</span> i &gt; <span class="hljs-number">0</span> &amp;&amp; kl.onRepeatedHeartbeatFailure != <span class="hljs-literal">nil</span> &#123;<br>             kl.onRepeatedHeartbeatFailure()<br>          &#125;<br>          klog.ErrorS(err, <span class="hljs-string">&quot;Error updating node status, will retry&quot;</span>)<br>       &#125; <span class="hljs-keyword">else</span> &#123;<br>          <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>       &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;update node status exceeds retry count&quot;</span>)<br>&#125;<br><br><span class="hljs-comment">// tryUpdateNodeStatus tries to update node status to master if there is any</span><br><span class="hljs-comment">// change or enough time passed from the last sync.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(kl *Kubelet)</span></span> tryUpdateNodeStatus(ctx context.Context, tryNumber <span class="hljs-type">int</span>) <span class="hljs-type">error</span> &#123;<br>    <span class="hljs-comment">// In large clusters, GET and PUT operations on Node objects coming</span><br>    <span class="hljs-comment">// from here are the majority of load on apiserver and etcd.</span><br>    <span class="hljs-comment">// To reduce the load on control-plane, we are serving GET operations from</span><br>    <span class="hljs-comment">// local lister (the data might be slightly delayed but it doesn&#x27;t</span><br>    <span class="hljs-comment">// seem to cause more conflict - the delays are pretty small).</span><br>    <span class="hljs-comment">// If it result in a conflict, all retries are served directly from etcd.</span><br>    <span class="hljs-keyword">var</span> originalNode *v1.Node<br>    <span class="hljs-keyword">var</span> err <span class="hljs-type">error</span><br><br>    <span class="hljs-keyword">if</span> tryNumber == <span class="hljs-number">0</span> &#123;<br>       originalNode, err = kl.nodeLister.Get(<span class="hljs-type">string</span>(kl.nodeName))<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>       opts := metav1.GetOptions&#123;&#125;<br>       originalNode, err = kl.heartbeatClient.CoreV1().Nodes().Get(ctx, <span class="hljs-type">string</span>(kl.nodeName), opts)<br>    &#125;<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>       <span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;error getting node %q: %v&quot;</span>, kl.nodeName, err)<br>    &#125;<br>    <span class="hljs-keyword">if</span> originalNode == <span class="hljs-literal">nil</span> &#123;<br>       <span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;nil %q node object&quot;</span>, kl.nodeName)<br>    &#125;<br><br>    node, changed := kl.updateNode(ctx, originalNode)<br>    shouldPatchNodeStatus := changed || kl.clock.Since(kl.lastStatusReportTime) &gt;= kl.nodeStatusReportFrequency<br><br>    <span class="hljs-keyword">if</span> !shouldPatchNodeStatus &#123;<br>       kl.markVolumesFromNode(node)<br>       <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>    &#125;<br><br>    updatedNode, err := kl.patchNodeStatus(originalNode, node)<br>    <span class="hljs-keyword">if</span> err == <span class="hljs-literal">nil</span> &#123;<br>       kl.markVolumesFromNode(updatedNode)<br>    &#125;<br>    <span class="hljs-keyword">return</span> err<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="fastStatusUpdateOnce"><a href="#fastStatusUpdateOnce" class="headerlink" title="fastStatusUpdateOnce"></a>fastStatusUpdateOnce</h3>]]></content>
    
    
    <categories>
      
      <category>k8s</category>
      
      <category>kubelet</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【k8s kubelet 源代码阅读（一）】-Pod管理</title>
    <link href="/2025/08/11/k8s%20kubelet%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%90%E4%B8%80%E3%80%91-Pod%E7%AE%A1%E7%90%86/"/>
    <url>/2025/08/11/k8s%20kubelet%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%90%E4%B8%80%E3%80%91-Pod%E7%AE%A1%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="关键组件介绍"><a href="#关键组件介绍" class="headerlink" title="关键组件介绍"></a>关键组件介绍</h1><h2 id="probeManager"><a href="#probeManager" class="headerlink" title="probeManager"></a>probeManager</h2><p>当一个pod被kubelet感知到之后，就会调用<code>probeManager</code>的<code>AddPod</code>来为各个容器启动探针，监控状态变化。具体执行如下：</p><ul><li><p>获取所有的pod.Spec.Containers以及pod中支持重新启动的initContainers，然后逐个处理</p></li><li><p>如果这个容器配置了StartupProbe或者ReadinessProbe或者LivenessProbe，那么就会为其创建对应类型的newWorker，并启动协程来运行</p></li><li><p>newWorker协程会通过一个循环不断运行，其在实际检查前会有一些专门的处理，例如看容器是否是已经重启的新容器（即ContainerID是否发生改变），或者是否处在<code>onHold</code>的重启状态，容器是否是Running状态，是否超过了InitialDelaySeconds初始等待时间，检查通过后才会去具体执行探针。</p></li><li><p>如果当前状态不是Started，那么就不会执行ReadinessProbe和LivenessProbe，只会执行StartupProbe。如果已经是Started那么就不会去执行StartupProbe。</p></li><li><p>然后目前支持的探针类型有：</p><ul><li><p>Exec：执行指定的命令</p></li><li><p>HTTPGet：执行Http请求</p></li><li><p>TCPSocket：建立TCPSocket</p></li><li><p>GRPC：调用GRPC</p></li></ul></li><li><p>如果连续探测成功或者连续失败超过阈值，就会将结果记录在自己的缓存中（缓存会被PLEG用来计算pod的事件），然后将事件发送到对应的<code>updates</code> 通道，然后通过<code>syncLoopIteration</code>函数来继续处理，在处理中会给podWorkers发送一个这个pod的<em>SyncPodSync</em>类型的事件来触发更新。</p></li></ul><h2 id="StatusManager"><a href="#StatusManager" class="headerlink" title="StatusManager"></a>StatusManager</h2><p>StatusManager主要职责是批量地将 Kubelet 本地缓存的 Pod 状态同步到 API Server。</p><p>其主循环如下，有两类更新的时机：</p><ul><li><p>增量更新：通过<code>podStatusChannel</code>触发</p></li><li><p>全量更新：每10s周期性触发一次</p></li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">go</span> wait.Forever(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>    <span class="hljs-keyword">for</span> &#123;<br>       <span class="hljs-keyword">select</span> &#123;<br>       <span class="hljs-keyword">case</span> &lt;-m.podStatusChannel:<br>          klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Syncing updated statuses&quot;</span>)<br>          m.syncBatch(<span class="hljs-literal">false</span>) <span class="hljs-comment">// 增量更新</span><br>       <span class="hljs-keyword">case</span> &lt;-syncTicker:<br>          klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Syncing all statuses&quot;</span>)<br>          m.syncBatch(<span class="hljs-literal">true</span>) <span class="hljs-comment">// 全量更新</span><br>       &#125;<br>    &#125;<br>&#125;, <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><p>其核心的<code>syncBatch(all bool)</code>执行流程如下：</p><ul><li><p>首先通过与缓存的apiserver中pod的状态与本地维护的pod的状态比较，收集哪些pod需要更新状态：</p><ul><li><p>增量更新：</p><ul><li>收集当前status version &gt; apiserver中 version的pod</li></ul></li><li><p>全量更新：</p><ul><li><p>version变大的pod</p></li><li><p>有deletionTimestamp并且是终态的的pod</p></li><li><p>逐condition进行比较收集有不同的pod</p></li></ul></li></ul></li><li><p>然后会对这些收集到的pod通过<code>syncPod</code>来向APIServer更新状态</p><ul><li><p>首先get到原本的pod</p></li><li><p>然后比较uid是否相同，如果相同说明是同名新建的pod，需要跳过这次更新，并删除这旧的缓存</p></li><li><p>然后进行状态合并<code>mergePodStatus</code>，状态合并而不是替换的原因在于有些status字段可能不是kubelet管理的，具体合并规则为：</p><ul><li><p>对于condition，保留不由kubelet管理的旧的condition，对于由kubelet管理的采用最新的condition，目前由kubelet管理的condition type有：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs go">v1.PodScheduled,<br>v1.PodReady,<br>v1.PodInitialized,<br>v1.ContainersReady,<br>v1.PodResizeInProgress,<br>v1.PodResizePending,<br>v1.PodReadyToStartContainers,(如果开启相关特性)<br></code></pre></td></tr></table></figure></li><li><p><code>ResourceClaimStatuses</code>字段还是采用旧字段，因为其不属于kubelet管理</p></li><li><p>如果pod进入了终态但是还有容器在运行，对于<code>Phase</code>、<code>Reason</code>、<code>Message</code>字段还是采用原本的旧字段，防止出现明明状态是终态但是还有容器在运行的假死状态</p></li><li><p>如果pod的phase是终态，就强制把<code>containerReady</code>和<code>podReady</code>字段设置为false，以防止上报一个逻辑上矛盾的状态。</p></li></ul></li><li><p>得到merge后的状态后会通过Patch的方式来更新status，根据patch返回的新pod来更新缓存</p></li><li><p>最后还会处理有<code>deletionTimestamp</code>并且是终态的的pod，其会调用Apiserver的<code>Delete</code>去删除对应的pod，并删除对应的缓存。</p></li></ul></li></ul><h2 id="PLEG"><a href="#PLEG" class="headerlink" title="PLEG"></a>PLEG</h2><p>目前最新的k8s支持两类PLEG：</p><ul><li><p>GenericPLEG：</p><ul><li><p>传统的PLEG，核心的执行循环是：<code>go wait.Until(g.Relist, g.relistDuration.RelistPeriod, g.stopCh)</code>，默认间隔是1s。</p></li><li><p>在g.Relist函数中，它会通过容器运行时获取到当前节点的所有pod的容器的状态，然后逐个pod检查，通过与新旧容器状态的比较来获取事件，主要的事件类型有<em>ContainerStarted、ContainerDied、ContainerRemoved、ContainerChanged</em>，比较的代码如下：</p></li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">generateEvents</span><span class="hljs-params">(logger klog.Logger, podID types.UID, cid <span class="hljs-type">string</span>, oldState, newState plegContainerState)</span></span> []*PodLifecycleEvent &#123;<br>    <span class="hljs-keyword">if</span> newState == oldState &#123;<br>       <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>    &#125;<br><br>    logger.V(<span class="hljs-number">4</span>).Info(<span class="hljs-string">&quot;GenericPLEG&quot;</span>, <span class="hljs-string">&quot;podUID&quot;</span>, podID, <span class="hljs-string">&quot;containerID&quot;</span>, cid, <span class="hljs-string">&quot;oldState&quot;</span>, oldState, <span class="hljs-string">&quot;newState&quot;</span>, newState)<br>    <span class="hljs-keyword">switch</span> newState &#123;<br>    <span class="hljs-keyword">case</span> plegContainerRunning:<br>       <span class="hljs-keyword">return</span> []*PodLifecycleEvent&#123;&#123;ID: podID, Type: ContainerStarted, Data: cid&#125;&#125;<br>    <span class="hljs-keyword">case</span> plegContainerExited:<br>       <span class="hljs-keyword">return</span> []*PodLifecycleEvent&#123;&#123;ID: podID, Type: ContainerDied, Data: cid&#125;&#125;<br>    <span class="hljs-keyword">case</span> plegContainerUnknown:<br>       <span class="hljs-keyword">return</span> []*PodLifecycleEvent&#123;&#123;ID: podID, Type: ContainerChanged, Data: cid&#125;&#125;<br>    <span class="hljs-keyword">case</span> plegContainerNonExistent:<br>       <span class="hljs-keyword">switch</span> oldState &#123;<br>       <span class="hljs-keyword">case</span> plegContainerExited:<br>          <span class="hljs-comment">// We already reported that the container died before.</span><br>          <span class="hljs-keyword">return</span> []*PodLifecycleEvent&#123;&#123;ID: podID, Type: ContainerRemoved, Data: cid&#125;&#125;<br>       <span class="hljs-keyword">default</span>:<br>          <span class="hljs-keyword">return</span> []*PodLifecycleEvent&#123;&#123;ID: podID, Type: ContainerDied, Data: cid&#125;, &#123;ID: podID, Type: ContainerRemoved, Data: cid&#125;&#125;<br>       &#125;<br>    <span class="hljs-keyword">default</span>:<br>       <span class="hljs-built_in">panic</span>(fmt.Sprintf(<span class="hljs-string">&quot;unrecognized container state: %v&quot;</span>, newState))<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><ul><li>如果有事件发生了，那么就会更新kubelet的cache中pod的状态，缓存的pod的状态如下：</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// PodStatus represents the status of the pod and its containers.</span><br><span class="hljs-comment">// v1.PodStatus can be derived from examining PodStatus and v1.Pod.</span><br><span class="hljs-keyword">type</span> PodStatus <span class="hljs-keyword">struct</span> &#123;<br>    <span class="hljs-comment">// ID of the pod.</span><br>    ID types.UID<br>    <span class="hljs-comment">// Name of the pod.</span><br>    Name <span class="hljs-type">string</span><br>    <span class="hljs-comment">// Namespace of the pod.</span><br>    Namespace <span class="hljs-type">string</span><br>    <span class="hljs-comment">// All IPs assigned to this pod</span><br>    IPs []<span class="hljs-type">string</span><br>    <span class="hljs-comment">// Status of containers in the pod.</span><br>    ContainerStatuses []*Status<br>    <span class="hljs-comment">// Statuses of containers of the active sandbox in the pod.</span><br>    ActiveContainerStatuses []*Status<br>    <span class="hljs-comment">// Status of the pod sandbox.</span><br>    <span class="hljs-comment">// Only for kuberuntime now, other runtime may keep it nil.</span><br>    SandboxStatuses []*runtimeapi.PodSandboxStatus<br>    <span class="hljs-comment">// Timestamp at which container and pod statuses were recorded</span><br>    TimeStamp time.Time<br>&#125;<br><br><span class="hljs-comment">// Status represents the status of a container.</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// Status does not contain VolumeMap because CRI API is unaware of volume names.</span><br><span class="hljs-keyword">type</span> Status <span class="hljs-keyword">struct</span> &#123;<br>    <span class="hljs-comment">// ID of the container.</span><br>    ID ContainerID<br>    <span class="hljs-comment">// Name of the container.</span><br>    Name <span class="hljs-type">string</span><br>    <span class="hljs-comment">// Status of the container.</span><br>    State State<br>    <span class="hljs-comment">// Creation time of the container.</span><br>    CreatedAt time.Time<br>    <span class="hljs-comment">// Start time of the container.</span><br>    StartedAt time.Time<br>    <span class="hljs-comment">// Finish time of the container.</span><br>    FinishedAt time.Time<br>    <span class="hljs-comment">// Exit code of the container.</span><br>    ExitCode <span class="hljs-type">int</span><br>    <span class="hljs-comment">// Name of the image, this also includes the tag of the image,</span><br>    <span class="hljs-comment">// the expected form is &quot;NAME:TAG&quot;.</span><br>    Image <span class="hljs-type">string</span><br>    <span class="hljs-comment">// ID of the image.</span><br>    ImageID <span class="hljs-type">string</span><br>    <span class="hljs-comment">// The digested reference of the image used by the container.</span><br>    ImageRef <span class="hljs-type">string</span><br>    <span class="hljs-comment">// Runtime handler used to pull the image if any.</span><br>    ImageRuntimeHandler <span class="hljs-type">string</span><br>    <span class="hljs-comment">// Hash of the container, used for comparison.</span><br>    Hash <span class="hljs-type">uint64</span><br>    <span class="hljs-comment">// Number of times that the container has been restarted.</span><br>    RestartCount <span class="hljs-type">int</span><br>    <span class="hljs-comment">// A string explains why container is in such a status.</span><br>    Reason <span class="hljs-type">string</span><br>    <span class="hljs-comment">// Message written by the container before exiting (stored in</span><br>    <span class="hljs-comment">// TerminationMessagePath).</span><br>    Message <span class="hljs-type">string</span><br>    <span class="hljs-comment">// CPU and memory resources for this container</span><br>    Resources *ContainerResources<br>    <span class="hljs-comment">// User identity information of the first process of this container</span><br>    User *ContainerUser<br>    <span class="hljs-comment">// Mounts are the volume mounts of the container</span><br>    Mounts []Mount<br>    <span class="hljs-comment">// StopSignal is used to show the container&#x27;s effective stop signal in the Status</span><br>    StopSignal *v1.Signal<br>&#125;<br></code></pre></td></tr></table></figure><ul><li>最后会将event（除<em>ContainerChanged</em>）发送给对应的管道，通过<code>g.eventChannel &lt;- events[i]</code></li></ul></li><li><p>eventedPleg：</p><ul><li><p>这是基于事件实现的PLEG，通过 CRI 事件直接获取状态更新，相比依赖轮询的传统方法减少了节点资源消耗，但是这需要 CRI 运行时支持 CRI 事件。</p></li><li><p>其主循环有两个：<code>go wait.Until(e.watchEventsChannel, 0, e.stopCh)</code>和<br><code>go wait.Until(e.updateGlobalCache, </code><em><code>globalCacheUpdatePeriod</code></em><code>, e.stopCacheUpdateCh)</code>。</p></li><li><p>对于<code>e.watchEventsChannel</code>，</p><ul><li><p>其会向容器运行时发起请求，要求运行时将所有容器的生命周期事件（如创建、启动、停止、删除）通过一个 channel (<code>containerEventsResponseCh</code>) 发送过来。如果多次连接失败会停止eventPleg，转而启动GenericPLEG。</p></li><li><p>它还会不断从<code>containerEventsResponseCh</code>读取事件，然后获取对应pod的status，更新自身的cache，然后将事件发送给<code>eventChannel</code></p></li></ul></li></ul></li></ul><h1 id="主循环syncLoopIteration"><a href="#主循环syncLoopIteration" class="headerlink" title="主循环syncLoopIteration"></a>主循环<code>syncLoopIteration</code></h1><p>其主循环处理为<code>syncLoopIteration</code>，其核心是使用一个 select 语句同时监听来自多个不同源的事件通道，一旦任何一个通道有事件到达，它就会立即处理，从而驱动节点上的 Pod 达到其期望状态。代码如下。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// syncLoopIteration reads from various channels and dispatches pods to the</span><br><span class="hljs-comment">// given handler.</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// Arguments:</span><br><span class="hljs-comment">// 1.  configCh:       a channel to read config events from</span><br><span class="hljs-comment">// 2.  handler:        the SyncHandler to dispatch pods to</span><br><span class="hljs-comment">// 3.  syncCh:         a channel to read periodic sync events from</span><br><span class="hljs-comment">// 4.  housekeepingCh: a channel to read housekeeping events from</span><br><span class="hljs-comment">// 5.  plegCh:         a channel to read PLEG updates from</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// Events are also read from the kubelet liveness manager&#x27;s update channel.</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// The workflow is to read from one of the channels, handle that event, and</span><br><span class="hljs-comment">// update the timestamp in the sync loop monitor.</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// Here is an appropriate place to note that despite the syntactical</span><br><span class="hljs-comment">// similarity to the switch statement, the case statements in a select are</span><br><span class="hljs-comment">// evaluated in a pseudorandom order if there are multiple channels ready to</span><br><span class="hljs-comment">// read from when the select is evaluated.  In other words, case statements</span><br><span class="hljs-comment">// are evaluated in random order, and you can not assume that the case</span><br><span class="hljs-comment">// statements evaluate in order if multiple channels have events.</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// With that in mind, in truly no particular order, the different channels</span><br><span class="hljs-comment">// are handled as follows:</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">//   - configCh: dispatch the pods for the config change to the appropriate</span><br><span class="hljs-comment">//     handler callback for the event type</span><br><span class="hljs-comment">//   - plegCh: update the runtime cache; sync pod</span><br><span class="hljs-comment">//   - syncCh: sync all pods waiting for sync</span><br><span class="hljs-comment">//   - housekeepingCh: trigger cleanup of pods</span><br><span class="hljs-comment">//   - health manager: sync pods that have failed or in which one or more</span><br><span class="hljs-comment">//     containers have failed health checks</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(kl *Kubelet)</span></span> syncLoopIteration(ctx context.Context, configCh &lt;-<span class="hljs-keyword">chan</span> kubetypes.PodUpdate, handler SyncHandler,<br>    syncCh &lt;-<span class="hljs-keyword">chan</span> time.Time, housekeepingCh &lt;-<span class="hljs-keyword">chan</span> time.Time, plegCh &lt;-<span class="hljs-keyword">chan</span> *pleg.PodLifecycleEvent) <span class="hljs-type">bool</span> &#123;<br>    <span class="hljs-keyword">select</span> &#123;<br>    <span class="hljs-keyword">case</span> u, open := &lt;-configCh:<br>       <span class="hljs-comment">// Update from a config source; dispatch it to the right handler</span><br>       <span class="hljs-comment">// callback.</span><br>       <span class="hljs-keyword">if</span> !open &#123;<br>          klog.ErrorS(<span class="hljs-literal">nil</span>, <span class="hljs-string">&quot;Update channel is closed, exiting the sync loop&quot;</span>)<br>          <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span><br>       &#125;<br><br>       <span class="hljs-keyword">switch</span> u.Op &#123;<br>       <span class="hljs-keyword">case</span> kubetypes.ADD:<br>          klog.V(<span class="hljs-number">2</span>).InfoS(<span class="hljs-string">&quot;SyncLoop ADD&quot;</span>, <span class="hljs-string">&quot;source&quot;</span>, u.Source, <span class="hljs-string">&quot;pods&quot;</span>, klog.KObjSlice(u.Pods))<br>          <span class="hljs-comment">// After restarting, kubelet will get all existing pods through</span><br>          <span class="hljs-comment">// ADD as if they are new pods. These pods will then go through the</span><br>          <span class="hljs-comment">// admission process and *may* be rejected. This can be resolved</span><br>          <span class="hljs-comment">// once we have checkpointing.</span><br>          handler.HandlePodAdditions(u.Pods)<br>       <span class="hljs-keyword">case</span> kubetypes.UPDATE:<br>          klog.V(<span class="hljs-number">2</span>).InfoS(<span class="hljs-string">&quot;SyncLoop UPDATE&quot;</span>, <span class="hljs-string">&quot;source&quot;</span>, u.Source, <span class="hljs-string">&quot;pods&quot;</span>, klog.KObjSlice(u.Pods))<br>          handler.HandlePodUpdates(u.Pods)<br>       <span class="hljs-keyword">case</span> kubetypes.REMOVE:<br>          klog.V(<span class="hljs-number">2</span>).InfoS(<span class="hljs-string">&quot;SyncLoop REMOVE&quot;</span>, <span class="hljs-string">&quot;source&quot;</span>, u.Source, <span class="hljs-string">&quot;pods&quot;</span>, klog.KObjSlice(u.Pods))<br>          handler.HandlePodRemoves(u.Pods)<br>       <span class="hljs-keyword">case</span> kubetypes.RECONCILE:<br>          klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;SyncLoop RECONCILE&quot;</span>, <span class="hljs-string">&quot;source&quot;</span>, u.Source, <span class="hljs-string">&quot;pods&quot;</span>, klog.KObjSlice(u.Pods))<br>          handler.HandlePodReconcile(u.Pods)<br>       <span class="hljs-keyword">case</span> kubetypes.DELETE:<br>          klog.V(<span class="hljs-number">2</span>).InfoS(<span class="hljs-string">&quot;SyncLoop DELETE&quot;</span>, <span class="hljs-string">&quot;source&quot;</span>, u.Source, <span class="hljs-string">&quot;pods&quot;</span>, klog.KObjSlice(u.Pods))<br>          <span class="hljs-comment">// DELETE is treated as a UPDATE because of graceful deletion.</span><br>          handler.HandlePodUpdates(u.Pods)<br>       <span class="hljs-keyword">case</span> kubetypes.SET:<br>          <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span> Do we want to support this?</span><br>          klog.ErrorS(<span class="hljs-literal">nil</span>, <span class="hljs-string">&quot;Kubelet does not support snapshot update&quot;</span>)<br>       <span class="hljs-keyword">default</span>:<br>          klog.ErrorS(<span class="hljs-literal">nil</span>, <span class="hljs-string">&quot;Invalid operation type received&quot;</span>, <span class="hljs-string">&quot;operation&quot;</span>, u.Op)<br>       &#125;<br><br>       kl.sourcesReady.AddSource(u.Source)<br><br>    <span class="hljs-keyword">case</span> e := &lt;-plegCh:<br>       <span class="hljs-keyword">if</span> isSyncPodWorthy(e) &#123;<br>          <span class="hljs-comment">// PLEG event for a pod; sync it.</span><br>          <span class="hljs-keyword">if</span> pod, ok := kl.podManager.GetPodByUID(e.ID); ok &#123;<br>             klog.V(<span class="hljs-number">2</span>).InfoS(<span class="hljs-string">&quot;SyncLoop (PLEG): event for pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;event&quot;</span>, e)<br>             handler.HandlePodSyncs([]*v1.Pod&#123;pod&#125;)<br>          &#125; <span class="hljs-keyword">else</span> &#123;<br>             <span class="hljs-comment">// If the pod no longer exists, ignore the event.</span><br>             klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;SyncLoop (PLEG): pod does not exist, ignore irrelevant event&quot;</span>, <span class="hljs-string">&quot;event&quot;</span>, e)<br>          &#125;<br>       &#125;<br><br>       <span class="hljs-keyword">if</span> e.Type == pleg.ContainerDied &#123;<br>          <span class="hljs-keyword">if</span> containerID, ok := e.Data.(<span class="hljs-type">string</span>); ok &#123;<br>             kl.cleanUpContainersInPod(e.ID, containerID)<br>          &#125;<br>       &#125;<br>    <span class="hljs-keyword">case</span> &lt;-syncCh:<br>       <span class="hljs-comment">// Sync pods waiting for sync</span><br>       podsToSync := kl.getPodsToSync()<br>       <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(podsToSync) == <span class="hljs-number">0</span> &#123;<br>          <span class="hljs-keyword">break</span><br>       &#125;<br>       klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;SyncLoop (SYNC) pods&quot;</span>, <span class="hljs-string">&quot;total&quot;</span>, <span class="hljs-built_in">len</span>(podsToSync), <span class="hljs-string">&quot;pods&quot;</span>, klog.KObjSlice(podsToSync))<br>       handler.HandlePodSyncs(podsToSync)<br>    <span class="hljs-keyword">case</span> update := &lt;-kl.livenessManager.Updates():<br>       <span class="hljs-keyword">if</span> update.Result == proberesults.Failure &#123;<br>          handleProbeSync(kl, update, handler, <span class="hljs-string">&quot;liveness&quot;</span>, <span class="hljs-string">&quot;unhealthy&quot;</span>)<br>       &#125;<br>    <span class="hljs-keyword">case</span> update := &lt;-kl.readinessManager.Updates():<br>       ready := update.Result == proberesults.Success<br>       kl.statusManager.SetContainerReadiness(update.PodUID, update.ContainerID, ready)<br><br>       status := <span class="hljs-string">&quot;not ready&quot;</span><br>       <span class="hljs-keyword">if</span> ready &#123;<br>          status = <span class="hljs-string">&quot;ready&quot;</span><br>       &#125;<br>       handleProbeSync(kl, update, handler, <span class="hljs-string">&quot;readiness&quot;</span>, status)<br>    <span class="hljs-keyword">case</span> update := &lt;-kl.startupManager.Updates():<br>       started := update.Result == proberesults.Success<br>       kl.statusManager.SetContainerStartup(update.PodUID, update.ContainerID, started)<br><br>       status := <span class="hljs-string">&quot;unhealthy&quot;</span><br>       <span class="hljs-keyword">if</span> started &#123;<br>          status = <span class="hljs-string">&quot;started&quot;</span><br>       &#125;<br>       handleProbeSync(kl, update, handler, <span class="hljs-string">&quot;startup&quot;</span>, status)<br>    <span class="hljs-keyword">case</span> update := &lt;-kl.containerManager.Updates():<br>       pods := []*v1.Pod&#123;&#125;<br>       <span class="hljs-keyword">for</span> _, p := <span class="hljs-keyword">range</span> update.PodUIDs &#123;<br>          <span class="hljs-keyword">if</span> pod, ok := kl.podManager.GetPodByUID(types.UID(p)); ok &#123;<br>             klog.V(<span class="hljs-number">3</span>).InfoS(<span class="hljs-string">&quot;SyncLoop (containermanager): event for pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;event&quot;</span>, update)<br>             pods = <span class="hljs-built_in">append</span>(pods, pod)<br>          &#125; <span class="hljs-keyword">else</span> &#123;<br>             <span class="hljs-comment">// If the pod no longer exists, ignore the event.</span><br>             klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;SyncLoop (containermanager): pod does not exist, ignore devices updates&quot;</span>, <span class="hljs-string">&quot;event&quot;</span>, update)<br>          &#125;<br>       &#125;<br>       <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(pods) &gt; <span class="hljs-number">0</span> &#123;<br>          <span class="hljs-comment">// Updating the pod by syncing it again</span><br>          <span class="hljs-comment">// We do not apply the optimization by updating the status directly, but can do it later</span><br>          handler.HandlePodSyncs(pods)<br>       &#125;<br><br>    <span class="hljs-keyword">case</span> &lt;-housekeepingCh:<br>       <span class="hljs-keyword">if</span> !kl.sourcesReady.AllReady() &#123;<br>          <span class="hljs-comment">// If the sources aren&#x27;t ready or volume manager has not yet synced the states,</span><br>          <span class="hljs-comment">// skip housekeeping, as we may accidentally delete pods from unready sources.</span><br>          klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;SyncLoop (housekeeping, skipped): sources aren&#x27;t ready yet&quot;</span>)<br>       &#125; <span class="hljs-keyword">else</span> &#123;<br>          start := time.Now()<br>          klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;SyncLoop (housekeeping)&quot;</span>)<br>          <span class="hljs-keyword">if</span> err := handler.HandlePodCleanups(ctx); err != <span class="hljs-literal">nil</span> &#123;<br>             klog.ErrorS(err, <span class="hljs-string">&quot;Failed cleaning pods&quot;</span>)<br>          &#125;<br>          duration := time.Since(start)<br>          <span class="hljs-keyword">if</span> duration &gt; housekeepingWarningDuration &#123;<br>             klog.ErrorS(fmt.Errorf(<span class="hljs-string">&quot;housekeeping took too long&quot;</span>), <span class="hljs-string">&quot;Housekeeping took longer than expected&quot;</span>, <span class="hljs-string">&quot;expected&quot;</span>, housekeepingWarningDuration, <span class="hljs-string">&quot;actual&quot;</span>, duration.Round(time.Millisecond))<br>          &#125;<br>          klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;SyncLoop (housekeeping) end&quot;</span>, <span class="hljs-string">&quot;duration&quot;</span>, duration.Round(time.Millisecond))<br>       &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span><br>&#125;<br></code></pre></td></tr></table></figure><p>监控的事件来源有：</p><ul><li><p><code>configCh</code> (配置变更)</p><ul><li><p><strong>来源:&#x20;</strong>&#x41;PI Server、静态 Pod 文件目录、HTTP 端点。当有 Pod 的增、删、改、查（ADD, UPDATE, REMOVE, RECONCILE, DELETE）等期望状态变更时，事件会发送到这个通道。</p></li><li><p><strong>处理:&#x20;</strong>&#x51FD;数会根据事件类型（<code>u.op</code>），调用 <code>SyncHandler</code> 对应的处理方法（如 <code>HandlePodAdditions</code>, <code>HandlePodUpdates</code>,<code>HandlePodRemoves</code>,<code>HandlePodReconcile</code> ），将变更分发下去，最终触发 podWorkers 去执行具体的 Pod 操作。</p></li></ul></li><li><p><code>plegCh</code> (Pod 生命周期事件)</p><ul><li><p><strong>来源:&#x20;</strong>&#x50;LEG (Pod Lifecycle Event Generator)通过监控底层的容器运行时（如 containerd）获取的事件，它反映容器的实际状态变化，例如容器启动、死亡、创建失败等。</p></li><li><p><strong>处理:&#x20;</strong>&#x5F53;收到值得同步的 PLEG 事件时（<code>isSyncPodWorthy</code>），Kubelet 会为对应的 Pod 触发一次同步（<code>HandlePodSyncs</code>），以更新其在 API Server 上的状态，使其与节点的真实情况保持一致。</p></li></ul></li><li><p><code>syncCh</code> (周期性同步)</p><ul><li><p><strong>来源:&#x20;</strong>&#x4E00;个定时器，在 <code>syncLoop</code> 中定义，每秒触发一次。</p></li><li><p><strong>处理:&#x20;</strong>&#x8C03;用 <code>getPodsToSync()</code> 获取所有需要同步的 Pod（比如之前同步失败的、或有内部模块请求同步的），然后调用 <code>HandlePodSyncs</code> 对它们进行批量同步。这是一个“兜底”机制，确保即使有事件丢失，Pod 状态最终也能被校准。</p></li></ul></li><li><p><code>livenessManager</code>, <code>readinessManager</code>, <code>startupManager</code> 的 <code>Updates()</code> 通道 (探针结果)</p><ul><li><p><strong>来源:&#x20;</strong>&#x5065;康探针（Liveness&#x2F;Readiness&#x2F;Startup Probes）探测到某个容器的探针结果发生变化（如从成功变为失败）后会发送事件到通道中。</p></li><li><p><strong>处理:&#x20;</strong>&#x5F53;收到探针失败或成功的更新时，Kubelet 会立即为该 Pod 触发一次同步（<code>HandlePodSyncs</code>），以更新 Pod 的 <code>status.conditions</code>。例如，Readiness 探针失败会导致 Pod 的 <code>Ready</code> 条件变为 <code>False</code>。</p></li></ul></li><li><p><code>housekeepingCh</code> (内务管理)</p><ul><li><p><strong>来源:&#x20;</strong>&#x4E00;个定时器，每隔 <code>housekeepingPeriod</code>（默认2秒）触发一次。</p></li><li><p><strong>处理:&#x20;</strong>&#x8C03;用 <code>handler.HandlePodCleanups()</code> 执行清理工作，比如垃圾回收已终止的 Pod 和容器所占用的资源（如网络、挂载目录等）。这个操作只有在所有配置源都就绪后才会执行，以防误删。</p></li></ul></li></ul><h2 id="关键事件处理"><a href="#关键事件处理" class="headerlink" title="关键事件处理"></a>关键事件处理</h2><p><code>SyncHandler</code>主要包含以下的一些处理</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// SyncHandler is an interface implemented by Kubelet, for testability</span><br><span class="hljs-keyword">type</span> SyncHandler <span class="hljs-keyword">interface</span> &#123;<br>    HandlePodAdditions(pods []*v1.Pod)<br>    HandlePodUpdates(pods []*v1.Pod)<br>    HandlePodRemoves(pods []*v1.Pod)<br>    HandlePodReconcile(pods []*v1.Pod)<br>    HandlePodSyncs(pods []*v1.Pod)<br>    HandlePodCleanups(ctx context.Context) <span class="hljs-type">error</span><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="HandlePodAdditions"><a href="#HandlePodAdditions" class="headerlink" title="HandlePodAdditions"></a>HandlePodAdditions</h3><p>处理由API Server、静态 Pod 文件目录、HTTP 导致的pod添加的事件，流程如下：</p><ul><li><p>对所有添加的pod按创建时间进行排序，再逐个处理</p></li><li><p>把pod加入podManager用来表示pod以及被kubelet感知到</p></li><li><p>如果是静态pod：</p><ul><li>kubelet 不会走常规的创建流程，而是会找到它对应的真实 Pod（由 API Server 管理），并向 podWorkers 发送一个 SyncPodUpdate 请求。这通常意味着要根据静态 Pod 文件的变化来更新正在运行的 Pod。</li></ul></li><li><p>如果是普通的pod：</p><ul><li><p>进行准入检查：检查pod是否已经被标记为需要终止。例如，Kubelet 可能刚重启，就收到了一个已经被删除的 Pod 的 ADD 事件，这时需要忽略掉对其的处理。</p></li><li><p>进行准入控制：获取到所有的allocated pods，然后通过各个admitHandlers来计算当前的资源是否可以提交这个pod。下面简单介绍predicateAdmitHandler和evicition_manager的检查逻辑：</p><ul><li><p>predicateAdmitHandler：</p><ul><li><p>首先检查一些就算抢占其他pod也无法解决的限制，如系统、主机名；</p></li><li><p>根据节点基本信息、已有pod、额外插件资源得到完整的节点信息；</p></li><li><p>执行与调度器中filter类似的检查，检查节点的 CPU、内存、存储、Pod 数量等是否足够， Pod 请求的主机端口是否已被占用，节点的标签是否满足 Pod 的节点亲和性&#x2F;选择器要求， Pod 是否能容忍节点上的 NoExecute 污点（Taint）等</p></li><li><p>如果当前检查没有通过，但这个pod是关键函数则会尝试执行抢占，抢占的逻辑大致为通过贪心找到能满足需要的资源且已占有的资源最少的pod。</p></li><li><p>如果还是不成功就会返回第一个不成功的原因。</p></li></ul></li><li><p>evicition_manager：</p><ul><li><p>如果没有condition就说明节点目前没有压力，直接放行</p></li><li><p>查看当前pod是否是关键pod，如果是就直接放行。判断的依据是查看其是否是静态pod或者优先级是否大于等于 2 * <em>HighestUserDefinablePriority&#x20;</em>&#x3D; 2000000000。</p></li><li><p>如果condition中只有内存压力，那么就会放行非 BestEffort Pod，然后对于 BestEffort Pod会检查它的容忍度，如果配置了对“内存压力”污点（Taint）的容忍度（Toleration）也会放心。</p></li><li><p>此外在有压力的情况下就不会放行了。</p></li></ul></li><li><p>最后调用podWorkers去执行UpdatePod函数，并传入pod及kubetypes.<em>SyncPodCreate</em>事件去继续处理，UpdatePod函数介绍如下：</p><ul><li><p>获取或创建<code>podSyncStatus</code></p></li><li><p>处理状态转化，对于要终止的pod，会设置<code>status.terminatingAt</code>字段为当前时间</p></li><li><p>会启动或通知 <code>PodWorker</code> Goroutine。</p></li></ul></li></ul></li></ul></li></ul><h1 id="PodWorker循环podWorkerLoop"><a href="#PodWorker循环podWorkerLoop" class="headerlink" title="PodWorker循环podWorkerLoop"></a>PodWorker循环podWorkerLoop</h1><p><code>podWorkerLoop</code> 是 Kubelet 中<strong>每个 Pod 专属的生命周期管理者</strong>。当 <code>UpdatePod</code> 函数第一次接收到关于某个新 Pod 的更新时，它就会为这个 Pod 启动一个独立的 <code>podWorkerLoop</code> goroutine。这个 goroutine 的职责就是<strong>串行地、顺序地</strong>处理这个 Pod 从生到死的所有状态转换，确保每一步都正确执行。</p><p>这个函数的核心是一个 <code>for range podUpdates</code> 循环。<code>podUpdates</code> 是一个 channel，每当 <code>UpdatePod</code> 函数接收到关于这个 Pod 的新指令时，就会向这个 channel 发送一个信号，从而唤醒这个循环。</p><ol><li><p>等待并获取工作 (<code>for range podUpdates</code>):</p><ul><li>循环会阻塞在这里，等待新的更新信号。</li></ul></li><li><p>准备同步 (<code>p.startPodSync(podUID)</code>):</p><ul><li><p>一旦被唤醒，它首先会调用 <code>startPodSync</code>。这个函数负责：</p><ul><li><p>从 <code>podSyncStatus</code> 中取出待处理的更新任务 (<code>pendingUpdate</code>)。</p></li><li><p>检查 Pod 是否可以开始工作（例如，对于静态 Pod，要确保没有同名的旧 Pod 还在运行）。</p></li><li><p>如果不能立即开始，<code>startPodSync</code> 会返回 <code>canStart: false</code>，<code>podWorkerLoop</code> 就会 <code>continue</code>，继续等待下一次更新。</p></li><li><p>如果 Pod 已经被标记为“永远无法启动”（例如，在启动前就被删除了），<code>startPodSync</code> 会返回 <code>canEverStart: false</code>，<code>podWorkerLoop</code> 就会直接 <code>return</code>，彻底退出。</p></li><li><p>如果一切正常，<code>startPodSync</code> 会返回 <code>canStart: true</code> 和 <code>canEverStart: true</code>，以及包含了具体工作内容的 <code>update</code> 对象。</p></li></ul></li></ul></li><li><p>执行核心同步逻辑 (<code>err := func() error &#123; ... &#125;</code>):</p><ul><li><p>这是一个匿名函数，主要是为了方便地处理 <code>err</code>。</p></li><li><p>获取最新状态: <code>status, err = p.podCache.GetNewerThan(...)</code>。在执行任何操作前，它会尝试从 PLEG（Pod Lifecycle Event Generator）的缓存中获取 Pod 的最新运行时状态（<code>kubecontainer.PodStatus</code>）。这确保了接下来的决策是基于最新的容器状态做出的。</p></li><li><p>根据工作类型分派任务: <code>switch &#123; case update.WorkType == ... &#125;</code></p><ul><li><p><code>case TerminatedPod</code>: 如果工作类型是“已终止”，则调用 <code>p.podSyncer.SyncTerminatedPod</code>。这个函数负责最后的清理工作，比如卸载卷、清理网络等。</p></li><li><p><code>case TerminatingPod</code>: 如果工作类型是“正在终止”，则调用 <code>p.podSyncer.SyncTerminatingPod</code>。这个函数的核心任务是杀死 Pod 中的所有容器，并遵循优雅终止的宽限期。</p></li><li><p><code>default</code> (即 <code>SyncPod</code>): 这是最常见的类型，调用 <code>p.podSyncer.SyncPod</code>。这个函数是 Kubelet 的核心同步逻辑，它会：</p><ul><li><p>对比 Pod 的期望状态（<code>pod.Spec</code>）和实际状态（从容器运行时获取）。</p></li><li><p>创建缺失的容器。</p></li><li><p>杀死多余或不健康的容器。</p></li><li><p>设置沙箱（Pod 的网络命名空间等）。</p></li><li><p><code>SyncPod</code> 会返回一个布尔值 <code>isTerminal</code>，如果 Pod 的所有容器都已运行完毕并且根据重启策略（<code>RestartPolicy</code>）不需要再重启，<code>isTerminal</code> 就会是 <code>true</code>。</p></li></ul></li></ul></li></ul></li><li><p>处理同步结果:</p><ul><li><p><code>switch &#123; case err == context.Canceled: ... &#125;</code>: 如果错误是 <code>context.Canceled</code>，说明 <code>UpdatePod</code> 函数因为收到了更紧急的指令（如立即终止）而取消了这次同步。<code>podWorkerLoop</code> 会直接等待下一次被唤醒。</p></li><li><p><code>case err != nil</code>: 如果发生了其他错误，会打印错误日志，并通过 <code>completeWork</code> 函数将这个 Pod 加入到工作队列中，以便稍后进行退避重试（backoff retry）。</p></li><li><p><code>case update.WorkType == TerminatedPod</code>: 如果 <code>SyncTerminatedPod</code> 成功完成，说明 Pod 的生命周期彻底结束。调用 <code>completeTerminated</code> 来清理 <code>podWorkers</code> 的内部状态，然后 <code>podWorkerLoop</code> 就可以 <code>return</code>，这个 goroutine 也就退出了。</p></li><li><p><code>case update.WorkType == TerminatingPod</code>: 如果 <code>SyncTerminatingPod</code> 成功完成，说明所有容器都已被杀死。调用 <code>completeTerminating</code> 将 Pod 状态更新为“已终止”，并立即触发一次新的更新，以便进入 <code>TerminatedPod</code> 阶段进行清理。</p></li><li><p><code>case isTerminal</code>: 如果 <code>SyncPod</code> 返回 <code>isTerminal: true</code>，说明 Pod 自然运行结束。调用 <code>completeSync</code> 将 Pod 状态更新为“正在终止”，并立即触发一次新的更新，以便进入 <code>TerminatingPod</code> 阶段。</p></li></ul></li><li><p>完成工作并准备下一次循环 (<code>p.completeWork(...)</code>):</p><ul><li><p>这个函数会根据同步结果（成功、失败、阶段转换）来决定下一次重试的时间间隔，并将 Pod 的 UID 重新加入到 <code>workQueue</code> 中。</p></li><li><p>它还会检查 <code>podSyncStatus</code> 中是否已经有新的 <code>pendingUpdate</code>，如果有，就立即向 <code>podUpdates</code> channel 发送信号，触发下一次循环，实现状态转换的无缝衔接。</p></li></ul></li></ol><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// podWorkerLoop manages sequential state updates to a pod in a goroutine, exiting once the final</span><br><span class="hljs-comment">// state is reached. The loop is responsible for driving the pod through four main phases:</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// 1. Wait to start, guaranteeing no two pods with the same UID or same fullname are running at the same time</span><br><span class="hljs-comment">// 2. Sync, orchestrating pod setup by reconciling the desired pod spec with the runtime state of the pod</span><br><span class="hljs-comment">// 3. Terminating, ensuring all running containers in the pod are stopped</span><br><span class="hljs-comment">// 4. Terminated, cleaning up any resources that must be released before the pod can be deleted</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// The podWorkerLoop is driven by updates delivered to UpdatePod and by SyncKnownPods. If a particular</span><br><span class="hljs-comment">// sync method fails, p.workerQueue is updated with backoff but it is the responsibility of the kubelet</span><br><span class="hljs-comment">// to trigger new UpdatePod calls. SyncKnownPods will only retry pods that are no longer known to the</span><br><span class="hljs-comment">// caller. When a pod transitions working-&gt;terminating or terminating-&gt;terminated, the next update is</span><br><span class="hljs-comment">// queued immediately and no kubelet action is required.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *podWorkers)</span></span> podWorkerLoop(podUID types.UID, podUpdates &lt;-<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;) &#123;<br>    <span class="hljs-keyword">var</span> lastSyncTime time.Time<br>    <span class="hljs-keyword">for</span> <span class="hljs-keyword">range</span> podUpdates &#123;<br>       ctx, update, canStart, canEverStart, ok := p.startPodSync(podUID)<br>       <span class="hljs-comment">// If we had no update waiting, it means someone initialized the channel without filling out pendingUpdate.</span><br>       <span class="hljs-keyword">if</span> !ok &#123;<br>          <span class="hljs-keyword">continue</span><br>       &#125;<br>       <span class="hljs-comment">// If the pod was terminated prior to the pod being allowed to start, we exit the loop.</span><br>       <span class="hljs-keyword">if</span> !canEverStart &#123;<br>          <span class="hljs-keyword">return</span><br>       &#125;<br>       <span class="hljs-comment">// If the pod is not yet ready to start, continue and wait for more updates.</span><br>       <span class="hljs-keyword">if</span> !canStart &#123;<br>          <span class="hljs-keyword">continue</span><br>       &#125;<br><br>       podUID, podRef := podUIDAndRefForUpdate(update.Options)<br><br>       klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Processing pod event&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, podRef, <span class="hljs-string">&quot;podUID&quot;</span>, podUID, <span class="hljs-string">&quot;updateType&quot;</span>, update.WorkType)<br>       <span class="hljs-keyword">var</span> isTerminal <span class="hljs-type">bool</span><br>       err := <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> <span class="hljs-type">error</span> &#123;<br>          <span class="hljs-comment">// The worker is responsible for ensuring the sync method sees the appropriate</span><br>          <span class="hljs-comment">// status updates on resyncs (the result of the last sync), transitions to</span><br>          <span class="hljs-comment">// terminating (no wait), or on terminated (whatever the most recent state is).</span><br>          <span class="hljs-comment">// Only syncing and terminating can generate pod status changes, while terminated</span><br>          <span class="hljs-comment">// pods ensure the most recent status makes it to the api server.</span><br>          <span class="hljs-keyword">var</span> status *kubecontainer.PodStatus<br>          <span class="hljs-keyword">var</span> err <span class="hljs-type">error</span><br>          <span class="hljs-keyword">switch</span> &#123;<br>          <span class="hljs-keyword">case</span> update.Options.RunningPod != <span class="hljs-literal">nil</span>:<br>             <span class="hljs-comment">// when we receive a running pod, we don&#x27;t need status at all because we are</span><br>             <span class="hljs-comment">// guaranteed to be terminating and we skip updates to the pod</span><br>          <span class="hljs-keyword">default</span>:<br>             <span class="hljs-comment">// wait until we see the next refresh from the PLEG via the cache (max 2s)</span><br>             <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span> this adds ~1s of latency on all transitions from sync to terminating</span><br>             <span class="hljs-comment">//  to terminated, and on all termination retries (including evictions). We should</span><br>             <span class="hljs-comment">//  improve latency by making the pleg continuous and by allowing pod status</span><br>             <span class="hljs-comment">//  changes to be refreshed when key events happen (killPod, sync-&gt;terminating).</span><br>             <span class="hljs-comment">//  Improving this latency also reduces the possibility that a terminated</span><br>             <span class="hljs-comment">//  container&#x27;s status is garbage collected before we have a chance to update the</span><br>             <span class="hljs-comment">//  API server (thus losing the exit code).</span><br>             status, err = p.podCache.GetNewerThan(update.Options.Pod.UID, lastSyncTime)<br><br>             <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>                <span class="hljs-comment">// This is the legacy event thrown by manage pod loop all other events are now dispatched</span><br>                <span class="hljs-comment">// from syncPodFn</span><br>                p.recorder.Eventf(update.Options.Pod, v1.EventTypeWarning, events.FailedSync, <span class="hljs-string">&quot;error determining status: %v&quot;</span>, err)<br>                <span class="hljs-keyword">return</span> err<br>             &#125;<br>          &#125;<br><br>          <span class="hljs-comment">// Take the appropriate action (illegal phases are prevented by UpdatePod)</span><br>          <span class="hljs-keyword">switch</span> &#123;<br>          <span class="hljs-keyword">case</span> update.WorkType == TerminatedPod:<br>             err = p.podSyncer.SyncTerminatedPod(ctx, update.Options.Pod, status)<br><br>          <span class="hljs-keyword">case</span> update.WorkType == TerminatingPod:<br>             <span class="hljs-keyword">var</span> gracePeriod *<span class="hljs-type">int64</span><br>             <span class="hljs-keyword">if</span> opt := update.Options.KillPodOptions; opt != <span class="hljs-literal">nil</span> &#123;<br>                gracePeriod = opt.PodTerminationGracePeriodSecondsOverride<br>             &#125;<br>             podStatusFn := p.acknowledgeTerminating(podUID)<br><br>             <span class="hljs-comment">// if we only have a running pod, terminate it directly</span><br>             <span class="hljs-keyword">if</span> update.Options.RunningPod != <span class="hljs-literal">nil</span> &#123;<br>                err = p.podSyncer.SyncTerminatingRuntimePod(ctx, update.Options.RunningPod)<br>             &#125; <span class="hljs-keyword">else</span> &#123;<br>                err = p.podSyncer.SyncTerminatingPod(ctx, update.Options.Pod, status, gracePeriod, podStatusFn)<br>             &#125;<br><br>          <span class="hljs-keyword">default</span>:<br>             isTerminal, err = p.podSyncer.SyncPod(ctx, update.Options.UpdateType, update.Options.Pod, update.Options.MirrorPod, status)<br>          &#125;<br><br>          lastSyncTime = p.clock.Now()<br>          <span class="hljs-keyword">return</span> err<br>       &#125;()<br><br>       <span class="hljs-keyword">var</span> phaseTransition <span class="hljs-type">bool</span><br>       <span class="hljs-keyword">switch</span> &#123;<br>       <span class="hljs-keyword">case</span> err == context.Canceled:<br>          <span class="hljs-comment">// when the context is cancelled we expect an update to already be queued</span><br>          klog.V(<span class="hljs-number">2</span>).InfoS(<span class="hljs-string">&quot;Sync exited with context cancellation error&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, podRef, <span class="hljs-string">&quot;podUID&quot;</span>, podUID, <span class="hljs-string">&quot;updateType&quot;</span>, update.WorkType)<br><br>       <span class="hljs-keyword">case</span> err != <span class="hljs-literal">nil</span>:<br>          <span class="hljs-comment">// we will queue a retry</span><br>          klog.ErrorS(err, <span class="hljs-string">&quot;Error syncing pod, skipping&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, podRef, <span class="hljs-string">&quot;podUID&quot;</span>, podUID)<br><br>       <span class="hljs-keyword">case</span> update.WorkType == TerminatedPod:<br>          <span class="hljs-comment">// we can shut down the worker</span><br>          p.completeTerminated(podUID)<br>          <span class="hljs-keyword">if</span> start := update.Options.StartTime; !start.IsZero() &#123;<br>             metrics.PodWorkerDuration.WithLabelValues(<span class="hljs-string">&quot;terminated&quot;</span>).Observe(metrics.SinceInSeconds(start))<br>          &#125;<br>          klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Processing pod event done&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, podRef, <span class="hljs-string">&quot;podUID&quot;</span>, podUID, <span class="hljs-string">&quot;updateType&quot;</span>, update.WorkType)<br>          <span class="hljs-keyword">return</span><br><br>       <span class="hljs-keyword">case</span> update.WorkType == TerminatingPod:<br>          <span class="hljs-comment">// pods that don&#x27;t exist in config don&#x27;t need to be terminated, other loops will clean them up</span><br>          <span class="hljs-keyword">if</span> update.Options.RunningPod != <span class="hljs-literal">nil</span> &#123;<br>             p.completeTerminatingRuntimePod(podUID)<br>             <span class="hljs-keyword">if</span> start := update.Options.StartTime; !start.IsZero() &#123;<br>                metrics.PodWorkerDuration.WithLabelValues(update.Options.UpdateType.String()).Observe(metrics.SinceInSeconds(start))<br>             &#125;<br>             klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Processing pod event done&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, podRef, <span class="hljs-string">&quot;podUID&quot;</span>, podUID, <span class="hljs-string">&quot;updateType&quot;</span>, update.WorkType)<br>             <span class="hljs-keyword">return</span><br>          &#125;<br>          <span class="hljs-comment">// otherwise we move to the terminating phase</span><br>          p.completeTerminating(podUID)<br>          phaseTransition = <span class="hljs-literal">true</span><br><br>       <span class="hljs-keyword">case</span> isTerminal:<br>          <span class="hljs-comment">// if syncPod indicated we are now terminal, set the appropriate pod status to move to terminating</span><br>          klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Pod is terminal&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, podRef, <span class="hljs-string">&quot;podUID&quot;</span>, podUID, <span class="hljs-string">&quot;updateType&quot;</span>, update.WorkType)<br>          p.completeSync(podUID)<br>          phaseTransition = <span class="hljs-literal">true</span><br>       &#125;<br><br>       <span class="hljs-comment">// queue a retry if necessary, then put the next event in the channel if any</span><br>       p.completeWork(podUID, phaseTransition, err)<br>       <span class="hljs-keyword">if</span> start := update.Options.StartTime; !start.IsZero() &#123;<br>          metrics.PodWorkerDuration.WithLabelValues(update.Options.UpdateType.String()).Observe(metrics.SinceInSeconds(start))<br>       &#125;<br>       klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Processing pod event done&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, podRef, <span class="hljs-string">&quot;podUID&quot;</span>, podUID, <span class="hljs-string">&quot;updateType&quot;</span>, update.WorkType)<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="Kubelet中的SyncPod"><a href="#Kubelet中的SyncPod" class="headerlink" title="Kubelet中的SyncPod"></a>Kubelet中的SyncPod</h2><p><code>SyncPod</code> 是 Kubelet 中负责<strong>创建和维持 Pod 运行状态</strong>的核心工作函数。当 <code>podWorkerLoop</code> 决定要对一个 Pod 进行同步时，就会调用这个函数。它的核心目标是：<strong>将 Pod 的实际状态（Actual State）驱动为其期望状态（Desired State）</strong>。</p><p>这个函数是<strong>可重入的（reentrant）</strong>，这意味着它可以被反复调用，并且每次调用都会尝试让 Pod 更接近其最终的期望状态。如果中途发生可恢复的错误，下一次调用 <code>SyncPod</code> 时会从失败的地方继续尝试。</p><h3 id="SyncPod-的核心工作流程："><a href="#SyncPod-的核心工作流程：" class="headerlink" title="SyncPod 的核心工作流程："></a><code>SyncPod</code> 的核心工作流程：</h3><p>这个函数执行一个非常长的、事务性的操作序列，以确保一个 Pod 被正确地建立起来。如果其中任何一步失败，函数会返回错误，<code>podWorkerLoop</code> 会在稍后重试整个流程。</p><ol><li><p>前置准备和状态生成:</p><ul><li><p>记录延迟: 如果是第一次创建 Pod (<code>SyncPodCreate</code>)，会记录从 Kubelet 首次看到 Pod 到 <code>podWorker</code> 开始处理之间的延迟，用于性能监控。</p></li><li><p>处理资源伸缩 (Resize): 如果启用了 <code>InPlacePodVerticalScaling</code> 特性，它会先检查并处理 Pod 的资源伸缩请求。</p></li><li><p>生成 API 状态 (<code>generateAPIPodStatus</code>): 这是非常关键的一步。它会结合 Pod 的 <code>spec</code>、<code>statusManager</code> 中缓存的状态以及从容器运行时获取的最新 <code>podStatus</code>，生成一个最终要上报给 API Server 的 <code>v1.PodStatus</code> 对象。</p></li><li><p>检查是否已终结: 如果生成的 <code>apiPodStatus</code> 显示 Pod 已经处于 <code>Succeeded</code> 或 <code>Failed</code> 状态，说明 Pod 已经运行结束。<code>SyncPod</code> 会将这个最终状态更新到 <code>statusManager</code>，然后返回 <code>isTerminal: true</code>，通知 <code>podWorkerLoop</code> 进入终止流程。</p></li></ul></li><li><p>状态更新和前置检查:</p><ul><li><p>更新状态管理器 (<code>statusManager.SetPodStatus</code>): 将上一步生成的 <code>apiPodStatus</code> 更新到 Kubelet 的状态管理器中。这是 Kubelet 内部对 Pod 状态的权威记录。</p></li><li><p>检查网络插件: 如果网络插件还没准备好，并且 Pod 不是 <code>hostNetwork</code> 模式，那么就报错并退出，等待网络就绪。</p></li><li><p>注册 Secret&#x2F;ConfigMap: 通知 <code>secretManager</code> 和 <code>configMapManager</code>，这个 Pod 依赖了某些 Secret 和 ConfigMap，以便它们可以开始监视和挂载这些资源。</p></li></ul></li><li><p>资源准备和创建:</p><ul><li><p>创建 Cgroup (<code>pcm.EnsureExists</code>): 如果启用了 <code>cgroups-per-qos</code>，它会为 Pod 创建对应的 Cgroup，并应用资源限制（如 CPU、内存限制）。这里有一个特殊逻辑：如果 Kubelet 重启后发现一个已存在的 Pod 没有 Cgroup，它会先杀死这个 Pod 的所有容器，然后再创建 Cgroup 并重新拉起容器，以确保 Pod 运行在正确的 Cgroup 控制下。</p></li><li><p>创建 Mirror Pod (<code>tryReconcileMirrorPods</code>): 如果这是一个静态 Pod（Static Pod），并且还没有对应的镜像 Pod（Mirror Pod），<code>SyncPod</code> 会负责创建它。镜像 Pod 是静态 Pod 在 API Server 中的一个只读映射，目的是让集群的其他组件（如调度器）能够看到这个 Pod 并计算其资源占用。</p></li><li><p>创建数据目录 (<code>makePodDataDirs</code>): 为 Pod 创建所需的数据目录，例如 <code>/var/lib/kubelet/pods/&lt;pod-uid&gt;/volumes</code> 等。</p></li><li><p>等待卷挂载 (<code>volumeManager.WaitForAttachAndMount</code>): 这是非常重要的一步。它会阻塞在这里，直到 Pod 所需的所有存储卷（Volume）都已经被成功地附加（Attach）到节点并挂载（Mount）到 Pod 的数据目录中。</p></li></ul></li><li><p>容器运行时同步:</p><ul><li><p>获取拉取镜像的密钥 (<code>getPullSecretsForPod</code>): 从 Secret Manager 中获取拉取 Pod 镜像所需的 <code>imagePullSecrets</code>。</p></li><li><p>启动探针 (<code>probeManager.AddPod</code>): 通知 <code>probeManager</code> 开始对这个 Pod 进行存活探针（Liveness Probe）和就绪探针（Readiness Probe）。</p></li><li><p>调用容器运行时 (<code>kl.containerRuntime.SyncPod</code>): 这是整个流程中最核心的调用。Kubelet 将 Pod 的 <code>spec</code>、<code>podStatus</code>、拉取密钥等所有信息打包，传递给底层的容器运行时（如 containerd 或 CRI-O）。容器运行时会负责：</p><ul><li><p>创建或更新 Pod 的沙箱（Sandbox）。</p></li><li><p>拉取容器镜像。</p></li><li><p>创建和启动容器。</p></li><li><p>应用容器级别的配置。</p></li></ul></li><li><p><code>containerRuntime.SyncPod</code> 会返回一个结果，包含了同步过程中发生的任何错误。</p></li></ul></li><li><p>收尾工作:</p><ul><li><p>更新 Reason 缓存: 将容器运行时的同步结果更新到 <code>reasonCache</code>，这有助于调试。</p></li><li><p>处理原地伸缩结果: 如果有容器原地伸缩的操作，根据结果更新 Pod 的 <code>PodResizeInProgress</code> 状况。</p></li><li><p>返回结果: 将 <code>containerRuntime.SyncPod</code> 的错误返回给 <code>podWorkerLoop</code>。如果返回 <code>nil</code> 错误，则表示本次同步成功，Pod 已经处于其期望的运行状态。</p></li></ul></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><code>SyncPod</code> 是一个精心设计的、健壮的事务性脚本。它像一个建筑工头，按照一份详细的蓝图（Pod Spec），一步一步地协调各种资源（Cgroup、网络、存储、Secret），并最终指挥容器运行时这个“施工队”来完成 Pod 的“建造”工作。它的可重入性和详细的步骤确保了即使在复杂的环境中，Pod 也能够被可靠地创建和维护。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br></pre></td><td class="code"><pre><code class="hljs go"><br><span class="hljs-comment">// SyncPod is the transaction script for the sync of a single pod (setting up)</span><br><span class="hljs-comment">// a pod. This method is reentrant and expected to converge a pod towards the</span><br><span class="hljs-comment">// desired state of the spec. The reverse (teardown) is handled in</span><br><span class="hljs-comment">// SyncTerminatingPod and SyncTerminatedPod. If SyncPod exits without error,</span><br><span class="hljs-comment">// then the pod runtime state is in sync with the desired configuration state</span><br><span class="hljs-comment">// (pod is running). If SyncPod exits with a transient error, the next</span><br><span class="hljs-comment">// invocation of SyncPod is expected to make progress towards reaching the</span><br><span class="hljs-comment">// desired state. SyncPod exits with isTerminal when the pod was detected to</span><br><span class="hljs-comment">// have reached a terminal lifecycle phase due to container exits (for</span><br><span class="hljs-comment">// RestartNever or RestartOnFailure) and the next method invoked will be</span><br><span class="hljs-comment">// SyncTerminatingPod. If the pod terminates for any other reason, SyncPod</span><br><span class="hljs-comment">// will receive a context cancellation and should exit as soon as possible.</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// Arguments:</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// updateType - whether this is a create (first time) or an update, should</span><br><span class="hljs-comment">// only be used for metrics since this method must be reentrant</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// pod - the pod that is being set up</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// mirrorPod - the mirror pod known to the kubelet for this pod, if any</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// podStatus - the most recent pod status observed for this pod which can</span><br><span class="hljs-comment">// be used to determine the set of actions that should be taken during</span><br><span class="hljs-comment">// this loop of SyncPod</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// The workflow is:</span><br><span class="hljs-comment">//   - If the pod is being created, record pod worker start latency</span><br><span class="hljs-comment">//   - Call generateAPIPodStatus to prepare an v1.PodStatus for the pod</span><br><span class="hljs-comment">//   - If the pod is being seen as running for the first time, record pod</span><br><span class="hljs-comment">//     start latency</span><br><span class="hljs-comment">//   - Update the status of the pod in the status manager</span><br><span class="hljs-comment">//   - Stop the pod&#x27;s containers if it should not be running due to soft</span><br><span class="hljs-comment">//     admission</span><br><span class="hljs-comment">//   - Ensure any background tracking for a runnable pod is started</span><br><span class="hljs-comment">//   - Create a mirror pod if the pod is a static pod, and does not</span><br><span class="hljs-comment">//     already have a mirror pod</span><br><span class="hljs-comment">//   - Create the data directories for the pod if they do not exist</span><br><span class="hljs-comment">//   - Wait for volumes to attach/mount</span><br><span class="hljs-comment">//   - Fetch the pull secrets for the pod</span><br><span class="hljs-comment">//   - Call the container runtime&#x27;s SyncPod callback</span><br><span class="hljs-comment">//   - Update the traffic shaping for the pod&#x27;s ingress and egress limits</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// If any step of this workflow errors, the error is returned, and is repeated</span><br><span class="hljs-comment">// on the next SyncPod call.</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// This operation writes all events that are dispatched in order to provide</span><br><span class="hljs-comment">// the most accurate information possible about an error situation to aid debugging.</span><br><span class="hljs-comment">// Callers should not write an event if this operation returns an error.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(kl *Kubelet)</span></span> SyncPod(ctx context.Context, updateType kubetypes.SyncPodType, pod, mirrorPod *v1.Pod, podStatus *kubecontainer.PodStatus) (isTerminal <span class="hljs-type">bool</span>, err <span class="hljs-type">error</span>) &#123;<br>    ctx, otelSpan := kl.tracer.Start(ctx, <span class="hljs-string">&quot;syncPod&quot;</span>, trace.WithAttributes(<br>       semconv.K8SPodUIDKey.String(<span class="hljs-type">string</span>(pod.UID)),<br>       attribute.String(<span class="hljs-string">&quot;k8s.pod&quot;</span>, klog.KObj(pod).String()),<br>       semconv.K8SPodNameKey.String(pod.Name),<br>       attribute.String(<span class="hljs-string">&quot;k8s.pod.update_type&quot;</span>, updateType.String()),<br>       semconv.K8SNamespaceNameKey.String(pod.Namespace),<br>    ))<br>    klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;SyncPod enter&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;podUID&quot;</span>, pod.UID)<br>    <span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>       <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>          otelSpan.RecordError(err)<br>          otelSpan.SetStatus(codes.Error, err.Error())<br>       &#125;<br>       klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;SyncPod exit&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;podUID&quot;</span>, pod.UID, <span class="hljs-string">&quot;isTerminal&quot;</span>, isTerminal)<br>       otelSpan.End()<br>    &#125;()<br><br>    <span class="hljs-comment">// Latency measurements for the main workflow are relative to the</span><br>    <span class="hljs-comment">// first time the pod was seen by kubelet.</span><br>    <span class="hljs-keyword">var</span> firstSeenTime time.Time<br>    <span class="hljs-keyword">if</span> firstSeenTimeStr, ok := pod.Annotations[kubetypes.ConfigFirstSeenAnnotationKey]; ok &#123;<br>       firstSeenTime = kubetypes.ConvertToTimestamp(firstSeenTimeStr).Get()<br>    &#125;<br><br>    <span class="hljs-comment">// Record pod worker start latency if being created</span><br>    <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span> make pod workers record their own latencies</span><br>    <span class="hljs-keyword">if</span> updateType == kubetypes.SyncPodCreate &#123;<br>       <span class="hljs-keyword">if</span> !firstSeenTime.IsZero() &#123;<br>          <span class="hljs-comment">// This is the first time we are syncing the pod. Record the latency</span><br>          <span class="hljs-comment">// since kubelet first saw the pod if firstSeenTime is set.</span><br>          metrics.PodWorkerStartDuration.Observe(metrics.SinceInSeconds(firstSeenTime))<br>       &#125; <span class="hljs-keyword">else</span> &#123;<br>          klog.V(<span class="hljs-number">3</span>).InfoS(<span class="hljs-string">&quot;First seen time not recorded for pod&quot;</span>,<br>             <span class="hljs-string">&quot;podUID&quot;</span>, pod.UID,<br>             <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>       &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// handlePodResourcesResize updates the pod to use the allocated resources. This should come</span><br>    <span class="hljs-comment">// before the main business logic of SyncPod, so that a consistent view of the pod is used</span><br>    <span class="hljs-comment">// across the sync loop.</span><br>    <span class="hljs-keyword">if</span> utilfeature.DefaultFeatureGate.Enabled(features.InPlacePodVerticalScaling) &#123;<br>       <span class="hljs-comment">// Handle pod resize here instead of doing it in HandlePodUpdates because</span><br>       <span class="hljs-comment">// this conveniently retries any Deferred resize requests</span><br>       <span class="hljs-comment">// TODO(vinaykul,InPlacePodVerticalScaling): Investigate doing this in HandlePodUpdates + periodic SyncLoop scan</span><br>       <span class="hljs-comment">//     See: https://github.com/kubernetes/kubernetes/pull/102884#discussion_r663160060</span><br>       pod, err = kl.handlePodResourcesResize(pod, podStatus)<br>       <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>          <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>, err<br>       &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// Generate final API pod status with pod and status manager status</span><br>    apiPodStatus := kl.generateAPIPodStatus(pod, podStatus, <span class="hljs-literal">false</span>)<br>    <span class="hljs-comment">// The pod IP may be changed in generateAPIPodStatus if the pod is using host network. (See #24576)</span><br>    <span class="hljs-comment">// TODO(random-liu): After writing pod spec into container labels, check whether pod is using host network, and</span><br>    <span class="hljs-comment">// set pod IP to hostIP directly in runtime.GetPodStatus</span><br>    podStatus.IPs = <span class="hljs-built_in">make</span>([]<span class="hljs-type">string</span>, <span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(apiPodStatus.PodIPs))<br>    <span class="hljs-keyword">for</span> _, ipInfo := <span class="hljs-keyword">range</span> apiPodStatus.PodIPs &#123;<br>       podStatus.IPs = <span class="hljs-built_in">append</span>(podStatus.IPs, ipInfo.IP)<br>    &#125;<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(podStatus.IPs) == <span class="hljs-number">0</span> &amp;&amp; <span class="hljs-built_in">len</span>(apiPodStatus.PodIP) &gt; <span class="hljs-number">0</span> &#123;<br>       podStatus.IPs = []<span class="hljs-type">string</span>&#123;apiPodStatus.PodIP&#125;<br>    &#125;<br><br>    <span class="hljs-comment">// If the pod is terminal, we don&#x27;t need to continue to setup the pod</span><br>    <span class="hljs-keyword">if</span> apiPodStatus.Phase == v1.PodSucceeded || apiPodStatus.Phase == v1.PodFailed &#123;<br>       kl.statusManager.SetPodStatus(pod, apiPodStatus)<br>       isTerminal = <span class="hljs-literal">true</span><br>       <span class="hljs-keyword">return</span> isTerminal, <span class="hljs-literal">nil</span><br>    &#125;<br><br>    <span class="hljs-comment">// Record the time it takes for the pod to become running</span><br>    <span class="hljs-comment">// since kubelet first saw the pod if firstSeenTime is set.</span><br>    existingStatus, ok := kl.statusManager.GetPodStatus(pod.UID)<br>    <span class="hljs-keyword">if</span> !ok || existingStatus.Phase == v1.PodPending &amp;&amp; apiPodStatus.Phase == v1.PodRunning &amp;&amp;<br>       !firstSeenTime.IsZero() &#123;<br>       metrics.PodStartDuration.Observe(metrics.SinceInSeconds(firstSeenTime))<br>    &#125;<br><br>    kl.statusManager.SetPodStatus(pod, apiPodStatus)<br><br>    <span class="hljs-comment">// If the network plugin is not ready, only start the pod if it uses the host network</span><br>    <span class="hljs-keyword">if</span> err := kl.runtimeState.networkErrors(); err != <span class="hljs-literal">nil</span> &amp;&amp; !kubecontainer.IsHostNetworkPod(pod) &#123;<br>       kl.recorder.Eventf(pod, v1.EventTypeWarning, events.NetworkNotReady, <span class="hljs-string">&quot;%s: %v&quot;</span>, NetworkNotReadyErrorMsg, err)<br>       <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>, fmt.Errorf(<span class="hljs-string">&quot;%s: %v&quot;</span>, NetworkNotReadyErrorMsg, err)<br>    &#125;<br><br>    <span class="hljs-comment">// ensure the kubelet knows about referenced secrets or configmaps used by the pod</span><br>    <span class="hljs-keyword">if</span> !kl.podWorkers.IsPodTerminationRequested(pod.UID) &#123;<br>       <span class="hljs-keyword">if</span> kl.secretManager != <span class="hljs-literal">nil</span> &#123;<br>          kl.secretManager.RegisterPod(pod)<br>       &#125;<br>       <span class="hljs-keyword">if</span> kl.configMapManager != <span class="hljs-literal">nil</span> &#123;<br>          kl.configMapManager.RegisterPod(pod)<br>       &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// Create Cgroups for the pod and apply resource parameters</span><br>    <span class="hljs-comment">// to them if cgroups-per-qos flag is enabled.</span><br>    pcm := kl.containerManager.NewPodContainerManager()<br>    <span class="hljs-comment">// If pod has already been terminated then we need not create</span><br>    <span class="hljs-comment">// or update the pod&#x27;s cgroup</span><br>    <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span> once context cancellation is added this check can be removed</span><br>    <span class="hljs-keyword">if</span> !kl.podWorkers.IsPodTerminationRequested(pod.UID) &#123;<br>       <span class="hljs-comment">// When the kubelet is restarted with the cgroups-per-qos</span><br>       <span class="hljs-comment">// flag enabled, all the pod&#x27;s running containers</span><br>       <span class="hljs-comment">// should be killed intermittently and brought back up</span><br>       <span class="hljs-comment">// under the qos cgroup hierarchy.</span><br>       <span class="hljs-comment">// Check if this is the pod&#x27;s first sync</span><br>       firstSync := <span class="hljs-literal">true</span><br>       <span class="hljs-keyword">for</span> _, containerStatus := <span class="hljs-keyword">range</span> apiPodStatus.ContainerStatuses &#123;<br>          <span class="hljs-keyword">if</span> containerStatus.State.Running != <span class="hljs-literal">nil</span> &#123;<br>             firstSync = <span class="hljs-literal">false</span><br>             <span class="hljs-keyword">break</span><br>          &#125;<br>       &#125;<br>       <span class="hljs-comment">// Don&#x27;t kill containers in pod if pod&#x27;s cgroups already</span><br>       <span class="hljs-comment">// exists or the pod is running for the first time</span><br>       podKilled := <span class="hljs-literal">false</span><br>       <span class="hljs-keyword">if</span> !pcm.Exists(pod) &amp;&amp; !firstSync &#123;<br>          p := kubecontainer.ConvertPodStatusToRunningPod(kl.getRuntime().Type(), podStatus)<br>          <span class="hljs-keyword">if</span> err := kl.killPod(ctx, pod, p, <span class="hljs-literal">nil</span>); err == <span class="hljs-literal">nil</span> &#123;<br>             podKilled = <span class="hljs-literal">true</span><br>          &#125; <span class="hljs-keyword">else</span> &#123;<br>             <span class="hljs-keyword">if</span> wait.Interrupted(err) &#123;<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>, <span class="hljs-literal">nil</span><br>             &#125;<br>             klog.ErrorS(err, <span class="hljs-string">&quot;KillPod failed&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;podStatus&quot;</span>, podStatus)<br>          &#125;<br>       &#125;<br>       <span class="hljs-comment">// Create and Update pod&#x27;s Cgroups</span><br>       <span class="hljs-comment">// Don&#x27;t create cgroups for run once pod if it was killed above</span><br>       <span class="hljs-comment">// The current policy is not to restart the run once pods when</span><br>       <span class="hljs-comment">// the kubelet is restarted with the new flag as run once pods are</span><br>       <span class="hljs-comment">// expected to run only once and if the kubelet is restarted then</span><br>       <span class="hljs-comment">// they are not expected to run again.</span><br>       <span class="hljs-comment">// We don&#x27;t create and apply updates to cgroup if its a run once pod and was killed above</span><br>       <span class="hljs-keyword">if</span> !(podKilled &amp;&amp; pod.Spec.RestartPolicy == v1.RestartPolicyNever) &#123;<br>          <span class="hljs-keyword">if</span> !pcm.Exists(pod) &#123;<br>             <span class="hljs-keyword">if</span> err := kl.containerManager.UpdateQOSCgroups(); err != <span class="hljs-literal">nil</span> &#123;<br>                klog.V(<span class="hljs-number">2</span>).InfoS(<span class="hljs-string">&quot;Failed to update QoS cgroups while syncing pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;err&quot;</span>, err)<br>             &#125;<br>             <span class="hljs-keyword">if</span> err := pcm.EnsureExists(pod); err != <span class="hljs-literal">nil</span> &#123;<br>                kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToCreatePodContainer, <span class="hljs-string">&quot;unable to ensure pod container exists: %v&quot;</span>, err)<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>, fmt.Errorf(<span class="hljs-string">&quot;failed to ensure that the pod: %v cgroups exist and are correctly applied: %v&quot;</span>, pod.UID, err)<br>             &#125;<br>          &#125;<br>       &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// Create Mirror Pod for Static Pod if it doesn&#x27;t already exist</span><br>    kl.tryReconcileMirrorPods(pod, mirrorPod)<br><br>    <span class="hljs-comment">// Make data directories for the pod</span><br>    <span class="hljs-keyword">if</span> err := kl.makePodDataDirs(pod); err != <span class="hljs-literal">nil</span> &#123;<br>       kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedToMakePodDataDirectories, <span class="hljs-string">&quot;error making pod data directories: %v&quot;</span>, err)<br>       klog.ErrorS(err, <span class="hljs-string">&quot;Unable to make pod data directories for pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>       <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>, err<br>    &#125;<br><br>    <span class="hljs-comment">// Wait for volumes to attach/mount</span><br>    <span class="hljs-keyword">if</span> err := kl.volumeManager.WaitForAttachAndMount(ctx, pod); err != <span class="hljs-literal">nil</span> &#123;<br>       <span class="hljs-keyword">if</span> !wait.Interrupted(err) &#123;<br>          kl.recorder.Eventf(pod, v1.EventTypeWarning, events.FailedMountVolume, <span class="hljs-string">&quot;Unable to attach or mount volumes: %v&quot;</span>, err)<br>          klog.ErrorS(err, <span class="hljs-string">&quot;Unable to attach or mount volumes for pod; skipping pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>       &#125;<br>       <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>, err<br>    &#125;<br><br>    <span class="hljs-comment">// Fetch the pull secrets for the pod</span><br>    pullSecrets := kl.getPullSecretsForPod(pod)<br><br>    <span class="hljs-comment">// Ensure the pod is being probed</span><br>    kl.probeManager.AddPod(pod)<br><br>    <span class="hljs-comment">// TODO(#113606): use cancellation from the incoming context parameter, which comes from the pod worker.</span><br>    <span class="hljs-comment">// Currently, using cancellation from that context causes test failures. To remove this WithoutCancel,</span><br>    <span class="hljs-comment">// any wait.Interrupted errors need to be filtered from result and bypass the reasonCache - cancelling</span><br>    <span class="hljs-comment">// the context for SyncPod is a known and deliberate error, not a generic error.</span><br>    <span class="hljs-comment">// Use WithoutCancel instead of a new context.TODO() to propagate trace context</span><br>    <span class="hljs-comment">// Call the container runtime&#x27;s SyncPod callback</span><br>    sctx := context.WithoutCancel(ctx)<br>    result := kl.containerRuntime.SyncPod(sctx, pod, podStatus, pullSecrets, kl.crashLoopBackOff)<br>    kl.reasonCache.Update(pod.UID, result)<br><br>    <span class="hljs-keyword">for</span> _, r := <span class="hljs-keyword">range</span> result.SyncResults &#123;<br>       <span class="hljs-keyword">if</span> r.Action == kubecontainer.ResizePodInPlace &#123;<br>          <span class="hljs-keyword">if</span> r.Error == <span class="hljs-literal">nil</span> &#123;<br>             <span class="hljs-comment">// The pod was resized successfully, clear any pod resize errors in the PodResizeInProgress condition.</span><br>             kl.statusManager.SetPodResizeInProgressCondition(pod.UID, <span class="hljs-string">&quot;&quot;</span>, <span class="hljs-string">&quot;&quot;</span>, <span class="hljs-literal">true</span>)<br>          &#125; <span class="hljs-keyword">else</span> &#123;<br>             kl.statusManager.SetPodResizeInProgressCondition(pod.UID, v1.PodReasonError, r.Message, <span class="hljs-literal">false</span>)<br>          &#125;<br>       &#125;<br>    &#125;<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>, result.Error()<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="kubeGenericRuntimeManager的syncPod"><a href="#kubeGenericRuntimeManager的syncPod" class="headerlink" title="kubeGenericRuntimeManager的syncPod"></a>kubeGenericRuntimeManager的syncPod</h2><p>这个函数是 Kubelet 与底层容器运行时（如 containerd, CRI-O）交互的<strong>核心</strong>。当 Kubelet 的 <code>podWorkerLoop</code> 决定要同步一个 Pod 的状态时，最终就会调用这个函数。它的职责是<strong>将 Pod 的实际状态（Actual State）调整为期望状态（Desired State）</strong>。</p><p>整个函数可以看作一个精心编排的、包含多个步骤的事务性过程。如果中途某一步失败，函数会返回错误，上层逻辑（<code>podWorkerLoop</code>）会在稍后重试整个 <code>SyncPod</code> 流程。</p><h3 id="SyncPod-的-8-个核心步骤："><a href="#SyncPod-的-8-个核心步骤：" class="headerlink" title="SyncPod 的 8 个核心步骤："></a><code>SyncPod</code> 的 8 个核心步骤：</h3><p>函数注释中清晰地列出了它的工作流程，我们来逐一解析：</p><h4 id="步骤-1-计算-Pod-的变更-m-computePodActions"><a href="#步骤-1-计算-Pod-的变更-m-computePodActions" class="headerlink" title="步骤 1: 计算 Pod 的变更 (m.computePodActions)"></a><strong>步骤 1: 计算 Pod 的变更 (</strong><code>m.computePodActions</code><strong>)</strong></h4><p>这是 <code>SyncPod</code> 的“大脑”。它会比较 Pod 的期望配置 (<code>pod</code> spec) 和从容器运行时获取的当前实际状态 (<code>podStatus</code>)，然后计算出需要执行的一系列具体操作，封装在 <code>podContainerChanges</code> 对象里。这些操作包括：</p><ul><li><p><code>KillPod</code>: 是否需要杀死整个 Pod（包括它的网络沙箱）。</p></li><li><p><code>CreateSandbox</code>: 是否需要创建一个新的网络沙箱。</p></li><li><p><code>ContainersToKill</code>: 一个列表，包含需要被杀死的容器（比如：健康检查失败、定义已变更等）。</p></li><li><p><code>ContainersToStart</code>: 一个列表，包含需要被启动或重启的应用容器。</p></li><li><p><code>InitContainersToStart</code>: 需要启动的 Init 容器。</p></li><li><p><code>EphemeralContainersToStart</code>: 需要启动的临时容器（Ephemeral Container）。</p></li><li><p><code>ContainersToUpdate</code>: 需要原地更新资源的容器（用于垂直扩缩容）。</p></li></ul><h4 id="步骤-2-如有必要，杀死整个-Pod-if-podContainerChanges-KillPod"><a href="#步骤-2-如有必要，杀死整个-Pod-if-podContainerChanges-KillPod" class="headerlink" title="步骤 2: 如有必要，杀死整个 Pod (if podContainerChanges.KillPod)"></a><strong>步骤 2: 如有必要，杀死整个 Pod (</strong><code>if podContainerChanges.KillPod</code><strong>)</strong></h4><p>如果步骤 1 计算出需要杀死整个 Pod（通常是因为网络沙箱的配置变了，或者所有容器都已终止且无需重启），这一步就会执行。它会调用 <code>killPodWithSyncResult</code> 来：</p><ol><li><p>停止 Pod 内所有的容器。</p></li><li><p>停止并销毁 Pod 的网络沙箱。 如果此步骤失败，<code>SyncPod</code> 会立即返回。</p></li></ol><h4 id="步骤-3-杀死不需要的容器"><a href="#步骤-3-杀死不需要的容器" class="headerlink" title="步骤 3: 杀死不需要的容器"></a><strong>步骤 3: 杀死不需要的容器</strong></h4><p>如果不需要杀死整个 Pod，这一步会遍历步骤 1 计算出的 <code>ContainersToKill</code> 列表，并调用 <code>m.killContainer</code> 来逐个停止那些不再需要的容器。</p><h4 id="步骤-4-如有必要，创建-Pod-沙箱-if-podContainerChanges-CreateSandbox"><a href="#步骤-4-如有必要，创建-Pod-沙箱-if-podContainerChanges-CreateSandbox" class="headerlink" title="步骤 4: 如有必要，创建 Pod 沙箱 (if podContainerChanges.CreateSandbox)"></a><strong>步骤 4: 如有必要，创建 Pod 沙箱 (</strong><code>if podContainerChanges.CreateSandbox</code><strong>)</strong></h4><p>如果步骤 1 计算出需要创建新的沙箱，这一步就会执行。它会调用 <code>m.createPodSandbox</code>，通过 CRI (Container Runtime Interface) 请求底层容器运行时创建一个新的网络环境（即 Pod Sandbox）。创建成功后，会获取并记录沙箱的 IP 地址。</p><h4 id="步骤-5-创建临时容器-Ephemeral-Containers"><a href="#步骤-5-创建临时容器-Ephemeral-Containers" class="headerlink" title="步骤 5: 创建临时容器 (Ephemeral Containers)"></a><strong>步骤 5: 创建临时容器 (Ephemeral Containers)</strong></h4><p>遍历 <code>EphemeralContainersToStart</code> 列表，启动所有需要的临时容器。临时容器通常用于调试，它们可以在 Pod 运行后动态添加进来。</p><h4 id="步骤-6-创建-Init-容器"><a href="#步骤-6-创建-Init-容器" class="headerlink" title="步骤 6: 创建 Init 容器"></a><strong>步骤 6: 创建 Init 容器</strong></h4><p>遍历 <code>InitContainersToStart</code> 列表，按顺序启动 Init 容器。<strong>这是关键的一步</strong>：</p><ul><li><p>对于不可重启的 Init 容器，如果任何一个启动失败，<code>SyncPod</code> 会立即中止并返回错误，因为后续的容器都不能再启动。</p></li><li><p>对于可重启的 Init 容器（Sidecar），如果启动失败，可能会跳过并继续尝试启动其他容器。</p></li></ul><h4 id="步骤-7-调整运行中容器的资源"><a href="#步骤-7-调整运行中容器的资源" class="headerlink" title="步骤 7: 调整运行中容器的资源"></a><strong>步骤 7: 调整运行中容器的资源</strong></h4><p>如果启用了原地垂直扩缩容（In-Place Pod Vertical Scaling）功能，并且步骤 1 计算出了需要更新资源的容器列表 (ContainersToUpdate)，这一步会调用 CRI 接口去更新正在运行容器的资源限制（如 CPU、内存）。</p><h4 id="步骤-8-创建应用容器"><a href="#步骤-8-创建应用容器" class="headerlink" title="步骤 8: 创建应用容器"></a><strong>步骤 8: 创建应用容器</strong></h4><p>这是最后一步，遍历 <code>ContainersToStart</code> 列表，启动所有常规的应用容器。与 Init 容器不同，如果某个应用容器启动失败，<code>SyncPod</code> 通常不会立即中止，而是会记录下这个失败，然后继续尝试启动其他容器。</p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p><code>SyncPod</code> 是 Kubelet 中一个非常健壮和核心的函数。它通过一个定义清晰的、可重试的步骤序列，确保了 Pod 的状态能够可靠地从任何当前状态收敛到其期望状态。它处理了从网络、存储、配置到容器生命周期的方方面面，是 Pod 得以在节点上正确运行的根本保障。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br></pre></td><td class="code"><pre><code class="hljs go"><br><span class="hljs-comment">// SyncPod syncs the running pod into the desired pod by executing following steps:</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">//  1. Compute sandbox and container changes.</span><br><span class="hljs-comment">//  2. Kill pod sandbox if necessary.</span><br><span class="hljs-comment">//  3. Kill any containers that should not be running.</span><br><span class="hljs-comment">//  4. Create sandbox if necessary.</span><br><span class="hljs-comment">//  5. Create ephemeral containers.</span><br><span class="hljs-comment">//  6. Create init containers.</span><br><span class="hljs-comment">//  7. Resize running containers (if InPlacePodVerticalScaling==true)</span><br><span class="hljs-comment">//  8. Create normal containers.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(m *kubeGenericRuntimeManager)</span></span> SyncPod(ctx context.Context, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) &#123;<br>    <span class="hljs-comment">// Step 1: Compute sandbox and container changes.</span><br>    podContainerChanges := m.computePodActions(ctx, pod, podStatus)<br>    klog.V(<span class="hljs-number">3</span>).InfoS(<span class="hljs-string">&quot;computePodActions got for pod&quot;</span>, <span class="hljs-string">&quot;podActions&quot;</span>, podContainerChanges, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>    <span class="hljs-keyword">if</span> podContainerChanges.CreateSandbox &#123;<br>       ref, err := ref.GetReference(legacyscheme.Scheme, pod)<br>       <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>          klog.ErrorS(err, <span class="hljs-string">&quot;Couldn&#x27;t make a ref to pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>       &#125;<br>       <span class="hljs-keyword">if</span> podContainerChanges.SandboxID != <span class="hljs-string">&quot;&quot;</span> &#123;<br>          m.recorder.Eventf(ref, v1.EventTypeNormal, events.SandboxChanged, <span class="hljs-string">&quot;Pod sandbox changed, it will be killed and re-created.&quot;</span>)<br>       &#125; <span class="hljs-keyword">else</span> &#123;<br>          klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;SyncPod received new pod, will create a sandbox for it&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>       &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// Step 2: Kill the pod if the sandbox has changed.</span><br>    <span class="hljs-keyword">if</span> podContainerChanges.KillPod &#123;<br>       <span class="hljs-keyword">if</span> podContainerChanges.CreateSandbox &#123;<br>          klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Stopping PodSandbox for pod, will start new one&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>       &#125; <span class="hljs-keyword">else</span> &#123;<br>          klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Stopping PodSandbox for pod, because all other containers are dead&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>       &#125;<br><br>       killResult := m.killPodWithSyncResult(ctx, pod, kubecontainer.ConvertPodStatusToRunningPod(m.runtimeName, podStatus), <span class="hljs-literal">nil</span>)<br>       result.AddPodSyncResult(killResult)<br>       <span class="hljs-keyword">if</span> killResult.Error() != <span class="hljs-literal">nil</span> &#123;<br>          klog.ErrorS(killResult.Error(), <span class="hljs-string">&quot;killPodWithSyncResult failed&quot;</span>)<br>          <span class="hljs-keyword">return</span><br>       &#125;<br><br>       <span class="hljs-keyword">if</span> podContainerChanges.CreateSandbox &#123;<br>          m.purgeInitContainers(ctx, pod, podStatus)<br>       &#125;<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>       <span class="hljs-comment">// Step 3: kill any running containers in this pod which are not to keep.</span><br>       <span class="hljs-keyword">for</span> containerID, containerInfo := <span class="hljs-keyword">range</span> podContainerChanges.ContainersToKill &#123;<br>          klog.V(<span class="hljs-number">3</span>).InfoS(<span class="hljs-string">&quot;Killing unwanted container for pod&quot;</span>, <span class="hljs-string">&quot;containerName&quot;</span>, containerInfo.name, <span class="hljs-string">&quot;containerID&quot;</span>, containerID, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>          killContainerResult := kubecontainer.NewSyncResult(kubecontainer.KillContainer, containerInfo.name)<br>          result.AddSyncResult(killContainerResult)<br>          <span class="hljs-keyword">if</span> err := m.killContainer(ctx, pod, containerID, containerInfo.name, containerInfo.message, containerInfo.reason, <span class="hljs-literal">nil</span>, <span class="hljs-literal">nil</span>); err != <span class="hljs-literal">nil</span> &#123;<br>             killContainerResult.Fail(kubecontainer.ErrKillContainer, err.Error())<br>             klog.ErrorS(err, <span class="hljs-string">&quot;killContainer for pod failed&quot;</span>, <span class="hljs-string">&quot;containerName&quot;</span>, containerInfo.name, <span class="hljs-string">&quot;containerID&quot;</span>, containerID, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>             <span class="hljs-keyword">return</span><br>          &#125;<br>       &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// Keep terminated init containers fairly aggressively controlled</span><br>    <span class="hljs-comment">// This is an optimization because container removals are typically handled</span><br>    <span class="hljs-comment">// by container garbage collector.</span><br>    m.pruneInitContainersBeforeStart(ctx, pod, podStatus)<br><br>    <span class="hljs-comment">// We pass the value of the PRIMARY podIP and list of podIPs down to</span><br>    <span class="hljs-comment">// generatePodSandboxConfig and generateContainerConfig, which in turn</span><br>    <span class="hljs-comment">// passes it to various other functions, in order to facilitate functionality</span><br>    <span class="hljs-comment">// that requires this value (hosts file and downward API) and avoid races determining</span><br>    <span class="hljs-comment">// the pod IP in cases where a container requires restart but the</span><br>    <span class="hljs-comment">// podIP isn&#x27;t in the status manager yet. The list of podIPs is used to</span><br>    <span class="hljs-comment">// generate the hosts file.</span><br>    <span class="hljs-comment">//</span><br>    <span class="hljs-comment">// We default to the IPs in the passed-in pod status, and overwrite them if the</span><br>    <span class="hljs-comment">// sandbox needs to be (re)started.</span><br>    <span class="hljs-keyword">var</span> podIPs []<span class="hljs-type">string</span><br>    <span class="hljs-keyword">if</span> podStatus != <span class="hljs-literal">nil</span> &#123;<br>       podIPs = podStatus.IPs<br>    &#125;<br><br>    <span class="hljs-comment">// Step 4: Create a sandbox for the pod if necessary.</span><br>    podSandboxID := podContainerChanges.SandboxID<br>    <span class="hljs-keyword">if</span> podContainerChanges.CreateSandbox &#123;<br>       <span class="hljs-keyword">var</span> msg <span class="hljs-type">string</span><br>       <span class="hljs-keyword">var</span> err <span class="hljs-type">error</span><br><br>       klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Creating PodSandbox for pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>       metrics.StartedPodsTotal.Inc()<br>       createSandboxResult := kubecontainer.NewSyncResult(kubecontainer.CreatePodSandbox, format.Pod(pod))<br>       result.AddSyncResult(createSandboxResult)<br><br>       <span class="hljs-comment">// ConvertPodSysctlsVariableToDotsSeparator converts sysctl variable</span><br>       <span class="hljs-comment">// in the Pod.Spec.SecurityContext.Sysctls slice into a dot as a separator.</span><br>       <span class="hljs-comment">// runc uses the dot as the separator to verify whether the sysctl variable</span><br>       <span class="hljs-comment">// is correct in a separate namespace, so when using the slash as the sysctl</span><br>       <span class="hljs-comment">// variable separator, runc returns an error: &quot;sysctl is not in a separate kernel namespace&quot;</span><br>       <span class="hljs-comment">// and the podSandBox cannot be successfully created. Therefore, before calling runc,</span><br>       <span class="hljs-comment">// we need to convert the sysctl variable, the dot is used as a separator to separate the kernel namespace.</span><br>       <span class="hljs-comment">// When runc supports slash as sysctl separator, this function can no longer be used.</span><br>       sysctl.ConvertPodSysctlsVariableToDotsSeparator(pod.Spec.SecurityContext)<br><br>       <span class="hljs-comment">// Prepare resources allocated by the Dynammic Resource Allocation feature for the pod</span><br>       <span class="hljs-keyword">if</span> utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) &#123;<br>          <span class="hljs-keyword">if</span> err := m.runtimeHelper.PrepareDynamicResources(ctx, pod); err != <span class="hljs-literal">nil</span> &#123;<br>             ref, referr := ref.GetReference(legacyscheme.Scheme, pod)<br>             <span class="hljs-keyword">if</span> referr != <span class="hljs-literal">nil</span> &#123;<br>                klog.ErrorS(referr, <span class="hljs-string">&quot;Couldn&#x27;t make a ref to pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>                <span class="hljs-keyword">return</span><br>             &#125;<br>             m.recorder.Eventf(ref, v1.EventTypeWarning, events.FailedPrepareDynamicResources, <span class="hljs-string">&quot;Failed to prepare dynamic resources: %v&quot;</span>, err)<br>             klog.ErrorS(err, <span class="hljs-string">&quot;Failed to prepare dynamic resources&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>             <span class="hljs-keyword">return</span><br>          &#125;<br>       &#125;<br><br>       podSandboxID, msg, err = m.createPodSandbox(ctx, pod, podContainerChanges.Attempt)<br>       <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>          <span class="hljs-comment">// createPodSandbox can return an error from CNI, CSI,</span><br>          <span class="hljs-comment">// or CRI if the Pod has been deleted while the POD is</span><br>          <span class="hljs-comment">// being created. If the pod has been deleted then it&#x27;s</span><br>          <span class="hljs-comment">// not a real error.</span><br>          <span class="hljs-comment">//</span><br>          <span class="hljs-comment">// SyncPod can still be running when we get here, which</span><br>          <span class="hljs-comment">// means the PodWorker has not acked the deletion.</span><br>          <span class="hljs-keyword">if</span> m.podStateProvider.IsPodTerminationRequested(pod.UID) &#123;<br>             klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Pod was deleted and sandbox failed to be created&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;podUID&quot;</span>, pod.UID)<br>             <span class="hljs-keyword">return</span><br>          &#125;<br>          metrics.StartedPodsErrorsTotal.Inc()<br>          createSandboxResult.Fail(kubecontainer.ErrCreatePodSandbox, msg)<br>          klog.ErrorS(err, <span class="hljs-string">&quot;CreatePodSandbox for pod failed&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>          ref, referr := ref.GetReference(legacyscheme.Scheme, pod)<br>          <span class="hljs-keyword">if</span> referr != <span class="hljs-literal">nil</span> &#123;<br>             klog.ErrorS(referr, <span class="hljs-string">&quot;Couldn&#x27;t make a ref to pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>          &#125;<br>          m.recorder.Eventf(ref, v1.EventTypeWarning, events.FailedCreatePodSandBox, <span class="hljs-string">&quot;Failed to create pod sandbox: %v&quot;</span>, err)<br>          <span class="hljs-keyword">return</span><br>       &#125;<br>       klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Created PodSandbox for pod&quot;</span>, <span class="hljs-string">&quot;podSandboxID&quot;</span>, podSandboxID, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br><br>       resp, err := m.runtimeService.PodSandboxStatus(ctx, podSandboxID, <span class="hljs-literal">false</span>)<br>       <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>          ref, referr := ref.GetReference(legacyscheme.Scheme, pod)<br>          <span class="hljs-keyword">if</span> referr != <span class="hljs-literal">nil</span> &#123;<br>             klog.ErrorS(referr, <span class="hljs-string">&quot;Couldn&#x27;t make a ref to pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>          &#125;<br>          m.recorder.Eventf(ref, v1.EventTypeWarning, events.FailedStatusPodSandBox, <span class="hljs-string">&quot;Unable to get pod sandbox status: %v&quot;</span>, err)<br>          klog.ErrorS(err, <span class="hljs-string">&quot;Failed to get pod sandbox status; Skipping pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>          result.Fail(err)<br>          <span class="hljs-keyword">return</span><br>       &#125;<br>       <span class="hljs-keyword">if</span> resp.GetStatus() == <span class="hljs-literal">nil</span> &#123;<br>          result.Fail(errors.New(<span class="hljs-string">&quot;pod sandbox status is nil&quot;</span>))<br>          <span class="hljs-keyword">return</span><br>       &#125;<br><br>       <span class="hljs-comment">// If we ever allow updating a pod from non-host-network to</span><br>       <span class="hljs-comment">// host-network, we may use a stale IP.</span><br>       <span class="hljs-keyword">if</span> !kubecontainer.IsHostNetworkPod(pod) &#123;<br>          <span class="hljs-comment">// Overwrite the podIPs passed in the pod status, since we just started the pod sandbox.</span><br>          podIPs = m.determinePodSandboxIPs(pod.Namespace, pod.Name, resp.GetStatus())<br>          klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Determined the ip for pod after sandbox changed&quot;</span>, <span class="hljs-string">&quot;IPs&quot;</span>, podIPs, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>       &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// the start containers routines depend on pod ip(as in primary pod ip)</span><br>    <span class="hljs-comment">// instead of trying to figure out if we have 0 &lt; len(podIPs)</span><br>    <span class="hljs-comment">// everytime, we short circuit it here</span><br>    podIP := <span class="hljs-string">&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(podIPs) != <span class="hljs-number">0</span> &#123;<br>       podIP = podIPs[<span class="hljs-number">0</span>]<br>    &#125;<br><br>    <span class="hljs-comment">// Get podSandboxConfig for containers to start.</span><br>    configPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID)<br>    result.AddSyncResult(configPodSandboxResult)<br>    podSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>       message := fmt.Sprintf(<span class="hljs-string">&quot;GeneratePodSandboxConfig for pod %q failed: %v&quot;</span>, format.Pod(pod), err)<br>       klog.ErrorS(err, <span class="hljs-string">&quot;GeneratePodSandboxConfig for pod failed&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>       configPodSandboxResult.Fail(kubecontainer.ErrConfigPodSandbox, message)<br>       <span class="hljs-keyword">return</span><br>    &#125;<br><br>    imageVolumePullResults, err := m.getImageVolumes(ctx, pod, podSandboxConfig, pullSecrets)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>       klog.ErrorS(err, <span class="hljs-string">&quot;Get image volumes for pod failed&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>       configPodSandboxResult.Fail(kubecontainer.ErrConfigPodSandbox, err.Error())<br>       <span class="hljs-keyword">return</span><br>    &#125;<br><br>    <span class="hljs-comment">// Helper containing boilerplate common to starting all types of containers.</span><br>    <span class="hljs-comment">// typeName is a description used to describe this type of container in log messages,</span><br>    <span class="hljs-comment">// currently: &quot;container&quot;, &quot;init container&quot; or &quot;ephemeral container&quot;</span><br>    <span class="hljs-comment">// metricLabel is the label used to describe this type of container in monitoring metrics.</span><br>    <span class="hljs-comment">// currently: &quot;container&quot;, &quot;init_container&quot; or &quot;ephemeral_container&quot;</span><br>    start := <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(ctx context.Context, typeName, metricLabel <span class="hljs-type">string</span>, spec *startSpec)</span></span> <span class="hljs-type">error</span> &#123;<br>       startContainerResult := kubecontainer.NewSyncResult(kubecontainer.StartContainer, spec.container.Name)<br>       result.AddSyncResult(startContainerResult)<br><br>       isInBackOff, msg, err := m.doBackOff(pod, spec.container, podStatus, backOff)<br>       <span class="hljs-keyword">if</span> isInBackOff &#123;<br>          startContainerResult.Fail(err, msg)<br>          klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Backing Off restarting container in pod&quot;</span>, <span class="hljs-string">&quot;containerType&quot;</span>, typeName, <span class="hljs-string">&quot;container&quot;</span>, spec.container, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>          <span class="hljs-keyword">return</span> err<br>       &#125;<br><br>       metrics.StartedContainersTotal.WithLabelValues(metricLabel).Inc()<br>       <span class="hljs-keyword">if</span> sc.HasWindowsHostProcessRequest(pod, spec.container) &#123;<br>          metrics.StartedHostProcessContainersTotal.WithLabelValues(metricLabel).Inc()<br>       &#125;<br>       klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Creating container in pod&quot;</span>, <span class="hljs-string">&quot;containerType&quot;</span>, typeName, <span class="hljs-string">&quot;container&quot;</span>, spec.container, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br><br>       <span class="hljs-comment">// We fail late here to populate the &quot;ErrImagePull&quot; and &quot;ImagePullBackOff&quot; correctly to the end user.</span><br>       imageVolumes, err := m.toKubeContainerImageVolumes(imageVolumePullResults, spec.container, pod, startContainerResult)<br>       <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>          <span class="hljs-keyword">return</span> err<br>       &#125;<br><br>       <span class="hljs-comment">// NOTE (aramase) podIPs are populated for single stack and dual stack clusters. Send only podIPs.</span><br>       msg, err = m.startContainer(ctx, podSandboxID, podSandboxConfig, spec, pod, podStatus, pullSecrets, podIP, podIPs, imageVolumes)<br>       incrementImageVolumeMetrics(err, msg, spec.container, imageVolumes)<br>       <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>          <span class="hljs-comment">// startContainer() returns well-defined error codes that have reasonable cardinality for metrics and are</span><br>          <span class="hljs-comment">// useful to cluster administrators to distinguish &quot;server errors&quot; from &quot;user errors&quot;.</span><br>          metrics.StartedContainersErrorsTotal.WithLabelValues(metricLabel, err.Error()).Inc()<br>          <span class="hljs-keyword">if</span> sc.HasWindowsHostProcessRequest(pod, spec.container) &#123;<br>             metrics.StartedHostProcessContainersErrorsTotal.WithLabelValues(metricLabel, err.Error()).Inc()<br>          &#125;<br>          startContainerResult.Fail(err, msg)<br>          <span class="hljs-comment">// known errors that are logged in other places are logged at higher levels here to avoid</span><br>          <span class="hljs-comment">// repetitive log spam</span><br>          <span class="hljs-keyword">switch</span> &#123;<br>          <span class="hljs-keyword">case</span> err == images.ErrImagePullBackOff:<br>             klog.V(<span class="hljs-number">3</span>).InfoS(<span class="hljs-string">&quot;Container start failed in pod&quot;</span>, <span class="hljs-string">&quot;containerType&quot;</span>, typeName, <span class="hljs-string">&quot;container&quot;</span>, spec.container, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;containerMessage&quot;</span>, msg, <span class="hljs-string">&quot;err&quot;</span>, err)<br>          <span class="hljs-keyword">default</span>:<br>             utilruntime.HandleError(fmt.Errorf(<span class="hljs-string">&quot;%v %+v start failed in pod %v: %v: %s&quot;</span>, typeName, spec.container, format.Pod(pod), err, msg))<br>          &#125;<br>          <span class="hljs-keyword">return</span> err<br>       &#125;<br><br>       <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>    &#125;<br><br>    <span class="hljs-comment">// Step 5: start ephemeral containers</span><br>    <span class="hljs-comment">// These are started &quot;prior&quot; to init containers to allow running ephemeral containers even when there</span><br>    <span class="hljs-comment">// are errors starting an init container. In practice init containers will start first since ephemeral</span><br>    <span class="hljs-comment">// containers cannot be specified on pod creation.</span><br>    <span class="hljs-keyword">for</span> _, idx := <span class="hljs-keyword">range</span> podContainerChanges.EphemeralContainersToStart &#123;<br>       start(ctx, <span class="hljs-string">&quot;ephemeral container&quot;</span>, metrics.EphemeralContainer, ephemeralContainerStartSpec(&amp;pod.Spec.EphemeralContainers[idx]))<br>    &#125;<br><br>    <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span> Remove this code path as logically it is the subset of the next</span><br>    <span class="hljs-comment">// code path.</span><br>    <span class="hljs-keyword">if</span> !types.HasRestartableInitContainer(pod) &amp;&amp; utilfeature.DefaultFeatureGate.Enabled(features.LegacySidecarContainers) &#123;<br>       <span class="hljs-comment">// Step 6: start the init container.</span><br>       <span class="hljs-keyword">if</span> container := podContainerChanges.NextInitContainerToStart; container != <span class="hljs-literal">nil</span> &#123;<br>          <span class="hljs-comment">// Start the next init container.</span><br>          <span class="hljs-keyword">if</span> err := start(ctx, <span class="hljs-string">&quot;init container&quot;</span>, metrics.InitContainer, containerStartSpec(container)); err != <span class="hljs-literal">nil</span> &#123;<br>             <span class="hljs-keyword">return</span><br>          &#125;<br><br>          <span class="hljs-comment">// Successfully started the container; clear the entry in the failure</span><br>          klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Completed init container for pod&quot;</span>, <span class="hljs-string">&quot;containerName&quot;</span>, container.Name, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>       &#125;<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>       <span class="hljs-comment">// Step 6: start init containers.</span><br>       <span class="hljs-keyword">for</span> _, idx := <span class="hljs-keyword">range</span> podContainerChanges.InitContainersToStart &#123;<br>          container := &amp;pod.Spec.InitContainers[idx]<br>          <span class="hljs-comment">// Start the next init container.</span><br>          <span class="hljs-keyword">if</span> err := start(ctx, <span class="hljs-string">&quot;init container&quot;</span>, metrics.InitContainer, containerStartSpec(container)); err != <span class="hljs-literal">nil</span> &#123;<br>             <span class="hljs-keyword">if</span> podutil.IsRestartableInitContainer(container) &#123;<br>                klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Failed to start the restartable init container for the pod, skipping&quot;</span>, <span class="hljs-string">&quot;initContainerName&quot;</span>, container.Name, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>                <span class="hljs-keyword">continue</span><br>             &#125;<br>             klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Failed to initialize the pod, as the init container failed to start, aborting&quot;</span>, <span class="hljs-string">&quot;initContainerName&quot;</span>, container.Name, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>             <span class="hljs-keyword">return</span><br>          &#125;<br><br>          <span class="hljs-comment">// Successfully started the container; clear the entry in the failure</span><br>          klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Completed init container for pod&quot;</span>, <span class="hljs-string">&quot;containerName&quot;</span>, container.Name, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>       &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// Step 7: For containers in podContainerChanges.ContainersToUpdate[CPU,Memory] list, invoke UpdateContainerResources</span><br>    <span class="hljs-keyword">if</span> resizable, _ := IsInPlacePodVerticalScalingAllowed(pod); resizable &#123;<br>       <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(podContainerChanges.ContainersToUpdate) &gt; <span class="hljs-number">0</span> || podContainerChanges.UpdatePodResources &#123;<br>          result.SyncResults = <span class="hljs-built_in">append</span>(result.SyncResults, m.doPodResizeAction(pod, podContainerChanges))<br>       &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// Step 8: start containers in podContainerChanges.ContainersToStart.</span><br>    <span class="hljs-keyword">for</span> _, idx := <span class="hljs-keyword">range</span> podContainerChanges.ContainersToStart &#123;<br>       start(ctx, <span class="hljs-string">&quot;container&quot;</span>, metrics.Container, containerStartSpec(&amp;pod.Spec.Containers[idx]))<br>    &#125;<br><br>    <span class="hljs-keyword">return</span><br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>k8s</category>
      
      <category>kubelet</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【pytorch-fsdp 源代码阅读（二）】-参数流转</title>
    <link href="/2025/07/12/pytorch-fsdp-2/"/>
    <url>/2025/07/12/pytorch-fsdp-2/</url>
    
    <content type="html"><![CDATA[<h1 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h1><ol><li><p>获取module下需要展开的tensors</p></li><li><p>将tensors放入到一个数组中，然后使用cat拼接到一起。这部分展开的数据会放到FlatParamHandle.flat_param中</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><code class="hljs python">    <span class="hljs-keyword">def</span> <span class="hljs-title function_">flatten_tensors</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        tensors: <span class="hljs-built_in">list</span>[Tensor],</span><br><span class="hljs-params">        aligned_numel: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    </span>) -&gt; Tensor:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Flatten ``tensors`` into a single flat tensor.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        The flattening optionally includes</span><br><span class="hljs-string">        padding if ``aligned_numel`` is greater than 0, where ``aligned_numel``</span><br><span class="hljs-string">        gives the numel required to have address alignment.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        NOTE: The padding alignment algorithm must be kept in sync with</span><br><span class="hljs-string">        :meth:`_init_flat_param_metadata`. We separate the two methods because</span><br><span class="hljs-string">        the initialization happens once, whereas this method may be called</span><br><span class="hljs-string">        multiple times throughout training (e.g. for checkpointing).</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(tensors) == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Expects non-empty `tensors`&quot;</span>)<br>        <span class="hljs-keyword">if</span> aligned_numel &lt; <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<br>                <span class="hljs-string">f&quot;Expects non-negative `aligned_numel` but got <span class="hljs-subst">&#123;aligned_numel&#125;</span>&quot;</span><br>            )<br>        dtype, _, device = self._validate_tensors_to_flatten(tensors)<br>        flat_tensors: <span class="hljs-built_in">list</span>[Tensor] = []<br>        <span class="hljs-keyword">if</span> aligned_numel &gt; <span class="hljs-number">0</span>:<br>            total_numel = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">for</span> tensor <span class="hljs-keyword">in</span> tensors:<br>                numel_to_pad = aligned_numel - (total_numel % aligned_numel)<br>                <span class="hljs-keyword">if</span> numel_to_pad &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> numel_to_pad &lt; aligned_numel:<br>                    padding_tensor = _construct_padding_tensor(<br>                        numel_to_pad, dtype, <span class="hljs-literal">False</span>, device<br>                    )<br>                    flat_tensors.append(padding_tensor)<br>                    total_numel += numel_to_pad<br>                flat_tensors.append(<br>                    torch.flatten(_detach_if_needed(tensor))<br>                    <span class="hljs-keyword">if</span> _is_truly_contiguous(tensor)<br>                    <span class="hljs-keyword">else</span> _detach_if_needed(tensor).as_strided((tensor.numel(),), (<span class="hljs-number">1</span>,))<br>                )<br>                total_numel += tensor.numel()<br>            numel_to_pad = self.world_size - (total_numel % self.world_size)<br>            <span class="hljs-keyword">if</span> numel_to_pad &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> numel_to_pad &lt; self.world_size:<br>                padding_tensor = _construct_padding_tensor(<br>                    numel_to_pad, dtype, <span class="hljs-literal">False</span>, device<br>                )<br>                flat_tensors.append(padding_tensor)<br>                total_numel += numel_to_pad<br>        <span class="hljs-keyword">else</span>:<br>            flat_tensors = [<br>                torch.flatten(_detach_if_needed(tensor))<br>                <span class="hljs-keyword">if</span> _is_truly_contiguous(tensor)<br>                <span class="hljs-keyword">else</span> _detach_if_needed(tensor).as_strided((tensor.numel(),), (<span class="hljs-number">1</span>,))<br>                <span class="hljs-keyword">for</span> tensor <span class="hljs-keyword">in</span> tensors<br>            ]<br>        <span class="hljs-keyword">return</span> torch.cat(flat_tensors, dim=<span class="hljs-number">0</span>)<br>         <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">flatten_tensors_into_flat_param</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        tensors: <span class="hljs-built_in">list</span>[Tensor],</span><br><span class="hljs-params">        aligned_numel: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        requires_grad: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">    </span>) -&gt; FlatParameter:<br>        flat_param_data = self.flatten_tensors(tensors, aligned_numel)<br>        <span class="hljs-keyword">return</span> FlatParameter(flat_param_data, requires_grad=requires_grad)<br>      <br>      <br>        <br>      self.flat_param: FlatParameter = self.flatten_tensors_into_flat_param(<br>            params_to_flatten,<br>            aligned_numel=<span class="hljs-number">0</span>,<br>            requires_grad=flat_param_requires_grad,<br>        )<br></code></pre></td></tr></table></figure><ul><li>根据各个tensors的参数量划分FlatParamHandle.flat_param，得到views，</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_unflat_views_unaligned</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    tensor: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>) -&gt; Iterator[Tensor]:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Return unflattened ``Tensor`` views into ``tensor``.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    If `tensor`` is ``None``,  ``flat_param`` is used. The unflattening is based</span><br><span class="hljs-string">    on ``flat_param`` &#x27;s metadata.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Examples for ``tensor`` include ``flat_param.grad`` or unsharded</span><br><span class="hljs-string">    tensor optimizer state.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    flat_param = self.flat_param<br>    <span class="hljs-keyword">if</span> tensor <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        tensor = flat_param<br>    views = (<br>        _ext_post_unflatten_transform(<br>            subtensor.view(shape)<br>            <span class="hljs-keyword">if</span> contiguous<br>            <span class="hljs-keyword">else</span> subtensor.as_strided(shape, stride),<br>            param_extension,<br>            self._fsdp_extension,<br>        )<br>        <span class="hljs-keyword">for</span> (subtensor, shape, stride, contiguous, param_extension) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<br>            torch.split(tensor, flat_param._numels, dim=<span class="hljs-number">0</span>),<br>            flat_param._shapes,<br>            flat_param._strides,<br>            flat_param._contiguities,<br>            flat_param._param_extensions,<br>        )<br>    )<br>    <span class="hljs-keyword">return</span> views<br></code></pre></td></tr></table></figure><ul><li>将这些views设置为module的attr，即进行替换</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">param_var: Tensor = view<br>self._setattr_tensor(module, param_name, param_var)<br><br></code></pre></td></tr></table></figure><h2 id="lazy-init"><a href="#lazy-init" class="headerlink" title="_lazy_init"></a>_lazy_init</h2><p>会调用init_flat_param_attributes()</p><ol><li><p>设置flat_param._local_shard &#x3D; flat_param.data</p></li><li><p>设置flat_param._full_param_padded为padded_unsharded_numel大小的torch.empty</p></li><li><p>设置flat_param._padded_unsharded_size&#x20;</p></li><li><p>释放flat_param._full_param_padded的底层存储</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> self.uses_sharded_strategy:<br>    <span class="hljs-comment"># We maintain a padded unsharded tensor that serves as the</span><br>    <span class="hljs-comment"># all-gather destination and owns the original parameter storages.</span><br>    unsharded_param_dtype = (<br>        self._fwd_bwd_param_dtype<br>        <span class="hljs-keyword">if</span> self._uses_param_mixed_precision<br>        <span class="hljs-keyword">else</span> flat_param.dtype<br>    )  <span class="hljs-comment"># use low precision if parameter mixed precision is enabled</span><br>    padded_unsharded_numel = flat_param.numel() * self.world_size<br>    flat_param._full_param_padded = torch.empty(<br>        padded_unsharded_numel,<br>        device=self.device,<br>        dtype=unsharded_param_dtype,<br>    )<br>    flat_param._padded_unsharded_size = flat_param._full_param_padded.size()<br>    _free_storage(flat_param._full_param_padded)<br><br>    <span class="hljs-keyword">if</span> self._uses_param_mixed_precision:<br>        <span class="hljs-comment"># For parameter mixed precision, we maintain a full precision</span><br>        <span class="hljs-comment"># padded unsharded tensor for when we force full precision.</span><br>        flat_param._full_prec_full_param_padded = torch.empty(<br>            padded_unsharded_numel,<br>            device=self.device,<br>            dtype=flat_param.dtype,  <span class="hljs-comment"># full precision</span><br>        )<br>        _free_storage(flat_param._full_prec_full_param_padded)<br></code></pre></td></tr></table></figure><h1 id="Shard"><a href="#Shard" class="headerlink" title="Shard"></a>Shard</h1><h2 id="post-forward-reshard"><a href="#post-forward-reshard" class="headerlink" title="_post_forward_reshard"></a>_post_forward_reshard</h2><ol><li>注意只有非root且RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES才会进行参数的reshard:</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_forward_reshard</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Reshards parameters in the post-forward.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle:<br>        <span class="hljs-keyword">return</span><br>    <span class="hljs-comment"># Do not free the root&#x27;s parameters in the post-forward for `FULL_SHARD`</span><br>    <span class="hljs-comment"># with the intention that they are immediately used for backward</span><br>    <span class="hljs-comment"># computation (though this may not be true)</span><br>    free_unsharded_flat_param = (<br>        <span class="hljs-keyword">not</span> state._is_root<br>        <span class="hljs-keyword">and</span> handle._sharding_strategy <span class="hljs-keyword">in</span> RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES<br>    )<br>    _reshard(state, handle, free_unsharded_flat_param)<br></code></pre></td></tr></table></figure><ul><li>将FlatParamHanle.flat_param.data设置为FlatParamHanle.flat_param._local_shard</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reshard</span>(<span class="hljs-params">self, free_unsharded_flat_param: <span class="hljs-built_in">bool</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Run the reshard logic.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    This includes freeing the unsharded flat</span><br><span class="hljs-string">    parameter if ``free_unsharded_flat_param`` and switching to using the</span><br><span class="hljs-string">    sharded flat parameter. Note that this also implicitly offloads</span><br><span class="hljs-string">    the sharded flat parameter (if CPU offload is enabled) by pointing</span><br><span class="hljs-string">    it to the ``_local_shard`` attribute which resides on CPU.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># Switch to the sharded `FlatParameter` before freeing to prevent</span><br>    <span class="hljs-comment"># &quot;use-after-free&quot;-type bugs with external profiling tools, where for</span><br>    <span class="hljs-comment"># `use_orig_params=True`, the `param` does not point to valid memory</span><br>    <span class="hljs-comment"># when setting `param.data = ...` in `_use_sharded_views()`.</span><br>    self._use_sharded_flat_param()<br>    <span class="hljs-keyword">if</span> free_unsharded_flat_param:<br>        self._free_unsharded_flat_param()<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">flat_param.data = flat_param._local_shard  <span class="hljs-comment"># type: ignore[attr-defined]</span><br></code></pre></td></tr></table></figure><ul><li>得到FlatParamHanle.unsharded_flat_param，即FlatParamHanle.flat_param._full_param_padded:</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs bash">unsharded_flat_param = flat_param._full_param_padded  <span class="hljs-comment"># type: ignore[attr-defined]</span><br><br>    def _free_unsharded_flat_param(self):<br>        <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string">        Free the padded unsharded flat parameter. We allow this</span><br><span class="hljs-string">        function to be called even when storage is not allocated</span><br><span class="hljs-string"></span><br><span class="hljs-string">        The tensor to free depends</span><br><span class="hljs-string">        on the calling context since the unshard may have forced full</span><br><span class="hljs-string">        precision, in which case a different tensor is used.</span><br><span class="hljs-string">        &quot;</span><span class="hljs-string">&quot;&quot;</span><br>        self._check_sharded_strategy()<br>        unsharded_flat_param = self._get_padded_unsharded_flat_param()<br>        self._check_on_compute_device(unsharded_flat_param)<br>        <span class="hljs-comment"># Do not free the memory until all ops in the current stream finish</span><br>        _no_dispatch_record_stream(<br>            unsharded_flat_param, self._device_handle.current_stream()<br>        )<br>        _free_storage(unsharded_flat_param)<br></code></pre></td></tr></table></figure><ul><li>释放这部分存储</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_free_storage</span>(<span class="hljs-params">tensor: torch.Tensor</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Frees the underlying storage of ``tensor``.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        bool: ``True`` if the method freed the storage and ``False`` if the</span><br><span class="hljs-string">        storage was already freed.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>            already_freed = tensor._typed_storage()._size() == <span class="hljs-number">0</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> already_freed:<br>                _p_assert(<br>                    tensor.storage_offset() == <span class="hljs-number">0</span>,<br>                    <span class="hljs-string">&quot;Freeing a tensor&#x27;s storage is unsafe when it is not the sole occupant\n&quot;</span><br>                    <span class="hljs-string">f&quot;storage offset: <span class="hljs-subst">&#123;tensor.storage_offset()&#125;</span>\n&quot;</span><br>                    <span class="hljs-string">f&quot;storage size: <span class="hljs-subst">&#123;tensor._typed_storage()._size()&#125;</span>\n&quot;</span><br>                    <span class="hljs-string">f&quot;tensor shape: <span class="hljs-subst">&#123;tensor.shape&#125;</span>&quot;</span>,<br>                )<br>                tensor._typed_storage()._resize_(<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h1 id="Unshard"><a href="#Unshard" class="headerlink" title="Unshard"></a>Unshard</h1><h2 id="pre-forward-unshard"><a href="#pre-forward-unshard" class="headerlink" title="_pre_forward_unshard"></a>_pre_forward_unshard</h2><ol><li>获取到FlatParamHanle.flat_param._full_param_padded，这是一个tensor</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">flat_param = self.flat_param<br>    unsharded_flat_param = flat_param._full_param_padded  <span class="hljs-comment"># type: ignore[attr-defined]</span><br><span class="hljs-built_in">return</span> unsharded_flat_param<br></code></pre></td></tr></table></figure><ul><li><p>检查存储是不是真释放了</p></li><li><p>给FlatParamHanle.flat_param._full_param_padded分配存储</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">_alloc_storage(unsharded_flat_param, flat_param._padded_unsharded_size) <br><br>def _alloc_storage(tensor: torch.Tensor, size: torch.Size) -&gt; None:<br> <span class="hljs-string">&quot;&quot;</span><span class="hljs-string">&quot;</span><br><span class="hljs-string"> Allocate storage for ``tensor`` with the given size.</span><br><span class="hljs-string"></span><br><span class="hljs-string"> Returns:</span><br><span class="hljs-string">     bool: ``True`` if this method allocated storage and ``False`` if the</span><br><span class="hljs-string">     storage was already allocated.</span><br><span class="hljs-string"> &quot;</span><span class="hljs-string">&quot;&quot;</span><br> with torch.no_grad():<br>     <span class="hljs-keyword">if</span> not torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>         already_allocated = tensor._typed_storage()._size() == size.numel()<br>         <span class="hljs-keyword">if</span> not already_allocated:<br>             tensor_storage_size = tensor._typed_storage()._size()<br>             _p_assert(<br>                 tensor_storage_size == 0,<br>                 <span class="hljs-string">&quot;Tensor storage should have been resized to be 0 but got PLACEHOLDEr&quot;</span>,<br>             )<br>             tensor._typed_storage()._resize_(size.numel())<br></code></pre></td></tr></table></figure><ul><li>通过all gather来将各个GPU上的FlatParamHanle.flat_param.data收集给FlatParamHanle.flat_param._full_param_padded</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_all_gather_flat_param</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    padded_unsharded_flat_param: Tensor,</span><br><span class="hljs-params"></span>) -&gt; Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    All-gather the handle&#x27;s flat parameter to the destination ``padded_unsharded_flat_param``.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Then switch to use the all-gathered tensor.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    _p_assert(<br>        <span class="hljs-built_in">hasattr</span>(self, <span class="hljs-string">&quot;process_group&quot;</span>) <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(self, <span class="hljs-string">&quot;world_size&quot;</span>),<br>        <span class="hljs-string">&quot;Expects a process group and world size to have been set via `shard()`&quot;</span>,<br>    )<br>    sharded_flat_param = self.flat_param.data<br>    expected_numel = sharded_flat_param.numel() * self.world_size<br>    _p_assert(<br>        padded_unsharded_flat_param.numel() == expected_numel,<br>        <span class="hljs-string">f&quot;Expects <span class="hljs-subst">&#123;expected_numel&#125;</span> numel but got <span class="hljs-subst">&#123;padded_unsharded_flat_param.numel()&#125;</span>&quot;</span>,<br>    )<br><br>    pg = (<br>        self._fake_process_group<br>        <span class="hljs-keyword">if</span> self._use_fake_all_gather<br>        <span class="hljs-keyword">else</span> self.process_group<br>    )<br><br>    <span class="hljs-comment"># HACK this should be handled by C10D</span><br>    <span class="hljs-keyword">if</span> sharded_flat_param.is_cpu:  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>        tensor_list = <span class="hljs-built_in">list</span>(<br>            torch.chunk(<br>                padded_unsharded_flat_param,<br>                dist.get_world_size(pg),  <span class="hljs-comment"># type: ignore[arg-<span class="hljs-built_in">type</span>]</span><br>            )<br>        )<br>        dist.all_gather(tensor_list, sharded_flat_param, group=pg)<br>    <span class="hljs-keyword">else</span>:<br>        dist.all_gather_into_tensor(<br>            padded_unsharded_flat_param,<br>            sharded_flat_param,<br>            pg,<br>        )<br><br>    <span class="hljs-keyword">if</span> self._offload_params:<br>        <span class="hljs-comment"># In case of offloading, `flat_param.data` (i.e. sharded param) is</span><br>        <span class="hljs-comment"># created on the pre-unshard stream. We need to hand it over to the</span><br>        <span class="hljs-comment"># unshard stream for all-gather</span><br>        _no_dispatch_record_stream(<br>            sharded_flat_param,<br>            self._device_handle.current_stream(),  <span class="hljs-comment"># unshard_stream</span><br>        )<br>    <span class="hljs-keyword">return</span> padded_unsharded_flat_param<br><br></code></pre></td></tr></table></figure><ol start="4"><li>使用收集到的FlatParamHanle.flat_param._full_param_padded，将self.flat_param.data更新为它，然后调用_use_unsharded_views，得到views然后赋值给各个param</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">unsharded_size = self.flat_param._unpadded_unsharded_size<br>flat_param_part = padded_unsharded_flat_param[: unsharded_size.numel()]<br><span class="hljs-comment"># slicing [:] is not visible to autograd because of .data</span><br>self.flat_param.data = flat_param_part<br>self._use_unsharded_views(as_params=False)<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>FSDP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【pytorch-fsdp 源代码阅读（一）】-全流程概览</title>
    <link href="/2025/07/02/pytorch-fsdp-1/"/>
    <url>/2025/07/02/pytorch-fsdp-1/</url>
    
    <content type="html"><![CDATA[<h1 id="专有名词解释"><a href="#专有名词解释" class="headerlink" title="专有名词解释"></a>专有名词解释</h1><ul><li><p>warp：对模型进行包裹，使其具备fsdp的相关的分布式能力</p></li><li><p>shard: 对参数进行切分，得到每个rank sharded的参数</p></li><li><p>unshard: 将切分的参数allgather，得到完整的参数</p></li><li><p>reshard：将完整的参数释放，只保留每个rank的sharded的参数</p></li><li><p>sharded：切分后的参数</p></li><li><p>unsharded：完整的参数</p></li></ul><h1 id="fsdp概览"><a href="#fsdp概览" class="headerlink" title="fsdp概览"></a>fsdp概览</h1><p>如下图所示，首先对于Zero算法来说：</p><ul><li><p>Zero-1切分了优化器状态</p></li><li><p>Zero-2切分了优化器状态和梯度</p></li><li><p>Zero-3切分了优化器状态和梯度和参数</p></li></ul><p><img src="/2025/07/02/pytorch-fsdp-1/image-2.png"></p><p>对于fsdp来说，它实际上就是Zero-3。传统的数据并行会在每个GPU上维护一份模型参数，梯度，优化器状态的副本，但是FSDP将这些状态分片到所有的数据并行worker中，并且可以选择将分片的模型参数卸载到CPU上，从而使得若现在有 $n$个GPU，某一层的参数量为 $m$，那么每个GPU会维护这一层 $m&#x2F;n$个参数。</p><p><img src="/2025/07/02/pytorch-fsdp-1/image-3.png"></p><p>通常，模型层以嵌套方式用 FSDP 包装，因此在前向或后向计算期间，只有单个 FSDP 实例中的层需要将完整参数收集到单个设备。聚合到的完整参数会在计算后立即释放，释放的内存可以用于下一层的计算。通过这种方式，可以节省峰值 GPU 内存，因此可以扩展训练以使用更大的模型大小或更大的批量大小。为了进一步最大化内存效率，当实例在计算中不活动时，FSDP 可以将参数、梯度和优化器状态卸载到 CPU。</p><h1 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h1><p>如下是一个使用示例，简单来说有这几个关键步骤：</p><ol><li><p>定义自动 wrap 策略：只 wrap nn.Linear层</p></li><li><p>将模型用FSDP进行包裹，从而转变为fsdp_model</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># torchrun --nproc_per_node=2 --master_port=47123 test.py</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> torch.distributed.fsdp <span class="hljs-keyword">import</span> FullyShardedDataParallel <span class="hljs-keyword">as</span> FSDP<br><span class="hljs-keyword">from</span> torch.distributed.fsdp.wrap <span class="hljs-keyword">import</span> lambda_auto_wrap_policy<br><span class="hljs-keyword">from</span> torch.distributed.fsdp <span class="hljs-keyword">import</span> ShardingStrategy<br><span class="hljs-keyword">import</span> torch.distributed <span class="hljs-keyword">as</span> dist<br><span class="hljs-keyword">import</span> torch.multiprocessing <span class="hljs-keyword">as</span> mp<br><span class="hljs-keyword">from</span> torch.nn.parallel <span class="hljs-keyword">import</span> DistributedDataParallel <span class="hljs-keyword">as</span> DDP<br><span class="hljs-keyword">from</span> torch.distributed.fsdp.wrap <span class="hljs-keyword">import</span> lambda_auto_wrap_policy<br><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> partial<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">my_wrap_criteria</span>(<span class="hljs-params">module</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">isinstance</span>(module, nn.Linear)<br><br><span class="hljs-comment"># 定义模型</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, H</span>):<br>        <span class="hljs-built_in">super</span>(Net, self).__init__()<br>        self.fc0 = nn.Linear(H, H, bias=<span class="hljs-literal">False</span>)<br>        self.fc1 = nn.Linear(H, H, bias=<span class="hljs-literal">False</span>)<br>        self.fc2 = nn.Linear(H, H, bias=<span class="hljs-literal">False</span>)<br>        self.fc3 = nn.Linear(H, H, bias=<span class="hljs-literal">False</span>)<br>        self.fc4 = nn.Linear(H, H, bias=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.fc0(x)<br>        x = self.fc1(x)<br>        x = self.fc2(x)<br>        x = self.fc3(x)<br>        x = self.fc4(x)<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment"># 启动函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    rank = <span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;RANK&quot;</span>])<br>    world_size = <span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;WORLD_SIZE&quot;</span>])<br>    local_rank = <span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;LOCAL_RANK&quot;</span>])<br><br>    dist.init_process_group(<span class="hljs-string">&quot;nccl&quot;</span>, rank=rank, world_size=world_size)<br>    torch.cuda.set_device(rank)<br><br>    H = <span class="hljs-number">512</span><br>    model = Net(H).cuda()<br><br>    <span class="hljs-comment"># 定义自动 wrap 策略：只 wrap nn.Linear</span><br>    policy = partial(lambda_auto_wrap_policy, lambda_fn=my_wrap_criteria)<br><br>    fsdp_model = FSDP(<br>        model,<br>        device_id=torch.cuda.current_device(),<br>        sharding_strategy=ShardingStrategy.FULL_SHARD,<br>        auto_wrap_policy=policy<br>    )<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;fsdp_model:&quot;</span>, fsdp_model)<br><br>    optimizer = optim.Adam(fsdp_model.parameters(), lr=<span class="hljs-number">1e-3</span>)<br><br>    <span class="hljs-comment"># 模拟数据</span><br>    x = torch.randn(<span class="hljs-number">32</span>, H).cuda()<br>    target = torch.randn(<span class="hljs-number">32</span>, H).cuda()<br>    criterion = nn.MSELoss()<br><br>    fsdp_model.train()<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):<br>        optimizer.zero_grad()<br>        output = fsdp_model(x)<br>        loss = criterion(output, target)<br>        loss.backward()<br>        optimizer.step()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Rank <span class="hljs-subst">&#123;rank&#125;</span>] Epoch <span class="hljs-subst">&#123;epoch&#125;</span> Loss: <span class="hljs-subst">&#123;loss.item()&#125;</span>&quot;</span>)<br><br>    dist.destroy_process_group()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br><br></code></pre></td></tr></table></figure><h1 id="fsdp初始化"><a href="#fsdp初始化" class="headerlink" title="fsdp初始化"></a>fsdp初始化</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FullyShardedDataParallel</span>(nn.Module, _FSDPState):<br><br></code></pre></td></tr></table></figure><p>注意fsdp的初始化过程是“惰性”的（lazy），只有在forward调用的时候才会进行初始化，从而对模型进行shard。</p><h2 id="Warp"><a href="#Warp" class="headerlink" title="Warp"></a>Warp</h2><p>具体的_auto_wrap的调用是在使用FullyShardedDataParallel包裹module后进行初始化的时候实现的，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">_auto_wrap(<br>    module,<br>    auto_wrap_policy,<br>    self._ignored_modules,<br>    self._ignored_params,<br>    root_kwargs,<br>    FullyShardedDataParallel,<br>)<br></code></pre></td></tr></table></figure><h3 id="判断模块是否需要划分的函数"><a href="#判断模块是否需要划分的函数" class="headerlink" title="判断模块是否需要划分的函数"></a>判断模块是否需要划分的函数</h3><p>pytorch中提供了多种对模型进行自动切分和包装的方法，下面介绍几个常用的：</p><h4 id="CustomPolicy"><a href="#CustomPolicy" class="headerlink" title="CustomPolicy"></a>CustomPolicy</h4><p>这支持自定义包装策略，关键是允许通过lambda_fn来进行自定义。lambda_fn可以返回bool值，这代表和root执行同样的分片参数，也可以返回args，这代表自定义的分片参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomPolicy</span>(<span class="hljs-title class_ inherited__">_Policy</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    这是一个高度灵活的 FSDP（Fully Sharded Data Parallel）自动包装策略。</span><br><span class="hljs-string">    它允许用户通过提供一个自定义的 lambda 函数来精确控制对哪个模块应用 FSDP 包装，</span><br><span class="hljs-string">    以及使用什么样的参数。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    工作机制：</span><br><span class="hljs-string">    策略的核心是用户传入的 `lambda_fn`。FSDP 会遍历模型中的每一个模块，并用该模块</span><br><span class="hljs-string">    作为参数调用 `lambda_fn`。根据 `lambda_fn` 的返回值，决定如何操作：</span><br><span class="hljs-string">    - 返回 `False`：不包装当前模块。</span><br><span class="hljs-string">    - 返回 `True`：使用 FSDP 的默认参数包装当前模块。</span><br><span class="hljs-string">    - 返回一个非空字典：包装当前模块，并使用该字典中的键值对来覆盖或补充 FSDP 的默认参数。</span><br><span class="hljs-string">      这允许为特定模块设置不同的分片策略（ShardingStrategy）或其他配置。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    使用场景：</span><br><span class="hljs-string">    当你需要比 `ModuleWrapPolicy`（基于类名包装）更精细的控制时，此策略非常有用。</span><br><span class="hljs-string">    例如，你可能想为模型的大部分 Transformer 层使用默认包装，但为最后的输出层（如 lm_head）</span><br><span class="hljs-string">    指定一个不同的分片策略，或者完全不包装某个特定的层。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, lambda_fn: <span class="hljs-type">Callable</span>[[nn.Module], <span class="hljs-type">Union</span>[<span class="hljs-built_in">bool</span>, <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]]]</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        构造函数。</span><br><span class="hljs-string"></span><br><span class="hljs-string">        参数:</span><br><span class="hljs-string">        - lambda_fn (Callable): 一个函数，它接受一个 `nn.Module` 实例作为输入，</span><br><span class="hljs-string">          并返回一个布尔值或一个字典，用于决定包装行为。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self._lambda_fn = lambda_fn<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_run_policy</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        root_module: nn.Module,</span><br><span class="hljs-params">        ignored_modules: <span class="hljs-built_in">set</span>[nn.Module],</span><br><span class="hljs-params">        root_kwargs: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>],</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-built_in">dict</span>[nn.Module, <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]]:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        （内部方法）执行策略的核心逻辑，遍历所有模块并应用 lambda 函数来决定包装方案。</span><br><span class="hljs-string"></span><br><span class="hljs-string">        参数:</span><br><span class="hljs-string">        - root_module (nn.Module): 整个模型。</span><br><span class="hljs-string">        - ignored_modules (set[nn.Module]): 需要忽略的模块集合。</span><br><span class="hljs-string">        - root_kwargs (dict[str, Any]): 应用于根 FSDP 模块的参数，作为包装子模块时的默认参数。</span><br><span class="hljs-string"></span><br><span class="hljs-string">        返回:</span><br><span class="hljs-string">        一个字典，键是需要被包装的目标模块实例，值是应用于该模块的 FSDP 参数。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        target_module_to_kwargs: <span class="hljs-built_in">dict</span>[nn.Module, <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]] = &#123;&#125;<br>        <span class="hljs-comment"># 遍历整个模型的所有子模块</span><br>        <span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> root_module.modules():<br>            <span class="hljs-comment"># 如果模块在忽略列表中，则跳过</span><br>            <span class="hljs-keyword">if</span> module <span class="hljs-keyword">in</span> ignored_modules:<br>                <span class="hljs-keyword">continue</span><br>            <br>            <span class="hljs-comment"># 对当前模块调用用户提供的 lambda 函数</span><br>            res = self._lambda_fn(module)<br>            <br>            <span class="hljs-comment"># 验证 lambda 函数的返回值是否合法（必须是布尔型或字典）</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(res, (<span class="hljs-built_in">dict</span>, <span class="hljs-built_in">bool</span>)):<br>                <span class="hljs-keyword">raise</span> ValueError(<br>                    <span class="hljs-string">f&quot;传递给 CustomPolicy 的 lambda_fn 应返回 &quot;</span><br>                    <span class="hljs-string">f&quot;False/True 或一个 kwarg 字典，但它返回了 <span class="hljs-subst">&#123;res&#125;</span>&quot;</span><br>                )<br>            <br>            <span class="hljs-comment"># 如果返回 False 或一个空字典，表示不包装该模块，直接跳过</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> res:<br>                <span class="hljs-keyword">continue</span><br>            <br>            <span class="hljs-comment"># 如果需要包装，首先浅拷贝根 FSDP 的参数作为默认值</span><br>            <span class="hljs-comment"># 这样做是为了防止不同 FSDP 实例间共享和意外修改同一份配置</span><br>            kwargs = copy.copy(root_kwargs)<br>            <br>            <span class="hljs-comment"># 如果 lambda 函数返回的是一个字典，用它的内容更新（覆盖）默认参数</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(res, <span class="hljs-built_in">dict</span>):<br>                kwargs.update(res)<br>            <br>            <span class="hljs-comment"># 将最终确定要包装的模块及其配置参数存入结果字典</span><br>            target_module_to_kwargs[module] = kwargs<br>            <br>        <span class="hljs-keyword">return</span> target_module_to_kwargs<br></code></pre></td></tr></table></figure><p>使用示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">model = init_transformer_model(...)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lambda_fn</span>(<span class="hljs-params">module: nn.Module</span>):<br>    <span class="hljs-keyword">if</span> module <span class="hljs-keyword">is</span> model.lm_head:<br>        <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&quot;sharding_strategy&quot;</span>: ShardingStrategy.SHARD_GRAD_OP&#125;<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(module, TransformerBlock):<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>policy = CustomPolicy(lambda_fn)<br>fsdp_model = FSDP(model, auto_wrap_policy=policy)<br></code></pre></td></tr></table></figure><h5 id="Module结构学习"><a href="#Module结构学习" class="headerlink" title="Module结构学习"></a>Module结构学习</h5><p>对于module，其记录子结构的变量为<code>_modules: dict[str, Optional[&quot;Module&quot;]]</code>，即对于函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">self.block1 = SomeModule()<br></code></pre></td></tr></table></figure><p>底层实际做了如下事情（在 <code>setattr</code> 中）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">self._modules[<span class="hljs-string">&quot;block1&quot;</span>] = SomeModule()<br></code></pre></td></tr></table></figure><p>也就是说所有子模块都保存在 <code>self._modules</code> 中（有顺序的字典）。</p><p>这里调用了root_module.modules()来获取root_module的子modules()，该函数实际上就是用深度优先遍历的方式去遍历<code>self._modules</code> ，具体的方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">modules</span>(<span class="hljs-params">self</span>) -&gt; Iterator[<span class="hljs-string">&quot;Module&quot;</span>]:<br>    <span class="hljs-string">r&quot;&quot;&quot;返回一个遍历网络中所有模块的迭代器。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    功能:</span><br><span class="hljs-string">    - 这是一个便捷方法，用于获取模型及其所有子模块的实例，而不需要它们的名称。</span><br><span class="hljs-string">    - 它在内部调用 `self.named_modules()`，但忽略了每个元组中的名称部分。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    产生:</span><br><span class="hljs-string">        Module: 网络中的一个模块。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    注意:</span><br><span class="hljs-string">        与 `named_modules` 一样，重复的模块实例默认只返回一次。在下面的例子中，</span><br><span class="hljs-string">        `l` 只会被返回一次。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    示例::</span><br><span class="hljs-string"></span><br><span class="hljs-string">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span><br><span class="hljs-string">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span><br><span class="hljs-string">        &gt;&gt;&gt; for idx, m in enumerate(net.modules()):</span><br><span class="hljs-string">        ...     print(idx, &#x27;-&gt;&#x27;, m)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        0 -&gt; Sequential(</span><br><span class="hljs-string">          (0): Linear(in_features=2, out_features=2, bias=True)</span><br><span class="hljs-string">          (1): Linear(in_features=2, out_features=2, bias=True)</span><br><span class="hljs-string">        )</span><br><span class="hljs-string">        1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 核心实现：</span><br>    <span class="hljs-comment"># 1. 调用 `self.named_modules()`，这个方法会返回一个 (名称, 模块) 元组的迭代器。</span><br>    <span class="hljs-comment"># 2. 在 for 循环中，使用 `_` 来接收并“丢弃”元组中的第一个元素（即模块的名称）。</span><br>    <span class="hljs-comment"># 3. `module` 变量接收元组中的第二个元素（即模块对象本身）。</span><br>    <span class="hljs-comment"># 4. `yield module` 将模块对象作为生成器的下一个值返回。</span><br>    <span class="hljs-comment"># 这种实现方式非常优雅，因为它将所有复杂的遍历逻辑（如递归、处理重复）</span><br>    <span class="hljs-comment"># 全部委托给了 `named_modules` 方法，自身保持了极度的简洁。</span><br>    <span class="hljs-keyword">for</span> _, module <span class="hljs-keyword">in</span> self.named_modules():<br>        <span class="hljs-keyword">yield</span> module<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">named_modules</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    memo: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">set</span>[<span class="hljs-string">&quot;Module&quot;</span>]] = <span class="hljs-literal">None</span>, <span class="hljs-comment"># 用于记录已访问模块的集合，防止重复处理</span></span><br><span class="hljs-params">    prefix: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;&quot;</span>, <span class="hljs-comment"># 当前模块的名称前缀</span></span><br><span class="hljs-params">    remove_duplicate: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>, <span class="hljs-comment"># 是否移除重复的模块实例</span></span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">r&quot;&quot;&quot;返回一个迭代器，该迭代器遍历网络中的所有模块，同时产生模块的名称和模块本身。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这是一个深度优先（pre-order，先序）的遍历。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">        memo: 用于存储已添加到结果中的模块集合的备忘录。主要用于内部递归调用。</span><br><span class="hljs-string">        prefix: 将被添加到模块名称前面的前缀。主要用于内部递归调用。</span><br><span class="hljs-string">        remove_duplicate: 是否在结果中移除重复的模块实例。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    产生:</span><br><span class="hljs-string">        (str, Module): (名称, 模块) 的元组。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    注意:</span><br><span class="hljs-string">        默认情况下，重复的模块只返回一次。在下面的例子中，`l` 只会被返回一次。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    示例::</span><br><span class="hljs-string"></span><br><span class="hljs-string">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span><br><span class="hljs-string">        &gt;&gt;&gt; net = nn.Sequential(l, l) # net 中有两个对同一 l 实例的引用</span><br><span class="hljs-string">        &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):</span><br><span class="hljs-string">        ...     print(idx, &#x27;-&gt;&#x27;, m)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        0 -&gt; (&#x27;&#x27;, Sequential(</span><br><span class="hljs-string">          (0): Linear(in_features=2, out_features=2, bias=True)</span><br><span class="hljs-string">          (1): Linear(in_features=2, out_features=2, bias=True)</span><br><span class="hljs-string">        ))</span><br><span class="hljs-string">        1 -&gt; (&#x27;0&#x27;, Linear(in_features=2, out_features=2, bias=True))</span><br><span class="hljs-string">        # 注意：尽管有两个 l，但 (&#x27;1&#x27;, l) 不会再次出现，因为 l 已经被访问过。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 1. 初始化备忘录（memo）</span><br>    <span class="hljs-comment"># 如果是顶层调用（非递归调用），memo 为 None，此时创建一个新的集合。</span><br>    <span class="hljs-comment"># 在递归调用中，memo 会被传递下去。</span><br>    <span class="hljs-keyword">if</span> memo <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        memo = <span class="hljs-built_in">set</span>()<br>    <br>    <span class="hljs-comment"># 2. 处理当前模块（self）</span><br>    <span class="hljs-comment"># 检查当前模块实例是否已经被访问过。</span><br>    <span class="hljs-keyword">if</span> self <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> memo:<br>        <span class="hljs-comment"># 如果 remove_duplicate 为 True，则将当前模块添加到备忘录中，</span><br>        <span class="hljs-comment"># 以确保后续遇到同一个实例时不再处理。</span><br>        <span class="hljs-keyword">if</span> remove_duplicate:<br>            memo.add(self)<br>        <br>        <span class="hljs-comment"># 3. 产生当前模块的名称和实例</span><br>        <span class="hljs-comment"># 这是先序遍历的体现：先访问根节点（当前模块）。</span><br>        <span class="hljs-keyword">yield</span> prefix, self<br>        <br>        <span class="hljs-comment"># 4. 递归遍历所有子模块</span><br>        <span class="hljs-comment"># self._modules 是一个有序字典，存储了所有直接子模块（例如 self.layer1, self.conv2）。</span><br>        <span class="hljs-keyword">for</span> name, module <span class="hljs-keyword">in</span> self._modules.items():<br>            <span class="hljs-comment"># 如果某个子模块是 None（例如，通过 delattr 删除后），则跳过。</span><br>            <span class="hljs-keyword">if</span> module <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">continue</span><br>            <br>            <span class="hljs-comment"># 5. 构建子模块的完整名称</span><br>            <span class="hljs-comment"># 在当前前缀的基础上，添加子模块的名称。</span><br>            <span class="hljs-comment"># 例如，如果当前 prefix 是 &#x27;encoder&#x27;，子模块 name 是 &#x27;layer1&#x27;，</span><br>            <span class="hljs-comment"># 那么 submodule_prefix 就是 &#x27;encoder.layer1&#x27;。</span><br>            submodule_prefix = prefix + (<span class="hljs-string">&quot;.&quot;</span> <span class="hljs-keyword">if</span> prefix <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;&quot;</span>) + name<br>            <br>            <span class="hljs-comment"># 6. 递归调用</span><br>            <span class="hljs-comment"># 使用 `yield from` 将递归调用产生的生成器内容直接“转发”出去。</span><br>            <span class="hljs-comment"># 将 memo 和新构建的 submodule_prefix 传递给下一次递归。</span><br>            <span class="hljs-keyword">yield</span> <span class="hljs-keyword">from</span> module.named_modules(<br>                memo, submodule_prefix, remove_duplicate<br>            )<br></code></pre></td></tr></table></figure><h4 id="transformer-auto-wrap-policy"><a href="#transformer-auto-wrap-policy" class="headerlink" title="transformer_auto_wrap_policy"></a>transformer_auto_wrap_policy</h4><p>这是transformer模型中比较常用的策略，主要就是自定义要划分的模块的类型，然后自动划分。具体使用的时候需要用functools.partial进行包装。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">transformer_auto_wrap_policy</span>(<span class="hljs-params"></span><br><span class="hljs-params">    module: nn.Module,</span><br><span class="hljs-params">    recurse: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">    nonwrapped_numel: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    transformer_layer_cls: <span class="hljs-type">Set</span>[<span class="hljs-type">Type</span>[nn.Module]],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    这是一个专门为 Transformer 模型设计的便捷包装策略。它本质上是 `_module_wrap_policy` 的一个别名或封装。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    目的：</span><br><span class="hljs-string">    提供一个语义上更清晰的函数名 (`transformer_auto_wrap_policy`)，让用户在处理 Transformer 模型时，</span><br><span class="hljs-string">    能更直观地理解其作用。它特别适用于包装 Transformer 的编码器/解码器层（例如 `TransformerEncoderLayer`）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    要点：</span><br><span class="hljs-string">    - 它直接调用 `_module_wrap_policy`，并将 `transformer_layer_cls` 作为 `module_classes` 传递过去。</span><br><span class="hljs-string">    - 正确地包装共享参数（如词嵌入层）非常重要，因为它们必须位于同一个 FSDP 实例中。</span><br><span class="hljs-string">      这个策略通过将所有指定的层（通常是 Transformer block）包装起来，有助于确保模型中其他部分（如共享的嵌入层）</span><br><span class="hljs-string">      最终被包含在更高层级的 FSDP 实例中，从而被正确处理。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 直接调用通用的模块包装策略函数，实现完全相同的功能。</span><br>    <span class="hljs-keyword">return</span> _module_wrap_policy(module, recurse, nonwrapped_numel, transformer_layer_cls)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_module_wrap_policy</span>(<span class="hljs-params"></span><br><span class="hljs-params">    module: nn.Module,</span><br><span class="hljs-params">    recurse: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">    nonwrapped_numel: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    module_classes: <span class="hljs-type">Set</span>[<span class="hljs-type">Type</span>[nn.Module]],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    这是一个核心的辅助函数，用于实现基于模块类的自动包装策略。</span><br><span class="hljs-string">    FSDP 的自动包装过程是一个从上到下（top-down）的遍历，但包装决策是从下到上（bottom-up）做出的。</span><br><span class="hljs-string">    此函数在这个过程中被调用，以决定是否应该包装当前的模块。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    工作机制：</span><br><span class="hljs-string">    该函数的行为取决于 `recurse` 参数：</span><br><span class="hljs-string">    1. 当 `recurse=True` 时：表示 FSDP 正在递归地深入模块树（DFS 过程）。</span><br><span class="hljs-string">       在这种情况下，函数总是返回 `True`，告诉 FSDP 继续向下遍历，直到到达叶子模块或一个已经被包装的子模块。</span><br><span class="hljs-string">    2. 当 `recurse=False` 时：表示 FSDP 已经完成对当前模块所有子模块的遍历，现在需要对当前模块本身做出决策。</span><br><span class="hljs-string">       这时，函数会检查 `module` 是否是 `module_classes` 中指定的任何一个类的实例。</span><br><span class="hljs-string">       - 如果是，则返回 `True`，表示“请包装我这个模块”。</span><br><span class="hljs-string">       - 如果不是，则返回 `False`，表示“不要包装我，继续向上返回”。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">        module (nn.Module): 当前正在被考虑的模块。</span><br><span class="hljs-string">        recurse (bool): 控制函数行为的标志。`True` 表示继续递归，`False` 表示需要做出包装决策。</span><br><span class="hljs-string">        nonwrapped_numel (int): 尚未被包装的参数数量（在此函数中未使用，但在其他更复杂的策略中可能有用）。</span><br><span class="hljs-string">        module_classes (Set[Type[nn.Module]]): 一个包含模块类的集合。任何属于这些类的模块都将被包装。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">        一个布尔值。如果 `recurse=True`，总是返回 `True`。如果 `recurse=False`，返回是否应该包装 `module`。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 如果标志为 True，意味着我们仍在递归地深入模块树，所以总是返回 True 以继续递归。</span><br>    <span class="hljs-keyword">if</span> recurse:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>    <br>    <span class="hljs-comment"># 如果标志为 False，意味着已经到达决策点。</span><br>    <span class="hljs-comment"># 检查当前模块的类型是否在用户指定的需要包装的类型列表中。</span><br>    <span class="hljs-comment"># isinstance 的第二个参数需要是元组，所以我们将集合转换为元组。</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">isinstance</span>(module, <span class="hljs-built_in">tuple</span>(module_classes))<br><br></code></pre></td></tr></table></figure><p>使用示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">fsdp_m = FSDP(<br>    m,<br>    auto_wrap_policy=functools.partial(<br>        transformer_auto_wrap_policy, transformer_layer_cls=(nn.Linear,)<br>    ),<br>    use_orig_params=<span class="hljs-literal">True</span>,<br>)<br></code></pre></td></tr></table></figure><h3 id="具体进行自动划分"><a href="#具体进行自动划分" class="headerlink" title="具体进行自动划分"></a>具体进行自动划分</h3><p>注意观察下面的函数中的fsdp_fn为FullyShardedDataParallel，即这个fsdp_fn的作用是把module包装成FullyShardedDataParallel类型。</p><p>这里有两种划分的调用方式：</p><ol><li><p>如果 <code>policy</code>是 <code>_Policy</code>的实例（推荐方式），则使用策略对象来决定哪些模块需要被包装。</p></li><li><p>如果 <code>policy</code> 是一个可调用对象（旧版方式），则使用递归的方式进行包装。</p></li></ol><p>暂时先只看第一种<code>_Policy</code>的实例的方法，其执行顺序如下：</p><ol><li><p>执行<code>_run_policy</code>得到root_module下所有符合包装规则的module以及args</p></li><li><p>如果配置了混合精度就特殊处理一下</p></li><li><p>验证要包装的模块中的冻结参数（即 requires_grad&#x3D;False 的参数）</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_auto_wrap</span>(<span class="hljs-params"></span><br><span class="hljs-params">    root_module: nn.Module,</span><br><span class="hljs-params">    policy: <span class="hljs-type">Union</span>[<span class="hljs-type">Callable</span>, _Policy], <span class="hljs-comment"># 包装策略，可以是_Policy对象或一个可调用函数</span></span><br><span class="hljs-params">    ignored_modules: <span class="hljs-built_in">set</span>[nn.Module], <span class="hljs-comment"># 应该忽略不进行包装的模块集合</span></span><br><span class="hljs-params">    ignored_params: <span class="hljs-built_in">set</span>[nn.Parameter], <span class="hljs-comment"># 应该忽略不进行包装的参数集合</span></span><br><span class="hljs-params">    root_kwargs: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>], <span class="hljs-comment"># FSDP的根配置参数</span></span><br><span class="hljs-params">    fsdp_fn: <span class="hljs-type">Callable</span>,  <span class="hljs-comment"># FSDP的包装函数，例如 `FullyShardedDataParallel` 类或 `fully_shard` 函数</span></span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    根据 `policy`，以后序遍历的方式自动包装 `root_module` 模块树中的模块。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    此函数是 FSDP 自动包装功能的核心入口。</span><br><span class="hljs-string">    它根据传入的 `policy` 类型，选择不同的包装逻辑：</span><br><span class="hljs-string">    1.  如果 `policy` 是 `_Policy` 的实例（推荐方式），则使用策略对象来决定哪些模块需要被包装。</span><br><span class="hljs-string">    2.  如果 `policy` 是一个可调用对象（旧版方式），则使用递归的方式进行包装。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    前提条件: `root_kwargs` 应该包含除 `module` 之外的所有FSDP构造函数参数。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 检查模块是否已经被FSDP包装过。自动包装不支持对已包装的模块再次进行包装（嵌套包装）。</span><br>    _check_nested_wrapping(root_module)<br><br>    <span class="hljs-comment"># --- 分支1：基于 _Policy 对象的策略化自动包装 (推荐方式) ---</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(policy, _Policy):<br>        <span class="hljs-comment"># 运行策略，获取一个从目标模块到其对应FSDP参数的映射字典</span><br>        target_module_to_kwargs = policy._run_policy(<br>            root_module, ignored_modules, root_kwargs<br>        )<br>        <span class="hljs-comment"># 如果配置了混合精度(mixed_precision)</span><br>        <span class="hljs-keyword">if</span> root_kwargs.get(<span class="hljs-string">&quot;mixed_precision&quot;</span>) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># 运行混合精度覆盖策略，这可能会修改 target_module_to_kwargs，</span><br>            <span class="hljs-comment"># 例如，将某些模块的混合精度设置为特定的类型或禁用它。</span><br>            target_module_to_kwargs = _run_mixed_precision_override_policy(<br>                root_module,<br>                root_kwargs[<span class="hljs-string">&quot;mixed_precision&quot;</span>]._module_classes_to_ignore,<br>                ignored_modules,<br>                root_kwargs,<br>                target_module_to_kwargs,<br>            )<br>            <span class="hljs-comment"># 对指定的不使用混合精度的模块类别，通过注册前向钩子来覆盖其行为</span><br>            overridden_module_classes = _override_module_mixed_precision(<br>                root_module, root_kwargs[<span class="hljs-string">&quot;mixed_precision&quot;</span>]._module_classes_to_ignore<br>            )<br>            <span class="hljs-comment"># 如果有模块的混合精度设置被覆盖，发出警告</span><br>            _warn_on_overridden_mixed_precision(overridden_module_classes)<br>        <br>        <span class="hljs-comment"># 验证要包装的模块中的冻结参数（即 requires_grad=False 的参数）</span><br>        <span class="hljs-comment"># 确保所有参数的 `requires_grad` 状态在所有进程中是一致的</span><br>        _validate_frozen_params(<br>            root_module,<br>            <span class="hljs-built_in">set</span>(target_module_to_kwargs.keys()),<br>            ignored_params,<br>            root_kwargs.get(<span class="hljs-string">&quot;use_orig_params&quot;</span>, <span class="hljs-literal">False</span>),<br>        )<br>        <span class="hljs-comment"># 根据 target_module_to_kwargs 构建一个包装函数</span><br>        wrap_fn = _construct_wrap_fn(root_module, target_module_to_kwargs, fsdp_fn)<br>        <span class="hljs-comment"># 以后序遍历的方式，将包装函数应用到模块树上</span><br>        _post_order_apply(root_module, wrap_fn)<br>        <span class="hljs-keyword">return</span> <span class="hljs-comment"># 完成包装，直接返回</span><br><br>    <span class="hljs-comment"># --- 分支2：基于可调用函数的递归自动包装 (旧版方式) ---</span><br>    <span class="hljs-comment"># 准备递归包装所需的参数</span><br>    recursive_wrap_kwargs = &#123;<br>        <span class="hljs-string">&quot;module&quot;</span>: root_module,<br>        <span class="hljs-string">&quot;auto_wrap_policy&quot;</span>: policy,<br>        <span class="hljs-string">&quot;wrapper_cls&quot;</span>: fsdp_fn,<br>        <span class="hljs-string">&quot;ignored_modules&quot;</span>: ignored_modules,<br>        <span class="hljs-string">&quot;ignored_params&quot;</span>: ignored_params,<br>        <span class="hljs-string">&quot;only_wrap_children&quot;</span>: <span class="hljs-literal">True</span>,  <span class="hljs-comment"># 表示只对子模块进行递归包装</span><br>    &#125;<br>    <span class="hljs-comment"># 如果配置了混合精度</span><br>    <span class="hljs-keyword">if</span> root_kwargs.get(<span class="hljs-string">&quot;mixed_precision&quot;</span>) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># 对指定的不使用混合精度的模块类别，通过注册前向钩子来覆盖其行为</span><br>        overridden_module_classes = _override_module_mixed_precision(<br>            root_module, root_kwargs[<span class="hljs-string">&quot;mixed_precision&quot;</span>]._module_classes_to_ignore<br>        )<br>        <span class="hljs-comment"># 创建一个组合策略：它会同时应用用户提供的原始策略，</span><br>        <span class="hljs-comment"># 以及一个单独包装被忽略混合精度模块的策略。</span><br>        policy = functools.partial(<br>            _or_policy, <span class="hljs-comment"># _or_policy 会依次尝试列表中的每个策略</span><br>            policies=[<br>                policy, <span class="hljs-comment"># 用户原始策略</span><br>                partial( <span class="hljs-comment"># 一个新策略，用于单独包装需要忽略混合精度的模块</span><br>                    _wrap_module_cls_individually,<br>                    module_classes=root_kwargs[<span class="hljs-string">&quot;mixed_precision&quot;</span>]._module_classes_to_ignore,<br>                ),<br>            ],<br>        )<br>        recursive_wrap_kwargs[<span class="hljs-string">&quot;auto_wrap_policy&quot;</span>] = policy<br>        <span class="hljs-comment"># 如果有模块的混合精度设置被覆盖，发出警告</span><br>        _warn_on_overridden_mixed_precision(overridden_module_classes)<br>    <br>    <span class="hljs-comment"># 执行递归包装。将根配置和递归配置合并后传递给 _recursive_wrap</span><br>    _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  <span class="hljs-comment"># type: ignore[arg-<span class="hljs-built_in">type</span>]</span><br><br></code></pre></td></tr></table></figure><h4 id="construct-wrap-fn-post-order-apply"><a href="#construct-wrap-fn-post-order-apply" class="headerlink" title="_construct_wrap_fn&amp;_post_order_apply"></a>_construct_wrap_fn&amp;_post_order_apply</h4><p>首先构建一个warp函数，该函数对于target model且不是root的会调用fsdp_fn执行</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_construct_wrap_fn</span>(<span class="hljs-params"></span><br><span class="hljs-params">    root_module: nn.Module, <span class="hljs-comment"># 整个模型的根模块</span></span><br><span class="hljs-params">    target_module_to_kwargs: <span class="hljs-built_in">dict</span>[nn.Module, <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]], <span class="hljs-comment"># 一个字典，映射了需要被包装的模块到它们的FSDP参数</span></span><br><span class="hljs-params">    fsdp_fn: <span class="hljs-type">Callable</span>, <span class="hljs-comment"># 实际执行包装的函数，例如 `FullyShardedDataParallel`</span></span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Callable</span>[[nn.Module], <span class="hljs-type">Optional</span>[nn.Module]]:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    此函数是一个高阶函数，它的作用是构建并返回一个&quot;包装函数&quot;（`fn`）。</span><br><span class="hljs-string">    这个返回的函数将被传递给 `_post_order_apply`，用于在后序遍历中实际应用FSDP包装。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    - 它利用闭包捕获 `root_module`、`target_module_to_kwargs` 和 `fsdp_fn`。</span><br><span class="hljs-string">    - 返回的 `fn` 封装了决定是否包装一个特定模块以及如何包装它的全部逻辑。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">    - root_module (nn.Module): 模型的根模块，用于在包装时进行排除检查。</span><br><span class="hljs-string">    - target_module_to_kwargs (dict): 由包装策略生成的字典，指明了哪些模块需要被包装以及它们各自的FSDP配置。</span><br><span class="hljs-string">    - fsdp_fn (Callable): FSDP的包装器，如 `FullyShardedDataParallel` 类。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">    - 一个可调用对象 `fn`，该函数接受一个模块作为输入，如果该模块需要被包装，则返回包装后的模块，否则返回 `None`。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fn</span>(<span class="hljs-params">module: nn.Module</span>) -&gt; <span class="hljs-type">Optional</span>[nn.Module]:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        这个内部函数是实际执行替换逻辑的单元。</span><br><span class="hljs-string">        它会被 `_post_order_apply` 在遍历模型树时对每个模块调用。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 检查当前遍历到的 `module` 是否在我们的目标包装列表 `target_module_to_kwargs` 中</span><br>        <span class="hljs-comment"># 同时，显式地避免包装根模块 `root_module`，因为根模块的包装通常由调用FSDP的用户在最外层手动完成。</span><br>        <span class="hljs-keyword">if</span> module <span class="hljs-keyword">in</span> target_module_to_kwargs <span class="hljs-keyword">and</span> module <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> root_module:<br>            <span class="hljs-comment"># 如果模块是目标模块，就从字典中获取其对应的FSDP参数</span><br>            kwargs = target_module_to_kwargs[module]<br>            <span class="hljs-comment"># 使用传入的 `fsdp_fn` (例如 `FullyShardedDataParallel`) 和对应的参数来包装当前模块</span><br>            <span class="hljs-comment"># `_post_order_apply` 会用这里返回的新模块替换掉原始模块</span><br>            <span class="hljs-keyword">return</span> fsdp_fn(module, **kwargs)<br>        <br>        <span class="hljs-comment"># 如果模块不是目标包装模块，或者它就是根模块，则返回 None。</span><br>        <span class="hljs-comment"># `_post_order_apply` 看到返回 `None` 时，不会对原始模块做任何改动。</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">return</span> fn<br></code></pre></td></tr></table></figure><p>然后对于_post_order_apply函数，他就是会执行上面构造的fn，然后以后序遍历的方式去遍历所有的子模块（注意不会替换root 模块），替换的方法就是setattr。</p><blockquote><p>为什么要避免替换root呢，因为root这时候已经是FullyShardedDataParallel类型了，不需要重复进行。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 注意：我们有意保持此函数简单，并将复杂性隔离到 `fn` 中，</span><br><span class="hljs-comment"># 以便能够通用地使用此函数。我们将来可能会将其移动到非 FSDP 特定的文件夹和/或使其公开。</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_order_apply</span>(<span class="hljs-params"></span><br><span class="hljs-params">    root_module: nn.Module,</span><br><span class="hljs-params">    fn: <span class="hljs-type">Callable</span>[[nn.Module], <span class="hljs-type">Optional</span>[nn.Module]],</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    此函数遵循后序遍历（post-order traversal）将 `fn` 应用于 `root_module` 模块树中的每个模块。</span><br><span class="hljs-string">    如果 `fn` 返回一个 :class:`nn.Module`，那么它将在树中用新返回的模块替换原始模块。</span><br><span class="hljs-string">    否则，`fn` 应返回 `None`，在这种情况下，模块不会被更改。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    后序遍历意味着，对于任何给定的模块，函数 `fn` 会先被应用于其所有子模块，然后再应用于该模块本身。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">    - root_module (nn.Module): 整个模型层级结构的根模块。</span><br><span class="hljs-string">    - fn (Callable[[nn.Module], Optional[nn.Module]]): 一个可调用对象，它接收一个模块作为输入。</span><br><span class="hljs-string">        - 如果需要替换该模块，则返回一个新的 nn.Module 实例。</span><br><span class="hljs-string">        - 如果不需要改变该模块，则返回 None。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 跟踪已访问的模块，以避免多次访问共享的模块实例。</span><br>    visited_modules: <span class="hljs-built_in">set</span>[nn.Module] = &#123;root_module&#125;<br><br>    <span class="hljs-comment"># 定义一个内部辅助函数来进行递归的后序遍历和应用。</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_order_apply_inner</span>(<span class="hljs-params"></span><br><span class="hljs-params">        module: nn.Module, <span class="hljs-comment"># 当前正在处理的模块</span></span><br><span class="hljs-params">        module_name: <span class="hljs-built_in">str</span>, <span class="hljs-comment"># 当前模块在其父模块中的属性名</span></span><br><span class="hljs-params">        parent_module: <span class="hljs-type">Optional</span>[nn.Module], <span class="hljs-comment"># 当前模块的父模块</span></span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-comment"># 1. 遍历当前模块的所有直接子模块。</span><br>        <span class="hljs-keyword">for</span> child_module_name, child_module <span class="hljs-keyword">in</span> module.named_children():<br>            <span class="hljs-comment"># 如果子模块还没有被访问过（防止重复处理共享模块）</span><br>            <span class="hljs-keyword">if</span> child_module <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> visited_modules:<br>                visited_modules.add(child_module) <span class="hljs-comment"># 标记为已访问</span><br>                <span class="hljs-comment"># 2. 对子模块进行递归调用。这是实现后序遍历的关键：先深入子树。</span><br>                _post_order_apply_inner(child_module, child_module_name, module)<br>        <br>        <span class="hljs-comment"># 3. 在所有子模块都处理完毕后，对当前模块应用 `fn` 函数。</span><br>        <span class="hljs-comment">#    这就是“后序”的含义：先处理子节点，再处理父节点。</span><br>        optional_module = fn(module)<br>        <br>        <span class="hljs-comment"># 4. 如果 `fn` 返回了一个新的模块实例（而不是 None），则替换原始模块。</span><br>        <span class="hljs-keyword">if</span> optional_module <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># 断言确保非根模块必须有一个父模块，否则替换操作无法进行。</span><br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">isinstance</span>(parent_module, nn.Module), (<br>                <span class="hljs-string">f&quot;非根模块应该设置其父模块，但对于 <span class="hljs-subst">&#123;module&#125;</span> 得到了 <span class="hljs-subst">&#123;parent_module&#125;</span>&quot;</span><br>            )<br>            <span class="hljs-comment"># 断言确保非根模块必须有一个名称，否则无法通过名称在父模块中找到并替换它。</span><br>            <span class="hljs-keyword">assert</span> module_name, (<br>                <span class="hljs-string">f&quot;非根模块应该设置其模块名称，但对于 <span class="hljs-subst">&#123;module&#125;</span> 得到了一个空模块名&quot;</span><br>            )<br>            <span class="hljs-comment"># 断言确保 `fn` 的返回值要么是 None，要么是 nn.Module 的实例。</span><br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">isinstance</span>(optional_module, nn.Module), (<br>                <span class="hljs-string">f&quot;fn 应返回 None 或 nn.Module，但得到了 <span class="hljs-subst">&#123;optional_module&#125;</span>&quot;</span><br>            )<br>            <span class="hljs-comment"># 使用 setattr 动态地将父模块中的 `module_name` 属性设置为新的 `optional_module`，</span><br>            <span class="hljs-comment"># 从而完成模块的替换。</span><br>            <span class="hljs-built_in">setattr</span>(parent_module, module_name, optional_module)<br><br>    <span class="hljs-comment"># 从根模块开始启动整个后序应用过程。</span><br>    <span class="hljs-comment"># 根模块没有父模块（None）和名称（&quot;&quot;）。</span><br>    _post_order_apply_inner(root_module, <span class="hljs-string">&quot;&quot;</span>, <span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure><h2 id="初始化param-handle"><a href="#初始化param-handle" class="headerlink" title="初始化param_handle"></a>初始化param_handle</h2><p>在Warp之后，初始化param_handle的顶层调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">_init_param_handle_from_module(<br>    self,<br>    module,<br>    device_id,<br>    param_init_fn,<br>    sync_module_states,<br>)<br></code></pre></td></tr></table></figure><h3 id="init-param-handle-from-module"><a href="#init-param-handle-from-module" class="headerlink" title="_init_param_handle_from_module"></a>_init_param_handle_from_module</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_param_handle_from_module</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState, <span class="hljs-comment"># FSDP 状态对象，用于跟踪和管理 FSDP 实例的各种状态</span></span><br><span class="hljs-params">    fully_sharded_module: nn.Module, <span class="hljs-comment"># 需要被 FSDP 完全分片的模块</span></span><br><span class="hljs-params">    device_id: <span class="hljs-type">Optional</span>[<span class="hljs-type">Union</span>[<span class="hljs-built_in">int</span>, torch.device]], <span class="hljs-comment"># 目标设备 ID</span></span><br><span class="hljs-params">    param_init_fn: <span class="hljs-type">Optional</span>[<span class="hljs-type">Callable</span>[[nn.Module], <span class="hljs-literal">None</span>]], <span class="hljs-comment"># 可选的参数初始化函数，用于在物化（materialize）模块时调用</span></span><br><span class="hljs-params">    sync_module_states: <span class="hljs-built_in">bool</span>, <span class="hljs-comment"># 是否在初始化时同步模块的状态（参数和缓冲区）</span></span><br><span class="hljs-params"></span>) -&gt; _FSDPState:<br>    <span class="hljs-string">&quot;&quot;&quot;从一个模块 `fully_sharded_module` 初始化一个 `FlatParamHandle`。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    `FlatParamHandle` 是 FSDP 的核心组件，它将模块的多个原始参数展平（flatten）</span><br><span class="hljs-string">    并合并成一个单一的、连续的 `FlatParameter`。这个函数负责完成这一过程。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><span class="hljs-string">&quot;&quot;</span><br>    <span class="hljs-comment"># 1. 检查和准备设备</span><br>    <span class="hljs-comment"># 确保模块的所有参数都在同一个设备上，或者在 CPU 上，或者在 &#x27;meta&#x27; 设备上</span><br>    _check_single_device_module(fully_sharded_module, state._ignored_params, device_id)<br>    <span class="hljs-comment"># 根据传入的 device_id 获取实际的 torch.device 对象</span><br>    device_from_device_id = _get_device_from_device_id(<br>        device_id, state.rank, state._device_handle<br>    )<br><br>    <span class="hljs-comment"># 2. 模块物化（Materialization）</span><br>    <span class="hljs-comment"># 检查模块是否需要在设备上进行物化。如果模块的参数在 &#x27;meta&#x27; 设备上，</span><br>    <span class="hljs-comment"># 或者使用了 torchdistX 的延迟初始化，就需要进行物化，即为参数分配实际的内存。</span><br>    is_meta_module, is_torchdistX_deferred_init = _need_to_materialize_module(<br>        fully_sharded_module, state._ignored_params, state._ignored_modules<br>    )<br>    <span class="hljs-comment"># 如果需要物化并且用户提供了自定义的参数初始化函数 `param_init_fn`</span><br>    <span class="hljs-keyword">if</span> (is_meta_module <span class="hljs-keyword">or</span> is_torchdistX_deferred_init) <span class="hljs-keyword">and</span> param_init_fn <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># 使用用户提供的函数来初始化并物化模块</span><br>        _materialize_with_param_init_fn(<br>            fully_sharded_module, param_init_fn, state._ignored_modules<br>        )<br>    <span class="hljs-comment"># 如果是 &#x27;meta&#x27; 模块，但没有提供初始化函数</span><br>    <span class="hljs-keyword">elif</span> is_meta_module:<br>        <span class="hljs-comment"># 使用默认方式在目标设备上物化模块</span><br>        _materialize_meta_module(<br>            fully_sharded_module,<br>            device_id,<br>            state._ignored_modules,<br>            state._device_handle,<br>        )<br>    <span class="hljs-comment"># 如果使用了 torchdistX 的延迟初始化</span><br>    <span class="hljs-keyword">elif</span> is_torchdistX_deferred_init:<br>        <span class="hljs-comment"># 使用 torchdistX 的 API 来物化模块</span><br>        deferred_init.materialize_module(<br>            fully_sharded_module,<br>            <span class="hljs-comment"># 检查函数确保我们不会重复物化已经被 FSDP 管理或被忽略的子模块</span><br>            check_fn=<span class="hljs-keyword">lambda</span> submodule: _get_module_fsdp_state(submodule) <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">and</span> submodule <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> state._ignored_modules,<br>        )<br><br>    <span class="hljs-comment"># 3. 将模块移动到目标设备</span><br>    <span class="hljs-comment"># 收集所有被忽略的模块中的缓冲区，这些缓冲区将不会被移动</span><br>    ignored_buffers = &#123;<br>        buffer<br>        <span class="hljs-keyword">for</span> ignored_module <span class="hljs-keyword">in</span> state._ignored_modules<br>        <span class="hljs-keyword">for</span> buffer <span class="hljs-keyword">in</span> ignored_module.buffers()<br>    &#125;<br>    <span class="hljs-comment"># 将整个模块（包括其参数和缓冲区）移动到目标设备</span><br>    _move_module_to_device(<br>        fully_sharded_module,<br>        state._ignored_params, <span class="hljs-comment"># 忽略指定参数</span><br>        ignored_buffers, <span class="hljs-comment"># 忽略指定缓冲区</span><br>        device_from_device_id,<br>    )<br>    <span class="hljs-comment"># 确定计算设备（通常是 GPU），并更新 FSDP 状态</span><br>    state.compute_device = _get_compute_device(<br>        fully_sharded_module,<br>        state._ignored_params,<br>        device_from_device_id,<br>        state.rank,<br>        state._device_handle,<br>    )<br><br>    <span class="hljs-comment"># 4. 参数同步</span><br>    <span class="hljs-comment"># 获取 FSDP 需要管理的所有原始参数</span><br>    managed_params = <span class="hljs-built_in">list</span>(_get_orig_params(fully_sharded_module, state._ignored_params))<br>    <span class="hljs-comment"># 验证这些参数是否符合 FSDP 的要求</span><br>    _verify_managed_params(fully_sharded_module, managed_params)<br>    <span class="hljs-comment"># 如果设置了 `sync_module_states`，则在所有 rank 之间同步参数和缓冲区</span><br>    <span class="hljs-comment"># 这确保了在训练开始前，所有进程上的模型状态是完全一致的</span><br>    <span class="hljs-keyword">if</span> sync_module_states:<br>        _sync_module_params_and_buffers(<br>            fully_sharded_module, managed_params, state.process_group<br>        )<br>        <span class="hljs-comment"># 对于混合分片策略，还需要在跨节点的进程组中进行同步</span><br>        <span class="hljs-keyword">if</span> state.sharding_strategy <span class="hljs-keyword">in</span> HYBRID_SHARDING_STRATEGIES:<br>            _sync_module_params_and_buffers(<br>                fully_sharded_module, managed_params, state._inter_node_pg<br>            )<br>            <br>    <span class="hljs-comment"># 5. 创建 FlatParamHandle</span><br>    <span class="hljs-comment"># 这是最后一步，使用准备好的参数列表来实际创建和初始化 FlatParamHandle</span><br>    _init_param_handle_from_params(state, managed_params, fully_sharded_module)<br>    <br>    <span class="hljs-comment"># 返回更新后的 FSDP 状态</span><br>    <span class="hljs-keyword">return</span> state<br></code></pre></td></tr></table></figure><h3 id="get-orig-params"><a href="#get-orig-params" class="headerlink" title="_get_orig_params"></a>_get_orig_params</h3><p>得到fully_sharded_module中所有的参数，也就是tensor矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_orig_params</span>(<span class="hljs-params"></span><br><span class="hljs-params">    module: nn.Module, <span class="hljs-comment"># 要从中提取参数的模块</span></span><br><span class="hljs-params">    ignored_params: <span class="hljs-built_in">set</span>[nn.Parameter], <span class="hljs-comment"># 一个包含应被忽略的参数的集合</span></span><br><span class="hljs-params"></span>) -&gt; Iterator[nn.Parameter]: <span class="hljs-comment"># 返回一个参数的迭代器（生成器）</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    返回一个遍历 `module` 中原始参数的迭代器。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这个迭代器不会返回以下几种参数：</span><br><span class="hljs-string">    1. 在 `ignored_params` 集合中的参数。</span><br><span class="hljs-string">    2. 任何 `FlatParameter` 实例（这可能因为嵌套使用 FSDP 而出现）。</span><br><span class="hljs-string">    3. 任何已经被展平的原始参数（这只在 `use_orig_params=True` 模式下有意义）。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 获取模块所有参数的生成器</span><br>    param_gen = module.parameters()<br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-comment"># 使用一个无限循环来手动迭代生成器</span><br>        <span class="hljs-comment"># 这样做是为了清晰地处理 StopIteration 异常</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>            <span class="hljs-comment"># 从生成器中获取下一个参数</span><br>            param = <span class="hljs-built_in">next</span>(param_gen)<br>            <br>            <span class="hljs-comment"># 这是核心的过滤逻辑：</span><br>            <span class="hljs-comment"># 1. `param not in ignored_params`：确保该参数不是用户明确指定要忽略的参数。</span><br>            <span class="hljs-comment"># 2. `not _is_fsdp_flattened(param)`：检查该参数是否已经被 FSDP 处理过。</span><br>            <span class="hljs-comment">#    `_is_fsdp_flattened` 会检查参数是否是 `FlatParameter` 的实例，</span><br>            <span class="hljs-comment">#    或者是否已经被合并到另一个 `FlatParameter` 中。这对于处理嵌套 FSDP</span><br>            <span class="hljs-comment">#    至关重要，可以防止重复包装和管理同一个参数。</span><br>            <span class="hljs-keyword">if</span> param <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> ignored_params <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> _is_fsdp_flattened(param):<br>                <span class="hljs-comment"># 如果参数通过了所有检查，就将其 yield 出去</span><br>                <span class="hljs-keyword">yield</span> param<br>    <span class="hljs-keyword">except</span> StopIteration:<br>        <span class="hljs-comment"># 当 `module.parameters()` 迭代完成并抛出 StopIteration 异常时，</span><br>        <span class="hljs-comment"># 捕获它并正常退出循环。`pass` 表示什么也不做。</span><br>        <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure><h3 id="init-param-handle-from-params"><a href="#init-param-handle-from-params" class="headerlink" title="_init_param_handle_from_params"></a>_init_param_handle_from_params</h3><p>注意初始化了FlatParamHandle后立刻进行了shard</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_param_handle_from_params</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState, <span class="hljs-comment"># FSDP 状态对象，用于跟踪和管理 FSDP 实例的各种状态</span></span><br><span class="hljs-params">    params: <span class="hljs-built_in">list</span>[nn.Parameter], <span class="hljs-comment"># 从模块中收集到的、需要被 FSDP 管理的原始参数列表</span></span><br><span class="hljs-params">    fully_sharded_module: nn.Module, <span class="hljs-comment"># 这些参数所属的、需要被 FSDP 完全分片的模块</span></span><br><span class="hljs-params"></span>):<br>    <span class="hljs-comment"># 如果没有需要管理的参数，则直接返回，不做任何操作</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(params) == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span><br><br>    <span class="hljs-comment"># 1. 实例化 FlatParamHandle</span><br>    <span class="hljs-comment"># FlatParamHandle 是 FSDP 的核心，它负责将 `params` 列表中的多个参数</span><br>    <span class="hljs-comment"># “展平”（flatten）并合并成一个单一的、连续的张量（FlatParameter）。</span><br>    <span class="hljs-comment"># 这里传入了所有必要的配置，如分片策略、混合精度设置、进程组等。</span><br>    handle = FlatParamHandle(<br>        params, <span class="hljs-comment"># 原始参数列表</span><br>        fully_sharded_module, <span class="hljs-comment"># 所属模块</span><br>        state.compute_device, <span class="hljs-comment"># 计算设备 (例如, &#x27;cuda:0&#x27;)</span><br>        SHARDING_STRATEGY_MAP[state.sharding_strategy], <span class="hljs-comment"># 分片策略</span><br>        state.cpu_offload.offload_params, <span class="hljs-comment"># 是否启用 CPU offload</span><br>        state.mixed_precision.param_dtype, <span class="hljs-comment"># 参数的数据类型 (例如, torch.float16)</span><br>        state.mixed_precision.reduce_dtype, <span class="hljs-comment"># all-reduce 操作的数据类型</span><br>        state.mixed_precision.keep_low_precision_grads, <span class="hljs-comment"># 是否保留低精度梯度</span><br>        state.process_group, <span class="hljs-comment"># 分布式通信的进程组</span><br>        state._use_orig_params, <span class="hljs-comment"># 是否使用原始参数的视图（一种优化）</span><br>        fsdp_extension=state._fsdp_extension, <span class="hljs-comment"># FSDP 扩展</span><br>    )<br><br>    <span class="hljs-comment"># 2. 对 FlatParameter 进行分片</span><br>    <span class="hljs-comment"># 调用 .shard() 方法，根据指定的分片策略，将完整的 FlatParameter 分割成</span><br>    <span class="hljs-comment"># 多个分片，每个 rank 只保留自己负责的那一部分。这是实现显存优化的关键。</span><br>    handle.shard()<br><br>    <span class="hljs-comment"># 3. 更新 FSDP 状态</span><br>    <span class="hljs-comment"># 确保当前 FSDP 实例还没有关联任何 handle</span><br>    <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> state._handle<br>    <span class="hljs-comment"># 将新创建的 FlatParameter 添加到 FSDP 实例的参数列表中，以便优化器可以找到它</span><br>    state.params.append(handle.flat_param)<br>    <span class="hljs-comment"># 将新创建的 handle 保存到 FSDP 状态中</span><br>    state._handle = handle<br>    <span class="hljs-comment"># 建立从模块到其对应 handle 的映射关系</span><br>    state._fully_sharded_module_to_handle[handle._fully_sharded_module] = handle<br><br>    <span class="hljs-comment"># 4. 处理 CPU Offload</span><br>    <span class="hljs-comment"># 如果启用了 CPU offload，并且分片后的 FlatParameter 当前不在 CPU 上</span><br>    cpu_device = torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)<br>    <span class="hljs-keyword">if</span> state.cpu_offload.offload_params <span class="hljs-keyword">and</span> handle.flat_param.device != cpu_device:<br>        <span class="hljs-comment"># 将该分片移动到 CPU，以释放 GPU 显存</span><br>        handle.flat_param_to(cpu_device)<br></code></pre></td></tr></table></figure><h3 id="FlatParamHandle"><a href="#FlatParamHandle" class="headerlink" title="FlatParamHandle"></a>FlatParamHandle</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FlatParamHandle</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    一个管理扁平化参数（:class:`FlatParameter`）的句柄。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这包括分片和视图管理。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">        params (Sequence[nn.Parameter]): 要被展平到扁平化参数中的参数序列。</span><br><span class="hljs-string">        fully_sharded_module (nn.Module): 被 FSDP 包装的模块。</span><br><span class="hljs-string">        device (torch.device): 计算和通信设备，通常是 GPU。</span><br><span class="hljs-string">        sharding_strategy (ShardingStrategy): 应用于此句柄的 `FlatParameter` 的分片策略。</span><br><span class="hljs-string">        offload_params (bool): 是否将此句柄的 `FlatParameter` 卸载到 CPU。</span><br><span class="hljs-string">        mp_param_dtype (Optional[torch.dtype]): 用于参数的混合精度类型。</span><br><span class="hljs-string">        mp_reduce_dtype (Optional[torch.dtype]): 用于梯度归约的混合精度类型。</span><br><span class="hljs-string">        keep_low_precision_grads (bool): 是否保持低精度的梯度。</span><br><span class="hljs-string">        use_orig_params (bool): 如果为 True，FSDP 会保留原始参数变量，并从 `named_parameters()` 返回它们。</span><br><span class="hljs-string">                               这允许在同一个 `FlatParameter` 内对不同原始参数使用不同的优化器超参数。</span><br><span class="hljs-string">                               如果为 False，FSDP 会在每次迭代中重新构建参数。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment">##################</span><br>    <span class="hljs-comment"># INITIALIZATION #</span><br>    <span class="hljs-comment">##################</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        params: <span class="hljs-type">Sequence</span>[<span class="hljs-type">Union</span>[nn.Parameter, Tensor]],</span><br><span class="hljs-params">        fully_sharded_module: nn.Module,</span><br><span class="hljs-params">        device: torch.device,</span><br><span class="hljs-params">        sharding_strategy: HandleShardingStrategy,</span><br><span class="hljs-params">        offload_params: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        mp_param_dtype: <span class="hljs-type">Optional</span>[torch.dtype],</span><br><span class="hljs-params">        mp_reduce_dtype: <span class="hljs-type">Optional</span>[torch.dtype],</span><br><span class="hljs-params">        keep_low_precision_grads: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        process_group: dist.ProcessGroup,</span><br><span class="hljs-params">        use_orig_params: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        *,</span><br><span class="hljs-params">        fsdp_extension: <span class="hljs-type">Optional</span>[FSDPExtensions] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 确保传入的参数列表不为空</span><br>        params = <span class="hljs-built_in">list</span>(params)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(params) == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<br>                <span class="hljs-string">f&quot;不能用一个空的参数列表来构造 <span class="hljs-subst">&#123;self.__class__.__name__&#125;</span>&quot;</span><br>            )<br>        <br>        <span class="hljs-comment"># 初始化一些内部函数</span><br>        self._init_setattr_fns()<br><br>        <span class="hljs-comment"># 从环境变量中读取一些用于调试和性能分析的高级配置</span><br>        self._skip_writeback_check = (<br>            os.environ.get(_FSDP_SKIP_WRITEBACK_CHECK, <span class="hljs-string">&quot;&quot;</span>) == <span class="hljs-string">&quot;1&quot;</span><br>        )<br>        self._use_full_prec_in_eval = (<br>            os.environ.get(_FSDP_USE_FULL_PREC_IN_EVAL, <span class="hljs-string">&quot;&quot;</span>) == <span class="hljs-string">&quot;1&quot;</span><br>        )<br>        <span class="hljs-comment"># 这些 &quot;fake&quot; 选项用于性能分析，它们会跳过实际的通信操作</span><br>        self._use_fake_all_gather = os.environ.get(_FSDP_USE_FAKE_ALL_GATHER, <span class="hljs-string">&quot;&quot;</span>) == <span class="hljs-string">&quot;1&quot;</span><br>        self._use_fake_reduce = os.environ.get(_FSDP_USE_FAKE_REDUCE, <span class="hljs-string">&quot;&quot;</span>) == <span class="hljs-string">&quot;1&quot;</span><br>        <br>        <span class="hljs-comment"># ... (处理上述环境变量的警告信息) ...</span><br><br>        <span class="hljs-comment"># 是否对齐内存地址，目前仅在 use_orig_params=True 时启用</span><br>        align_addresses = use_orig_params<br>        self._init_get_unflat_views_fn(align_addresses)<br><br>        <span class="hljs-comment"># --- 初始化核心属性 ---</span><br>        self.device = device  <span class="hljs-comment"># 计算设备 (e.g., &#x27;cuda:0&#x27;)</span><br>        self._device_handle = _FSDPDeviceHandle.from_device(self.device)<br>        self.process_group = process_group  <span class="hljs-comment"># 分布式通信组</span><br>        self.rank = process_group.rank()      <span class="hljs-comment"># 当前进程的排名</span><br>        self.world_size = process_group.size() <span class="hljs-comment"># 总进程数</span><br><br>        <span class="hljs-comment"># --- 存储 FSDP 的主要配置 ---</span><br>        self._sharding_strategy = sharding_strategy <span class="hljs-comment"># 分片策略 (e.g., SHARD_GRAD_OP)</span><br>        self._offload_params = offload_params       <span class="hljs-comment"># 是否卸载到 CPU</span><br>        self._use_orig_params = use_orig_params     <span class="hljs-comment"># 是否使用原始参数</span><br>        self._keep_low_precision_grads = keep_low_precision_grads <span class="hljs-comment"># 是否保留低精度梯度</span><br><br>        <span class="hljs-comment"># --- 初始化状态变量 ---</span><br>        self._training_state = HandleTrainingState.IDLE  <span class="hljs-comment"># 初始状态为空闲</span><br>        self._debug_level = dist.get_debug_level()<br>        self._fully_sharded_module = fully_sharded_module <span class="hljs-comment"># 关联的模块</span><br>        <br>        <span class="hljs-comment"># ... (初始化一些用于 prefetch 和执行顺序跟踪的内部状态变量) ...</span><br>        self._handle_index: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span><br>        self._needs_pre_forward_unshard = <span class="hljs-literal">False</span><br>        self._needs_pre_backward_unshard = <span class="hljs-literal">False</span><br>        self._prefetched = <span class="hljs-literal">False</span><br><br>        <span class="hljs-comment"># --- 初始化数据类型 (dtype) ---</span><br>        <span class="hljs-comment"># 原始参数的数据类型</span><br>        self._orig_param_dtype = params[<span class="hljs-number">0</span>].dtype<br>        <span class="hljs-comment"># 初始化用于前向/后向传播和梯度计算的混合精度数据类型</span><br>        self._init_param_reduce_dtypes(mp_param_dtype, mp_reduce_dtype)<br>        <span class="hljs-keyword">assert</span> self._fwd_bwd_param_dtype <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>  <span class="hljs-comment"># mypy</span><br>        <br>        <span class="hljs-comment"># 计算对齐所需的元素数量</span><br>        self._aligned_numel = (<br>            _get_aligned_numel(unsharded_dtype=self._fwd_bwd_param_dtype)<br>            <span class="hljs-keyword">if</span> align_addresses<br>            <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>        )<br>        self._fsdp_extension = fsdp_extension<br><br>        <span class="hljs-comment"># --- 最关键的步骤：创建扁平化参数和元数据 ---</span><br>        <span class="hljs-comment"># 这个方法会执行以下操作:</span><br>        <span class="hljs-comment"># 1. 计算所有参数的总元素数量。</span><br>        <span class="hljs-comment"># 2. 创建一个大的、一维的 `FlatParameter` 来容纳所有参数。</span><br>        <span class="hljs-comment"># 3. 将原始参数的数据复制到这个 `FlatParameter` 中。</span><br>        <span class="hljs-comment"># 4. 记录每个原始参数在 `FlatParameter` 中的位置、形状等元数据。</span><br>        self._init_flat_param_and_metadata(<br>            params,<br>            fully_sharded_module,<br>            self._aligned_numel,<br>            use_orig_params,  <span class="hljs-comment"># type: ignore[arg-<span class="hljs-built_in">type</span>]</span><br>        )<br><br>        <span class="hljs-comment"># --- 最后一步：设置参数视图 ---</span><br>        <span class="hljs-comment"># 让原始模块的参数成为 `FlatParameter` 的“视图”（view）。</span><br>        <span class="hljs-comment"># 这意味着对原始参数的任何修改都会反映在 `FlatParameter` 上，反之亦然。</span><br>        self._use_unsharded_views(as_params=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><h4 id="init-flat-param-and-metadata"><a href="#init-flat-param-and-metadata" class="headerlink" title="_init_flat_param_and_metadata"></a>_init_flat_param_and_metadata</h4><p>这个方法是 FSDP 魔法的起点。它像一个高效的管家，将一堆零散的参数（ params ）整齐地排列、打包，并贴上详细的标签（元数据），最终形成一个易于管理的单一实体（ FlatParameter ）。这个过程不仅处理了复杂的共享参数和内存对齐问题，还为后续的分布式操作（如 reduce-scatter ）做好了准备。一旦这个方法执行完毕， FlatParamHandle 就拥有了一个完整的、随时可以被分片和恢复的扁平化参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_flat_param_and_metadata</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    params: <span class="hljs-built_in">list</span>[<span class="hljs-type">Union</span>[Tensor, nn.Parameter]],</span><br><span class="hljs-params">    module: nn.Module,</span><br><span class="hljs-params">    aligned_numel: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    use_orig_params: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    初始化 ``FlatParameter`` 及其元数据。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    注意：此方法只应在构造时调用一次，之后 ``FlatParameter`` 的元数据被假定为静态的。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># --- 1. 输入验证 ---</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(params) == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;期望非空的 `params`&quot;</span>)<br>    <span class="hljs-keyword">if</span> aligned_numel &lt; <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<br>            <span class="hljs-string">f&quot;期望非负的 `aligned_numel` 但得到了 <span class="hljs-subst">&#123;aligned_numel&#125;</span>&quot;</span><br>        )<br>    <span class="hljs-comment"># 验证所有待展平的张量具有相同的 dtype、requires_grad 和 device</span><br>    (<br>        dtype,<br>        flat_param_requires_grad,<br>        device,<br>    ) = self._validate_tensors_to_flatten(params)<br>    params_set = <span class="hljs-built_in">set</span>(params) <span class="hljs-comment"># 转换为集合以提高查找效率</span><br><br>    <span class="hljs-comment"># --- 2. 初始化用于存储元数据的列表 ---</span><br>    param_infos: <span class="hljs-built_in">list</span>[ParamInfo] = []  <span class="hljs-comment"># 参数信息 (名称, 所属模块, 模块名)</span><br>    numels: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>] = []             <span class="hljs-comment"># 每个参数的元素数量</span><br>    shapes: <span class="hljs-built_in">list</span>[torch.Size] = []      <span class="hljs-comment"># 每个参数的原始形状</span><br>    strides: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">tuple</span>[<span class="hljs-built_in">int</span>, ...]] = [] <span class="hljs-comment"># 每个参数的原始步长</span><br>    fqns: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">str</span>] = []               <span class="hljs-comment"># 每个参数的完全限定名 (e.g., &#x27;layer1.0.conv.weight&#x27;)</span><br>    shared_param_infos: <span class="hljs-built_in">list</span>[SharedParamInfo] = [] <span class="hljs-comment"># 共享参数的信息</span><br>    <span class="hljs-comment"># 用于跟踪已处理参数，以识别共享参数</span><br>    shared_param_memo: <span class="hljs-built_in">dict</span>[<br>        <span class="hljs-type">Union</span>[Tensor, nn.Parameter], <span class="hljs-built_in">tuple</span>[nn.Module, <span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>]<br>    ] = &#123;&#125;<br>    params_to_flatten: <span class="hljs-built_in">list</span>[<span class="hljs-type">Union</span>[Tensor, nn.Parameter]] = [] <span class="hljs-comment"># 最终要展平的张量列表（包括填充）</span><br>    shared_params: <span class="hljs-built_in">list</span>[<span class="hljs-type">Union</span>[Tensor, nn.Parameter]] = []     <span class="hljs-comment"># 识别出的共享参数列表</span><br>    is_padding_mask: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">bool</span>] = [] <span class="hljs-comment"># 标记 `params_to_flatten` 中哪些是填充</span><br>    total_numel = total_numel_without_padding = <span class="hljs-number">0</span> <span class="hljs-comment"># 计数器</span><br><br>    <span class="hljs-comment"># --- 3. 遍历模块，收集参数和元数据 ---</span><br>    <span class="hljs-comment"># 遍历模块的所有子模块和参数，以确保参数的顺序是确定的</span><br>    <span class="hljs-keyword">for</span> submodule_name, submodule <span class="hljs-keyword">in</span> module.named_modules(remove_duplicate=<span class="hljs-literal">False</span>):<br>        <span class="hljs-keyword">for</span> param_name, param <span class="hljs-keyword">in</span> _named_parameters_with_duplicates(<br>            submodule, recurse=<span class="hljs-literal">False</span><br>        ):<br>            <span class="hljs-keyword">if</span> param <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> params_set:<br>                <span class="hljs-keyword">continue</span> <span class="hljs-comment"># 只处理在输入 `params` 列表中的参数</span><br>            <br>            <span class="hljs-comment"># 如果参数已经在 memo 中，说明它是共享参数</span><br>            <span class="hljs-keyword">if</span> param <span class="hljs-keyword">in</span> shared_param_memo:<br>                <span class="hljs-comment"># ... 记录共享参数信息 ...</span><br>                shared_params.append(param)<br>                <span class="hljs-comment"># ...</span><br>            <span class="hljs-keyword">else</span>: <span class="hljs-comment"># 这是一个新的、未见过的参数</span><br>                <span class="hljs-comment"># --- 3a. 处理内存对齐填充 ---</span><br>                <span class="hljs-keyword">if</span> aligned_numel &gt; <span class="hljs-number">0</span>:<br>                    numel_to_pad = aligned_numel - (total_numel % aligned_numel)<br>                    <span class="hljs-keyword">if</span> numel_to_pad &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> numel_to_pad &lt; aligned_numel:<br>                        <span class="hljs-comment"># 如果需要，插入一个填充张量</span><br>                        padding_tensor = _construct_padding_tensor(<br>                            numel_to_pad, dtype, <span class="hljs-literal">False</span>, device<br>                        )<br>                        params_to_flatten.append(padding_tensor)<br>                        is_padding_mask.append(<span class="hljs-literal">True</span>)<br>                        numels.append(numel_to_pad)<br>                        total_numel += numel_to_pad<br>                <br>                <span class="hljs-comment"># --- 3b. 记录主参数的元数据 ---</span><br>                shared_param_memo[param] = (submodule, submodule_name, param_name)<br>                params_to_flatten.append(param)<br>                is_padding_mask.append(<span class="hljs-literal">False</span>)<br>                param_infos.append(ParamInfo(param_name, submodule, submodule_name))<br>                numels.append(param.numel())<br>                shapes.append(param.shape)<br>                strides.append(param.stride())<br>                <span class="hljs-comment"># ... 记录其他元数据 ...</span><br>                total_numel += param.numel()<br>                total_numel_without_padding += param.numel()<br><br>    <span class="hljs-comment"># --- 4. 处理 reduce-scatter 的填充 ---</span><br>    <span class="hljs-comment"># 为了让 reduce-scatter 操作更高效，需要确保总元素数能被 world_size 整除</span><br>    <span class="hljs-keyword">if</span> aligned_numel &gt; <span class="hljs-number">0</span>:<br>        numel_to_pad = self.world_size - (total_numel % self.world_size)<br>        <span class="hljs-keyword">if</span> numel_to_pad &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> numel_to_pad &lt; self.world_size:<br>            <span class="hljs-comment"># ... 如果需要，再次插入填充张量 ...</span><br>            padding_tensor = _construct_padding_tensor(<br>                numel_to_pad, dtype, <span class="hljs-literal">False</span>, device<br>            )<br>            params_to_flatten.append(padding_tensor)<br>            is_padding_mask.append(<span class="hljs-literal">True</span>)<br>            numels.append(numel_to_pad)<br>            total_numel += numel_to_pad<br><br>    <span class="hljs-comment"># --- 5. 执行展平操作 ---</span><br>    <span class="hljs-comment"># 调用 `flatten_tensors_into_flat_param` 将 `params_to_flatten` 列表中的所有张量</span><br>    <span class="hljs-comment"># 合并成一个大的、一维的 `FlatParameter`</span><br>    self.flat_param: FlatParameter = self.flatten_tensors_into_flat_param(<br>        params_to_flatten,<br>        aligned_numel=<span class="hljs-number">0</span>, <span class="hljs-comment"># 此时已手动处理完对齐，故传 0</span><br>        requires_grad=flat_param_requires_grad,<br>    )<br><br>    <span class="hljs-comment"># --- 6. 将元数据附加到 FlatParameter 上 ---</span><br>    <span class="hljs-comment"># 调用 `FlatParameter` 的静态方法，将之前收集的所有元数据（形状、步长、名称等）</span><br>    <span class="hljs-comment"># 作为属性附加到新创建的 `self.flat_param` 对象上。</span><br>    FlatParameter._init_metadata(<br>        self.flat_param,<br>        param_infos,<br>        numels,<br>        shapes,<br>        strides,<br>        <span class="hljs-comment"># ... 传递所有其他元数据列表 ...</span><br>    )<br></code></pre></td></tr></table></figure><h5 id="flatten-tensors-into-flat-param"><a href="#flatten-tensors-into-flat-param" class="headerlink" title="flatten_tensors_into_flat_param"></a>flatten_tensors_into_flat_param</h5><p>最后的结果就是进行了张量展开，获得了一个扁平的张量，形状为：[参数数量，参数长度]。即每个参数param都变成了一维，最后各个param都拼接在了一起。所以这些参数最后在物理地址上都是连续的，方便操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">flatten_tensors_into_flat_param</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    tensors: <span class="hljs-built_in">list</span>[Tensor],</span><br><span class="hljs-params">    aligned_numel: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    requires_grad: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params"></span>) -&gt; FlatParameter:<br>    flat_param_data = self.flatten_tensors(tensors, aligned_numel)<br>    <span class="hljs-keyword">return</span> FlatParameter(flat_param_data, requires_grad=requires_grad)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">flatten_tensors</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    tensors: <span class="hljs-built_in">list</span>[Tensor], <span class="hljs-comment"># 输入：一个待展平的张量列表</span></span><br><span class="hljs-params">    aligned_numel: <span class="hljs-built_in">int</span>,    <span class="hljs-comment"># 输入：用于内存对齐的元素数量。如果为0，则不进行对齐填充</span></span><br><span class="hljs-params"></span>) -&gt; Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    将 `tensors` 展平为单个扁平张量。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    如果 `aligned_numel` 大于 0，展平过程会包含可选的填充，</span><br><span class="hljs-string">    其中 `aligned_numel` 给出了实现地址对齐所需的元素数量。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    注意：填充对齐算法必须与 `_init_flat_param_metadata` 方法保持同步。</span><br><span class="hljs-string">    我们分离这两个方法是因为初始化只发生一次，而此方法可能在训练过程中</span><br><span class="hljs-string">    被多次调用（例如，用于保存检查点）。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># --- 1. 输入校验 ---</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(tensors) == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;期望 `tensors` 列表不为空&quot;</span>)<br>    <span class="hljs-keyword">if</span> aligned_numel &lt; <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<br>            <span class="hljs-string">f&quot;期望 `aligned_numel` 为非负数，但得到了 <span class="hljs-subst">&#123;aligned_numel&#125;</span>&quot;</span><br>        )<br>    <span class="hljs-comment"># 校验所有输入张量的数据类型(dtype)和设备(device)是否一致</span><br>    dtype, _, device = self._validate_tensors_to_flatten(tensors)<br>    <br>    flat_tensors: <span class="hljs-built_in">list</span>[Tensor] = [] <span class="hljs-comment"># 用于存储最终要拼接的张量（包括填充）</span><br><br>    <span class="hljs-comment"># --- 2. 处理对齐填充 (如果需要) ---</span><br>    <span class="hljs-keyword">if</span> aligned_numel &gt; <span class="hljs-number">0</span>:<br>        total_numel = <span class="hljs-number">0</span> <span class="hljs-comment"># 记录当前已处理的元素总数</span><br>        <span class="hljs-keyword">for</span> tensor <span class="hljs-keyword">in</span> tensors:<br>            <span class="hljs-comment"># 计算在添加当前张量之前需要多少填充，以使其起始位置按 `aligned_numel` 对齐</span><br>            numel_to_pad = aligned_numel - (total_numel % aligned_numel)<br>            <span class="hljs-keyword">if</span> numel_to_pad &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> numel_to_pad &lt; aligned_numel:<br>                <span class="hljs-comment"># 创建一个指定大小的填充张量</span><br>                padding_tensor = _construct_padding_tensor(<br>                    numel_to_pad, dtype, <span class="hljs-literal">False</span>, device<br>                )<br>                flat_tensors.append(padding_tensor)<br>                total_numel += numel_to_pad<br>            <br>            <span class="hljs-comment"># 添加实际的张量。首先将其展平为一维</span><br>            <span class="hljs-comment"># 如果张量是内存连续的，直接 flatten；否则使用 as_strided 避免额外拷贝</span><br>            flat_tensors.append(<br>                torch.flatten(_detach_if_needed(tensor))<br>                <span class="hljs-keyword">if</span> _is_truly_contiguous(tensor)<br>                <span class="hljs-keyword">else</span> _detach_if_needed(tensor).as_strided((tensor.numel(),), (<span class="hljs-number">1</span>,))<br>            )<br>            total_numel += tensor.numel()<br>        <br>        <span class="hljs-comment"># --- 3. 处理分片填充 (为了能被 world_size 整除) ---</span><br>        numel_to_pad = self.world_size - (total_numel % self.world_size)<br>        <span class="hljs-keyword">if</span> numel_to_pad &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> numel_to_pad &lt; self.world_size:<br>            padding_tensor = _construct_padding_tensor(<br>                numel_to_pad, dtype, <span class="hljs-literal">False</span>, device<br>            )<br>            flat_tensors.append(padding_tensor)<br>            total_numel += numel_to_pad<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># --- 4. 无对齐填充的简单情况 ---</span><br>        <span class="hljs-comment"># 直接将每个张量展平并放入列表</span><br>        flat_tensors = [<br>            torch.flatten(_detach_if_needed(tensor))<br>            <span class="hljs-keyword">if</span> _is_truly_contiguous(tensor)<br>            <span class="hljs-keyword">else</span> _detach_if_needed(tensor).as_strided((tensor.numel(),), (<span class="hljs-number">1</span>,))<br>            <span class="hljs-keyword">for</span> tensor <span class="hljs-keyword">in</span> tensors<br>        ]<br>    <br>    <span class="hljs-comment"># --- 5. 最终拼接 ---</span><br>    <span class="hljs-comment"># 将列表中的所有张量（包括原始张量和所有填充张量）拼接成一个最终的扁平化张量</span><br>    <span class="hljs-keyword">return</span> torch.cat(flat_tensors, dim=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h1 id="fsdp模型forward"><a href="#fsdp模型forward" class="headerlink" title="fsdp模型forward"></a>fsdp模型forward</h1><p>最外部的代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, *args: <span class="hljs-type">Any</span>, **kwargs: <span class="hljs-type">Any</span></span>) -&gt; <span class="hljs-type">Any</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Run the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.&quot;&quot;&quot;</span><br>    handle = self._handle<br>    <span class="hljs-keyword">with</span> torch.autograd.profiler.record_function(<br>        <span class="hljs-string">&quot;FullyShardedDataParallel.forward&quot;</span><br>    ):<br>        args, kwargs = _root_pre_forward(self, self, args, kwargs)<br>        unused = <span class="hljs-literal">None</span><br>        args, kwargs = _pre_forward(<br>            self,<br>            handle,<br>            _pre_forward_unshard,<br>            self._fsdp_wrapped_module,<br>            args,<br>            kwargs,<br>        )<br>        <span class="hljs-keyword">if</span> handle:<br>            _p_assert(<br>                handle.flat_param.device == self.compute_device,<br>                <span class="hljs-string">&quot;Expected `FlatParameter` to be on the compute device &quot;</span><br>                <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;self.compute_device&#125;</span> but got <span class="hljs-subst">&#123;handle.flat_param.device&#125;</span>&quot;</span>,<br>            )<br>        output = self._fsdp_wrapped_module(*args, **kwargs)<br>        <span class="hljs-keyword">return</span> _post_forward(<br>            self, handle, _post_forward_reshard, self, unused, output<br>        )<br><br></code></pre></td></tr></table></figure><h2 id="pre-forward"><a href="#pre-forward" class="headerlink" title="_pre_forward"></a>_pre_forward</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_pre_forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: <span class="hljs-type">Optional</span>[FlatParamHandle],</span><br><span class="hljs-params">    unshard_fn: <span class="hljs-type">Callable</span>,</span><br><span class="hljs-params">    module: nn.Module,</span><br><span class="hljs-params">    args: <span class="hljs-built_in">tuple</span>[<span class="hljs-type">Any</span>, ...],</span><br><span class="hljs-params">    kwargs: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-built_in">tuple</span>[<span class="hljs-built_in">tuple</span>[<span class="hljs-type">Any</span>, ...], <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]]:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    执行前向传播前的逻辑。这包括：</span><br><span class="hljs-string">    1. 对当前分片的参数进行反分片（unshard），使其恢复为完整参数。</span><br><span class="hljs-string">    2. 为这些参数注册后向传播钩子（post-backward hooks）。</span><br><span class="hljs-string">    3. 将前向传播的输入（args, kwargs）转换为指定的计算精度。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 使用 PyTorch profiler 记录函数执行，便于性能分析</span><br>    <span class="hljs-keyword">with</span> torch.profiler.record_function(<span class="hljs-string">&quot;FullyShardedDataParallel._pre_forward&quot;</span>):<br>        <span class="hljs-comment"># 这是一个针对梯度检查点（gradient checkpointing）的特殊处理。</span><br>        <span class="hljs-comment"># 在梯度检查点的重计算阶段，模块会再次执行前向传播，但此时参数已经 unshard 过了，</span><br>        <span class="hljs-comment"># 无需重复执行 unshard 和注册 hook 等操作，直接返回即可。</span><br>        <span class="hljs-keyword">if</span> handle <span class="hljs-keyword">and</span> handle._training_state == HandleTrainingState.BACKWARD_PRE:<br>            <span class="hljs-keyword">return</span> args, kwargs<br><br>        <span class="hljs-comment"># 1. 更新 FSDP 状态，标记当前正处于前向或后向传播阶段。</span><br>        state.training_state = TrainingState.FORWARD_BACKWARD<br>        <span class="hljs-comment"># 记录当前模块的执行顺序，这对于后续的预取（prefetching）和梯度同步至关重要。</span><br>        state._exec_order_data.record_pre_forward(handle, module.training)<br>        <span class="hljs-keyword">if</span> handle:<br>            <span class="hljs-comment"># 更新当前参数句柄（handle）的状态为“正在前向传播”。</span><br>            handle._training_state = HandleTrainingState.FORWARD<br><br>        <span class="hljs-comment"># 2. 执行核心操作：反分片（Unsharding）。</span><br>        <span class="hljs-comment"># 这是最关键的一步。如果 unshard_fn 存在，就调用它。</span><br>        <span class="hljs-comment"># 这个函数内部会触发 all-gather 操作，从所有 GPU 上收集参数分片，</span><br>        <span class="hljs-comment"># 在当前设备上重建完整的、未分片的参数，以供模块的 forward 方法使用。</span><br>        <span class="hljs-keyword">if</span> unshard_fn <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            unshard_fn(state, handle)<br><br>        <span class="hljs-comment"># 3. 注册后向传播钩子（Post-Backward Hook）。</span><br>        <span class="hljs-comment"># 这个钩子会在反向传播计算完当前参数的梯度之后被触发。</span><br>        <span class="hljs-comment"># 它的主要作用是：</span><br>        <span class="hljs-comment">#   a. 将参数重新分片（reshard），释放完整参数占用的内存。</span><br>        <span class="hljs-comment">#   b. 对计算出的完整梯度进行 reduce-scatter 操作，完成梯度同步。</span><br>        <span class="hljs-comment"># 因为计算图（grad_fn）每次都可能变化，所以这个钩子需要在每次前向传播时都重新注册。</span><br>        _register_post_backward_hook(state, handle)<br><br>        <span class="hljs-comment"># 针对 CPU Offload 的特殊处理：如果优化器在反向传播中将 CPU 上的梯度清空了，</span><br>        <span class="hljs-comment"># 这里需要重新分配一块内存空间给它，为下一次梯度累积做准备。</span><br>        <span class="hljs-keyword">if</span> handle <span class="hljs-keyword">and</span> handle._offload_params <span class="hljs-keyword">and</span> handle.flat_param._cpu_grad <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            handle.flat_param._cpu_grad = torch.zeros_like(<br>                handle.flat_param._local_shard, device=torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)<br>            ).pin_memory()<br><br>        <span class="hljs-comment"># 检查是否需要将模型输入转换为低精度（如 fp16）。</span><br>        should_cast_forward_inputs = (<br>            state._handle <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> state._handle._force_full_precision<br>        )<br><br>        <span class="hljs-comment"># 4. 如果启用了混合精度（Mixed Precision）并且设置了 cast_forward_inputs，</span><br>        <span class="hljs-comment"># 就将 `args` 和 `kwargs` 中的张量递归地转换为指定的参数数据类型（param_dtype）。</span><br>        <span class="hljs-keyword">if</span> should_cast_forward_inputs <span class="hljs-keyword">and</span> state.mixed_precision.cast_forward_inputs:<br>            input_dtype: <span class="hljs-type">Optional</span>[torch.dtype] = state.mixed_precision.param_dtype<br>            args, kwargs = _cast_forward_inputs(input_dtype, *args, **kwargs)<br><br>        <span class="hljs-comment"># 注册一个只做 reshard 的后向钩子，用于不需要计算梯度的场景。</span><br>        _register_post_backward_reshard_only_hook(state, handle, args, kwargs)<br>        <br>        <span class="hljs-comment"># 返回处理过（可能已转换精度）的输入，传递给原始模块的 forward 方法。</span><br>        <span class="hljs-keyword">return</span> args, kwargs<br></code></pre></td></tr></table></figure><h3 id="pre-forward-unshard"><a href="#pre-forward-unshard" class="headerlink" title="_pre_forward_unshard"></a>_pre_forward_unshard</h3><p>对于_pre_forward，可以看到其传入的是<code>_pre_forward_unshard</code>函数，该函数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_pre_forward_unshard</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: <span class="hljs-type">Optional</span>[FlatParamHandle],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Unshards parameters in the pre-forward.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle:<br>        <span class="hljs-keyword">return</span><br>    <span class="hljs-comment"># If the handles have been prefetched, then there is no need to call</span><br>    <span class="hljs-comment"># `_unshard()` again</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle._prefetched:<br>        _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br>    handle._needs_pre_forward_unshard = <span class="hljs-literal">False</span><br>    <span class="hljs-comment"># Don&#x27;t wait during trace</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>        current_stream = state._device_handle.current_stream()<br>        <span class="hljs-keyword">if</span> state._unshard_event <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            current_stream.wait_event(state._unshard_event)<br>            state._unshard_event = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">else</span>:<br>            current_stream.wait_stream(state._unshard_stream)<br>    <span class="hljs-keyword">with</span> torch.profiler.record_function(<br>        <span class="hljs-string">&quot;FullyShardedDataParallel._pre_forward_prefetch&quot;</span><br>    ):<br>        _prefetch_handle(state, handle, _PrefetchMode.FORWARD)<br><br><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_unshard</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params">    unshard_stream: torch.Stream,</span><br><span class="hljs-params">    pre_unshard_stream: torch.Stream,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Unshards the handles in ``handles``. If the handles are in</span><br><span class="hljs-string">    :meth:`summon_full_params` and are using mixed precision, then they are</span><br><span class="hljs-string">    forced to full precision.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Postcondition: handle&#x27;s ``FlatParameter`` &#x27;s data is the padded</span><br><span class="hljs-string">    unsharded flat parameter on the compute device.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle:<br>        <span class="hljs-keyword">return</span><br>    <span class="hljs-keyword">with</span> state._device_handle.stream(pre_unshard_stream):<br>        ran_pre_unshard = handle.pre_unshard()<br>    <span class="hljs-keyword">if</span> ran_pre_unshard:<br>        unshard_stream.wait_stream(pre_unshard_stream)<br>    <span class="hljs-keyword">if</span> state.limit_all_gathers:<br>        event = state._free_event_queue.dequeue_if_needed()<br>        <span class="hljs-keyword">if</span> event:<br>            <span class="hljs-keyword">with</span> torch.profiler.record_function(<br>                <span class="hljs-string">&quot;FullyShardedDataParallel.rate_limiter&quot;</span><br>            ):<br>                event.synchronize()<br>    <span class="hljs-keyword">with</span> state._device_handle.stream(unshard_stream):<br>        handle.unshard()<br>        handle.post_unshard()<br><br></code></pre></td></tr></table></figure><p>其中最关键的是<code>unshard</code>函数，该函数用来将参数重新收集回来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python">// ... existing code ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">unshard</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Run the unshard logic.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This includes all-gathering the flat parameter</span><br><span class="hljs-string">        and switching to using the unsharded flat parameter. If the handle does</span><br><span class="hljs-string">        not need unsharding, then this only switches to using the unsharded</span><br><span class="hljs-string">        flat parameter. For ``NO_SHARD``, this is a no-op.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        If FSDP is in :meth:`summon_full_params` and the handle uses parameter</span><br><span class="hljs-string">        mixed precision, then the parameter is forced to full precision.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 1. 检查是否真的需要执行 unshard 操作。</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.needs_unshard():<br>            <span class="hljs-comment"># 如果不需要（例如，sharding 策略是 NO_SHARD，或者参数已经被 unshard），</span><br>            <span class="hljs-comment"># 也要确保后续计算使用的是 unsharded 参数的视图。</span><br>            unsharded_flat_param = (<br>                self._get_padded_unsharded_flat_param()<br>                <span class="hljs-keyword">if</span> self.uses_sharded_strategy<br>                <span class="hljs-keyword">else</span> self.flat_param<br>            )<br>            self._use_unsharded_flat_param(unsharded_flat_param)<br>            <span class="hljs-keyword">return</span><br>        <br>        <span class="hljs-comment"># 2. 如果需要 unshard，则执行以下核心步骤：</span><br>        <span class="hljs-comment"># 2a. 分配内存：为即将聚合的完整参数张量分配空间。</span><br>        unsharded_flat_param = self._alloc_padded_unsharded_flat_param()<br>        <span class="hljs-comment"># 2b. 执行 All-Gather：调用我们之前分析过的 `_all_gather_flat_param` 方法，</span><br>        <span class="hljs-comment">#     从所有进程收集参数分片，并填充到刚刚分配的内存中。</span><br>        padded_unsharded_flat_param = self._all_gather_flat_param(unsharded_flat_param)<br>        <span class="hljs-comment"># 2c. 切换状态：将模块内部的参数指针切换为指向这个刚刚聚合好的完整参数，</span><br>        <span class="hljs-comment">#     以便后续的前向或后向计算可以使用它。</span><br>        self._use_unsharded_flat_param(padded_unsharded_flat_param)<br></code></pre></td></tr></table></figure><h4 id="alloc-padded-unsharded-flat-param"><a href="#alloc-padded-unsharded-flat-param" class="headerlink" title="_alloc_padded_unsharded_flat_param"></a>_alloc_padded_unsharded_flat_param</h4><p>该函数负责获取收集全参数的变量，并给他分配足够的内存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">// ... existing code ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_alloc_padded_unsharded_flat_param</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Allocate the *padded* unsharded flat parameter.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        The unpadded unsharded</span><br><span class="hljs-string">        flat parameter is always a view into the padded one. This padded</span><br><span class="hljs-string">        parameter is saved to a different attribute on the ``FlatParameter``</span><br><span class="hljs-string">        depending on if we force full precision.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 1. 检查：确保当前正在使用分片策略。</span><br>        self._check_sharded_strategy()<br>        flat_param = self.flat_param<br>        <br>        <span class="hljs-comment"># 2. 获取目标张量：获取将要用于存储完整参数的那个张量对象。</span><br>        <span class="hljs-comment">#    此时它可能还只是一个没有分配实际存储空间的“空壳”。</span><br>        unsharded_flat_param = self._get_padded_unsharded_flat_param()<br>        <br>        <span class="hljs-comment"># 3. 检查存储：确保这个张量之前的存储已经被释放，防止内存泄漏。</span><br>        self._check_storage_freed(unsharded_flat_param)<br>        <br>        <span class="hljs-comment"># 4. 分配存储：这是核心操作。调用 `_alloc_storage` 为这个张量分配实际的内存空间。</span><br>        <span class="hljs-comment">#    分配的大小是 `_padded_unsharded_size`，即所有分片聚合后的总大小，可能还包含一些为了对齐而增加的 padding。</span><br>        _alloc_storage(unsharded_flat_param, flat_param._padded_unsharded_size)  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>        <br>        <span class="hljs-comment"># 5. 返回张量：返回这个已经分配好内存、准备好被填充的张量。</span><br>        <span class="hljs-keyword">return</span> unsharded_flat_param<br></code></pre></td></tr></table></figure><h5 id="get-padded-unsharded-flat-param"><a href="#get-padded-unsharded-flat-param" class="headerlink" title="_get_padded_unsharded_flat_param"></a>_get_padded_unsharded_flat_param</h5><p>注意这里判断了是否是强制使用全精度，如果是，就会返回<code>flat_param._full_prec_full_param_padded</code>，并且释放掉<code>flat_param._full_param_padded</code>，如果不是，就直接返回<code>flat_param._full_param_padded</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python">// ... existing code ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_padded_unsharded_flat_param</span>(<span class="hljs-params">self</span>) -&gt; torch.Tensor:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Return a reference to the padded unsharded flat parameter depending on the calling context.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This should only be called if using a sharded strategy.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self._check_sharded_strategy()<br>        flat_param = self.flat_param<br>        <span class="hljs-comment"># 关键的逻辑判断：是否需要强制使用全精度？</span><br>        <span class="hljs-keyword">if</span> self._force_full_precision <span class="hljs-keyword">and</span> self._uses_param_mixed_precision:<br>            <span class="hljs-comment"># --- 情况1: 需要强制全精度 --- </span><br>            <span class="hljs-comment"># 当 FSDP 进入一个需要全精度参数的上下文（例如 `summon_full_params`），</span><br>            <span class="hljs-comment"># 并且当前参数本身是使用混合精度（如 bfloat16）存储的。</span><br>            <br>            <span class="hljs-comment"># 1. 选择一个专门用于存储全精度（如 float32）参数的张量作为目标。</span><br>            unsharded_flat_param = flat_param._full_prec_full_param_padded  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>            _p_assert(<br>// ... existing code ...<br>            )<br>            <span class="hljs-comment"># 2. 释放可能存在的、旧的、低精度的完整参数的存储。</span><br>            <span class="hljs-comment">#    因为全精度版本接下来可能会被修改，这会导致低精度版本失效。</span><br>            <span class="hljs-comment">#    释放它是为了确保下次需要时，会重新执行 all-gather 获取最新的数据。</span><br>            <span class="hljs-keyword">if</span> flat_param._full_param_padded.untyped_storage().size() &gt; <span class="hljs-number">0</span>:<br>                _free_storage(flat_param._full_param_padded)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># --- 情况2: 标准情况 --- </span><br>            <span class="hljs-comment"># 在常规的前向/后向传播中，直接使用默认的、与参数计算类型一致的张量即可。</span><br>            <span class="hljs-comment"># 这个张量的数据类型通常是低精度（如 bfloat16）。</span><br>            unsharded_flat_param = flat_param._full_param_padded  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>        <span class="hljs-keyword">return</span> unsharded_flat_param<br></code></pre></td></tr></table></figure><h5 id="alloc-storage"><a href="#alloc-storage" class="headerlink" title="_alloc_storage"></a>_alloc_storage</h5><p>该函数调用了底层存储对象的 <em>resize</em> 方法，将其大小调整为所需的元素数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_alloc_storage</span>(<span class="hljs-params">tensor: torch.Tensor, size: torch.Size</span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Allocate storage for ``tensor`` with the given size.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        bool: ``True`` if this method allocated storage and ``False`` if the</span><br><span class="hljs-string">        storage was already allocated.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>            already_allocated = tensor._typed_storage()._size() == size.numel()<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> already_allocated:<br>                tensor_storage_size = tensor._typed_storage()._size()<br>                _p_assert(<br>                    tensor_storage_size == <span class="hljs-number">0</span>,<br>                    <span class="hljs-string">&quot;Tensor storage should have been resized to be 0 but got PLACEHOLDEr&quot;</span>,<br>                )<br>                tensor._typed_storage()._resize_(size.numel())<br></code></pre></td></tr></table></figure><h4 id="all-gather-flat-param"><a href="#all-gather-flat-param" class="headerlink" title="_all_gather_flat_param"></a>_all_gather_flat_param</h4><p>这个函数是通过all_gather操作来进行实际的参数收集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python">// ... existing code ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_all_gather_flat_param</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        padded_unsharded_flat_param: Tensor,</span><br><span class="hljs-params">    </span>) -&gt; Tensor:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        All-gather the handle&#x27;s flat parameter to the destination ``padded_unsharded_flat_param``.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Then switch to use the all-gathered tensor.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 1. 断言检查：确保分布式环境已初始化，并且目标张量的大小足以容纳所有分片。</span><br>        _p_assert(<br>// ... existing code ...<br>        )<br><br>        <span class="hljs-comment"># 2. 获取用于通信的进程组（Process Group）。</span><br>        pg = (<br>            self._fake_process_group<br>            <span class="hljs-keyword">if</span> self._use_fake_all_gather<br>            <span class="hljs-keyword">else</span> self.process_group<br>        )<br><br>        <span class="hljs-comment"># 3. 根据张量是在 CPU 还是 GPU 上，执行不同的 all-gather 操作。</span><br>        <span class="hljs-comment"># HACK this should be handled by C10D</span><br>        <span class="hljs-keyword">if</span> sharded_flat_param.is_cpu:  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>            <span class="hljs-comment"># 对于 CPU，将数据收集到一个 tensor 列表中。</span><br>            tensor_list = <span class="hljs-built_in">list</span>(<br>                torch.chunk(<br>                    padded_unsharded_flat_param,<br>                    dist.get_world_size(pg),  <span class="hljs-comment"># type: ignore[arg-<span class="hljs-built_in">type</span>]</span><br>                )<br>            )<br>            dist.all_gather(tensor_list, sharded_flat_param, group=pg)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 对于 GPU，使用更高效的 all_gather_into_tensor 直接填充目标张量。</span><br>            dist.all_gather_into_tensor(<br>                padded_unsharded_flat_param,<br>                sharded_flat_param,<br>                pg,<br>            )<br><br>        <span class="hljs-comment"># 4. 处理参数卸载（Offloading）的特殊情况。</span><br>        <span class="hljs-keyword">if</span> self._offload_params:<br>            <span class="hljs-comment"># 如果参数被卸载到 CPU，需要确保 CUDA stream 正确同步，防止数据竞争。</span><br>            _no_dispatch_record_stream(<br>                sharded_flat_param,<br>                self._device_handle.current_stream(),  <span class="hljs-comment"># unshard_stream</span><br>            )<br>        <span class="hljs-comment"># 5. 返回填充了完整参数的张量。</span><br>        <span class="hljs-keyword">return</span> padded_unsharded_flat_param<br></code></pre></td></tr></table></figure><h5 id="all-gather-into-tensor学习"><a href="#all-gather-into-tensor学习" class="headerlink" title="all_gather_into_tensor学习"></a>all_gather_into_tensor学习</h5><p>注意到这里对于GPU使用到了<code>dist.all_gather_into_tensor</code>操作。这个操作的示意图如下，即将各个GPU上的分片按序收集给各个GPU上，使得每个GPU都有一个整体：</p><p><img src="/2025/07/02/pytorch-fsdp-1/image.png"></p><p><img src="/2025/07/02/pytorch-fsdp-1/image-1.png"></p><p>这里有一个示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.distributed <span class="hljs-keyword">as</span> dist<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>():<br>    dist.init_process_group(backend=<span class="hljs-string">&quot;nccl&quot;</span>)  <span class="hljs-comment"># or &quot;gloo&quot; for CPU</span><br>    rank = dist.get_rank()<br>    world_size = dist.get_world_size()<br><br>    <span class="hljs-comment"># 每个进程构造自己的 input tensor</span><br>    input_tensor = torch.ones(<span class="hljs-number">2</span>, device=<span class="hljs-string">&#x27;cuda&#x27;</span>) * (rank + <span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 所有数据拼接的输出 tensor</span><br>    output_tensor = torch.empty(<span class="hljs-number">2</span> * world_size, device=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br><br>    <span class="hljs-comment"># All-gather into output tensor</span><br>    dist.all_gather_into_tensor(output_tensor, input_tensor)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[rank <span class="hljs-subst">&#123;rank&#125;</span>] output_tensor: <span class="hljs-subst">&#123;output_tensor.cpu().tolist()&#125;</span>&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    torch.cuda.set_device(<span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;LOCAL_RANK&quot;</span>]))  <span class="hljs-comment"># torchrun 自动设置</span><br>    run()<br><br><span class="hljs-comment"># 2机，每机2卡的运行指令</span><br><span class="hljs-comment"># 机器1: torchrun   --nproc_per_node=2   --nnodes=2   --node_rank=0   --master_addr=fdbd:dc03:16:266::86   --master_port=12345   test.py</span><br><span class="hljs-comment"># 机器2: torchrun   --nproc_per_node=2   --nnodes=2   --node_rank=1   --master_addr=fdbd:dc03:16:266::86   --master_port=12345   test.py</span><br><br><span class="hljs-comment"># 运行结果</span><br><span class="hljs-comment"># [rank 0] output_tensor: [1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0]</span><br><span class="hljs-comment"># [rank 1] output_tensor: [1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0]</span><br><span class="hljs-comment"># [rank 2] output_tensor: [1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0]</span><br><span class="hljs-comment"># [rank 3] output_tensor: [1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0]</span><br><br></code></pre></td></tr></table></figure><h5 id="all-gather学习"><a href="#all-gather学习" class="headerlink" title="all_gather学习"></a>all_gather学习</h5><p>此外注意到这里这里对CPU使用了all_gather，它是 PyTorch 最经典的分布式通信原语之一。</p><p>它负责把每个进程上的 <code>tensor</code> 收集起来，按 rank 顺序填入 <code>tensor_list</code> 中。</p><ul><li><code>tensor_list[i]</code> 就是第 i 个进程的 tensor。</li></ul><p>一个简单的示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.distributed <span class="hljs-keyword">as</span> dist<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>():<br>    dist.init_process_group(backend=<span class="hljs-string">&quot;gloo&quot;</span>)  <span class="hljs-comment"># &quot;nccl&quot; for GPU</span><br>    rank = dist.get_rank()<br>    world_size = dist.get_world_size()<br><br>    <span class="hljs-comment"># 当前进程持有的 tensor，标记自己的 rank</span><br>    input_tensor = torch.full((<span class="hljs-number">2</span>,), rank, dtype=torch.<span class="hljs-built_in">int</span>)<br><br>    <span class="hljs-comment"># tensor_list 是一个 list，会存放所有进程的 tensor</span><br>    tensor_list = [torch.empty_like(input_tensor) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(world_size)]<br><br>    <span class="hljs-comment"># 执行 all_gather：每个进程收集所有人的 tensor</span><br>    dist.all_gather(tensor_list, input_tensor)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[rank <span class="hljs-subst">&#123;rank&#125;</span>] tensor_list = <span class="hljs-subst">&#123;[t.tolist() <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> tensor_list]&#125;</span>&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    run()<br><br><span class="hljs-comment"># 单机2卡的运行指令</span><br><span class="hljs-comment"># torchrun   --nproc_per_node=2   --nnodes=1   --node_rank=0   --master_addr=fdbd:dc03:16:266::86   --master_port=12346   test.py</span><br><br><span class="hljs-comment"># 运行结果</span><br><span class="hljs-comment"># [rank 0] tensor_list = [[0, 0], [1, 1]]</span><br><span class="hljs-comment"># [rank 1] tensor_list = [[0, 0], [1, 1]]</span><br></code></pre></td></tr></table></figure><h5 id="record-stream学习"><a href="#record-stream学习" class="headerlink" title="record_stream学习"></a>record_stream学习</h5><p>这里对于off_load模型，即将参数卸载到CPU上的操作，会使用 <code>record_stream(stream)</code> 注册 stream 依赖，从而告诉系统这个 tensor 来自 CPU，是通过 <code>stream X</code> 拷贝到 GPU 的，请不要在这个 stream 执行完之前把它删掉。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> self._offload_params:<br>    <span class="hljs-comment"># In case of offloading, `flat_param.data` (i.e. sharded param) is</span><br>    <span class="hljs-comment"># created on the pre-unshard stream. We need to hand it over to the</span><br>    <span class="hljs-comment"># unshard stream for all-gather</span><br>    _no_dispatch_record_stream(<br>        sharded_flat_param,<br>        self._device_handle.current_stream(),  <span class="hljs-comment"># unshard_stream</span><br>    )<br></code></pre></td></tr></table></figure><h3 id="register-post-backward-hook"><a href="#register-post-backward-hook" class="headerlink" title="_register_post_backward_hook"></a>_register_post_backward_hook</h3><p>主要用于在梯度计算完后对参数进行重新分片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_register_post_backward_hook</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: <span class="hljs-type">Optional</span>[FlatParamHandle],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    在 FlatParameter 的 AccumulateGrad 对象上注册一个后向钩子(post-backward hook)，</span><br><span class="hljs-string">    用于在梯度计算完成后执行梯度的 reduce-scatter 操作以及参数的重新分片(reshard)。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    AccumulateGrad 对象是完成 FlatParameter 梯度计算的最后一个函数，</span><br><span class="hljs-string">    因此钩子能确保在参数的整个梯度计算完成后才运行。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    我们只在 FlatParameter 参与的 *第一个* 前向传播中注册一次钩子。</span><br><span class="hljs-string">    这依赖于 AccumulateGrad 对象在多次前向传播中被保留的特性。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 如果不需要计算梯度，则无需注册后向钩子。</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.is_grad_enabled():<br>        <span class="hljs-keyword">return</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle:<br>        <span class="hljs-keyword">return</span><br>    flat_param = handle.flat_param<br><br>    <span class="hljs-comment"># 根据是否在 TorchDynamo 编译模式下，选择不同的钩子注册方式。</span><br>    <span class="hljs-keyword">if</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>        <span class="hljs-comment"># 检查钩子是否已注册，或者参数是否需要梯度。</span><br>        already_registered = <span class="hljs-built_in">hasattr</span>(flat_param, <span class="hljs-string">&quot;_post_backward_hook_handle&quot;</span>)<br>        <span class="hljs-keyword">if</span> already_registered <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> flat_param.requires_grad:<br>            <span class="hljs-keyword">return</span><br>        <span class="hljs-comment"># 使用 functools.partial 包装钩子函数，传入 FSDP 状态和句柄。</span><br>        hook = functools.partial(_post_backward_hook, state, handle)<br>        <span class="hljs-comment"># 使用为编译模式设计的专用 API 注册钩子。</span><br>        hook_handle = flat_param.register_post_accumulate_grad_hook(hook)<br>        <span class="hljs-comment"># 保存钩子句柄，用于状态检查和可能的卸载。</span><br>        flat_param._post_backward_hook_handle = hook_handle  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>    <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># Eager mode (常规执行模式)</span><br>        <span class="hljs-comment"># 检查钩子是否已注册。</span><br>        already_registered = <span class="hljs-built_in">hasattr</span>(flat_param, <span class="hljs-string">&quot;_post_backward_hook_state&quot;</span>)<br>        <span class="hljs-keyword">if</span> already_registered <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> flat_param.requires_grad:<br>            <span class="hljs-keyword">return</span><br>        <br>        <span class="hljs-comment"># --- 获取 AccumulateGrad 对象 --- #</span><br>        <span class="hljs-comment"># 创建一个临时的、与 flat_param 相同大小的张量，这会创建一个简单的计算图，</span><br>        <span class="hljs-comment"># 从而使我们能够访问其 grad_fn。</span><br>        temp_flat_param = flat_param.expand_as(flat_param)<br>        _p_assert(<br>            temp_flat_param.grad_fn <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>,<br>            <span class="hljs-string">&quot;需要 grad_fn 来访问 AccumulateGrad 对象并注册后向钩子&quot;</span>,<br>        )<br>        <span class="hljs-comment"># AccumulateGrad 对象是与参数直接关联的梯度累积函数。</span><br>        acc_grad = temp_flat_param.grad_fn.next_functions[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]  <span class="hljs-comment"># type: ignore[union-attr]</span><br>        <span class="hljs-keyword">assert</span> acc_grad <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># --- 注册钩子 --- #</span><br>        <span class="hljs-comment"># 在 AccumulateGrad 对象上注册钩子，确保在梯度累积完成后执行。</span><br>        hook_handle = acc_grad.register_hook(<br>            functools.partial(_post_backward_hook, state, handle)<br>        )<br>        <span class="hljs-comment"># 保存 AccumulateGrad 对象和钩子句柄，用于状态检查和后续管理。</span><br>        flat_param._post_backward_hook_state = (acc_grad, hook_handle)  <span class="hljs-comment"># type: ignore[attr-defined]</span><br><br></code></pre></td></tr></table></figure><h4 id="acc-grad-register-hook学习"><a href="#acc-grad-register-hook学习" class="headerlink" title="acc_grad.register_hook学习"></a>acc_grad.register_hook学习</h4><p>为了在 <strong>反向传播过程中准确地知道哪些参数梯度已经计算完成</strong>，它会在 <strong>每个参数的 <code>AccumulateGrad</code> 节点上注册钩子（hook）</strong>。</p><p>首先需要获取 <code>grad_fn</code>，构建 autograd 路径</p><ul><li><p><code>flat_param</code> 是一个 leaf tensor（即 <code>requires_grad=True</code> 且没有 <code>grad_fn</code>）；</p></li><li><p>使用 <code>expand_as()</code> 创建一个临时 view tensor，**这个 view 有 <code>grad_fn</code>**；</p></li><li><p>这个 <code>grad_fn</code> 会链接到 <strong><code>AccumulateGrad</code> 节点</strong>，而这个节点才允许注册 hook。</p></li></ul><p>需要找到 <code>AccumulateGrad</code></p><ul><li><p><code>grad_fn.next_functions</code> 是 PyTorch autograd 中的下游节点；</p></li><li><p><code>.next_functions[0][0]</code> 正好是与参数 <code>flat_param</code> 直接绑定的 <code>AccumulateGrad</code> 节点。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">  grad_fn (ExpandBackward0)<br>        ↓<br>  next_functions[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br>        ↓<br>AccumulateGrad ← hook 就挂在这里！<br></code></pre></td></tr></table></figure><p>然后我们需要注册钩子：</p><ul><li><p>使用 <code>functools.partial()</code> 固定住状态（<code>state</code>, <code>handle</code>）；</p></li><li><p>注册的 <code>_post_backward_hook</code> 会在梯度写入 <code>.grad</code> 前后触发；</p></li><li><p>这是 FSDP 判断“这个参数的梯度已经完成了，可以 reshard &#x2F; reduce &#x2F; offload”的触发点。</p></li></ul><p>然后记录下这个钩子对应的对象和句柄，方便：</p><ul><li><p>判断是否已注册（防止重复）；</p></li><li><p>后续移除 hook；</p></li><li><p>debug 或控制生命周期。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>x = torch.tensor([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>], requires_grad=<span class="hljs-literal">True</span>)<br>y = (x * <span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y=&quot;</span>,y)<br><br>temp_x = x.expand_as(x)<br>acc_grad = temp_x.grad_fn.next_functions[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">my_hook</span>(<span class="hljs-params">grad_input, grad_output</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;HOOK: grad_input=<span class="hljs-subst">&#123;grad_input&#125;</span>, grad_output=<span class="hljs-subst">&#123;grad_output&#125;</span>&quot;</span>)<br>    g = grad_output[<span class="hljs-number">0</span>]<br>    g = g * <span class="hljs-number">0.5</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Modified grad:&quot;</span>, g)<br>    <span class="hljs-comment"># 不要 return！Node hook 不能返回任何值</span><br><br>acc_grad.register_hook(my_hook)<br><br>y.backward()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x.grad=&quot;</span>,x.grad)<br><br><span class="hljs-comment"># 运行结果</span><br><span class="hljs-comment"># y= tensor(12., grad_fn=&lt;SumBackward0&gt;)</span><br><span class="hljs-comment"># HOOK: grad_input=(), grad_output=(tensor([2., 2., 2.]),)</span><br><span class="hljs-comment"># Modified grad: tensor([1., 1., 1.])</span><br><span class="hljs-comment"># x.grad= tensor([2., 2., 2.])</span><br></code></pre></td></tr></table></figure><h4 id="post-backward-hook"><a href="#post-backward-hook" class="headerlink" title="_post_backward_hook"></a>_post_backward_hook</h4><p>这是 FSDP 的核心反向传播钩子，负责在本地梯度计算完成后，进行跨 GPU 的梯度同步（reduce-scatter）和参数重新分片（reshard）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-meta">@torch.no_grad()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_backward_hook</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params">    flat_param,  <span class="hljs-comment"># Note: this is a positional argument passed by the hook</span></span><br><span class="hljs-params">    *unused: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    对 `handle` 的 `FlatParameter` 的梯度执行 Reduce-scatter 操作。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这是 FSDP 的核心反向传播钩子，负责在本地梯度计算完成后，</span><br><span class="hljs-string">    进行跨 GPU 的梯度同步（reduce-scatter）和参数重新分片（reshard）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    前置条件:</span><br><span class="hljs-string">    - `FlatParameter` 的 `.grad` 属性包含了本地批次（local batch）的完整（unsharded）梯度。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    后置条件:</span><br><span class="hljs-string">    - 如果使用 `NO_SHARD` 策略，`.grad` 属性将是经过 all-reduce 后的完整梯度。</span><br><span class="hljs-string">    - 否则，`_saved_grad_shard` 属性将是经过 reduce-scatter 后的分片梯度（会与已有的梯度累加）。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    _log_post_backward_hook(state, handle, logger)<br>    flat_param = handle.flat_param<br>    <span class="hljs-comment"># 标记该参数的后向钩子已被调用</span><br>    flat_param._post_backward_called = <span class="hljs-literal">True</span><br>    <span class="hljs-keyword">with</span> torch.autograd.profiler.record_function(<br>        <span class="hljs-string">&quot;FullyShardedDataParallel._post_backward_hook&quot;</span><br>    ):<br>        _assert_in_training_states(state, [TrainingState.FORWARD_BACKWARD])<br>        <span class="hljs-comment"># 当对共享相同 `FlatParameter` 的子模块多次使用可重入的激活检查点（AC）时，</span><br>        <span class="hljs-comment"># 后向钩子可能会在一次反向传播中运行多次。在这种情况下，我们允许句柄的状态</span><br>        <span class="hljs-comment"># 已经是 `BACKWARD_POST`。</span><br>        _p_assert(<br>            handle._training_state<br>            <span class="hljs-keyword">in</span> (HandleTrainingState.BACKWARD_PRE, HandleTrainingState.BACKWARD_POST),<br>            <span class="hljs-string">f&quot;Expects `BACKWARD_PRE` or `BACKWARD_POST` state but got <span class="hljs-subst">&#123;handle._training_state&#125;</span>&quot;</span>,<br>        )<br>        handle._training_state = HandleTrainingState.BACKWARD_POST<br><br>        <span class="hljs-comment"># 如果没有梯度，直接返回</span><br>        <span class="hljs-keyword">if</span> flat_param.grad <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span><br>        <span class="hljs-comment"># FSDP 不支持对梯度本身再求梯度</span><br>        <span class="hljs-keyword">if</span> flat_param.grad.requires_grad:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&quot;FSDP does not support gradients of gradients&quot;</span>)<br><br>        <span class="hljs-comment"># 关键步骤1：在进行梯度通信之前，先尝试重新分片参数，以尽早释放内存</span><br>        _post_backward_reshard(state, handle)<br>        <br>        <span class="hljs-comment"># 如果不进行梯度同步（例如在 `no_sync()` 上下文中），则直接返回</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> state._sync_gradients:<br>            <span class="hljs-keyword">if</span> handle._use_orig_params:<br>                <span class="hljs-comment"># 如果使用了原始（未合并的）参数，需要将梯度视图指向正确的 unsharded grad</span><br>                handle._use_unsharded_grad_views()<br>            <span class="hljs-keyword">return</span><br><br>        <span class="hljs-comment"># 关键步骤2：等待当前计算流中的所有操作（如梯度计算）完成，</span><br>        <span class="hljs-comment"># 然后再开始 reduce-scatter 梯度。这确保了我们拥有完整的本地梯度。</span><br>        <span class="hljs-comment"># TorchDynamo 编译模式下跳过此步。</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>            state._post_backward_stream.wait_stream(<br>                state._device_handle.current_stream()<br>            )<br><br>        <span class="hljs-comment"># 在专用的后向流中执行梯度通信</span><br>        <span class="hljs-keyword">with</span> state._device_handle.stream(state._post_backward_stream):<br>            autograd_computed_grad = flat_param.grad.data<br>            <span class="hljs-comment"># 如果开启了低精度训练，且梯度类型与通信类型不符，则进行类型转换以降低通信开销</span><br>            <span class="hljs-keyword">if</span> (<br>                <span class="hljs-keyword">not</span> _low_precision_hook_enabled(state)<br>                <span class="hljs-keyword">and</span> flat_param.grad.dtype != handle._reduce_dtype<br>                <span class="hljs-comment"># 如果强制全精度（例如在 eval 模式下），则不降低梯度精度</span><br>                <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> handle._force_full_precision<br>            ):<br>                flat_param.grad.data = flat_param.grad.to(handle._reduce_dtype)<br>            <br>            <span class="hljs-comment"># 根据分片策略执行梯度规约</span><br>            <span class="hljs-keyword">if</span> handle.uses_sharded_strategy:<br>                _reduce_grad(state, handle)  <span class="hljs-comment"># Reduce-scatter</span><br>            <span class="hljs-keyword">else</span>:<br>                _reduce_grad_no_shard(state, handle)  <span class="hljs-comment"># All-reduce</span><br>            <br>            <span class="hljs-comment"># 由于未分片的梯度是在计算流中产生的，但在后向流中消耗，</span><br>            <span class="hljs-comment"># 我们需要通知缓存分配器，以避免内存被过早回收。</span><br>            _no_dispatch_record_stream(<br>                autograd_computed_grad, state._post_backward_stream<br>            )<br></code></pre></td></tr></table></figure><h5 id="post-backward-reshard"><a href="#post-backward-reshard" class="headerlink" title="_post_backward_reshard"></a>_post_backward_reshard</h5><p>在梯度计算好后我们可以提前将之前unshard的参数进行reshard，从而释放内存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_backward_reshard</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params">    *unused: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    在反向传播后执行参数的重新分片（reshard）和预取（prefetch）操作。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这个函数是后向钩子（post-backward hook）的核心逻辑之一，负责在梯度计算和</span><br><span class="hljs-string">    聚合之后，管理参数的内存状态，并为下一次迭代（的第一个前向传播）做准备。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 1. 决定在反向传播后是否应该释放当前 handle 的未分片（unsharded）参数内存</span><br>    free_unsharded_flat_param = _should_free_in_backward(state, handle)<br>    <br>    <span class="hljs-comment"># 2. 执行重新分片操作。如果 `free_unsharded_flat_param` 为 True，则会释放内存</span><br>    _reshard(state, handle, free_unsharded_flat_param)<br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 当前的后向预取（Post-backward prefetching）不支持一个模块包含多个 handle 的情况，</span><br>    <span class="hljs-comment"># 因为后向钩子是按 handle 触发的，而不是按一组 handle 触发的。</span><br>    <span class="hljs-keyword">with</span> torch.profiler.record_function(<br>        <span class="hljs-string">&quot;FullyShardedDataParallel._post_backward_prefetch&quot;</span><br>    ):<br>        <span class="hljs-comment"># 3. 为下一次迭代预取参数。这里的模式是 BACKWARD，意味着这个预取是在</span><br>        <span class="hljs-comment">#    反向传播阶段触发的，目的是为下一次迭代的第一个前向传播做准备，</span><br>        <span class="hljs-comment">#    从而实现计算和通信的重叠。</span><br>        _prefetch_handle(state, handle, _PrefetchMode.BACKWARD)<br><br></code></pre></td></tr></table></figure><p>此外为了计算和通行的重叠，会为了下一次迭代提前开启unshard。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_prefetch_handle</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    current_handle: <span class="hljs-type">Optional</span>[FlatParamHandle],</span><br><span class="hljs-params">    prefetch_mode: _PrefetchMode,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    根据需要（异步地）预取下一个 handle 的参数。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这个函数是 FSDP 实现计算和通信重叠的关键。它会在当前 handle 计算的同时，</span><br><span class="hljs-string">    提前将下一个 handle 所需的参数从分片状态（sharded）通过 all-gather 恢复为</span><br><span class="hljs-string">    完整状态（unsharded）。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> current_handle:<br>        <span class="hljs-keyword">return</span><br>    <br>    <span class="hljs-comment"># 1. 根据当前 handle 和预取模式（前向或后向），确定下一个需要预取的 handle</span><br>    handle = _get_handle_to_prefetch(state, current_handle)<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle:<br>        <span class="hljs-keyword">return</span><br>    <br>    <span class="hljs-comment"># 2. 临时模拟训练状态，以确保 `_unshard` 能够正确工作。</span><br>    <span class="hljs-comment">#    例如，在 `_unshard` 内部调用的 `_use_unsharded_views()` 需要根据正确的训练状态</span><br>    <span class="hljs-comment">#    来设置参数视图。</span><br>    prev_training_state = handle._training_state<br>    <span class="hljs-keyword">if</span> prefetch_mode == _PrefetchMode.BACKWARD:<br>        <span class="hljs-comment"># 在后向钩子中预取，是为下一次前向传播做准备</span><br>        handle._training_state = HandleTrainingState.BACKWARD_PRE<br>    <span class="hljs-keyword">elif</span> prefetch_mode == _PrefetchMode.FORWARD:<br>        <span class="hljs-comment"># 在前向钩子中预取，是为下一次前向传播做准备</span><br>        handle._training_state = HandleTrainingState.FORWARD<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;Invalid prefetch mode on rank <span class="hljs-subst">&#123;state.rank&#125;</span>: <span class="hljs-subst">&#123;prefetch_mode&#125;</span>&quot;</span>)<br>    <br>    <span class="hljs-comment"># 3. 异步地执行 unshard (all-gather) 操作，但不同步等待操作完成。</span><br>    <span class="hljs-comment">#    这使得 all-gather 通信可以与当前流中的计算（例如，前向/后向计算）重叠。</span><br>    <span class="hljs-comment">#    同步操作（`wait()`）会被推迟到真正需要使用该参数之前执行。</span><br>    _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br>    <br>    <span class="hljs-comment"># 4. 恢复 handle 原始的训练状态</span><br>    handle._training_state = prev_training_state<br>    <span class="hljs-comment"># 5. 标记该 handle 的参数已经被预取</span><br>    handle._prefetched = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure><h3 id="register-post-backward-reshard-only-hook"><a href="#register-post-backward-reshard-only-hook" class="headerlink" title="_register_post_backward_reshard_only_hook"></a>_register_post_backward_reshard_only_hook</h3><p>对于那些不需要梯度计算的参数，注册一个梯度计算结束后进行重分片的勾子函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_register_post_backward_reshard_only_hook</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: <span class="hljs-type">Optional</span>[FlatParamHandle],</span><br><span class="hljs-params">    args: <span class="hljs-built_in">tuple</span>[<span class="hljs-type">Any</span>, ...],</span><br><span class="hljs-params">    kwargs: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    为那些不需要计算梯度(requires_grad=False)的扁平化参数(FlatParameter)注册一个</span><br><span class="hljs-string">    仅用于重新分片(reshard)的后向钩子。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    我们通过在模块的输入激活(input activations)上注册一个多重梯度钩子(multi-post-grad hook)</span><br><span class="hljs-string">    来做到这一点。这么做的原因是，对于 requires_grad=False 的参数，我们无法像之前一样</span><br><span class="hljs-string">    在其自身的 AccumulateGrad 对象上注册钩子（因为它不存在）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    通过在输入张量上挂钩，我们可以确保在所有可能依赖于该参数的梯度都计算完毕后，</span><br><span class="hljs-string">    才执行重新分片操作，从而安全地释放内存。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 如果当前上下文不计算梯度，则无需执行任何后向逻辑。</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.is_grad_enabled():<br>        <span class="hljs-keyword">return</span><br>    <br>    <span class="hljs-comment"># `inp_tensors` 会被懒加载，以避免在所有参数都计算梯度的常规情况下产生不必要的CPU开销。</span><br>    inp_tensors: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">list</span>[torch.Tensor]] = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle:<br>        <span class="hljs-keyword">return</span><br>    flat_param = handle.flat_param<br><br>    <span class="hljs-comment"># 检查钩子是否已经注册过。</span><br>    <span class="hljs-keyword">if</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>        already_registered = <span class="hljs-built_in">hasattr</span>(flat_param, <span class="hljs-string">&quot;_post_backward_hook_handle&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        already_registered = <span class="hljs-built_in">hasattr</span>(flat_param, <span class="hljs-string">&quot;_post_backward_hook_state&quot;</span>)<br><br>    <span class="hljs-comment"># 如果钩子已注册，或者参数需要梯度（此函数只处理不需要梯度的参数），则直接返回。</span><br>    <span class="hljs-keyword">if</span> already_registered <span class="hljs-keyword">or</span> flat_param.requires_grad:<br>        <span class="hljs-keyword">return</span><br>    <br>    <span class="hljs-comment"># --- 查找需要梯度的输入张量 --- #</span><br>    <span class="hljs-comment"># 这是此函数的关键逻辑：找到所有需要计算梯度的输入张量，并将钩子挂在它们上面。</span><br>    <span class="hljs-keyword">if</span> inp_tensors <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># 将所有输入参数扁平化为一个列表。</span><br>        args_flat = pytree.arg_tree_leaves(*args, **kwargs)<br>        <span class="hljs-comment"># 筛选出其中是张量(Tensor)且需要梯度(requires_grad=True)的对象。</span><br>        inp_tensors = [<br>            obj <span class="hljs-keyword">for</span> obj <span class="hljs-keyword">in</span> args_flat <span class="hljs-keyword">if</span> torch.is_tensor(obj) <span class="hljs-keyword">and</span> obj.requires_grad<br>        ]<br>    <span class="hljs-keyword">assert</span> inp_tensors <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>  <span class="hljs-comment"># mypy</span><br><br>    <span class="hljs-comment"># --- 注册多重梯度钩子 --- #</span><br>    <span class="hljs-comment"># `register_multi_grad_hook` 会注册一个钩子，该钩子只有在 `inp_tensors` 列表</span><br>    <span class="hljs-comment"># 中所有张量的梯度都计算完毕后才会触发。</span><br>    hook_handle = register_multi_grad_hook(<br>        inp_tensors, functools.partial(_post_backward_reshard_only_hook, state, handle)<br>    )<br><br>    <span class="hljs-comment"># 保存钩子句柄，以防止重复注册。</span><br>    <span class="hljs-keyword">if</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>        flat_param._post_backward_hook_handle = hook_handle  <span class="hljs-comment"># type: ignore[attr-defined, assignment]</span><br>    <span class="hljs-keyword">else</span>:<br>        flat_param._post_backward_hook_state = (hook_handle,)  <span class="hljs-comment"># type: ignore[attr-defined, assignment]</span><br><br></code></pre></td></tr></table></figure><h4 id="post-backward-reshard-only-hook"><a href="#post-backward-reshard-only-hook" class="headerlink" title="_post_backward_reshard_only_hook"></a>_post_backward_reshard_only_hook</h4><p>这里是注册的勾子函数，该函数会在梯度计算完成后计算，其作用是对于不需要梯度计算的参数，也在反向传播完成后将参数进行分片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_backward_reshard_only_hook</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params">    *unused: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    仅用于重新分片的后向钩子（post-backward hook）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这个钩子专门为那些不需要梯度（`requires_grad=False`）的参数服务。</span><br><span class="hljs-string">    它的主要作用是在反向传播完成后，安全地将完整的（unsharded）参数重新分片（reshard），</span><br><span class="hljs-string">    从而释放内存。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">with</span> torch.profiler.record_function(<br>        <span class="hljs-string">&quot;FullyShardedDataParallel._post_backward_hook_reshard_only&quot;</span><br>    ):<br>        <span class="hljs-comment"># 如果前向传播的输出不需要梯度，`_pre_backward_hook` 可能不会被执行。</span><br>        <span class="hljs-comment"># 因此，这里需要显式地更新状态，以确保后续的后向预取（post-backward prefetching）逻辑能够正确运行。</span><br>        state.training_state = TrainingState.FORWARD_BACKWARD<br>        handle._training_state = HandleTrainingState.BACKWARD_POST<br>        <span class="hljs-comment"># 调用核心的重新分片逻辑</span><br>        _post_backward_reshard(state, handle)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_backward_reshard</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params">    *unused: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    执行后向传播后的重新分片和预取操作。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这个函数是后向钩子的核心部分，负责在梯度计算和聚合之后管理参数内存和为下一次迭代做准备。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 决定在反向传播后是否应该释放未分片的扁平参数（flat_param）</span><br>    free_unsharded_flat_param = _should_free_in_backward(state, handle)<br>    <span class="hljs-comment"># 执行重新分片操作，根据上面的标志决定是否释放内存</span><br>    _reshard(state, handle, free_unsharded_flat_param)<br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 当前的后向预取不支持一个模块有多个 handle 的情况，</span><br>    <span class="hljs-comment"># 因为后向钩子是按 handle 触发的，而不是按 handle 组触发的。</span><br>    <span class="hljs-keyword">with</span> torch.profiler.record_function(<br>        <span class="hljs-string">&quot;FullyShardedDataParallel._post_backward_prefetch&quot;</span><br>    ):<br>        <span class="hljs-comment"># 为下一次迭代的（前向）传播预取下一个 handle 的参数</span><br>        _prefetch_handle(state, handle, _PrefetchMode.BACKWARD)<br><br><br><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_should_free_in_backward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    决定 FSDP 是否应该在后向钩子中释放未分片的扁平参数。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">        bool: 如果应该释放则返回 True，否则返回 False。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 如果未使用分片策略，则不释放</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle.uses_sharded_strategy:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>    <span class="hljs-comment"># 如果不进行梯度同步（例如，在使用 `no_sync()` 上下文时），</span><br>    <span class="hljs-comment"># 并且参数的分片策略是在前向传播后不重新分片（reshard），</span><br>    <span class="hljs-comment"># 那么我们选择不释放参数。这是一种启发式策略，</span><br>    <span class="hljs-comment"># 目的是用较高的内存占用换取更高的吞吐量（因为避免了额外的 all-gather 操作）。</span><br>    <span class="hljs-comment"># 否则，如果需要同步梯度，或者策略本身就需要重新分片，则释放参数以节省内存。</span><br>    <span class="hljs-keyword">return</span> (<br>        state._sync_gradients<br>        <span class="hljs-keyword">or</span> handle._sharding_strategy <span class="hljs-keyword">in</span> RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES<br>    )<br></code></pre></td></tr></table></figure><h2 id="post-forward"><a href="#post-forward" class="headerlink" title="_post_forward"></a>_post_forward</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: <span class="hljs-type">Optional</span>[FlatParamHandle],</span><br><span class="hljs-params">    reshard_fn: <span class="hljs-type">Callable</span>,</span><br><span class="hljs-params">    module: nn.Module,</span><br><span class="hljs-params">    <span class="hljs-built_in">input</span>: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params">    output: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Any</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    运行前向传播后的逻辑。这包括一个机会来重新分片（reshard）当前未分片的参数</span><br><span class="hljs-string">    （例如在当前前向传播中使用的参数），并在前向传播的输出上注册 pre-backward 钩子。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    - 这是 FSDP 前向传播钩子的核心实现，在每个 FSDP 包装的模块的 `forward` 方法之后执行。</span><br><span class="hljs-string">    - 主要负责在前向计算完成后，将不再需要的完整参数重新分片，以释放 GPU 内存。</span><br><span class="hljs-string">    - 同时，在输出张量上注册 pre-backward 钩子，以便在反向传播开始时，能够及时地将分片参数恢复为完整参数，用于梯度计算。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        state (_FSDPState): FSDP 的全局状态。</span><br><span class="hljs-string">        handle (Optional[FlatParamHandle]): 当前前向传播中使用的参数句柄。</span><br><span class="hljs-string">        reshard_fn (Callable): 一个可调用对象，用于重新分片当前未分片的参数。如果为 `None`，则不执行任何重新分片操作。</span><br><span class="hljs-string">        module (nn.Module): 刚刚执行完 `forward` 的模块。</span><br><span class="hljs-string">        input (Any): 模块的输入（未使用，仅为满足钩子签名要求）。</span><br><span class="hljs-string">        output (Any): 前向传播的输出。Pre-backward 钩子会注册在该输出中需要梯度的张量上。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    后置条件：</span><br><span class="hljs-string">    - 每个 `FlatParameter` 的 `data` 属性将指向分片后的扁平化参数，从而释放内存。</span><br><span class="hljs-string">    - 输出张量上已注册 pre-backward 钩子。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    主要逻辑：</span><br><span class="hljs-string">    1. **处理激活检查点（Activation Checkpointing）**：如果与 `fully_shard` 和 `checkpoint` 一起使用，在重新计算的前向传播中会跳过此后向钩子逻辑，因为参数状态由激活检查点管理。</span><br><span class="hljs-string">    2. **记录执行顺序**：记录当前 handle 的前向传播完成事件，用于后续的乱序执行优化。</span><br><span class="hljs-string">    3. **重新分片 (Resharding)**：如果提供了 `reshard_fn`，则调用它来执行参数的重新分片，将完整的参数转换回分片状态，释放内存。</span><br><span class="hljs-string">    4. **注册 Pre-Backward 钩子**：调用 `_register_pre_backward_hooks`，遍历 `output` 中的张量，为那些需要梯度的张量注册一个钩子。这个钩子将在反向传播到达该张量时触发，执行参数的 unshard 操作（all-gather）。</span><br><span class="hljs-string">    5. **更新状态**：将 FSDP 实例和 handle 的训练状态更新为 `IDLE`，表示前向传播阶段已完成。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">with</span> torch.profiler.record_function(<span class="hljs-string">&quot;FullyShardedDataParallel._post_forward&quot;</span>):<br>        <span class="hljs-comment"># 对于 `fully_shard` + `checkpoint`，在重新计算的前向传播中跳过 post-forward 逻辑</span><br>        <span class="hljs-keyword">if</span> handle <span class="hljs-keyword">and</span> handle._training_state == HandleTrainingState.BACKWARD_PRE:<br>            <span class="hljs-keyword">return</span> output<br><br>        state._exec_order_data.record_post_forward(handle)<br>        <span class="hljs-keyword">if</span> reshard_fn <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            reshard_fn(state, handle)<br>        <span class="hljs-comment"># 注册 pre-backward 钩子，以便为梯度计算（如果需要）unshard 扁平化参数</span><br>        output = _register_pre_backward_hooks(state, module, output, handle)<br>        state.training_state = TrainingState.IDLE<br>        <span class="hljs-keyword">if</span> handle:<br>            handle._training_state = HandleTrainingState.IDLE<br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure><h3 id="post-forward-reshard"><a href="#post-forward-reshard" class="headerlink" title="_post_forward_reshard"></a>_post_forward_reshard</h3><p>_post_forward中使用的reshard_fn就是_post_forward_reshard。</p><p>这是在前向传播后触发重新分片的入口函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_forward_reshard</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;在前向传播后重新分片参数。&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 功能：</span><br>    <span class="hljs-comment">#   - 作为前向传播钩子的一部分，决定是否以及如何重新分片（reshard）刚刚在前向计算中使用过的参数。</span><br>    <span class="hljs-comment">#   - 重新分片的目的是及时释放未分片（unsharded）参数占用的 GPU 内存。</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># 主要逻辑：</span><br>    <span class="hljs-comment"># 1. 检查 handle 是否存在，如果不存在则直接返回。</span><br>    <span class="hljs-comment"># 2. 决定是否要释放未分片的扁平化参数（`free_unsharded_flat_param`）。</span><br>    <span class="hljs-comment">#    - 通常情况下，参数在使用后会被立即释放以节省内存。</span><br>    <span class="hljs-comment">#    - 一个重要的例外是根（root）FSDP 模块。在 `FULL_SHARD` 策略下，根模块的参数在</span><br>    <span class="hljs-comment">#      前向传播后不会被立即释放，因为它们很可能马上就要用于反向传播的计算。</span><br>    <span class="hljs-comment">#      这是一种性能优化，避免了在前向后和反向前进行不必要的 `reshard` 和 `unshard` 操作。</span><br>    <span class="hljs-comment">#    - `RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES` 包含了需要这种行为的分片策略。</span><br>    <span class="hljs-comment"># 3. 调用 `_reshard` 函数，传入计算出的 `free_unsharded_flat_param` 标志，执行实际的重新分片操作。</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle:<br>        <span class="hljs-keyword">return</span><br>    <span class="hljs-comment"># 对于 `FULL_SHARD`，不要在 post-forward 中释放根模块的参数，</span><br>    <span class="hljs-comment"># 意图是它们能立即用于反向计算（尽管这可能不总是真的）</span><br>    free_unsharded_flat_param = (<br>        <span class="hljs-keyword">not</span> state._is_root<br>        <span class="hljs-keyword">and</span> handle._sharding_strategy <span class="hljs-keyword">in</span> RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES<br>    )<br>    _reshard(state, handle, free_unsharded_flat_param)<br></code></pre></td></tr></table></figure><p>这个函数调用参数句柄（handle）来执行实际的重新分片逻辑。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_reshard</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params">    free_unsharded_flat_param: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    重新分片句柄。`free_unsharded_flat_param` 指示是否释放</span><br><span class="hljs-string">    句柄的带填充的未分片扁平参数。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 功能：</span><br>    <span class="hljs-comment">#   - 协调参数句柄（handle）的重新分片过程。</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># 主要逻辑：</span><br>    <span class="hljs-comment"># 1. 调用 `handle.reshard()` 方法，将 `free_unsharded_flat_param` 标志传递下去，</span><br>    <span class="hljs-comment">#    由 handle 对象自己管理其内部状态和内存。</span><br>    <span class="hljs-comment"># 2. 如果 `limit_all_gathers` 选项被启用并且参数被释放，它会使用一个 CUDA 事件队列（`_free_event_queue`）</span><br>    <span class="hljs-comment">#    来确保在释放内存前，所有在当前流上的操作都已经完成，这是一种更精细的同步机制。</span><br>    <span class="hljs-comment"># 3. 调用 `handle.post_reshard()` 来执行任何 reshard 后的清理工作。</span><br>    <span class="hljs-comment"># 4. 将 `handle._prefetched` 标志设置为 `False`，表示参数现在是分片状态，</span><br>    <span class="hljs-comment">#    下次访问时需要通过 all-gather（即 unshard）来获取完整数据。</span><br>    handle.reshard(free_unsharded_flat_param)<br>    <span class="hljs-keyword">if</span> state.limit_all_gathers <span class="hljs-keyword">and</span> free_unsharded_flat_param:<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>            <span class="hljs-comment"># 在 torch compile 模式下，我们目前不为释放操作运行事件队列</span><br>            <span class="hljs-comment"># 但也许我们需要？TODO(voz): 研究一下</span><br>            free_event = state._device_handle.Event()<br>            free_event.record()<br>            state._free_event_queue.enqueue(free_event)<br>    handle.post_reshard()<br>    <span class="hljs-comment"># 无论扁平参数是否被释放，我们总是在下次访问时“unshard”参数</span><br>    <span class="hljs-comment"># 以获取其正确的形状。</span><br>    handle._prefetched = <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><p>执行重分片逻辑，需要先转为使用本地参数，再安全释放收集的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reshard</span>(<span class="hljs-params">self, free_unsharded_flat_param: <span class="hljs-built_in">bool</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    运行重新分片逻辑。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这包括如果 `free_unsharded_flat_param` 为真，则释放未分片的扁平参数，</span><br><span class="hljs-string">    并切换到使用分片的扁平参数。注意，这也隐式地将分片的扁平参数</span><br><span class="hljs-string">    卸载到 CPU（如果启用了 CPU offload），通过将其指向位于 CPU 上的 `_local_shard` 属性。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 功能：</span><br>    <span class="hljs-comment">#   - 在参数句柄（handle）级别上执行重新分片的核心操作。</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># 主要逻辑：</span><br>    <span class="hljs-comment"># 1. **切换指针**：首先调用 `_use_sharded_flat_param()`。这是一个关键步骤，它将 `FlatParameter`</span><br>    <span class="hljs-comment">#    的内部 `data` 指针重新指向分片后的张量（`_sharded_flat_param`）。</span><br>    <span class="hljs-comment">#    这样做可以防止在释放内存后发生“悬空指针”或“use-after-free”的 bug。</span><br>    <span class="hljs-comment"># 2. **释放内存**：如果 `free_unsharded_flat_param` 为 `True`，则调用 `_free_unsharded_flat_param()`</span><br>    <span class="hljs-comment">#    来释放之前未分片的、完整的参数所占用的内存。</span><br>    <span class="hljs-comment"># 在释放之前切换到分片的 `FlatParameter`，以防止外部性能分析工具出现“use-after-free”类型的 bug，</span><br>    <span class="hljs-comment"># 其中对于 `use_orig_params=True`，当在 `_use_sharded_views()` 中设置 `param.data = ...` 时，</span><br>    <span class="hljs-comment"># `param` 不会指向有效的内存。</span><br>    self._use_sharded_flat_param()<br>    <span class="hljs-keyword">if</span> free_unsharded_flat_param:<br>        self._free_unsharded_flat_param()<br></code></pre></td></tr></table></figure><p>主要作用是将 self.flat_param (一个 nn.Parameter) 的 .data 属性从指向完整的、未分片的张量，切换为指向本地的分片张量 (self.flat_param._local_shard)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_use_sharded_flat_param</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;切换到使用分片的扁平参数。&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 功能：</span><br>    <span class="hljs-comment">#   - 这是 reshard（重新分片）过程中的关键步骤。</span><br>    <span class="hljs-comment">#   - 主要作用是将 self.flat_param (一个 nn.Parameter) 的 .data 属性从指向完整的、</span><br>    <span class="hljs-comment">#     未分片的张量，切换为指向本地的分片张量 (self.flat_param._local_shard)。</span><br>    <span class="hljs-comment">#   - 这个切换是实现内存优化的核心：一旦 .data 指向了分片，之前完整张量所占用的</span><br>    <span class="hljs-comment">#     内存就可以被安全地释放。</span><br>    <span class="hljs-comment">#   - 如果 `use_orig_params` 为 True，此方法还负责更新原始模型参数，使其成为</span><br>    <span class="hljs-comment">#     分片张量的“视图”（view），并处理其梯度的视图。</span><br><br>    flat_param = self.flat_param<br>    <span class="hljs-keyword">if</span> self._use_orig_params:<br>        <span class="hljs-comment"># --- 特殊情况处理：决定是否跳过更新原始参数视图 --- #</span><br>        <span class="hljs-comment"># 在某些策略下（如 NO_SHARD），我们不在前向传播后立即重新分片。这是一种优化，</span><br>        <span class="hljs-comment"># 避免在前向和后向之间进行不必要的 unshard/reshard。</span><br>        <span class="hljs-comment"># `skip_use_sharded_views` 用于标识这种情况。</span><br>        in_forward = self._training_state == HandleTrainingState.FORWARD<br>        skip_use_sharded_views = (<br>            torch.is_grad_enabled()<br>            <span class="hljs-keyword">and</span> in_forward<br>            <span class="hljs-keyword">and</span> self._sharding_strategy<br>            <span class="hljs-keyword">in</span> NO_RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES<br>        )<br>        <span class="hljs-comment"># 如果需要跳过，提前保存未分片参数的引用</span><br>        <span class="hljs-keyword">if</span> skip_use_sharded_views:<br>            unsharded_flat_param = flat_param.data<br><br>    <span class="hljs-keyword">if</span> self._offload_params:<br>        <span class="hljs-comment"># --- CPU Offload 断言 --- #</span><br>        <span class="hljs-comment"># 如果启用了参数的 CPU 卸载，那么此时的本地分片理应在 CPU 上。</span><br>        device = flat_param._local_shard.device  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>        _p_assert(<br>            device == torch.device(<span class="hljs-string">&quot;cpu&quot;</span>),<br>            <span class="hljs-string">f&quot;期望本地分片在 CPU 上，但实际在 <span class="hljs-subst">&#123;device&#125;</span>&quot;</span>,<br>        )<br><br>    <span class="hljs-comment"># --- 核心操作：切换 .data 指针 --- #</span><br>    <span class="hljs-comment"># 这是此方法最核心的一行。它将 FlatParameter 的数据指针指向本地分片。</span><br>    <span class="hljs-comment"># 如果启用了 CPU Offload，_local_shard 就在 CPU 上，这个操作也完成了数据到 CPU 的“卸载”。</span><br>    flat_param.data = flat_param._local_shard  <span class="hljs-comment"># type: ignore[attr-defined]</span><br><br>    <span class="hljs-keyword">if</span> self._use_orig_params:<br>        <span class="hljs-comment"># --- 更新原始参数及其梯度视图 --- #</span><br>        <span class="hljs-keyword">if</span> skip_use_sharded_views:  <span class="hljs-comment"># type: ignore[possibly-undefined]</span><br>            <span class="hljs-comment"># 如果跳过了视图更新，只需保存未分片的参数引用即可。</span><br>            self._unsharded_flat_param_for_skipped_views = unsharded_flat_param  <span class="hljs-comment"># type: ignore[possibly-undefined]</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 否则，调用 _use_sharded_views()，将原始参数的 .data 更新为分片张量的视图。</span><br>            self._use_sharded_views()<br><br>        <span class="hljs-comment"># 在前向传播后的 reshard 中，我们可能尝试使用分片的梯度视图</span><br>        <span class="hljs-comment"># （或者，如果在 no_sync() 中累积了梯度，则使用未分片的梯度视图），</span><br>        <span class="hljs-comment"># 但在后向传播后的 reshard 中，我们将此调用推迟到 reduce-scatter 之后。</span><br>        <span class="hljs-keyword">if</span> (<br>            in_forward  <span class="hljs-comment"># type: ignore[possibly-undefined]</span><br>            <span class="hljs-comment"># 如果跳过了使用分片视图，则跳过使用梯度视图，</span><br>            <span class="hljs-comment"># 因为向用户暴露未分片的参数和分片的梯度可能会引起困惑</span><br>            <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self._skipped_use_sharded_views<br>        ):<br>            <span class="hljs-comment"># 检查在 no_sync() 上下文中是否累积了完整的梯度</span><br>            accumulated_grad_in_no_sync = (<br>                flat_param.grad <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">and</span> self.uses_sharded_strategy<br>                <span class="hljs-keyword">and</span> flat_param.grad.shape == flat_param._unpadded_unsharded_size<br>            )<br>            <span class="hljs-keyword">if</span> accumulated_grad_in_no_sync:<br>                <span class="hljs-comment"># 如果有完整的梯度，则原始参数的梯度视图也应指向这个完整的梯度。</span><br>                self._use_unsharded_grad_views()<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># 否则，梯度视图应指向分片后的梯度。</span><br>                self._use_sharded_grad_views()<br></code></pre></td></tr></table></figure><p>释放放带填充的未分片扁平参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_free_unsharded_flat_param</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    释放带填充的未分片扁平参数。我们允许在存储未分配时也调用此函数。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    要释放的张量取决于调用上下文，因为 unshard 可能强制使用了全精度，</span><br><span class="hljs-string">    在这种情况下，会使用一个不同的张量。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 功能：</span><br>    <span class="hljs-comment">#   - 定位到未分片的、完整的扁平化参数，并准备释放其内存。</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># 主要逻辑：</span><br>    <span class="hljs-comment"># 1. 获取正确的未分片参数张量 `unsharded_flat_param`。</span><br>    <span class="hljs-comment"># 2. 检查该张量是否在计算设备上（例如 GPU）。</span><br>    <span class="hljs-comment"># 3. **同步流**：调用 `_no_dispatch_record_stream()`，确保在释放张量内存之前，</span><br>    <span class="hljs-comment">#    当前 CUDA 流中所有使用该张量的操作都已完成。这是一个重要的同步步骤，</span><br>    <span class="hljs-comment">#    防止在 GPU 操作完成前就释放了其正在使用的内存。</span><br>    <span class="hljs-comment"># 4. 调用底层的 `_free_storage()` 工具函数来执行实际的内存释放。</span><br>    self._check_sharded_strategy()<br>    unsharded_flat_param = self._get_padded_unsharded_flat_param()<br>    self._check_on_compute_device(unsharded_flat_param)<br>    <span class="hljs-comment"># 在当前流中的所有操作完成之前，不要释放内存</span><br>    _no_dispatch_record_stream(<br>        unsharded_flat_param, self._device_handle.current_stream()<br>    )<br>    _free_storage(unsharded_flat_param)<br></code></pre></td></tr></table></figure><p>首先获取到之前带填充的、未分片的扁平参数的引用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_padded_unsharded_flat_param</span>(<span class="hljs-params">self</span>) -&gt; torch.Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    根据调用上下文，返回对带填充的、未分片的扁平参数的引用。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    - 此方法是获取用于 all-gather 操作的目标张量的核心逻辑。</span><br><span class="hljs-string">    - 它处理了混合精度训练中的一个重要情况：当需要强制使用全精度参数时，它会返回一个不同的、高精度的张量，并释放可能存在的旧的、低精度的张量，以确保数据一致性。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    主要逻辑：</span><br><span class="hljs-string">    1. **检查分片策略**：确保此方法仅在使用了分片策略（如 `FULL_SHARD` 或 `SHARD_GRAD_OP`）时被调用。</span><br><span class="hljs-string">    2. **处理强制全精度和混合精度**：</span><br><span class="hljs-string">       - 如果 `_force_full_precision`（例如，在 `summon_full_params` 中）和 `_uses_param_mixed_precision` 都为 `True`，则意味着我们需要一个全精度的参数副本进行操作。</span><br><span class="hljs-string">       - 在这种情况下，返回 `_full_prec_full_param_padded`，这是一个专门用于存储全精度参数的张量。</span><br><span class="hljs-string">       - **关键操作**：如果低精度的 `_full_param_padded` 张量仍然占用内存（意味着它可能来自上一次前向传播且未被释放），则必须将其释放。这是因为对全精度参数的修改会使这个低精度副本失效。释放后，下一次计算将强制执行新的 all-gather 来获取最新的数据，而不是使用过时的低精度缓存。</span><br><span class="hljs-string">    3. **标准情况**：</span><br><span class="hljs-string">       - 在其他所有情况下（例如，不强制全精度或不使用混合精度），直接返回标准的 `_full_param_padded` 张量，该张量将作为 all-gather 的目标。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 确认当前正在使用分片策略，因为此方法与获取未分片参数相关</span><br>    self._check_sharded_strategy()<br>    flat_param = self.flat_param<br>    <span class="hljs-comment"># 检查是否需要强制使用全精度参数，并且参数混合精度已启用</span><br>    <span class="hljs-keyword">if</span> self._force_full_precision <span class="hljs-keyword">and</span> self._uses_param_mixed_precision:<br>        <span class="hljs-comment"># 当启用参数混合精度时，我们使用一个不同的张量作为 all-gather 的目标，</span><br>        <span class="hljs-comment"># 以保持 `_full_param_padded` 始终是低精度这一不变性。</span><br>        unsharded_flat_param = flat_param._full_prec_full_param_padded  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>        <span class="hljs-comment"># 断言确保我们获取的确实是全精度张量，其类型不应与前向/后向传播中使用的低精度类型相同</span><br>        _p_assert(<br>            unsharded_flat_param.dtype != self._fwd_bwd_param_dtype,<br>            <span class="hljs-string">f&quot;期望全精度但得到了 <span class="hljs-subst">&#123;self._fwd_bwd_param_dtype&#125;</span>&quot;</span>,<br>        )<br>        <span class="hljs-comment"># 对于在 forward 后不重新分片的策略，`_full_param_padded` 可能仍被分配了内存。</span><br>        <span class="hljs-comment"># 由于我们在这里强制使用全精度，全精度副本可能会被修改，从而使现有的低精度副本失效。</span><br>        <span class="hljs-comment"># 因此，我们在这里释放它，以确保下一次前向/后向计算会进行新的 all-gather，以持久化修改。</span><br>        <span class="hljs-keyword">if</span> flat_param._full_param_padded.untyped_storage().size() &gt; <span class="hljs-number">0</span>:<br>            _free_storage(flat_param._full_param_padded)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># 在标准情况下，直接使用 `_full_param_padded` 作为未分片的参数</span><br>        unsharded_flat_param = flat_param._full_param_padded  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>    <span class="hljs-keyword">return</span> unsharded_flat_param<br></code></pre></td></tr></table></figure><p>这是一个通用的底层工具函数，通过将张量的存储大小调整为 0 来释放其内存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_free_storage</span>(<span class="hljs-params">tensor: torch.Tensor</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    释放 `tensor` 的底层存储。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">        bool: 如果方法释放了存储，则返回 `True`；如果存储已被释放，则返回 `False`。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 功能：</span><br>    <span class="hljs-comment">#   - 这是实际执行内存释放的最低级函数。</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># 主要逻辑：</span><br>    <span class="hljs-comment"># 1. 在 `torch.no_grad()` 上下文中操作，避免不必要的梯度跟踪。</span><br>    <span class="hljs-comment"># 2. 检查存储是否已经被释放（大小是否为 0）。</span><br>    <span class="hljs-comment"># 3. **安全检查**：断言（`_p_assert`）张量的 `storage_offset()` 为 0。这是一个重要的安全措施，</span><br>    <span class="hljs-comment">#    确保我们正在释放的张量是其底层存储的唯一所有者。如果一个存储被多个张量视图（view）共享，</span><br>    <span class="hljs-comment">#    释放它是不安全的。</span><br>    <span class="hljs-comment"># 4. **释放操作**：调用 `tensor._typed_storage()._resize_(0)`。这个内部方法会将张量的底层存储</span><br>    <span class="hljs-comment">#    大小调整为 0，从而有效地将内存返回给 PyTorch 的缓存分配器，使其可以被重用。</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>            already_freed = tensor._typed_storage()._size() == <span class="hljs-number">0</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> already_freed:<br>                _p_assert(<br>                    tensor.storage_offset() == <span class="hljs-number">0</span>,<br>                    <span class="hljs-string">&quot;当张量不是其存储的唯一占用者时，释放它的存储是不安全的\n&quot;</span><br>                    <span class="hljs-string">f&quot;storage offset: <span class="hljs-subst">&#123;tensor.storage_offset()&#125;</span>\n&quot;</span><br>                    <span class="hljs-string">f&quot;storage size: <span class="hljs-subst">&#123;tensor._typed_storage()._size()&#125;</span>\n&quot;</span><br>                    <span class="hljs-string">f&quot;tensor shape: <span class="hljs-subst">&#123;tensor.shape&#125;</span>&quot;</span>,<br>                )<br>                tensor._typed_storage()._resize_(<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><h3 id="register-pre-backward-hooks"><a href="#register-pre-backward-hooks" class="headerlink" title="_register_pre_backward_hooks"></a>_register_pre_backward_hooks</h3><p>主要是注册反向传播前置钩子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_register_pre_backward_hooks</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    module: nn.Module,</span><br><span class="hljs-params">    outputs: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    在 `outputs`（前向传播的输出）中需要梯度的张量上注册反向传播前置钩子（pre-backward hooks）。</span><br><span class="hljs-string">    这些输出是使用 `handle` 的 `FlatParameter` 计算得出的。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    - 这是 FSDP 实现自动、即时（just-in-time）参数 un-sharding 的核心机制。</span><br><span class="hljs-string">    - 通过在模块的输出张量上注册钩子，FSDP 可以在反向传播到达该模块之前，精确地触发相应参数的 all-gather 操作。</span><br><span class="hljs-string">    - 这样可以确保在计算梯度时，完整的、未分片的参数是可用的，同时在其他时间保持分片状态以节省内存。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    主要逻辑：</span><br><span class="hljs-string">    1. **检查梯度计算**：如果当前没有启用梯度计算（例如，在 `torch.no_grad()` 上下文中），则无需注册任何钩子，直接返回。</span><br><span class="hljs-string">    2. **重置状态**：</span><br><span class="hljs-string">       - 对于根模块，重置 `_post_backward_callback_queued` 标志，为新的反向传播做准备。</span><br><span class="hljs-string">       - 对于当前 `handle`，重置 `_needs_pre_backward_unshard` 和 `_ran_pre_backward_hook` 标志，以确保钩子逻辑的正确执行。</span><br><span class="hljs-string">    3. **定义钩子注册函数 `_register_hook`**：</span><br><span class="hljs-string">       - 此内部函数负责在单个张量上注册钩子。</span><br><span class="hljs-string">       - **条件**：仅当张量 `requires_grad` 时才注册，因为只有这些张量会参与反向传播。</span><br><span class="hljs-string">       - **注册**：使用 `t.register_hook()` 将 `_pre_backward_hook`（通过 `functools.partial` 包装）附加到张量上。</span><br><span class="hljs-string">       - **标记需求**：注册钩子后，将 `handle._needs_pre_backward_unshard` 设为 `True`，表明该 `handle` 对应的参数在反向传播中需要被 un-shard。</span><br><span class="hljs-string">    4. **递归应用钩子**：</span><br><span class="hljs-string">       - 使用 `_apply_to_tensors` 工具函数，将 `_register_hook` 应用于 `outputs` 中的所有张量。这可以处理复杂的输出结构（如元组、列表、字典等）。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 如果没有启用梯度计算（例如在 `torch.no_grad()` 中），则不需要反向传播逻辑</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.is_grad_enabled():<br>        <span class="hljs-keyword">return</span> outputs<br>    <span class="hljs-comment"># 如果是根 FSDP 实例，重置 post-backward 回调已排队的标志</span><br>    <span class="hljs-keyword">if</span> state._is_root:<br>        state._post_backward_callback_queued = <span class="hljs-literal">False</span>  <span class="hljs-comment"># 此标志仅在根节点上定义</span><br><br>    <span class="hljs-keyword">if</span> handle:<br>        <span class="hljs-comment"># 初始化标志，表示此 handle 尚不需要在反向传播前进行 un-shard</span><br>        handle._needs_pre_backward_unshard = <span class="hljs-literal">False</span><br>        <span class="hljs-comment"># 由于此 handle 的 FlatParameter 参与了前向传播，我们保守地假设</span><br>        <span class="hljs-comment"># 它将在反向传播中使用。重置此标志，用于跟踪 pre-backward 钩子是否已运行。</span><br>        handle._ran_pre_backward_hook = <span class="hljs-literal">False</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_register_hook</span>(<span class="hljs-params">t: torch.Tensor</span>) -&gt; torch.Tensor:<br>        <span class="hljs-comment"># 只在需要计算梯度的张量上注册钩子</span><br>        <span class="hljs-keyword">if</span> t.requires_grad:<br>            <span class="hljs-comment"># 注册一个不可序列化的钩子。`_pre_backward_hook` 将在反向传播到此张量时被调用。</span><br>            t.register_hook(<br>                torch.utils.hooks.unserializable_hook(<br>                    functools.partial(_pre_backward_hook, state, module, handle)<br>                )<br>            )<br>            <span class="hljs-comment"># 如果注册了钩子，说明这个 handle 对应的参数将需要 un-shard</span><br>            <span class="hljs-keyword">if</span> handle:<br>                handle._needs_pre_backward_unshard = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">return</span> t<br><br>    <span class="hljs-comment"># 递归地将 _register_hook 函数应用于 `outputs` 中的所有张量</span><br>    <span class="hljs-keyword">return</span> _apply_to_tensors(_register_hook, outputs)<br></code></pre></td></tr></table></figure><p>具体注册的勾子函数为_pre_backward_hook，该函数主要是执行_unshard来通过 all-gather 操作获取完整的参数，并且为了重叠计算和通信，它会立即触发下一个（在反向传播顺序中）模块参数的 prefetching（预取）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_pre_backward_hook</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    module: nn.Module,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params">    grad,</span><br><span class="hljs-params">    *unused: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Any</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    为梯度计算准备 `handle` 的 `FlatParameter`。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    - 这是 FSDP 的核心反向传播钩子，由 `_register_pre_backward_hooks` 注册。</span><br><span class="hljs-string">    - 当反向传播的梯度流到达一个模块的输出张量时，这个钩子被触发。</span><br><span class="hljs-string">    - 它的主要职责是：</span><br><span class="hljs-string">        1. **Un-shard 参数**：执行 all-gather 操作，将当前模块所需的 `FlatParameter` 从分片状态恢复为完整的、未分片的张量，以便进行梯度计算。</span><br><span class="hljs-string">        2. **预取下一个参数**：为了重叠计算和通信，它会立即触发下一个（在反向传播顺序中）模块参数的 prefetching（预取）。</span><br><span class="hljs-string">        3. **状态管理**：管理 FSDP 的内部状态，例如标记钩子已运行，以及为根模块注册最终的 post-backward 回调。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    主要逻辑：</span><br><span class="hljs-string">    1. **钩子执行保护**：检查 `_ran_pre_backward_hook` 标志，确保对于同一次前向计算涉及的同一组参数，此钩子只执行一次。</span><br><span class="hljs-string">    2. **根模块初始化**：如果是根 FSDP 模块，并且是反向传播的第一次调用，它会注册一个 `_post_backward_final_callback`。这个回调将在整个反向传播结束后执行，用于最终的清理工作（如梯度 reshard）。</span><br><span class="hljs-string">    3. **状态转换**：将 FSDP 状态机切换到 `FORWARD_BACKWARD` 和 `BACKWARD_PRE`，用于调试和断言。</span><br><span class="hljs-string">    4. **参数 Un-shard**：</span><br><span class="hljs-string">       - 检查 `_needs_pre_backward_unshard` 标志。</span><br><span class="hljs-string">       - 如果需要 un-shard 且参数尚未被预取（`_prefetched` 为 False），则调用 `_unshard` 执行 all-gather。</span><br><span class="hljs-string">       - 使用 `wait_stream` 确保计算流等待 un-shard 操作完成。</span><br><span class="hljs-string">    5. **反向预取（Backward Prefetch）**：</span><br><span class="hljs-string">       - 调用 `_prefetch_handle` 并传入 `_PrefetchMode.BACKWARD`，以启动下一个句柄的参数 un-sharding。这是 FSDP 的关键性能优化。</span><br><span class="hljs-string">    6. **梯度准备**：调用 `handle.prepare_gradient_for_backward()`，为即将到来的梯度计算做准备。</span><br><span class="hljs-string">    7. **标记完成**：设置 `_ran_pre_backward_hook = True`。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 对于同一次模块前向计算中涉及的同一组句柄，只运行一次 pre-backward 钩子</span><br>    <span class="hljs-keyword">if</span> (<br>        handle<br>        <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(handle, <span class="hljs-string">&quot;_ran_pre_backward_hook&quot;</span>)<br>        <span class="hljs-keyword">and</span> handle._ran_pre_backward_hook<br>    ):<br>        <span class="hljs-keyword">return</span> grad<br><br>    <span class="hljs-keyword">with</span> torch.profiler.record_function(<span class="hljs-string">&quot;FullyShardedDataParallel._pre_backward_hook&quot;</span>):<br>        <span class="hljs-comment"># 为根 FSDP 实例排队一次 post-backward 回调，将其附加到最外层的反向图任务上，</span><br>        <span class="hljs-comment"># 以便在所有反向调用完成后调用它。</span><br>        <span class="hljs-keyword">if</span> state._is_root <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> state._post_backward_callback_queued:<br>            _register_post_backward_final_callback(state, module)<br>            _reset_flat_param_grad_info_if_needed(state._all_handles)<br>        <span class="hljs-keyword">elif</span> handle:<br>            <span class="hljs-comment"># 断言 FSDP 模块处于正确的训练状态</span><br>            allowed_states = [TrainingState.IDLE]<br>            <span class="hljs-keyword">if</span> _is_composable(state):<br>                allowed_states.append(TrainingState.FORWARD_BACKWARD)<br>            _assert_in_training_states(state, allowed_states)<br>        <span class="hljs-comment"># 更新训练状态为正在进行反向传播</span><br>        state.training_state = TrainingState.FORWARD_BACKWARD<br>        <span class="hljs-comment"># 排队 post-backward 回调是 pre-backward 钩子中唯一不是按句柄处理的逻辑，</span><br>        <span class="hljs-comment"># 因此如果没有句柄，我们可以在这里提前返回。</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle:<br>            <span class="hljs-keyword">return</span> grad<br>        <span class="hljs-comment"># 更新句柄的训练状态为反向传播前</span><br>        handle._training_state = HandleTrainingState.BACKWARD_PRE<br><br>        <span class="hljs-keyword">if</span> handle._needs_pre_backward_unshard:<br>            <span class="hljs-comment"># 如果句柄已经被预取，则无需再次调用 `_unshard()`</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle._prefetched:<br>                _unshard(<br>                    state,<br>                    handle,<br>                    state._unshard_stream,      <span class="hljs-comment"># 用于 unshard 的 CUDA 流</span><br>                    state._pre_unshard_stream,  <span class="hljs-comment"># 用于 pre-unshard 的 CUDA 流</span><br>                )<br>            <span class="hljs-comment"># 在 tracing 期间不要等待，以避免图中断</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>                <span class="hljs-comment"># 确保计算流等待 unshard 操作完成</span><br>                state._device_handle.current_stream().wait_stream(state._unshard_stream)<br><br>        <span class="hljs-comment"># 将此标志设置为 `False`，以确保目标错误的预取不会实际 unshard 这些句柄</span><br>        handle._needs_pre_backward_unshard = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">with</span> torch.profiler.record_function(<br>            <span class="hljs-string">&quot;FullyShardedDataParallel._pre_backward_prefetch&quot;</span><br>        ):<br>            <span class="hljs-comment"># 预取下一个在反向传播中需要的句柄的参数</span><br>            _prefetch_handle(state, handle, _PrefetchMode.BACKWARD)<br>        <span class="hljs-comment"># 为反向传播准备梯度</span><br>        handle.prepare_gradient_for_backward()<br>        <span class="hljs-comment"># 标记此句柄的 pre-backward 钩子已运行</span><br>        handle._ran_pre_backward_hook = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">return</span> grad<br></code></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="勾子函数及运行流程"><a href="#勾子函数及运行流程" class="headerlink" title="勾子函数及运行流程"></a>勾子函数及运行流程</h2><p><img src="/2025/07/02/pytorch-fsdp-1/diagram.png"></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><p><a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/485208899">https://zhuanlan.zhihu.com/p/485208899</a></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>FSDP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Picotron-Tutorial】上下文并行</title>
    <link href="/2025/06/28/Picotron-Tutorial%20Context%20parallel/"/>
    <url>/2025/06/28/Picotron-Tutorial%20Context%20parallel/</url>
    
    <content type="html"><![CDATA[<h1 id="理论分析"><a href="#理论分析" class="headerlink" title="理论分析"></a>理论分析</h1><p><strong>上下文并行</strong>的<strong>核心思想</strong>是将<strong>序列并行的方法</strong>（也就是沿序列长度进行拆分）的思路<strong>应用到已经采用张量并行的模块上</strong>。</p><p>对于上下文并行，就像序列并行一样，我们将<strong>沿序列维度拆分输入</strong>，但这次我们对整个模型进行拆分，而不仅仅是对之前Tensor+Sequence并行中涉及的部分模型。</p><p>拆分序列是横向的切割，所以不会影响大多数模块，如MLP和LayerNorm，因为它们对每个token的处理是独立的。</p><p>在计算梯度后，会启动一次all-reduce操作以在上下文并行组内同步梯度。</p><p>不过注意力模块需要特别注意，在注意力模块中，每个token需要访问来自<strong>所有</strong>其他序列token的键&#x2F;值对，或者在因果注意力的情况下，至少需要关注每个前面的token。</p><p>由于上下文并行是沿序列维度将输入分布到各个GPU上，注意力模块将需要各个GPU之间进行充分通信，以交换必要的键&#x2F;值数据。为了降低通信的影响衍生了各种同步方式。</p><h3 id="环形注意力（Ring-Attention）"><a href="#环形注意力（Ring-Attention）" class="headerlink" title="环形注意力（Ring Attention）"></a>环形注意力（Ring Attention）</h3><p>假设我们有4个GPU和4个token的输入。最初，输入序列沿序列维度均匀拆分，因此每个GPU仅拥有一个token及其对应的Q&#x2F;K&#x2F;V值。</p><p>假设Q1、K1和V1分别表示第一个token的查询、键和值，并且它们位于第1个GPU上。</p><p>注意力计算需要4个时间步来完成。在每个时间步中，每个GPU依次执行以下三个操作：</p><ol><li><p>以非阻塞的方式将“当前的键和值”发送给下一台机器（在非阻塞模式下的最后一个时间步除外），以便在此步骤尚未完成时即可开始下一步骤</p></li><li><p>在本地对已拥有的“当前键和值”计算注意力得分，这通常涉及执行  $$\frac{Softmax(QK^T)}{\sqrt[]{d}}∗V$$</p></li><li><p>等待接收来自上一台GPU的键和值，然后返回到步骤1，此时“当前的键和值”即为刚刚从上一台GPU接收到的键&#x2F;值对。</p></li></ol><p><img src="/2025/06/28/Picotron-Tutorial%20Context%20parallel/image-3.png"></p><p>不过有一个大问题，那就是环形注意力的简单实现会导致因果注意力矩阵形状造成的GPU间工作不平衡。让我们通过考虑带有因果注意力掩码的注意力得分矩阵来观察Softmax计算：</p><p><img src="/2025/06/28/Picotron-Tutorial%20Context%20parallel/image-2.png"></p><p>Softmax是按行计算的，这意味着每当一个GPU收到一整行的所有token时，就可以进行计算。我们看到GPU1可以立即计算，因为它一开始就拥有token 1-4，而GPU1实际上不需要从其他GPU接收任何信息。然而，GPU2需要等待第二轮，才能收到token 1-4，从而获得token 1-8的所有值。同时，GPU1的工作量明显比其他GPU要少得多。</p><h3 id="之字形环形注意力（Zig-Zag-Ring-Attention）-–-平衡计算的实现"><a href="#之字形环形注意力（Zig-Zag-Ring-Attention）-–-平衡计算的实现" class="headerlink" title="之字形环形注意力（Zig-Zag Ring Attention） – 平衡计算的实现"></a>之字形环形注意力（Zig-Zag Ring Attention） – 平衡计算的实现</h3><p>为了更好地平衡计算负载，我们需要一种更好的方式来分配输入序列。</p><p>这可以通过不将token纯粹顺序地分配给各个GPU，而是稍微混合一下顺序，从而使每个GPU上都有较早和较晚的token。这种方法被称为之字形注意力，在这种新排列中，注意力掩码将显示出计算分布较为均匀。</p><p><img src="/2025/06/28/Picotron-Tutorial%20Context%20parallel/image-1.png"></p><p>与此同时，我们也会看到，为了完成所有行的计算，每个GPU都需要来自其他所有GPU的信息。</p><p>我们有两种常见方式来重叠计算和通信：一种是通过执行一次通用的all-gather操作，同时在每个GPU上重新组合所有KV（类似于Zero-3的方式）；另一种是根据需要从每个GPU逐个收集KV对：</p><p><img src="/2025/06/28/Picotron-Tutorial%20Context%20parallel/image.png"></p><p>这两种实现方式的关键区别在于它们的通信模式和内存使用：</p><ol><li><strong>AllGather实现：</strong></li></ol><ul><li><p>所有GPU同时收集来自其他所有GPU的完整键&#x2F;值对</p></li><li><p>需要更多的临时内存，因为每个GPU需要一次性存储完整的KV对</p></li><li><p>通信在一步内完成，但伴随较大的内存开销</p></li></ul><ul><li><strong>全对全（环形）实现：</strong></li></ul><ul><li><p>GPU以环形模式交换KV对，每次传输一个数据块</p></li><li><p>更节省内存，因为每个GPU只需临时存储一个数据块</p></li><li><p>通信被分散并与计算重叠，尽管由于多次通信步骤会带来一些额外的基础延迟</p></li></ul><p>全对全方法通常在内存效率上更优，但其通信模式稍显复杂；而AllGather方法则更简单，但在注意力计算过程中需要更多的临时内存。</p><h1 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h1><h2 id="ContextCommunicate"><a href="#ContextCommunicate" class="headerlink" title="ContextCommunicate"></a>ContextCommunicate</h2><ul><li><p>初始化时有点对点的send_rank和recv_rank。</p></li><li><p>对于send_recv函数，会创建一个异步的send_operation和recv_operation。然后会将这两个添加到_pending_operations中。</p></li><li><p>对于commit函数会批量提交所有_pending_operations中待处理的操作。</p></li><li><p>对于wait函数，它会等待所有已提交的任务都完成。</p></li></ul><h2 id="RingAttentionFunc"><a href="#RingAttentionFunc" class="headerlink" title="RingAttentionFunc"></a>RingAttentionFunc</h2><p>这里主要实现的是环形注意力机制。</p><ul><li><p>如果有Context parrallel那么就会通过apply_context_parallel设置环境变量”CONTEXT_PARALLEL”，然后根据环境变量在attention计算的时候如果是Context parrallel就会执行ring_attention(q, k, v, sm_scale, is_causal)函数。</p></li><li><p>对于forward，它会遍历所有的world_size来依次进行处理：</p><ul><li><p>如果不是最后一次，那么就对k、v执行send_recv，得到next_k、next_v，然后commit。</p></li><li><p>如果step &lt;&#x3D; comm.rank，那就说明当前的数据是在attention计算中是需要的。那么就调用ring_attention_forward来执行部分序列数据的标准的attention的计算</p></li><li><p>然后调用update_out_and_lse来执行对这种部分序列的attention的累计更新</p></li><li><p>如果不是最后一次，那么就将k、v替换为刚通过网络传输得到的next_k和next_v</p></li></ul></li><li><p>对于backward，首先会创建两个通信组ContextCommunicate，主要用于 K, V 环形通信的通信对象和 K, V 梯度的环形通信的通信对象，同样也是遍历world_size次来依次处理</p><ul><li><p>如果不是最后一次，那么就对k、v执行send_recv，得到next_k、next_v，然后commit。</p></li><li><p>如果当前step&lt;&#x3D;kv_comm.rank，那么就需要通过ring_attention_backward计算梯度，这里实际上是在手动计算反向传播出来的梯度。</p></li><li><p>得到梯度dq，还需要等待d_kv_comm，得到dk、dv</p></li><li><p>更新k、v为next_k、next_v</p></li><li><p>将dq、dv通过send_recv发送给下一个并进行接收</p></li></ul></li></ul><h2 id="update-rope-for-context-parallel"><a href="#update-rope-for-context-parallel" class="headerlink" title="update_rope_for_context_parallel"></a>update_rope_for_context_parallel</h2><p>由于现在每个GPU只有一部分的序列，所以在计算位置编码的时候不能依赖原始的位置，而是要加上当前rank之前的的。</p>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>Picotron-Tutorial</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Picotron-Tutorial】流水线并行</title>
    <link href="/2025/06/24/Picotron-Tutorial%20pipeline%20parallel/"/>
    <url>/2025/06/24/Picotron-Tutorial%20pipeline%20parallel/</url>
    
    <content type="html"><![CDATA[<h1 id="Afab并行"><a href="#Afab并行" class="headerlink" title="Afab并行"></a>Afab并行</h1><h2 id="理论分析"><a href="#理论分析" class="headerlink" title="理论分析"></a>理论分析</h2><p>最简单的pipeline并行就是将模型划分为好几层，然后分别放置在不同的GPU上依次进行前向传播和后先传播，如下图所示。但是这带来的最大的问题是效率过低，存在很多空闲时刻。</p><p><img src="/2025/06/24/Picotron-Tutorial%20pipeline%20parallel/image-2.png"></p><blockquote><p><em>一个16层模型的流水线并行示例，该模型分布在4块GPU上。数字表示层编号。</em></p></blockquote><p>假设$t_f$ 和$t_b$ 分别是单个微批次在流水线的一个阶段上进行前向传播和反向传播所需的时间（通常假设 $t_b\approx2\times t_f$，这在上图中可以观察到）。如果我们能够完美并行化，理想总时间应为 $t &#x3D;t_b+t_f$。但由于流水线气泡的存在，额外的时间为$t_p &#x3D;(p-1)*(t_b+t_f)$其中$p$ 是流水线并行度，即上图中的GPU数量），即每个GPU在其他GPU计算时的等待时间。</p><p>因此我们可以计算额外气泡时间与理想时间的比值：</p><p>$$r_{bubble}&#x3D;\frac{(p-1)*(t_f+t_b)}{t_f+t_b}&#x3D;p-1$$</p><p>当我们增加流水线阶段数时，气泡时间随之增加，GPU利用率下降。可以看出，在一个简单的实现中，流水线气泡可能会非常大！</p><p>为此需要提出一些优化方法来减少流水线中的气泡。一个经典的方法就是<strong>全前向-全反向（AFAB, All-Forward-All-Backward）</strong>&#x8C03;度。其整体思路是与微批次（microbatches）相绑定的。在微批次中，我们需要先对微批次中的所有的样本进行前向传播和反向传播，得到梯度后对各样本的梯度进行平均，然后通过优化器更新参数。</p><p>其优势在于前向和反向传播仍然是严格顺序的，因此可以保持模型训练代码的整体组织，使这种流水线并行实现方式成为最容易实现的一种。</p><p><img src="/2025/06/24/Picotron-Tutorial%20pipeline%20parallel/image.png"></p><blockquote><p>在之前的图表中，数字代表的是模型的层数，而从这一张图开始，所有流水线并行相关的图表中的数字都表示微批次。你可以将每个方块理解为包含多个层，就像前一张图所示的那样。</p></blockquote><p>在如此设计下，理想情况下处理m个批次所需要的时间为 $t_{id}&#x3D;m \times (t_f + t_b)$，实际过程中产生的气泡依旧为$(p-1)*(t_b+t_f)$，所以气泡比例为：</p><p>$$r_{bubble}&#x3D;\frac{(p-1)*(t_f+t_b)}{m\times ({t_f+t_b})}&#x3D;p-1$$</p><p>可以看到我们可以通过增加微批次的方法来减少气泡。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="1-pipeline-communicate"><a href="#1-pipeline-communicate" class="headerlink" title="1. pipeline_communicate"></a>1. pipeline_communicate</h3><p>实现了流水线并行中各个阶段之间点对点的发送和接收张量（激活值或梯度）的通用接口。</p><ul><li><p>其需要输入一个operation参数来表示操作的类型，包括有：</p><ul><li><p>recv_forward：接受前向传播的数据，如果是流水线第一个阶段，就不需要接受，直接返回none。</p></li><li><p>send_forward：发送前向传播的数据，如果是流水线最后一个阶段，就不需要发送，直接返回none。</p></li><li><p>recv_backward：</p></li><li><p>send_backward</p></li></ul></li><li><p>通过operation可以得到目标操作对象，然后就创建异步的点对点通信</p></li><li><p>等待通信操作完成</p></li><li><p>如果是recv操作，就需要返回接收到的数据</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ... existing code ...</span><br>STEP, VERBOSE = <span class="hljs-number">0</span>, os.environ.get(<span class="hljs-string">&quot;VERBOSE&quot;</span>, <span class="hljs-string">&quot;0&quot;</span>) == <span class="hljs-string">&quot;1&quot;</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pipeline_communicate</span>(<span class="hljs-params">operation, device, dtype, tensor=<span class="hljs-literal">None</span>, shapes=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    处理流水线阶段之间用于前向和反向传播的点对点通信。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        operation (str): 通信操作的类型 (&#x27;recv_forward&#x27;, &#x27;send_forward&#x27;, </span><br><span class="hljs-string">                        &#x27;recv_backward&#x27;, &#x27;send_backward&#x27;)</span><br><span class="hljs-string">        device: 张量操作的目标设备 (例如, CPU, GPU)</span><br><span class="hljs-string">        dtype: 张量的数据类型</span><br><span class="hljs-string">        tensor: 用于发送操作的输入张量 (默认: None)</span><br><span class="hljs-string">        shapes: 用于接收张量的形状规格 (默认: None)</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        torch.Tensor or None: 接收操作返回接收到的张量，发送操作返回 None</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">global</span> STEP <span class="hljs-comment"># 用于调试，追踪通信步骤</span><br>    <span class="hljs-keyword">global</span> VERBOSE <span class="hljs-comment"># 控制是否打印详细日志</span><br>    <br>    <span class="hljs-comment"># --- 根据操作类型确定源/目标和行为 ---</span><br>    <span class="hljs-keyword">if</span> operation == <span class="hljs-string">&#x27;recv_forward&#x27;</span>:<br>        <span class="hljs-comment"># 如果是第一个流水线阶段，则在前向传播中无需接收</span><br>        <span class="hljs-keyword">if</span> pgm.process_group_manager.pp_is_first_stage: <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>        <span class="hljs-comment"># 创建一个空的张量用于接收数据，requires_grad=True 因为它将是后续计算图的一部分</span><br>        tensor = torch.empty(shapes, requires_grad=<span class="hljs-literal">True</span>, device=device, dtype=dtype)<br>        src = pgm.process_group_manager.pp_prev_rank <span class="hljs-comment"># 从前一个流水线阶段接收</span><br>    <br>    <span class="hljs-keyword">elif</span> operation == <span class="hljs-string">&#x27;send_forward&#x27;</span>:<br>        <span class="hljs-comment"># 如果是最后一个流水线阶段，则在前向传播中无需发送</span><br>        <span class="hljs-keyword">if</span> pgm.process_group_manager.pp_is_last_stage: <span class="hljs-keyword">return</span><br>        dest = pgm.process_group_manager.pp_next_rank <span class="hljs-comment"># 发送到下一个流水线阶段</span><br>    <br>    <span class="hljs-keyword">elif</span> operation == <span class="hljs-string">&#x27;recv_backward&#x27;</span>:<br>        <span class="hljs-comment"># 如果是最后一个流水线阶段，则在反向传播中无需接收梯度</span><br>        <span class="hljs-keyword">if</span> pgm.process_group_manager.pp_is_last_stage: <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>        tensor = torch.empty(shapes, requires_grad=<span class="hljs-literal">True</span>, device=device, dtype=dtype) <span class="hljs-comment"># 接收的梯度也需要梯度</span><br>        src = pgm.process_group_manager.pp_next_rank <span class="hljs-comment"># 从下一个流水线阶段接收梯度</span><br>    <br>    <span class="hljs-keyword">elif</span> operation == <span class="hljs-string">&#x27;send_backward&#x27;</span>:<br>        <span class="hljs-comment"># 如果是第一个流水线阶段，则在反向传播中无需发送梯度</span><br>        <span class="hljs-keyword">if</span> pgm.process_group_manager.pp_is_first_stage: <span class="hljs-keyword">return</span><br>        dest = pgm.process_group_manager.pp_prev_rank <span class="hljs-comment"># 将梯度发送到前一个流水线阶段</span><br><br>    <span class="hljs-comment"># --- 执行通信 ---</span><br>    is_send = operation.startswith(<span class="hljs-string">&#x27;send&#x27;</span>)<br>    peer_rank = dest <span class="hljs-keyword">if</span> is_send <span class="hljs-keyword">else</span> src <span class="hljs-comment"># 确定通信对方的 rank</span><br>    <br>    <span class="hljs-comment"># 创建点对点操作 (P2POp) 对象，使用异步发送 (isend) 或接收 (irecv)</span><br>    op = dist.P2POp(dist.isend <span class="hljs-keyword">if</span> is_send <span class="hljs-keyword">else</span> dist.irecv, tensor, peer_rank)<br>    <br>    <span class="hljs-keyword">if</span> VERBOSE: <br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;operation&#125;</span> | <span class="hljs-subst">&#123;<span class="hljs-string">&#x27;sending&#x27;</span> <span class="hljs-keyword">if</span> is_send <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;receiving&#x27;</span>&#125;</span> <span class="hljs-subst">&#123;operation.split(<span class="hljs-string">&#x27;_&#x27;</span>)[<span class="hljs-number">1</span>]&#125;</span> &quot;</span><br>              <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;pgm.process_group_manager.pp_rank&#125;</span> <span class="hljs-subst">&#123;<span class="hljs-string">&#x27;→&#x27;</span> <span class="hljs-keyword">if</span> is_send <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;←&#x27;</span>&#125;</span> <span class="hljs-subst">&#123;peer_rank&#125;</span> | &quot;</span><br>              <span class="hljs-string">f&quot;STEP:<span class="hljs-subst">&#123;STEP&#125;</span> | RANK:<span class="hljs-subst">&#123;pgm.process_group_manager.pp_rank&#125;</span>&quot;</span>, flush=<span class="hljs-literal">True</span>)<br>    <br>    <span class="hljs-comment"># 执行批处理的异步发送/接收操作，并等待其完成</span><br>    <span class="hljs-comment"># 注意: dist.batch_isend_irecv 通常用于多个 P2POp，这里只用了一个</span><br>    [req.wait() <span class="hljs-keyword">for</span> req <span class="hljs-keyword">in</span> dist.batch_isend_irecv([op])]<br>    torch.cuda.synchronize() <span class="hljs-comment"># 确保 CUDA 操作完成</span><br>    <br>    <span class="hljs-keyword">if</span> VERBOSE: STEP += <span class="hljs-number">1</span><br>    <br>    <span class="hljs-keyword">return</span> tensor <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> is_send <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span> <span class="hljs-comment"># 如果是接收操作，返回接收到的张量</span><br><span class="hljs-comment"># ... existing code ...</span><br></code></pre></td></tr></table></figure><h3 id="2-PipelineParallel"><a href="#2-PipelineParallel" class="headerlink" title="2. PipelineParallel"></a>2. PipelineParallel</h3><ol><li><p>在初始化时，会将decoder_layers进行划分，根据pp分离的数量得到每个gpu负责多少layers，然后得到每个gpu负责的layer的起始与结束的decoder layer。还会看当前进程是否是pipeline的第一个进程，如果是就包含embedding层，不然就使用nn.Identity()。nn.Identity()的作用是将输入原封不动地返回。还会看如果当前进程时pipeline的最后一个进程，那么就会加上final_norm和final_proj，不然也是用nn.Identity()代替。</p></li><li><p>在forward的时候，其输入有input、position_ids和hidden_states。如果hidden_states不为空，那么输入就是input_ids，不然输入就是hidden_states，然后对输入使用embedding，因为如果不是第一个pipeline，embedding都是nn.Identity()，所以是可以的。然后执行自己所属的decoder_layers，然后在执行final_norma和final_proj。</p></li><li><p>在backward的时候，输入是input_tensor（当前阶段输入的tensor）、output_tensor（当前阶段输出的tensor）和output_tensor_grad（关于output_tensor的梯度）</p><ol><li><p>首先需要设置input_tensor需要保存梯度，需要额外设置的原因在于原本非叶子结点是不保存的梯度的，但是现在模型进行了拆分，所以也需要保存。</p></li><li><p>然后需要查看output_tensor_grad是不是None，如果是None那就说明现在其实是在最后一个阶段，那就需要将其初始化为1</p></li><li><p>然后执行反向传播计算当前阶段的梯度。</p></li><li><p>然后如果不是第一个阶段，就继续往前传递梯度。</p></li></ol></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ... existing code ...</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PipelineParallel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 1. 分配层到当前 GPU</span><br>        layer_distribution = self.distribute_layers(config.num_hidden_layers)<br>        <br>        <span class="hljs-comment"># 2. 根据当前 GPU 是否为第一个/最后一个阶段，选择性地包含 embedding, final_norm, final_proj</span><br>        <span class="hljs-comment"># 如果不是对应阶段，则使用 nn.Identity() 作为占位符（无操作）</span><br>        self.embedding = model.embedding <span class="hljs-keyword">if</span> pgm.process_group_manager.pp_is_first_stage <span class="hljs-keyword">else</span> nn.Identity()<br>        <span class="hljs-comment"># 只保留分配给当前 GPU 的 decoder_layers</span><br>        self.decoder_layers = nn.ModuleDict(&#123;<span class="hljs-built_in">str</span>(i): model.decoder_layers[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> layer_distribution&#125;)<br>        self.final_norm = model.final_norm <span class="hljs-keyword">if</span> pgm.process_group_manager.pp_is_last_stage <span class="hljs-keyword">else</span> nn.Identity()<br>        self.final_proj = model.final_proj <span class="hljs-keyword">if</span> pgm.process_group_manager.pp_is_last_stage <span class="hljs-keyword">else</span> nn.Identity()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">distribute_layers</span>(<span class="hljs-params">self, num_layers</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;计算当前流水线阶段应该负责哪些层。&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 计算每个 GPU 应分配的层数，处理余数</span><br>        layers_per_gpu = [num_layers // pgm.process_group_manager.pp_world_size + (<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> i &lt; num_layers % pgm.process_group_manager.pp_world_size <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(pgm.process_group_manager.pp_world_size)]<br>        <span class="hljs-comment"># 计算当前 GPU 的起始层索引</span><br>        start_layer = <span class="hljs-built_in">sum</span>(layers_per_gpu[:pgm.process_group_manager.pp_rank])<br>        <span class="hljs-comment"># 返回当前 GPU 负责的层索引列表</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(start_layer, start_layer + layers_per_gpu[pgm.process_group_manager.pp_rank]))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input_ids, position_ids, hidden_states</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;当前流水线阶段的前向传播。&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 如果 hidden_states 不为 None (来自前一阶段的激活)，则使用它；否则使用 input_ids (第一阶段)</span><br>        x = hidden_states <span class="hljs-keyword">if</span> hidden_states <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> input_ids<br>        x = self.embedding(x) <span class="hljs-comment"># 应用 embedding (如果存在)</span><br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.decoder_layers.values(): <span class="hljs-comment"># 通过分配给此阶段的 decoder 层</span><br>            x = layer(x, position_ids=position_ids)<br>        x = self.final_norm(x) <span class="hljs-comment"># 应用 final_norm (如果存在)</span><br>        <span class="hljs-keyword">return</span> self.final_proj(x) <span class="hljs-comment"># 应用 final_proj (如果存在) 并返回输出</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, input_tensor, output_tensor, output_tensor_grad</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;当前流水线阶段的反向传播。&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># input_tensor: 当前阶段前向传播的输入激活</span><br>        <span class="hljs-comment"># output_tensor: 当前阶段前向传播的输出激活</span><br>        <span class="hljs-comment"># output_tensor_grad: 相对于 output_tensor 的梯度 (来自下一阶段或损失函数)</span><br>        <br>        <span class="hljs-keyword">if</span> input_tensor <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>: <br>            input_tensor.retain_grad() <span class="hljs-comment"># 确保 input_tensor 的梯度会被计算并存储</span><br>        <br>        <span class="hljs-comment"># 如果是最后一个阶段，且没有显式提供 output_tensor_grad，则初始化为全1张量</span><br>        <span class="hljs-keyword">if</span> output_tensor_grad <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            output_tensor_grad = torch.ones_like(output_tensor, memory_format=torch.preserve_format)<br>        <br>        <span class="hljs-comment"># 执行反向传播，计算当前阶段参数的梯度，以及相对于 input_tensor 的梯度</span><br>        torch.autograd.backward(output_tensor, grad_tensors=output_tensor_grad, retain_graph=<span class="hljs-literal">False</span>, create_graph=<span class="hljs-literal">False</span>)<br>        <br>        <span class="hljs-comment"># 返回相对于 input_tensor 的梯度，这个梯度将传递给前一个阶段</span><br>        <span class="hljs-keyword">return</span> input_tensor.grad <span class="hljs-keyword">if</span> input_tensor <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br><span class="hljs-comment"># ... existing code ...</span><br></code></pre></td></tr></table></figure><h3 id="3-train-step-pipeline-afab"><a href="#3-train-step-pipeline-afab" class="headerlink" title="3. train_step_pipeline_afab"></a>3. train_step_pipeline_afab</h3><p>这个函数实现了使用 AFAB（Activation Forward - Activation Backward）流水线调度策略的一个完整训练步骤。AFAB 策略意味着所有微批次首先完成完整的前向传播阶段，然后所有微批次再完成完整的反向传播阶段。</p><p><strong>在前向传播阶段：</strong></p><ol><li><p>从前一个阶段接收前向传播的激活值。这里对于第一个阶段会直接返回None，对于其他阶段会阻塞，直到接收到了数据为止。</p></li><li><p>从data_loader获取当前微批次的数据。如果不是第一阶段获取到了前面的前向传播的激活值就放入<code>batch[&quot;hidden_states&quot;]</code>中</p></li><li><p>执行当前阶段的前向传播</p></li><li><p>将当前阶段的结果发送给下一个阶段，注意如果是最后一个阶段的话会直接返回None</p></li><li><p>如果是最后一个阶段，那么就根据输出和获取到的<code>batch[&quot;target_ids&quot;]</code>计算损失，注意需要将得到的loss除以data_loader.grad_acc_steps，以做到正确的累积。</p></li><li><p>存储当前微批次的输入和输出到数组中</p></li></ol><p><strong>在反向传播阶段：</strong></p><ol><li><p>如果启用了数据并行，那么需要在最后一个mini_batch中设置<code>model.require_backward_grad_sync=true</code>,从而使得模型在反向传播的时候会与其他的数据并行的模型的梯度进行平均</p></li><li><p>等待后一个阶段的模型的反向传播的输出梯度，这里对于最后一个阶段的进程会直接返回1，但是对于其他的进程一开始会有阻塞。</p></li><li><p>弹出当前微批次在前向传播时保存的输入和输出激活</p></li><li><p>调用PipelineParallel的backward计算反向传播的梯度</p></li><li><p>将梯度传输给下一个阶段</p></li><li><p>最后返回平均的损失（打印logger用）</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ... existing code ...</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_step_pipeline_afab</span>(<span class="hljs-params">model, data_loader, tensor_shapes, device, dtype</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    使用 AFAB 流水线并行执行一个训练步骤。</span><br><span class="hljs-string">    实现分离的前向和反向传播阶段以优化内存使用。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    logging_loss: torch.float32 = <span class="hljs-number">0.0</span> <span class="hljs-comment"># 用于记录损失</span><br>    input_tensors, output_tensors = [], [] <span class="hljs-comment"># 存储每个微批次在前向传播中的输入和输出激活，供反向传播使用</span><br>    requires_grad_sync = pgm.process_group_manager.dp_world_size &gt; <span class="hljs-number">1</span> <span class="hljs-comment"># 是否需要数据并行梯度同步</span><br><br>    <span class="hljs-comment"># === 全局前向传播阶段 ===</span><br>    <span class="hljs-comment"># 对梯度累积的每一步（即每个微批次）</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(data_loader.grad_acc_steps):<br>        <span class="hljs-comment"># 1. 从前一个阶段接收前向传播的激活值 (如果不是第一阶段)</span><br>        input_tensor = pipeline_communicate(operation=<span class="hljs-string">&#x27;recv_forward&#x27;</span>, shapes=tensor_shapes, device=device, dtype=dtype)<br>        <br>        <span class="hljs-comment"># 2. 获取当前微批次的数据</span><br>        batch = <span class="hljs-built_in">next</span>(data_loader)<br>        <span class="hljs-comment"># 将接收到的 input_tensor (如果存在) 设置为当前批次的 hidden_states</span><br>        batch[<span class="hljs-string">&quot;hidden_states&quot;</span>] = input_tensor.to(device) <span class="hljs-keyword">if</span> input_tensor <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> input_tensor<br>        <br>        <span class="hljs-comment"># 3. 执行当前阶段的前向传播</span><br>        output_tensor = model.forward(input_ids=batch[<span class="hljs-string">&quot;input_ids&quot;</span>].to(device), position_ids=batch[<span class="hljs-string">&quot;position_ids&quot;</span>].to(device), hidden_states=batch[<span class="hljs-string">&quot;hidden_states&quot;</span>])<br>        <br>        <span class="hljs-comment"># 4. 将当前阶段的输出激活发送到下一个阶段 (如果不是最后阶段)</span><br>        pipeline_communicate(operation=<span class="hljs-string">&#x27;send_forward&#x27;</span>, tensor=output_tensor, device=device, dtype=dtype)<br>        <br>        <span class="hljs-comment"># 5. 如果是最后一个阶段，计算损失</span><br>        <span class="hljs-keyword">if</span> pgm.process_group_manager.pp_is_last_stage:<br>            <span class="hljs-comment"># 注意: 这里的 output_tensor 是模型的 logits 输出</span><br>            <span class="hljs-comment"># F.cross_entropy 期望的输入形状是 (N, C, d1, ..., dk) 或 (N, C)，目标是 (N, d1, ..., dk) 或 (N)</span><br>            <span class="hljs-comment"># 通常模型的输出是 (batch_size, seq_len, vocab_size)，所以需要 transpose(1, 2) 变为 (batch_size, vocab_size, seq_len)</span><br>            loss_val = F.cross_entropy(output_tensor.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), batch[<span class="hljs-string">&quot;target_ids&quot;</span>].to(device), reduction=<span class="hljs-string">&#x27;mean&#x27;</span>)<br>            logging_loss += loss_val.item() / data_loader.grad_acc_steps <span class="hljs-comment"># 累积并平均损失</span><br><br>        <span class="hljs-comment"># 6. 存储当前微批次的输入和输出激活，供反向传播使用</span><br>        input_tensors.append(input_tensor)<br>        output_tensors.append(output_tensor) <span class="hljs-comment"># 注意：对于最后一个阶段，这里存的是损失值 (标量张量)</span><br><br>    <span class="hljs-comment"># === 全局反向传播阶段 ===</span><br>    <span class="hljs-comment"># 按照前向传播的顺序，对每个微批次进行反向传播</span><br>    <span class="hljs-keyword">for</span> ith_microbatch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(data_loader.grad_acc_steps):<br>        <span class="hljs-comment"># 如果启用了数据并行，则只在梯度累积的最后一步同步梯度</span><br>        <span class="hljs-keyword">if</span> requires_grad_sync:<br>            is_last_iteration = (ith_microbatch == data_loader.grad_acc_steps - <span class="hljs-number">1</span>)<br>            model.require_backward_grad_sync = is_last_iteration <span class="hljs-comment"># 假设 model 有这个属性来控制DP同步</span><br>            <br>        <span class="hljs-comment"># 1. 从下一个阶段接收反向传播的梯度 (如果不是最后阶段)</span><br>        output_tensor_grad = pipeline_communicate(operation=<span class="hljs-string">&#x27;recv_backward&#x27;</span>, shapes=tensor_shapes, device=device, dtype=dtype)<br>        <br>        <span class="hljs-comment"># 2. 弹出当前微批次在前向传播时保存的输入和输出激活</span><br>        input_tensor, output_tensor = input_tensors.pop(<span class="hljs-number">0</span>), output_tensors.pop(<span class="hljs-number">0</span>)<br>        <br>        <span class="hljs-comment"># 3. 执行当前阶段的反向传播</span><br>        <span class="hljs-comment"># 对于最后一个阶段，output_tensor 是损失值，output_tensor_grad 应该是 None (或由 pipeline_communicate 返回 None)</span><br>        <span class="hljs-comment"># model.backward 会处理 output_tensor_grad 为 None 的情况 (通常初始化为 torch.ones_like)</span><br>        input_tensor_grad = model.backward(input_tensor, output_tensor, output_tensor_grad)<br>        <br>        <span class="hljs-comment"># 4. 将计算得到的输入梯度发送到前一个阶段 (如果不是第一阶段)</span><br>        pipeline_communicate(operation=<span class="hljs-string">&#x27;send_backward&#x27;</span>, tensor=input_tensor_grad, device=device, dtype=dtype)<br><br>    <span class="hljs-keyword">return</span> logging_loss <span class="hljs-comment"># 返回平均损失</span><br><span class="hljs-comment"># ... existing code ...</span><br></code></pre></td></tr></table></figure><h1 id="1f1b并行"><a href="#1f1b并行" class="headerlink" title="1f1b并行"></a>1f1b并行</h1><h2 id="理论分析-1"><a href="#理论分析-1" class="headerlink" title="理论分析"></a>理论分析</h2><p>该调度方案称为 <em><strong>一前一后（1F1B）</strong></em>，因为在中间&#x2F;稳定状态下，交替执行一个前向传播和一个反向传播。其基本思想是尽早开始反向传播。该调度如下所示：</p><p><img src="/2025/06/24/Picotron-Tutorial%20pipeline%20parallel/image-1.png"></p><p>如果仔细计算的话会发现气泡的大小仍然相同，因此训练效率并未显著提升。然而，我们仅需存储 $p$ 个微批次的激活（其中 $p$ 为流水线并行度），而不是 $m$（其中 $m$ 是微批次数量）。这主要是因为观察最后一个阶段的GPU4，如果要做到1f1b的调度，那么在微批次1的梯度返回给第一个GPU的时候，第 $p&#x3D;4$ 个微批次的激活就应该已经到达了GPU4，所以在此之前对于GPU1必然已经计算完了前$p&#x3D;4$ 个微批次的激活。这减少了在 AFAB 调度中遇到的激活内存爆炸问题。因此，我们可以增加更多微批次，从而实际减少气泡的影响。</p><p>这种设置的主要复杂性（如上图所示）在于前向和反向传播不再是完全顺序执行的，各个GPU执行是并行交错执行的，每个GPU有自己调度的节奏，而不再能用同一套代码来统一控制了。这也是流水线并行通常需要对训练代码和建模代码进行大幅修改的原因之一。</p><h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><h3 id="1-bidirectional-pipeline-communicate"><a href="#1-bidirectional-pipeline-communicate" class="headerlink" title="1. bidirectional_pipeline_communicate"></a>1. bidirectional_pipeline_communicate</h3><p>该函数的作用是 处理一个流水线阶段同时向其邻居阶段发送数据和从该邻居阶段接收数据的双向通信 。</p><ol><li><p>支持两种操作：</p><ul><li><p>send_fwd_recv_bwd：发送前向传播接收反向传播的数据</p></li><li><p>send_bwd_recv_fwd：发送反向传播并接收前向传播的数据</p></li></ul></li><li><p>处理边界条件，如果是第一层就不需要执行send_bwd_recv_fwd，如果是最后一层就不需要执行send_fwd_recv_bwd</p></li><li><p>确定通信的对象，如果是send_fwd_recv_bwd，那么对象就是下一个流水线的GPU，不然就是上一个</p></li><li><p>创建一个空的张量用于接收数据</p></li><li><p>设置并启动同时进行的异步发送和异步接收操作</p></li><li><p>两个 P2POp 对象：一个用于发送，一个用于接收。异步执行</p></li><li><p>等待两个异步执行操作结束</p></li><li><p>返回接收到的数据</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ... existing code ...</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">bidirectional_pipeline_communicate</span>(<span class="hljs-params">operation, send_tensor, recv_shapes, device, dtype</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    处理流水线阶段之间的双向通信，允许同时进行发送和接收操作。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        operation (str): 双向操作的类型 (&#x27;send_fwd_recv_bwd&#x27; 或 &#x27;send_bwd_recv_fwd&#x27;)</span><br><span class="hljs-string">        send_tensor: 要发送的张量</span><br><span class="hljs-string">        recv_shapes: 要接收的张量的形状规格</span><br><span class="hljs-string">        device: 张量操作的目标设备</span><br><span class="hljs-string">        dtype: 张量的数据类型</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        torch.Tensor or None: 接收到的张量，如果在流水线的终端阶段则为 None</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">global</span> STEP <span class="hljs-comment"># 用于调试，追踪通信步骤</span><br>    <span class="hljs-keyword">global</span> VERBOSE <span class="hljs-comment"># 控制是否打印详细日志</span><br>    <br>    <span class="hljs-comment"># 1. 判断操作的主要方向 (是发送前向激活还是发送反向梯度)</span><br>    is_fwd = (operation == <span class="hljs-string">&#x27;send_fwd_recv_bwd&#x27;</span>)<br>    <br>    <span class="hljs-comment"># 2. 处理流水线的边界情况 (第一个或最后一个阶段)</span><br>    <span class="hljs-comment"># 如果是 &#x27;send_fwd_recv_bwd&#x27; (发送前向，接收反向) 且当前是最后一个阶段，</span><br>    <span class="hljs-comment"># 则没有下一个阶段可以发送前向激活，也没有下一个阶段可以接收反向梯度。</span><br>    <span class="hljs-keyword">if</span> (is_fwd <span class="hljs-keyword">and</span> pgm.process_group_manager.pp_is_last_stage) <span class="hljs-keyword">or</span> \<br>       (<span class="hljs-keyword">not</span> is_fwd <span class="hljs-keyword">and</span> pgm.process_group_manager.pp_is_first_stage): <br>        <span class="hljs-comment"># 如果是 &#x27;send_bwd_recv_fwd&#x27; (发送反向，接收前向) 且当前是第一个阶段，</span><br>        <span class="hljs-comment"># 则没有前一个阶段可以发送反向梯度，也没有前一个阶段可以接收前向激活。</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span> <span class="hljs-comment"># 在这些边界情况下，不进行通信，直接返回 None</span><br>    <br>    <span class="hljs-comment"># 3. 确定通信对方的 rank (peer_rank)</span><br>    <span class="hljs-comment"># 如果是发送前向 (is_fwd is True)，则对方是下一个流水线阶段 (pp_next_rank)</span><br>    <span class="hljs-comment"># 如果是发送反向 (is_fwd is False)，则对方是上一个流水线阶段 (pp_prev_rank)</span><br>    peer_rank = pgm.process_group_manager.pp_next_rank <span class="hljs-keyword">if</span> is_fwd <span class="hljs-keyword">else</span> pgm.process_group_manager.pp_prev_rank<br>    <br>    <span class="hljs-comment"># 4. 创建一个空的张量用于接收数据</span><br>    <span class="hljs-comment"># requires_grad=True 是因为接收到的张量 (无论是前向激活还是反向梯度) 都将参与后续的梯度计算</span><br>    recv_tensor = torch.empty(recv_shapes, requires_grad=<span class="hljs-literal">True</span>, device=device, dtype=dtype)<br>    <br>    <span class="hljs-comment"># 5. 设置并启动同时进行的异步发送和异步接收操作</span><br>    <span class="hljs-comment"># dist.batch_isend_irecv 接收一个 P2POp (点对点操作) 列表</span><br>    <span class="hljs-comment"># 这里我们创建了两个 P2POp 对象：一个用于发送，一个用于接收，都与同一个 peer_rank 通信</span><br>    reqs = dist.batch_isend_irecv([<br>        dist.P2POp(dist.isend, send_tensor, peer_rank), <span class="hljs-comment"># 异步发送 send_tensor 给 peer_rank</span><br>        dist.P2POp(dist.irecv, recv_tensor, peer_rank)  <span class="hljs-comment"># 从 peer_rank 异步接收数据到 recv_tensor</span><br>    ])<br>    <br>    <span class="hljs-keyword">if</span> VERBOSE: <br>        <span class="hljs-comment"># 打印详细的通信日志</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;operation&#125;</span> | sending <span class="hljs-subst">&#123;<span class="hljs-string">&#x27;next&#x27;</span> <span class="hljs-keyword">if</span> is_fwd <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;prev&#x27;</span>&#125;</span> &quot;</span><br>              <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;pgm.process_group_manager.pp_rank&#125;</span> -&gt; <span class="hljs-subst">&#123;peer_rank&#125;</span> | &quot;</span><br>              <span class="hljs-string">f&quot;receiving <span class="hljs-subst">&#123;<span class="hljs-string">&#x27;next&#x27;</span> <span class="hljs-keyword">if</span> is_fwd <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;prev&#x27;</span>&#125;</span> <span class="hljs-subst">&#123;peer_rank&#125;</span> -&gt; &quot;</span><br>              <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;pgm.process_group_manager.pp_rank&#125;</span> | STEP <span class="hljs-subst">&#123;STEP=&#125;</span> | &quot;</span><br>              <span class="hljs-string">f&quot;RANK:<span class="hljs-subst">&#123;pgm.process_group_manager.pp_rank&#125;</span>&quot;</span>, flush=<span class="hljs-literal">True</span>)<br>    <br>    <span class="hljs-comment"># 6. 等待发送和接收操作都完成</span><br>    <span class="hljs-comment"># [req.wait() for req in reqs] 会阻塞当前进程，直到 reqs 列表中的所有请求 (即发送和接收) 都完成</span><br>    [req.wait() <span class="hljs-keyword">for</span> req <span class="hljs-keyword">in</span> reqs]<br>    torch.cuda.synchronize() <span class="hljs-comment"># 确保所有在默认 CUDA 流上的操作完成，特别是在 GPU 通信后</span><br>    <br>    <span class="hljs-keyword">if</span> VERBOSE: STEP += <span class="hljs-number">1</span> <span class="hljs-comment"># 增加调试步骤计数器</span><br>    <br>    <span class="hljs-comment"># 7. 返回接收到的张量</span><br>    <span class="hljs-keyword">return</span> recv_tensor<br><span class="hljs-comment"># ... existing code ...</span><br></code></pre></td></tr></table></figure><h3 id="2-train-step-pipeline-1f1b"><a href="#2-train-step-pipeline-1f1b" class="headerlink" title="2. train_step_pipeline_1f1b"></a>2. train_step_pipeline_1f1b</h3><p>该函数实现了1f1b的调度策略，其旨在通过让每个流水线阶段（GPU）在稳定期同时处理一个微批次（micro-batch）的前向传播和另一个微批次的反向传播，并重叠计算与通信，来提高 GPU 的利用率并减少流水线“气泡”（即 GPU 空闲时间）。</p><p>整个函数可以分为三个主要阶段：</p><ol><li><p>预热（Warmup）阶段 ：用前向传播任务填充流水线。</p></li><li><p>稳定（Steady State）阶段 ：核心的 1F1B 操作，每个阶段同时执行一个前向和一个反向传播。</p></li><li><p>冷却（Cooldown）阶段 ：清空流水线中剩余的反向传播任务。</p></li></ol><p><strong>初始化阶段：</strong></p><p>我们假设如上图所示，GPU数量为4，<code>pp_world_size</code>为4，<code>grad_acc_steps</code>&#x3D;8。</p><ol><li><p>计算预热阶段的微批次的数量：**&#x20;&#x20;**<code>num_warmup_microbatches = min(pgm.process_group_manager.pp_world_size - pgm.process_group_manager.pp_rank - 1, data_loader.grad_acc_steps)</code>。如果<code>pp_rank</code>是0，那么其对应的<code>num_warmup_microbatches</code>为3，如果<code>pp_rank</code>是3，那么对应的<code>num_warmup_microbatches</code>为0。</p></li><li><p>计算在预热阶段之后，需要在稳定状态下处理的微批次数：<code>num_microbatches_remaining = data_loader.grad_acc_steps - num_warmup_microbatches</code>。</p></li><li><p>判断是否需要数据并行, 如果需要就标记<code>requires_grad_sync=true</code>。</p></li></ol><p><strong>预热阶段：</strong></p><p>预热阶段整体与Afab中的实现前向传播类似。</p><p>会执行<code>num_warmup_microbatches</code>次下面的操作来进行预热：</p><ol><li><p>通过recv_forward操作获取前一个阶段的input，注意对于第一阶段的pipeline，这会直接返回none，对于其他阶段会进行阻塞等待。</p></li><li><p>执行自定义的_forward_step函数，得倒output</p><ol><li><p>通过data_loader获取下一个batch</p></li><li><p>将input作为batch[“hidden_states”]</p></li><li><p>执行PipelineParallel的forward</p></li><li><p>如果是pipeline的最后一个阶段还需要计算loss并将loss作为output_tensor</p></li></ol></li><li><p>通过send_forward操作来发送输出</p></li><li><p>将input和output添加到数组中</p></li></ol><p>如果<code>num_microbatches_remaining&gt;0</code>，即稳定状态下需要微批次那么就再执行recv_forward操作来收集input，这是作为稳定状态循环的开始，一般都需要</p><p>如果启用了数据并行，那么还需要先设置require_backward_grad_sync为false，防止还没都最后一个梯度计算就开始执行了梯度平均。</p><p><strong>稳定阶段：</strong></p><p>在稳定阶段，每个GPU同时执行一个前向传播和反向传播</p><ol><li><p>执行刚刚最后收集到的input的前向传播_forward_step</p></li><li><p>执行send_fwd_recv_bwd操作发送前向传播的结果收集反向传播的梯度</p></li><li><p>将当前的input和output都放入到数组中</p></li><li><p>此时数组中的第一个input和第一个output是一对，直接取出</p></li><li><p>如果是在最后一个流水线阶段并且是稳定状态的最后一次迭代，那么就说明所有的微批次的反向传播在最后一个阶段都计算完了，设置require_backward_grad_sync为true</p></li><li><p>执行model.backward来计算反向传播</p></li><li><p>如果是流水线的最后一个阶段就执行send_backward发送回梯度，不然就执行send_bwd_recv_fwd，发送梯度，等待下一轮的激活值</p></li></ol><p><strong>冷却阶段：</strong></p><p>在冷却阶段需要把还剩余的梯度处理掉，因为之前预热阶段对<code>num_warmup_microbatches</code>批次只处理了前向传播，没有处理反向传播，所以这里需要把之前欠缺的反向传播补足。它会遍历需要的反向传播的次数：</p><ol><li><p>如果是最后一次迭代，就设置require_backward_grad_sync&#x3D;true，使得该批次的梯度可以得到平均</p></li><li><p>从数组中弹出input和output</p></li><li><p>调用recv_backward来接收上一个阶段的梯度</p></li><li><p>反向传播计算梯度</p></li><li><p>将计算得到梯度通过调用send_backward来将梯度传给前一个阶段</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_step_pipeline_1f1b</span>(<span class="hljs-params">model, data_loader, tensor_shapes, device, dtype</span>):    <br>    <span class="hljs-comment"># 1. 初始化和计算微批次数量</span><br>    <span class="hljs-comment"># num_warmup_microbatches: 当前 rank 需要执行的纯前向传播的微批次数，以启动流水线。</span><br>    <span class="hljs-comment"># - 对于 rank 0，它是 pp_world_size - 1 (如果梯度累积步数足够多)。</span><br>    <span class="hljs-comment"># - 对于最后一个 rank (pp_world_size - 1)，它是 0。</span><br>    <span class="hljs-comment"># - 这是为了确保在第一个反向传播开始前，流水线被适当地“填充”。</span><br>    num_warmup_microbatches = <span class="hljs-built_in">min</span>(pgm.process_group_manager.pp_world_size - pgm.process_group_manager.pp_rank - <span class="hljs-number">1</span>, data_loader.grad_acc_steps)<br>    <br>    <span class="hljs-comment"># num_microbatches_remaining: 在预热阶段之后，需要在稳定状态下处理的微批次数。</span><br>    num_microbatches_remaining = data_loader.grad_acc_steps - num_warmup_microbatches<br>    <br>    logging_loss, input_tensors, output_tensors  = <span class="hljs-number">0.0</span>, [], [] <span class="hljs-comment"># 用于记录损失和存储中间的输入/输出张量</span><br>    requires_grad_sync = pgm.process_group_manager.dp_world_size &gt; <span class="hljs-number">1</span> <span class="hljs-comment"># 判断是否需要数据并行梯度同步</span><br>    <br>    <span class="hljs-comment"># 2. 定义内部辅助函数 _forward_step</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_step</span>(<span class="hljs-params">input_tensor</span>):<br>        batch = <span class="hljs-built_in">next</span>(data_loader) <span class="hljs-comment"># 获取下一个微批次数据</span><br>        <span class="hljs-comment"># 如果 input_tensor 不是 None (即不是第一个流水线阶段的第一次迭代)，则将其作为隐藏状态</span><br>        batch[<span class="hljs-string">&quot;hidden_states&quot;</span>] = input_tensor.to(device) <span class="hljs-keyword">if</span> input_tensor <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> input_tensor<br>        <span class="hljs-comment"># 执行模型的前向传播</span><br>        output_tensor = model.forward(input_ids=batch[<span class="hljs-string">&quot;input_ids&quot;</span>].to(device), position_ids=batch[<span class="hljs-string">&quot;position_ids&quot;</span>].to(device), hidden_states=batch[<span class="hljs-string">&quot;hidden_states&quot;</span>])<br>        <br>        <span class="hljs-comment"># 如果是最后一个流水线阶段，计算损失</span><br>        <span class="hljs-keyword">if</span> pgm.process_group_manager.pp_is_last_stage:<br>            <span class="hljs-comment"># output_tensor 此时是 logits，需要与 target_ids 计算交叉熵损失</span><br>            output_tensor = F.cross_entropy(output_tensor.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), batch[<span class="hljs-string">&quot;target_ids&quot;</span>].to(device), reduction=<span class="hljs-string">&#x27;mean&#x27;</span>)<br>            <span class="hljs-keyword">nonlocal</span> logging_loss <span class="hljs-comment"># 允许修改外部作用域的 logging_loss</span><br>            logging_loss += output_tensor.item() / data_loader.grad_acc_steps <span class="hljs-comment"># 累积损失</span><br>        <span class="hljs-keyword">return</span> output_tensor <span class="hljs-comment"># 返回当前阶段的输出张量 (对于最后一个阶段，这是损失值)</span><br><br>    <span class="hljs-comment"># === 预热（Warmup）前向传播阶段 === (L214-L219)</span><br>    <span class="hljs-comment"># 这个循环的目的是填充流水线。每个阶段执行其分配到的 num_warmup_microbatches 次前向传播。</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_warmup_microbatches):<br>        <span class="hljs-comment"># 从前一个阶段接收前向传播的激活值 (或 None，如果是第一个阶段)</span><br>        input_tensor = pipeline_communicate(operation=<span class="hljs-string">&#x27;recv_forward&#x27;</span>, shapes=tensor_shapes, device=device, dtype=dtype)<br>        <span class="hljs-comment"># 执行当前微批次的前向计算</span><br>        output_tensor = _forward_step(input_tensor)<br>        <span class="hljs-comment"># 将计算得到的激活值发送给下一个阶段 (或 None，如果是最后一个阶段)</span><br>        pipeline_communicate(operation=<span class="hljs-string">&#x27;send_forward&#x27;</span>, tensor=output_tensor, device=device, dtype=dtype)<br>        <span class="hljs-comment"># 存储当前微批次的输入和输出，供后续的反向传播使用</span><br>        input_tensors.append(input_tensor)<br>        output_tensors.append(output_tensor)<br><br>    <span class="hljs-comment"># 在进入稳定状态之前，如果还有剩余的微批次需要处理，</span><br>    <span class="hljs-comment"># 当前阶段需要先接收一个前向传播的输入。</span><br>    <span class="hljs-comment"># 这是因为稳定状态的循环开始时，期望 input_tensor 已经被填充。</span><br>    <span class="hljs-keyword">if</span> num_microbatches_remaining &gt; <span class="hljs-number">0</span>: <span class="hljs-comment"># (L221-L222)</span><br>        input_tensor = pipeline_communicate(operation=<span class="hljs-string">&#x27;recv_forward&#x27;</span>, shapes=tensor_shapes, device=device, dtype=dtype)<br>    <br>    <span class="hljs-comment"># 如果启用了数据并行 (dp_world_size &gt; 1)，则需要处理梯度的同步。</span><br>    <span class="hljs-comment"># 在进入稳定状态前，通常先禁用梯度同步，因为梯度是在所有微批次的反向传播完成后才同步的。</span><br>    <span class="hljs-keyword">if</span> requires_grad_sync: <span class="hljs-comment"># (L224-L225)</span><br>        model.require_backward_grad_sync = <span class="hljs-literal">False</span><br><br>    <span class="hljs-comment"># === 1F1B 稳定状态（Steady State）阶段 === (L228-L247)</span><br>    <span class="hljs-comment"># 在这个阶段，每个 GPU 同时执行一个前向传播和一个反向传播（针对不同的微批次）。</span><br>    <span class="hljs-keyword">for</span> ith_microbatch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_microbatches_remaining):<br>        is_last_iteration = (ith_microbatch == num_microbatches_remaining - <span class="hljs-number">1</span>) <span class="hljs-comment"># 判断是否是稳定状态的最后一次迭代</span><br>        <br>        <span class="hljs-comment"># 1. 执行当前微批次的前向传播 (F)</span><br>        output_tensor = _forward_step(input_tensor) <span class="hljs-comment"># 使用上一次迭代接收或预热阶段最后接收的 input_tensor</span><br>        <br>        <span class="hljs-comment"># 2. 双向通信：发送当前前向结果 (F)，接收上一个微批次的反向梯度 (B)</span><br>        <span class="hljs-comment">#    - send_tensor=output_tensor: 将当前前向计算的结果发送给下一个阶段。</span><br>        <span class="hljs-comment">#    - recv_shapes=tensor_shapes: 准备接收从下一个阶段传来的梯度。</span><br>        <span class="hljs-comment">#      这个梯度对应的是上一个已完成前向传播并发送出去的微批次的输出。</span><br>        output_tensor_grad = bidirectional_pipeline_communicate(operation=<span class="hljs-string">&#x27;send_fwd_recv_bwd&#x27;</span>, send_tensor=output_tensor, recv_shapes=tensor_shapes, device=device, dtype=dtype)<br>        <br>        <span class="hljs-comment"># 存储当前完成前向传播的 input_tensor 和 output_tensor</span><br>        input_tensors.append(input_tensor)<br>        output_tensors.append(output_tensor)<br>        <br>        <span class="hljs-comment"># 取出之前存储的、现在需要进行反向传播的微批次的 input_tensor 和 output_tensor</span><br>        <span class="hljs-comment"># 这些是与刚接收到的 output_tensor_grad 相对应的。</span><br>        input_tensor, output_tensor = input_tensors.pop(<span class="hljs-number">0</span>), output_tensors.pop(<span class="hljs-number">0</span>)<br>        <br>        <span class="hljs-comment"># 梯度同步控制：</span><br>        <span class="hljs-comment"># - 只在最后一个流水线阶段 (num_warmup_microbatches == 0)</span><br>        <span class="hljs-comment"># - 并且是稳定状态的最后一次迭代时</span><br>        <span class="hljs-comment"># - 才将 model.require_backward_grad_sync 设置为 True。</span><br>        <span class="hljs-comment"># 这是为了确保在整个梯度累积周期的最后一次反向传播时进行梯度同步。</span><br>        <span class="hljs-keyword">if</span> num_warmup_microbatches == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> is_last_iteration: <span class="hljs-comment"># (L238-L239)</span><br>            model.require_backward_grad_sync = <span class="hljs-literal">True</span><br><br>        <span class="hljs-comment"># 3. 执行上一个微批次的反向传播 (B)</span><br>        input_tensor_grad = model.backward(input_tensor, output_tensor, output_tensor_grad)<br>        <br>        <span class="hljs-comment"># 4. 处理反向传播结果的通信</span><br>        <span class="hljs-keyword">if</span> is_last_iteration: <span class="hljs-comment"># 如果是稳定状态的最后一次迭代</span><br>            <span class="hljs-comment"># 对于最后一个1F1B周期的反向传播，不再需要接收新的前向输入，</span><br>            <span class="hljs-comment"># 只需将计算得到的梯度发送给前一个阶段。</span><br>            input_tensor = <span class="hljs-literal">None</span> <span class="hljs-comment"># 后续不再有前向传播，所以设为 None</span><br>            pipeline_communicate(operation=<span class="hljs-string">&#x27;send_backward&#x27;</span>, tensor=input_tensor_grad, device=device, dtype=dtype)<br>        <span class="hljs-keyword">else</span>: <span class="hljs-comment"># 如果不是稳定状态的最后一次迭代</span><br>            <span class="hljs-comment"># 双向通信：发送当前反向计算得到的梯度 (B)，接收下一个微批次的前向输入 (F)</span><br>            <span class="hljs-comment"># - send_tensor=input_tensor_grad: 将当前反向计算得到的输入梯度发送给前一个阶段。</span><br>            <span class="hljs-comment"># - recv_shapes=tensor_shapes: 准备从前一个阶段接收下一个微批次的前向激活值。</span><br>            <span class="hljs-comment"># 这个接收到的 input_tensor 将用于下一次 1F1B 循环的 _forward_step。</span><br>            input_tensor = bidirectional_pipeline_communicate(operation=<span class="hljs-string">&#x27;send_bwd_recv_fwd&#x27;</span>, send_tensor=input_tensor_grad, recv_shapes=tensor_shapes, device=device, dtype=dtype)<br><br>    <span class="hljs-comment"># === 冷却（Cooldown）反向传播阶段 === (L250-L258)</span><br>    <span class="hljs-comment"># 这个循环处理在预热阶段存入流水线、但在稳定状态未来得及进行反向传播的微批次。</span><br>    <span class="hljs-comment"># 循环次数等于预热阶段的微批次数。</span><br>    <span class="hljs-keyword">for</span> ith_warmup_microbatches <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_warmup_microbatches):<br>        <span class="hljs-keyword">if</span> requires_grad_sync:<br>            <span class="hljs-comment"># 梯度同步控制：只在整个梯度累积周期的最后一次反向传播时才同步。</span><br>            <span class="hljs-comment"># 这通常发生在冷却阶段的最后一次迭代，对于那些 num_warmup_microbatches &gt; 0 的 rank。</span><br>            model.require_backward_grad_sync = (ith_warmup_microbatches == num_warmup_microbatches - <span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-comment"># 取出预热阶段存储的输入和输出张量</span><br>        input_tensor, output_tensor = input_tensors.pop(<span class="hljs-number">0</span>), output_tensors.pop(<span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># 从下一个阶段接收反向梯度</span><br>        output_tensor_grad = pipeline_communicate(operation=<span class="hljs-string">&#x27;recv_backward&#x27;</span>, shapes=tensor_shapes, device=device, dtype=dtype)<br>        <span class="hljs-comment"># 执行反向传播</span><br>        input_tensor_grad = model.backward(input_tensor, output_tensor, output_tensor_grad)<br>        <span class="hljs-comment"># 将计算得到的输入梯度发送给前一个阶段</span><br>        pipeline_communicate(operation=<span class="hljs-string">&#x27;send_backward&#x27;</span>, tensor=input_tensor_grad, device=device, dtype=dtype)<br><br>    <span class="hljs-keyword">return</span> logging_loss <span class="hljs-comment"># 返回累积的损失值</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>Picotron-Tutorial</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Picotron-Tutorial】数据并行</title>
    <link href="/2025/06/14/Picotron-Tutorial%20%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C/"/>
    <url>/2025/06/14/Picotron-Tutorial%20%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C/</url>
    
    <content type="html"><![CDATA[<h1 id="原生数据并行"><a href="#原生数据并行" class="headerlink" title="原生数据并行"></a>原生数据并行</h1><h2 id="理论分析"><a href="#理论分析" class="headerlink" title="理论分析"></a>理论分析</h2><p>在原生的数据并行中，每个数据并行的组都会自己处理自己的数据，这带来的一个问题在于我们需要及时同步训练过程中的梯度以及优化器的状态。</p><p>最原生的方法就是我们在前向传播后，在对每一个层进行反向传播后进行一次同步，如下图所示。由于梯度得到了及时的同步，所有优化器的状态自然也就会变得相同。</p><p><img src="/2025/06/14/Picotron-Tutorial%20%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C/image-1.png"></p><h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><ol><li>修改<code>dataloader</code>为分布式，从而使得每个dp进程每次获取到的数据batch是不相同的，其主要修改是加入<code>DistributedSampler</code>：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">self.sampler = DistributedSampler(<br>    self.tokenized_dataset, <br>    num_replicas=pgm.process_group_manager.dp_world_size, <br>    rank=pgm.process_group_manager.dp_rank, <br>    seed=seed,<br>    shuffle=<span class="hljs-literal">False</span><br>)<br><br><span class="hljs-built_in">super</span>().__init__(<br>    self.tokenized_dataset,<br>    batch_size=micro_batch_size,<br>    collate_fn=self.collate_batch, <br>    pin_memory=<span class="hljs-literal">True</span>, <br>    num_workers=num_workers, <br>    sampler=self.sampler,<br>    shuffle=<span class="hljs-literal">False</span>,<br>)<br></code></pre></td></tr></table></figure><ul><li>对于原本的model需要包裹一个DataParallelNaive，即：<code>model = DataParallelNaive(model)</code>，这一层包裹会给每一个需要计算梯度的参数注册一个勾子函数，该函数的作用是如果model的<code>require_backward_grad_sync=true</code>，那么就会进行一次all_reduce获取到其他进程上的参数，然后进行平均，得到平均参数，如下：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">### begin Data Parallel (naive)</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DataParallelNaive</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, module</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.module = module<br>        <span class="hljs-comment"># whether to synchronize gradients during backward pass. Set to False when using gradient accumulation</span><br>        self.require_backward_grad_sync = <span class="hljs-literal">True</span><br>        self.register_backward_hook(self._allreduce_grads)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, *inputs, **kwargs</span>):<br>        <span class="hljs-keyword">return</span> self.module(*inputs, **kwargs)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">register_backward_hook</span>(<span class="hljs-params">self, hook</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Registers a backward hook for all parameters of the model that require gradients.&quot;&quot;&quot;</span> <br>        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.module.parameters():<br>            <span class="hljs-keyword">if</span> p.requires_grad <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span>:<br>                p.register_hook(hook)<br>                <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_allreduce_grads</span>(<span class="hljs-params">self, grad</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Performs an all-reduce operation to synchronize gradients across multiple processes.&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># No synchronization needed during gradient accumulation, except at the final accumulation step.</span><br>        <span class="hljs-keyword">if</span> self.require_backward_grad_sync:<br>            dist.all_reduce(grad, op=dist.ReduceOp.SUM, group=pgm.process_group_manager.dp_group)<br>            grad /= pgm.process_group_manager.dp_world_size<br>        <span class="hljs-keyword">return</span> grad<br><span class="hljs-comment">### end Data Parallel (naive)</span><br></code></pre></td></tr></table></figure><ul><li>修改原本的训练进程，添加一段对于<code>model.require_backward_grad_sync</code>赋值的控制，使得在最后一个dataloader.grad_acc_steps时会进行梯度平均。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> requires_grad_sync:<br>            model.require_backward_grad_sync = (i == dataloader.grad_acc_steps - <span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><h1 id="带bucket的数据并行"><a href="#带bucket的数据并行" class="headerlink" title="带bucket的数据并行"></a>带bucket的数据并行</h1><h2 id="理论分析-1"><a href="#理论分析-1" class="headerlink" title="理论分析"></a>理论分析</h2><p>对于原生的数据并行，其最大的问题在于每层进行一次反向传播的时候都需要一个网络传输，这导致整体的速度被拖慢了。所以有提出带bucket带数据并行的方案，其特点在于将多层作为一个bucket，然后在反向传播时，只有当bucket中的所有反向传播都结束的时候才进行一次同步，同时设置该同步为异步的同步，这样就不会阻塞整体的反向传播的进程了。</p><p><img src="/2025/06/14/Picotron-Tutorial%20%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C/image.png"></p><h2 id="代码分析-1"><a href="#代码分析-1" class="headerlink" title="代码分析"></a>代码分析</h2><p>主要是需要构建3个类：</p><ul><li><p><strong>Bucket&#x20;</strong></p></li><li><p><strong>BucketManager</strong></p></li><li><p><strong>DataParallelBucket</strong></p></li></ul><h3 id="Bucket"><a href="#Bucket" class="headerlink" title="Bucket"></a>Bucket</h3><p>Bucket 类代表一个梯度桶，它管理一组模型参数及其对应的梯度，并负责这些梯度的同步。</p><ul><li><p>其包含了一个grad_data来存储梯度信息，这个grad_data由多个参数的grad拼接而成。</p></li><li><p>然后有一个params_with_grad_ready来记录哪些对应的参数已经完成了梯度计算，并通过一个函数来支持标记params_with_grad_ready。如果对应的参数都完成梯度计算后，它支持通过异步的all-reduce操作来同步梯度，并支持通过wait函数来等待梯度同步完成。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ... existing code ...</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Bucket</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, params: <span class="hljs-type">List</span>[torch.nn.Parameter], grad_data: torch.Tensor, process_group: torch.distributed.ProcessGroup</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># params: 这个桶包含的参数集合。</span><br>        self.params = <span class="hljs-built_in">set</span>(params)<br>        <span class="hljs-comment"># params_with_grad_ready: 记录桶内哪些参数的梯度已经计算完毕并准备好同步。</span><br>        self.params_with_grad_ready = <span class="hljs-built_in">set</span>()<br>        <span class="hljs-comment"># grad_data: 一个预分配的张量，用于存储这个桶内所有参数的梯度。参数的梯度会被拷贝到这个张量中进行 all-reduce。</span><br>        self.grad_data = grad_data<br>        <span class="hljs-comment"># process_group: 用于梯度同步的分布式进程组 (通常是数据并行组)。</span><br>        self.process_group = process_group<br>        self.process_group_size = dist.get_world_size(group=self.process_group) <br>        <span class="hljs-comment"># handle: 异步 all-reduce 操作的句柄，用于后续等待操作完成。</span><br>        self.handle = <span class="hljs-literal">None</span><br>        <br>        self.reset() <span class="hljs-comment"># 初始化状态</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sync_gradient</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;发起一个异步的 all-reduce 操作来同步梯度。&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">assert</span> self.handle <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-comment"># 确保没有正在进行的同步操作</span><br>        <span class="hljs-comment"># 在 all-reduce 求和之前，先将梯度除以进程组大小，这样 all-reduce 之后就直接是平均梯度。</span><br>        self.grad_data /= self.process_group_size<br>        <span class="hljs-comment"># 发起异步 all-reduce，对 grad_data 进行求和操作。</span><br>        self.handle = dist.all_reduce(self.grad_data, group=self.process_group, async_op=<span class="hljs-literal">True</span>)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;重置桶的状态，通常在梯度同步完成后调用。&quot;&quot;&quot;</span><br>        self.handle = <span class="hljs-literal">None</span> <span class="hljs-comment"># 清除句柄</span><br>        self.params_with_grad_ready.clear() <span class="hljs-comment"># 清空已准备好的参数集合</span><br>        self.grad_data.zero_() <span class="hljs-comment"># 将梯度存储张量清零，为下一次迭代做准备</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">wait</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;等待 all-reduce 操作完成。&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">assert</span> self.handle <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;You should launch an allreduce operation before waiting for it to finish&quot;</span><br>        self.handle.wait() <span class="hljs-comment"># 阻塞等待异步操作完成</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mark_param_as_ready</span>(<span class="hljs-params">self, param: torch.nn.Parameter</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;标记一个参数的梯度已准备好进行同步。当桶内所有参数都准备好时，启动梯度同步。&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">assert</span> param <span class="hljs-keyword">in</span> self.params <span class="hljs-keyword">and</span> param <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> self.params_with_grad_ready<br>        self.params_with_grad_ready.add(param)<br>        <span class="hljs-comment"># 如果桶内所有参数的梯度都已准备好</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(self.params_with_grad_ready) == <span class="hljs-built_in">len</span>(self.params):<br>            self.sync_gradient() <span class="hljs-comment"># 则开始同步这个桶的梯度</span><br><span class="hljs-comment"># ... existing code ...</span><br></code></pre></td></tr></table></figure><h3 id="BucketManager"><a href="#BucketManager" class="headerlink" title="BucketManager"></a><strong>BucketManager</strong></h3><p>BucketManager 负责将模型的所有参数划分到多个 Bucket 中，并管理这些桶。</p><ul><li><p>用户需要指定每个桶的最大容量，以元素数量记。</p></li><li><p>会遍历模型中的各个参数，尝试将其放入桶中，如果放入不了就再新建一个桶放入</p></li><li><p>然后为每个桶创建一个连续内存来存储梯度，并将其与参数的grad进行映射，保障两个修改是同步的</p></li><li><p>支持标记param梯度计算完毕并将消息传递给对应的桶</p></li><li><p>支持等待所有的桶都同步完成</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ... existing code ...</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BucketManager</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, params: <span class="hljs-type">List</span>[torch.nn.Parameter], process_group: torch.distributed.ProcessGroup, bucket_size: <span class="hljs-built_in">int</span>, grad_type: torch.dtype = torch.float32</span>) -&gt; <span class="hljs-literal">None</span>:<br>        self.params = <span class="hljs-built_in">list</span>(params) <span class="hljs-comment"># 模型的所有参数</span><br>        self.buckets = [] <span class="hljs-comment"># 存储所有 Bucket 对象的列表</span><br>        self.process_group = process_group<br>        self.process_group_size = dist.get_world_size(group=self.process_group)<br>        <span class="hljs-comment"># params_to_bucket_location: 一个字典，映射每个参数到它所在的桶的索引以及在桶内梯度张量中的位置 (start, end, bucket_idx)。</span><br>        self.params_to_bucket_location = &#123;&#125;<br>        self.bucket_size = bucket_size <span class="hljs-comment"># 用户指定的每个桶的最大容量 (以元素数量计)。</span><br>        self.bucket_sizes = <span class="hljs-literal">None</span> <span class="hljs-comment"># 实际每个桶的大小</span><br>        self.grad_data_list = [] <span class="hljs-comment"># 存储每个桶的梯度数据张量 (grad_data) 的列表。</span><br>        self.grad_type = grad_type <span class="hljs-comment"># 梯度的数据类型，通常是 float32 以保证精度。</span><br>        <br>        self._initialize_buckets() <span class="hljs-comment"># 初始化分桶逻辑</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_initialize_buckets</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;根据 bucket_size 将模型参数划分到不同的桶中。&quot;&quot;&quot;</span><br>        cur_bucket_size = <span class="hljs-number">0</span> <br>        cur_bucket_idx = <span class="hljs-number">0</span><br>        <br>        <span class="hljs-comment"># 遍历所有需要梯度的参数，将它们分配到桶中</span><br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> self.params:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> param.requires_grad:<br>                <span class="hljs-keyword">continue</span><br>            <br>            num_elements = param.numel() <span class="hljs-comment"># 参数的元素数量</span><br>            <span class="hljs-comment"># 如果当前桶是空的，或者当前参数加入后会超过桶的容量，则创建一个新桶</span><br>            <span class="hljs-keyword">if</span> cur_bucket_size == <span class="hljs-number">0</span>: <span class="hljs-comment"># 新桶的第一个参数</span><br>                self.params_to_bucket_location[param] = (<span class="hljs-number">0</span>, num_elements, cur_bucket_idx)<br>                cur_bucket_size = num_elements<br>            <span class="hljs-keyword">elif</span> cur_bucket_size + num_elements &gt; self.bucket_size: <span class="hljs-comment"># 当前桶放不下，开新桶</span><br>                cur_bucket_idx += <span class="hljs-number">1</span><br>                self.params_to_bucket_location[param] = (<span class="hljs-number">0</span>, num_elements, cur_bucket_idx)<br>                cur_bucket_size = num_elements<br>            <span class="hljs-keyword">else</span>: <span class="hljs-comment"># 可以放入当前桶</span><br>                self.params_to_bucket_location[param] = (cur_bucket_size, cur_bucket_size + num_elements, cur_bucket_idx)<br>                cur_bucket_size += num_elements<br><br>        <span class="hljs-comment"># 收集每个桶的实际大小和包含的参数</span><br>        num_total_buckets = cur_bucket_idx + <span class="hljs-number">1</span><br>        actual_bucket_sizes = [<span class="hljs-number">0</span>] * num_total_buckets<br>        buckets_to_params_list = [[] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_total_buckets)]<br>        <span class="hljs-keyword">for</span> param, (start, end, idx) <span class="hljs-keyword">in</span> self.params_to_bucket_location.items():<br>            actual_bucket_sizes[idx] = <span class="hljs-built_in">max</span>(actual_bucket_sizes[idx], end) <span class="hljs-comment"># 桶的实际大小是最后一个参数的结束位置</span><br>            buckets_to_params_list[idx].append(param)<br>        <br>        self.bucket_sizes = actual_bucket_sizes<br><br>        <span class="hljs-comment"># 为每个桶创建梯度存储张量 (grad_data) 和 Bucket 对象</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(self.bucket_sizes)):<br>            <span class="hljs-comment"># 为每个桶预分配一块连续的内存来存储梯度</span><br>            grad_tensor = torch.zeros(self.bucket_sizes[i], dtype=self.grad_type, device=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>            self.grad_data_list.append(grad_tensor)<br>            self.buckets.append(Bucket(buckets_to_params_list[i], grad_tensor, self.process_group))<br>        <br>        <span class="hljs-comment"># 为每个参数创建一个指向其对应桶中梯度存储区视图的 &#x27;main_grad&#x27; 属性。</span><br>        <span class="hljs-comment"># 参数的梯度会先累加到这个 &#x27;main_grad&#x27; 中。</span><br>        <span class="hljs-comment"># 注意这里是倒序遍历参数，这与 PyTorch 反向传播计算梯度的顺序有关，</span><br>        <span class="hljs-comment"># 使得参数的梯度视图 (param.main_grad) 在其梯度实际计算出来之前就被创建。</span><br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> self.params[::-<span class="hljs-number">1</span>]: <br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> param.requires_grad:<br>                <span class="hljs-keyword">continue</span><br>            data_start_index, data_end_index, bucket_id = self.params_to_bucket_location[param]<br>            <span class="hljs-comment"># param.main_grad 是一个视图 (view)，它指向 self.grad_data_list[bucket_id] 中的特定区域。</span><br>            <span class="hljs-comment"># 对 param.main_grad 的修改会直接反映在 grad_data_list[bucket_id] 上。</span><br>            param.main_grad = self._get_view_from_tensor(self.grad_data_list[bucket_id], param.shape, data_start_index, data_end_index)<br>            <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_view_from_tensor</span>(<span class="hljs-params">self, tensor: torch.Tensor, shape: torch.Size, start: <span class="hljs-built_in">int</span>, end: <span class="hljs-built_in">int</span></span>) -&gt; torch.Tensor:<br>        <span class="hljs-string">&quot;&quot;&quot;从一个大张量中获取一个特定形状的视图。&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> tensor[start:end].view(shape)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;重置所有桶的状态。&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">for</span> bucket <span class="hljs-keyword">in</span> self.buckets:<br>            bucket.reset()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">wait</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;等待所有桶的梯度同步完成。&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">for</span> bucket <span class="hljs-keyword">in</span> self.buckets:<br>            <span class="hljs-keyword">if</span> bucket.handle <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>: <span class="hljs-comment"># 只等待已经启动了 all_reduce 的桶</span><br>                 bucket.wait()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mark_param_as_ready</span>(<span class="hljs-params">self, param: torch.nn.Parameter</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;标记一个参数的梯度已准备好，并通知其所在的桶。&quot;&quot;&quot;</span><br>        bucket_idx = self.params_to_bucket_location[param][<span class="hljs-number">2</span>]<br>        self.buckets[bucket_idx].mark_param_as_ready(param)<br><span class="hljs-comment"># ... existing code ...</span><br></code></pre></td></tr></table></figure><h3 id="DataParallelBucket"><a href="#DataParallelBucket" class="headerlink" title="DataParallelBucket"></a>DataParallelBucket</h3><p>这是梯度分桶数据并行策略的顶层封装，它继承自 nn.Module ，可以像普通的 PyTorch模块一样使用。它主要用来包装原始的model。</p><ul><li><p>它负责初始化bucketManager</p></li><li><p>给各个参数注册一个勾子函数，其负责</p><ul><li><p>累加梯度到main_grad</p></li><li><p>如果是acc_grad中的最后一个计算，就：</p><ul><li><p>注册一个post_backward函数，该函数会在在整个反向传播结束后被调用，其负责等待桶中所有的梯度同步完成，然后将同步后的梯度 (存储在 param.main_grad) 复制回 param.grad，以便优化器使用。</p></li><li><p>还会告诉bucketManager参数已经准备好，从而让bucketManager判断是否需要开始收集各个参数的grad</p></li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ... existing code ...</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DataParallelBucket</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, module, bucket_cap_mb=<span class="hljs-number">25</span>, grad_type = torch.float32</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.module = module <span class="hljs-comment"># 被包装的原始模型</span><br>        self.require_backward_grad_sync = <span class="hljs-literal">True</span> <span class="hljs-comment"># 控制是否进行梯度同步，用于梯度累积</span><br>        <br>        <span class="hljs-comment"># 计算每个桶的大小 (以元素数量计)</span><br>        <span class="hljs-comment"># grad_size: 假设梯度是 bfloat16 (2字节) 或 float32 (4字节)。这里代码写的是2，对应bfloat16</span><br>        <span class="hljs-comment"># bucket_cap_mb: 用户指定的桶容量上限 (MB)</span><br>        grad_element_size = torch.tensor([], dtype=grad_type).element_size() <span class="hljs-comment"># 获取梯度类型对应的字节数</span><br>        bucket_size_in_elements = bucket_cap_mb * <span class="hljs-number">1024</span> * <span class="hljs-number">1024</span> // grad_element_size<br>        <br>        self.bucket_manager = BucketManager(module.parameters(), pgm.process_group_manager.dp_group, bucket_size_in_elements, grad_type)<br>        self.register_backward_hook() <span class="hljs-comment"># 注册反向传播钩子</span><br>        self._post_backward_callback_set = <span class="hljs-literal">False</span> <span class="hljs-comment"># 标记是否已经注册了 post_backward 回调</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, *inputs, **kwargs</span>):<br>        <span class="hljs-comment"># 前向传播直接调用原始模块</span><br>        <span class="hljs-keyword">return</span> self.module(*inputs, **kwargs)<br><br>    <span class="hljs-comment"># backward 和 get_flops 方法是可选的，取决于原始模块是否需要它们</span><br>    <span class="hljs-comment"># def backward(self, input_tensor, output_tensor, output_tensor_grad):</span><br>    <span class="hljs-comment">#     return self.module.backward(input_tensor, output_tensor, output_tensor_grad)</span><br>    <br>    <span class="hljs-comment"># def get_flops(self, *args, **kwargs):</span><br>    <span class="hljs-comment">#     return self.module.get_flops(*args, **kwargs)</span><br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">register_backward_hook</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        为每个需要梯度的参数注册一个钩子 (hook)。</span><br><span class="hljs-string">        这个钩子会在该参数的梯度计算完成后被调用。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.grad_accs = [] <span class="hljs-comment"># 存储梯度累加器函数，防止被垃圾回收</span><br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> self.module.parameters():<br>            <span class="hljs-keyword">if</span> param.requires_grad:<br>                <span class="hljs-comment"># param_tmp.grad_fn.next_functions[0][0] 是获取参数对应的梯度累加器节点 (AccumulateGrad object)</span><br>                param_tmp = param.expand_as(param) <span class="hljs-comment"># 确保 param_tmp 有 grad_fn</span><br>                grad_acc_fn = param_tmp.grad_fn.next_functions[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br>                <span class="hljs-comment"># 为梯度累加器节点注册钩子</span><br>                grad_acc_fn.register_hook(self._make_param_hook(param, self.bucket_manager))<br>                self.grad_accs.append(grad_acc_fn)<br>                <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_param_hook</span>(<span class="hljs-params">self, param: torch.nn.Parameter, bucket_manager: BucketManager</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;创建一个参数特定的钩子函数。&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">param_hook</span>(<span class="hljs-params">*unused</span>):<br>            <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">            当参数 param 的梯度计算完成后，这个钩子会被调用。</span><br><span class="hljs-string">            1. 将计算得到的 param.grad 累加到 param.main_grad (即桶的梯度存储区)。</span><br><span class="hljs-string">            2. 将 param.grad 清空 (因为梯度已经拷贝到 main_grad)。</span><br><span class="hljs-string">            3. 如果需要同步，则标记该参数已准备好，并可能触发桶的同步。</span><br><span class="hljs-string">            4. 注册一个 _post_backward 回调，确保在整个反向传播完成后执行某些操作。</span><br><span class="hljs-string">            &quot;&quot;&quot;</span><br>            <span class="hljs-keyword">if</span> param.requires_grad:<br>                <span class="hljs-keyword">assert</span> param.grad <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>, <span class="hljs-string">f&quot;Gradient for <span class="hljs-subst">&#123;param.name&#125;</span> is None.&quot;</span> <span class="hljs-comment"># 确保梯度存在</span><br>                <span class="hljs-comment"># 1. 累加梯度到 main_grad (桶的存储区)</span><br>                param.main_grad.add_(param.grad.data) <br>                <span class="hljs-comment"># 2. 清空原始梯度，因为已经复制到 main_grad</span><br>                param.grad = <span class="hljs-literal">None</span> <br>                <br>                <span class="hljs-keyword">if</span> self.require_backward_grad_sync: <span class="hljs-comment"># 如果不是梯度累积的中间步骤</span><br>                    <span class="hljs-comment"># 3. 注册 _post_backward 回调 (如果还没注册的话)</span><br>                    <span class="hljs-comment"># 这个回调会在整个 backward() 调用完成后执行。</span><br>                    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self._post_backward_callback_set:<br>                        torch.autograd.Variable._execution_engine.queue_callback(self._post_backward)<br>                        self._post_backward_callback_set = <span class="hljs-literal">True</span><br>                        <br>                    <span class="hljs-comment"># 4. 标记参数已准备好，通知 BucketManager</span><br>                    bucket_manager.mark_param_as_ready(param) <br>        <span class="hljs-keyword">return</span> param_hook<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_backward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        在整个反向传播过程结束后执行的回调。</span><br><span class="hljs-string">        1. 等待所有桶的梯度同步完成。</span><br><span class="hljs-string">        2. 将同步后的梯度 (存储在 param.main_grad) 复制回 param.grad，以便优化器使用。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 1. 等待所有桶的 all-reduce 操作完成</span><br>        self.bucket_manager.wait()<br>        self._post_backward_callback_set = <span class="hljs-literal">False</span> <span class="hljs-comment"># 重置标记，为下一次 backward 做准备</span><br>        <br>        <span class="hljs-comment"># 2. 将同步并平均后的梯度从 param.main_grad 复制回 param.grad</span><br>        <span class="hljs-comment"># 优化器 (如 AdamW) 会读取 param.grad 来更新参数。</span><br>        <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.module.parameters():<br>            <span class="hljs-keyword">if</span> p.requires_grad:<br>                <span class="hljs-comment"># 需要确保数据类型匹配，优化器通常期望 param.grad 和 param.data 类型一致</span><br>                p.grad = p.main_grad.to(p.dtype) <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;重置 BucketManager 的状态，主要是清零所有桶的梯度。&quot;&quot;&quot;</span><br>        self.bucket_manager.reset()<br><span class="hljs-comment"># ... existing code ...</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>Picotron-Tutorial</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【Picotron-Tutorial】Tensor并行</title>
    <link href="/2025/06/07/Picotron-Tutorial%20Tensor%20Parallel/"/>
    <url>/2025/06/07/Picotron-Tutorial%20Tensor%20Parallel/</url>
    
    <content type="html"><![CDATA[<h1 id="理论分析"><a href="#理论分析" class="headerlink" title="理论分析"></a>理论分析</h1><p>分析的对象 $$Y&#x3D;X@W$$</p><h2 id="列并行"><a href="#列并行" class="headerlink" title="列并行"></a>列并行</h2><p>需要给每个GPU都复制一份X（往往都是早就有了），然后对于W进行列维度的切分。最后每个GPU会有不同列的结果，最后会对其进行all_gather拼接得到结果。</p><p><img src="/2025/06/07/Picotron-Tutorial%20Tensor%20Parallel/image-6.png"></p><h2 id="行并行"><a href="#行并行" class="headerlink" title="行并行"></a>行并行</h2><p>对于行并行，由于W的行数减小了，所以X的列数也要跟着变，所以首先需要将X进行列维度的拆分，划分到各个GPU卡上，然后与W进行相乘，得到的结果再进行all_reduce。</p><p><img src="/2025/06/07/Picotron-Tutorial%20Tensor%20Parallel/image-3.png"></p><h2 id="MLP模块的Tensor并行策略"><a href="#MLP模块的Tensor并行策略" class="headerlink" title="MLP模块的Tensor并行策略"></a>MLP模块的Tensor并行策略</h2><p>以大模型中的MLP模块为例，其结构往往为</p><ol><li><p>矩阵乘</p></li><li><p>Gelu</p></li><li><p>矩阵乘</p></li></ol><p>所以如何设置tensor并行的策略就非常重要。</p><p>首先由于我们希望将gelu操作与一开始的矩阵乘操作放在一起运算，而行并行中最后会通过all_reduce进行一次相加，由于 Gelu(Y_0)+Gelu(Y_1) !&#x3D; Gelu(Y_0+Y_1)，所以行并行并不能满足要求。而列并行中最后只是简单的拼接，所以还是可以做到的。所以一开始我们需要选择列并行。</p><p>然后需要讨论队后一个矩阵乘，我们需要选择什么矩阵并行的方法：</p><ol><li>如果采用列并行，那么我们就需要先进行一次all_gather操作得到结果，然后再broadcast给各个卡，最后再将结果进行all_gather汇聚在一起，注意这里相当于产生了3个通信操作。</li></ol><p><img src="/2025/06/07/Picotron-Tutorial%20Tensor%20Parallel/image-4.png"></p><ul><li>如果采用行并行，那么就不需要中间的进行结果汇聚的操作了，直接进行行并行的计算然后再进行all_reduce即可。注意这样做的话我们就只需要一次通信即可。</li></ul><p><img src="/2025/06/07/Picotron-Tutorial%20Tensor%20Parallel/image-5.png"></p><p>综上，最后采取列并行+行并行的矩阵并行运算的方法才是最合适的方法。</p><h2 id="Attention模块的Tensor并行策略"><a href="#Attention模块的Tensor并行策略" class="headerlink" title="Attention模块的Tensor并行策略"></a>Attention模块的Tensor并行策略</h2><p>attention模块内主要的计算步骤如下：</p><ol><li>与W_q, W_k, W_v进行矩阵乘得到Q、K、V</li></ol><p><img src="/2025/06/07/Picotron-Tutorial%20Tensor%20Parallel/image.png"></p><ul><li>得到各个注意力头的attention输出</li></ul><p><img src="/2025/06/07/Picotron-Tutorial%20Tensor%20Parallel/image-1.png"></p><ul><li>拼接各个attention，然后与W矩阵相乘得到最终的attention</li></ul><p><img src="/2025/06/07/Picotron-Tutorial%20Tensor%20Parallel/image-2.png"></p><p>其实整体与MLP模块的分析类似，我们会先采取列并行的方式来划分W_q, W_k, W_v，然后采用行并行的方式来划分W_o，这样最后计算的时候就不需要汇总了，而是直接计算即可。</p><h2 id="Embedding的Tensor并行策略"><a href="#Embedding的Tensor并行策略" class="headerlink" title="Embedding的Tensor并行策略"></a>Embedding的Tensor并行策略</h2><p>Embedding层的主要作用是通过各个token的id去embedding矩阵中获取对应的行作为输入。</p><p>所以在进行tensor并行的时候，只能对embedding矩阵采取行并行的切分方法，但是注意我们不会对输入进行切分，具体在使用的时候还会有一些其他的注意事项。</p><ol><li><p>由于每块GPU只有不同id范围的embedding，所以我们首先需要将各个token对id减去embedding矩阵的起始位置，得到新的坐标</p></li><li><p>然后得到所有不在当前范围内的token的坐标，并将这些坐标mask成0</p></li><li><p>然后依据一般的embedding获取的规则去获取所有token对应的embeddings</p></li><li><p>然后再将所有超出范围的token的坐标对应的embeddings层化为0</p></li><li><p>最后将各个GPU上的embeddings层进行all_reduce即可得到最后的结果</p></li></ol><h1 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h1><h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>首先调用<code>apply_tensor_parallel</code>函数来替换model中的部分层为矩阵并行的层。这里是直接写死各个层需要用什么并行方式。其整体来说就是先进行列并行然后再进行行并行，从而节省了中间的通信操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">apply_tensor_parallel</span>(<span class="hljs-params">model</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_replace_module</span>(<span class="hljs-params">_module, _linear_proj_name, _style, args=&#123;&#125;</span>):<br>        <span class="hljs-keyword">assert</span> _style <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;column&quot;</span>, <span class="hljs-string">&quot;row&quot;</span>, <span class="hljs-string">&#x27;vocab&#x27;</span>]<br>        linear_layer = <span class="hljs-built_in">getattr</span>(_module, _linear_proj_name)<br>        <br>        <span class="hljs-keyword">if</span> _style == <span class="hljs-string">&quot;column&quot;</span>:<br>            new_linear_layer = ColumnParallelLinear(<br>                in_features=linear_layer.in_features,<br>                out_features=linear_layer.out_features,<br>                bias=linear_layer.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>,<br>                gather_output=args.get(<span class="hljs-string">&quot;gather_output&quot;</span>, <span class="hljs-literal">False</span>)<br>            )<br>        <span class="hljs-keyword">elif</span> _style == <span class="hljs-string">&quot;row&quot;</span>:<br>            new_linear_layer = RowParallelLinear(<br>                in_features=linear_layer.in_features,<br>                out_features=linear_layer.out_features,<br>                bias=linear_layer.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>,<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            new_linear_layer = VocabParallelEmbedding(<br>                num_embeddings=linear_layer.num_embeddings,<br>                embedding_dim=linear_layer.embedding_dim,<br>            )<br>        <span class="hljs-built_in">setattr</span>(_module, _linear_proj_name, new_linear_layer)<br><br>    module_linear_name_stype_mapping_list = [<br>        (<span class="hljs-string">&quot;attention&quot;</span>, <span class="hljs-string">&quot;q_proj&quot;</span>, <span class="hljs-string">&quot;column&quot;</span>),<br>        (<span class="hljs-string">&quot;attention&quot;</span>, <span class="hljs-string">&quot;k_proj&quot;</span>, <span class="hljs-string">&quot;column&quot;</span>),<br>        (<span class="hljs-string">&quot;attention&quot;</span>, <span class="hljs-string">&quot;v_proj&quot;</span>, <span class="hljs-string">&quot;column&quot;</span>),<br>        (<span class="hljs-string">&quot;attention&quot;</span>, <span class="hljs-string">&quot;out_proj&quot;</span>, <span class="hljs-string">&quot;row&quot;</span>),<br>        (<span class="hljs-string">&quot;mlp&quot;</span>, <span class="hljs-string">&quot;up_proj&quot;</span>, <span class="hljs-string">&quot;column&quot;</span>),<br>        (<span class="hljs-string">&quot;mlp&quot;</span>, <span class="hljs-string">&quot;gate_proj&quot;</span>, <span class="hljs-string">&quot;column&quot;</span>),<br>        (<span class="hljs-string">&quot;mlp&quot;</span>, <span class="hljs-string">&quot;down_proj&quot;</span>, <span class="hljs-string">&quot;row&quot;</span>),<br>    ]<br><br>    <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> model.decoder_layers:<br>        <span class="hljs-keyword">for</span> module_name, linear_proj_name, style <span class="hljs-keyword">in</span> module_linear_name_stype_mapping_list:<br>            _replace_module(<span class="hljs-built_in">getattr</span>(layer, module_name), linear_proj_name, style)<br>            <br>    _replace_module(model, <span class="hljs-string">&quot;embedding&quot;</span>, <span class="hljs-string">&quot;vocab&quot;</span>)<br>    _replace_module(model, <span class="hljs-string">&quot;final_proj&quot;</span>, <span class="hljs-string">&quot;column&quot;</span>, args=&#123;<span class="hljs-string">&quot;gather_output&quot;</span>: <span class="hljs-literal">True</span>&#125;)<br>    <br>    <span class="hljs-keyword">return</span> model<br><br></code></pre></td></tr></table></figure><h2 id="列并行实现"><a href="#列并行实现" class="headerlink" title="列并行实现"></a>列并行实现</h2><p>其在初始化参数的时候会先按照原先的形状进行初始化，然后再将其按照并行维度进行划分，然后取自己rank对应的数据。需要注意对于矩阵乘，pytorch实现的时候是用X@W^T，所以对于列并行，实际上是会对W进行行并行。</p><p>收集结果的时候是用all_gather</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ColumnParallelLinear</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_features: <span class="hljs-built_in">int</span>, out_features: <span class="hljs-built_in">int</span>, bias: <span class="hljs-built_in">bool</span>, gather_output: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span></span>):<br>        <br>        <span class="hljs-built_in">super</span>(ColumnParallelLinear, self).__init__()<br><br>        self.tp_world_size = pgm.process_group_manager.tp_world_size<br>        self.tp_rank = pgm.process_group_manager.tp_rank <br><br>        self.in_features = in_features<br>        self.out_features = out_features<br>        <span class="hljs-keyword">assert</span> out_features % self.tp_world_size == <span class="hljs-number">0</span>, <span class="hljs-string">&quot;Hidden dimension must be divisible by the tensor parallel world size&quot;</span><br>        self.output_size_per_partition = out_features // self.tp_world_size<br>        self.gather_output = gather_output<br>     <br>        <span class="hljs-comment"># Note: torch.nn.functional.linear performs XW^T + b so we exchange the order of dimensions</span><br>        self.weight = nn.Parameter(torch.Tensor(self.output_size_per_partition, self.in_features)) <span class="hljs-comment"># W_i</span><br>        <span class="hljs-keyword">if</span> bias:<br>            self.bias = nn.Parameter(torch.Tensor(self.output_size_per_partition))<br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                self.bias.zero_()<br>        <span class="hljs-keyword">else</span>:<br>            self.register_parameter(<span class="hljs-string">&quot;bias&quot;</span>, <span class="hljs-literal">None</span>)<br><br>        self.reset_parameters()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset_parameters</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># Initialize weight tensor with the default initialization method used for nn.Linear in PyTorch</span><br>        <span class="hljs-keyword">if</span> self.tp_world_size == <span class="hljs-number">1</span>:<br>            <span class="hljs-comment">#  U(-sqrt(k), sqrt(k))</span><br>            k = <span class="hljs-number">1</span> / self.weight.size(<span class="hljs-number">1</span>)<br>            bound = math.sqrt(k)<br>            torch.nn.init.uniform_(self.weight, -bound, bound)<br>            <span class="hljs-keyword">return</span><br>    <br>        <span class="hljs-comment"># When TP &gt; 1, Initialize master weight</span><br>        master_weight = torch.empty(self.out_features, self.in_features, dtype=self.weight.dtype, requires_grad=<span class="hljs-literal">False</span>)<br>        <span class="hljs-comment"># Calculate bound based on master weight&#x27;s input dimension. U(-sqrt(k), sqrt(k))</span><br>        k = <span class="hljs-number">1</span> / master_weight.size(<span class="hljs-number">1</span>)<br>        bound = math.sqrt(k)<br>        torch.nn.init.uniform_(master_weight, -bound, bound)<br>        <br>        <span class="hljs-comment"># Split the model into size of self.output_size_per_partitio and take the corresponding partition</span><br>        weight_list = torch.split(master_weight, self.output_size_per_partition, dim=<span class="hljs-number">0</span>)<br>        self.weight.data = weight_list[self.tp_rank].contiguous()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        input_parallel = Copy.apply(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-comment"># XW_i^T + b, output is Y_i</span><br>        output = F.linear(input_parallel, self.weight, self.bias)<br>        <span class="hljs-keyword">if</span> self.gather_output:<br>            output = Gather.apply(output)<br>        <span class="hljs-keyword">return</span> output<br><br></code></pre></td></tr></table></figure><h2 id="行并行实现"><a href="#行并行实现" class="headerlink" title="行并行实现"></a>行并行实现</h2><p>与列并行基本一致，就是在实现的时候是对W的列进行划分，收集结果的时候是用All_reduce。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RowParallelLinear</span>(nn.Module):<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_features: <span class="hljs-built_in">int</span>, out_features: <span class="hljs-built_in">int</span>, bias: <span class="hljs-built_in">bool</span></span>):<br>        <span class="hljs-built_in">super</span>(RowParallelLinear, self).__init__()<br><br>        self.tp_world_size = pgm.process_group_manager.tp_world_size<br>        self.tp_rank = pgm.process_group_manager.tp_rank <br><br>        self.in_features = in_features<br>        self.out_features = out_features<br>        <span class="hljs-keyword">assert</span> in_features % self.tp_world_size == <span class="hljs-number">0</span>, <span class="hljs-string">&quot;Hidden dimension must be divisible by the tensor parallel world size&quot;</span><br>        self.input_size_per_partition = in_features // self.tp_world_size<br><br>        self.weight = nn.Parameter(torch.Tensor(self.out_features, self.input_size_per_partition))<br>        <span class="hljs-keyword">if</span> bias:<br>            self.bias = nn.Parameter(torch.Tensor(self.out_features))<br>            <span class="hljs-comment"># Always initialize bias to zero.</span><br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                self.bias.zero_()<br>        <span class="hljs-keyword">else</span>:<br>            self.register_parameter(<span class="hljs-string">&quot;bias&quot;</span>, <span class="hljs-literal">None</span>)<br><br>        self.reset_parameters()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset_parameters</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># Initialize weight tensor with the default initialization method used for nn.Linear in PyTorch</span><br>        <span class="hljs-keyword">if</span> self.tp_world_size == <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># U(-sqrt(k), sqrt(k))</span><br>            k = <span class="hljs-number">1</span> / self.weight.size(<span class="hljs-number">1</span>)<br>            bound = math.sqrt(k)<br>            torch.nn.init.uniform_(self.weight, -bound, bound)<br>            <span class="hljs-keyword">return</span><br>    <br>        <span class="hljs-comment"># When TP &gt; 1, Initialize master weight</span><br>        master_weight = torch.empty(self.out_features, self.in_features, dtype=self.weight.dtype, requires_grad=<span class="hljs-literal">False</span>)<br>        <span class="hljs-comment"># Calculate bound based on master weight&#x27;s input dimension. U(-sqrt(k), sqrt(k))</span><br>        k = <span class="hljs-number">1</span> / master_weight.size(<span class="hljs-number">1</span>)<br>        bound = math.sqrt(k)    <br>        torch.nn.init.uniform_(master_weight, -bound, bound)<br>        <br>        <span class="hljs-comment"># Split the model into size of self.input_size_per_partition and take the corresponding partition</span><br>        weight_list = torch.split(master_weight, self.input_size_per_partition, dim=<span class="hljs-number">1</span>)<br>        self.weight.data = weight_list[self.tp_rank].contiguous()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        <span class="hljs-comment"># X_i * W_i^T + b</span><br>        output_parallel = F.linear(<span class="hljs-built_in">input</span>, self.weight)<br>        <span class="hljs-comment"># All-reduce across all the partitions.</span><br>        output = Reduce.apply(output_parallel)<br>        <span class="hljs-keyword">return</span> output <span class="hljs-keyword">if</span> self.bias <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> output + self.bias<br><br></code></pre></td></tr></table></figure><h2 id="Embedding并行"><a href="#Embedding并行" class="headerlink" title="Embedding并行"></a>Embedding并行</h2><p>与之前谈论的类似，先得到input_mask，然后再将input id减去start id得到masked_input，然后将input_mask对应位置的mask_input标记为0，得到embedding的结果后，再将mask_input对应位置标记为0，最后进行reduce得到结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VocabParallelEmbedding</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        num_embeddings: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        embedding_dim: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        padding_idx: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        max_norm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">float</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        norm_type: <span class="hljs-built_in">float</span> = <span class="hljs-number">2.0</span>,</span><br><span class="hljs-params">        scale_grad_by_freq: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        sparse: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span></span><br><span class="hljs-params">    </span>):<br>        <br>        <span class="hljs-built_in">super</span>(VocabParallelEmbedding, self).__init__()<br><br>        self.tp_world_size = pgm.process_group_manager.tp_world_size<br>        self.tp_rank = pgm.process_group_manager.tp_rank<br><br>        self.num_embeddings = num_embeddings<br>        self.embedding_dim = embedding_dim<br>        self.padding_idx = padding_idx<br>        self.max_norm = max_norm<br>        self.norm_type = norm_type<br>        self.scale_grad_by_freq = scale_grad_by_freq<br>        self.sparse = sparse<br>        <span class="hljs-comment"># Divide the weight matrix along the vocaburaly dimension.</span><br>        self.vocab_start_index, self.vocab_end_index = self._vocab_range_from_global_vocab_size(<br>            self.num_embeddings, pgm.process_group_manager.tp_rank, pgm.process_group_manager.tp_world_size<br>        )<br>        self.num_embeddings_per_partition = self.vocab_end_index - self.vocab_start_index<br><br>        self.weight = nn.Parameter(torch.Tensor(self.num_embeddings_per_partition, self.embedding_dim))<br><br>        self.reset_parameters()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_vocab_range_from_global_vocab_size</span>(<span class="hljs-params">self, global_vocab_size: <span class="hljs-built_in">int</span>, rank: <span class="hljs-built_in">int</span>, world_size: <span class="hljs-built_in">int</span></span>):<br>        <span class="hljs-keyword">assert</span> global_vocab_size % world_size == <span class="hljs-number">0</span>, <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;global_vocab_size&#125;</span> is not divisible by <span class="hljs-subst">&#123;world_size&#125;</span>&quot;</span><br>        per_partition_vocab_size = global_vocab_size // world_size<br>        <span class="hljs-comment"># vocab_range_from_per_partition_vocab_size</span><br>        index_f = rank * per_partition_vocab_size<br>        index_l = index_f + per_partition_vocab_size<br>        <span class="hljs-keyword">return</span> index_f, index_l<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset_parameters</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">if</span> self.tp_world_size == <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># Initialize Vocab embedding with N(0, 1)</span><br>            torch.nn.init.normal_(self.weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">1.0</span>)<br>            <span class="hljs-keyword">return</span><br><br>        <span class="hljs-comment"># When TP &gt; 1, Initialize master weight</span><br>        master_weight = torch.empty(self.num_embeddings, self.embedding_dim, dtype=self.weight.dtype, requires_grad=<span class="hljs-literal">False</span>)<br>        torch.nn.init.normal_(master_weight, mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">1.0</span>)<br>        <br>        <span class="hljs-comment"># Split the model into size of self.num_embeddings_per_partition and take the corresponding partition</span><br>        weight_list = torch.split(master_weight, self.num_embeddings_per_partition, dim=<span class="hljs-number">0</span>)<br>        self.weight.data = weight_list[self.tp_rank].contiguous()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Performs an embedding lookup for input tokens in the parallelized embedding layer</span><br><span class="hljs-string">        1. Masks tokens that fall outside the specified vocabulary range and adjusts the input</span><br><span class="hljs-string">        2. Performs embedding lookups for valid tokens, setting embeddings of out-of-vocabulary tokens to zero</span><br><span class="hljs-string">        3. Reduces the embeddings across model parallel GPUs using all-reduce for synchronization</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Build the mask for out-of-vocabulary tokens.</span><br>        input_mask = (<span class="hljs-built_in">input</span> &lt; self.vocab_start_index) | (<span class="hljs-built_in">input</span> &gt;= self.vocab_end_index)<br>        <span class="hljs-comment"># Mask the input.</span><br>        masked_input = <span class="hljs-built_in">input</span>.clone() - self.vocab_start_index<br>        masked_input[input_mask] = <span class="hljs-number">0</span><br>        <span class="hljs-comment"># Get the embeddings for the valid tokens.</span><br>        output_parallel = F.embedding(<br>            masked_input,<br>            self.weight,<br>            self.padding_idx,<br>            self.max_norm,<br>            self.norm_type,<br>            self.scale_grad_by_freq,<br>            self.sparse,<br>        )<br>        <span class="hljs-comment"># Embedding of out-of-vocabulary tokens is set to 0.</span><br>        output_parallel[input_mask, :] = <span class="hljs-number">0.0</span><br>        output = Reduce.apply(output_parallel)<br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>Picotron-Tutorial</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习中反向传播及优化器使用详解</title>
    <link href="/2025/06/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%8F%8A%E4%BC%98%E5%8C%96%E5%99%A8%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/"/>
    <url>/2025/06/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%8F%8A%E4%BC%98%E5%8C%96%E5%99%A8%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h1><h2 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h2><p><strong>参考资料：</strong></p><ul><li><a href="https://wuli.wiki/online/ParDer.html">https://wuli.wiki/online/ParDer.html</a></li></ul><p><strong>简要说明：</strong></p><ul><li><p>对于复合函数，如果要求其对于某一个参数的偏导数，那么就需要把其他参数看为常数，然后求导。</p></li><li><p>几何意义：可以认为是沿着某一个轴的导数。如下图所示，对于曲面 $$z&#x3D;f(x,y)$$对x的偏导就是求这个曲面上的某点对于x轴的斜率。</p></li></ul><p><img src="/2025/06/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%8F%8A%E4%BC%98%E5%8C%96%E5%99%A8%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/image-1.png"></p><h2 id="全微分"><a href="#全微分" class="headerlink" title="全微分"></a>全微分</h2><p><strong>参考资料：</strong></p><ul><li><a href="https://wuli.wiki/online/TDiff.html">https://wuli.wiki/online/TDiff.html</a></li></ul><p><strong>简要说明：</strong></p><ul><li><p>对于多元函数，若每个参数都进行微小的变化，那么整个函数的z变化就是：$$\mathrm{d}z&#x3D;\sum_{i&#x3D;1}^N\frac{\partial f}{\partial x_i}\mathrm{~d}x_i$$</p></li><li><p>几何意义：对于曲面 $$z&#x3D;f(x,y)$$，若它在 $$(x_0, y_0)$$附近的曲面光滑，那么考虑一个足够小的区域，可以把这附近近似为一个平面，若在$x$与$y$方向都都进行小幅的移动，则$z$的增量就等于先在$x$方向进行移动的增量再加上$y$方向移动的增量。</p></li></ul><h2 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h2><p><strong>参考资料：</strong></p><ul><li><p><a href="https://wuli.wiki/online/PChain.html">https://wuli.wiki/online/PChain.html</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/85147199">https://zhuanlan.zhihu.com/p/85147199</a></p></li></ul><p><strong>简要说明：</strong></p><ul><li><p>证明：</p><ul><li><p>假设现在有函数： $$z(x,y)&#x3D;f[u(x,y),v(x,y)]$$</p></li><li><p>根据全微分关系有： $$\mathrm{d}z&#x3D;\frac{\partial f}{\partial u}\operatorname{d}u+\frac{\partial f}{\partial v}\operatorname{d}v$$</p></li><li><p>对于 $du$、$dv$又有：</p></li></ul></li></ul><p>$$\mathrm{d}u &#x3D; \frac{\partial u}{\partial x},\mathrm{d}x + \frac{\partial u}{\partial y},\mathrm{d}y,\quad<br>\mathrm{d}v &#x3D; \frac{\partial v}{\partial x},\mathrm{d}x + \frac{\partial v}{\partial y},\mathrm{d}y$$</p><ul><li><p>带入可得： $$\begin{gathered}<br>\mathbf{d}z&#x3D;\frac{\partial f}{\partial u}{\left(\frac{\partial u}{\partial x}\right.}\mathrm{d}x+\frac{\partial u}{\partial y}\left.\mathrm{d}y\right)+\frac{\partial f}{\partial v}{\left(\frac{\partial v}{\partial x}\right.}\mathrm{d}x+\frac{\partial v}{\partial y}\left.\mathrm{d}y\right) \ &#x3D;\left(\frac{\partial f}{\partial u}\frac{\partial u}{\partial x}+\frac{\partial f}{\partial v}\frac{\partial v}{\partial x}\right)\mathrm{d}x+\left(\frac{\partial f}{\partial u}\frac{\partial u}{\partial y}+\frac{\partial f}{\partial v}\frac{\partial v}{\partial y}\right)\mathrm{d}y<br>\end{gathered}$$</p></li><li><p>根据全微分的定义就可以得到：</p></li></ul><p>$$\begin{aligned}\frac{\partial z}{\partial x}  &= \frac{\partial f}{\partial u}\frac{\partial u}{\partial x}   + \frac{\partial f}{\partial v}\frac{\partial v}{\partial x}, \\\frac{\partial z}{\partial y}  &= \frac{\partial f}{\partial u}\frac{\partial u}{\partial y}   + \frac{\partial f}{\partial v}\frac{\partial v}{\partial y}.\end{aligned}$$</p><ul><li><p>拓展到任意多变量的情况：<br>$$z(x_1,\ldots,x_N)&#x3D;f[u_1(x_1,\ldots,x_N),\ldots,u_M(x_1,\ldots,x_N)]$$</p></li><li><p>我们就可以得到链式法则：$$\Large\frac{\partial z}{\partial x_i}&#x3D;\sum_j\frac{\partial f}{\partial u_j}\frac{\partial u_j}{\partial x_i}$$</p></li></ul><h2 id="举例分析"><a href="#举例分析" class="headerlink" title="举例分析"></a>举例分析</h2><p>假设现在有一个计算式为 ( e &#x3D; (a + b) * (b + 1) )，可以首先取 ( c &#x3D; a + b )、( d &#x3D; b + 1 )，<br>从而将其转化为 ( e &#x3D; c * d )，然后根据链式法则可以求 ( b ) 的偏导：</p><p>$$\frac{\partial e}{\partial b}=\frac{\partial e}{\partial c}\frac{\partial c}{\partial b}+\frac{\partial e}{\partial d}\frac{\partial d}{\partial b}=d*1+c*1=(b+1)+(a+b)$$。</p><p>取 $a&#x3D;3, b&#x3D;2$，将计算图绘制如下：</p><p><img src="/2025/06/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%8F%8A%E4%BC%98%E5%8C%96%E5%99%A8%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/image-2.png"></p><blockquote><p>该图中是将数值作为了节点，将运算符作为了边。注意也有一些说法是反过来的。</p></blockquote><p>在计算图中我们可以将各个变量的值计算出来，然后进行反向传播，计算去其对于各个变量的偏导数。对于 $b$，带入链式法则的公司，得到其偏导为: </p><p>$$\frac{\partial e}{\partial b}= \frac{\partial e}{\partial c}\frac{\partial c}{\partial b}+ \frac{\partial e}{\partial d}\frac{\partial d}{\partial b}= 3 \times 1 + 5 \times 1 = 8$$</p><p>带入计算图中可以更为简单地理解为其偏导就是各个链路的偏导乘积的和。所以如果我们需要同时计算 $a$和 $b$的偏导，我们可以将e节点认定为1，然后进行反向传播，求出各节点的偏导，然后进行复用，如下图所示：</p><p><img src="/2025/06/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%8F%8A%E4%BC%98%E5%8C%96%E5%99%A8%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3/image.png"></p><h2 id="举例pytorch验证"><a href="#举例pytorch验证" class="headerlink" title="举例pytorch验证"></a>举例pytorch验证</h2><p>使用pytorch实现上述的计算式，如下，最后的输出与我们上述分析的一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># 定义变量 a 和 b，并开启自动求导</span><br>a = torch.tensor(<span class="hljs-number">3.0</span>, requires_grad=<span class="hljs-literal">True</span>)<br>b = torch.tensor(<span class="hljs-number">2.0</span>, requires_grad=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 构造计算式 e = (a + b)(b + 1)</span><br>c = a + b<br>d = b + <span class="hljs-number">1</span><br>e = c * d  <span class="hljs-comment"># e = (a + b) * (b + 1)</span><br><br><span class="hljs-comment"># 保留计算过程的梯度，默认pytorch只保留叶子节点的，所以需要主动设置</span><br>c.retain_grad()<br>d.retain_grad()<br>e.retain_grad()<br><br><span class="hljs-comment"># 反向传播求导</span><br>e.backward()<br><br><span class="hljs-comment"># 打印结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;e = <span class="hljs-subst">&#123;e.item()&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;∂e/∂e = <span class="hljs-subst">&#123;e.grad.item()&#125;</span>&quot;</span>) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;∂e/∂c = <span class="hljs-subst">&#123;c.grad.item()&#125;</span>&quot;</span>) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;∂e/∂d = <span class="hljs-subst">&#123;d.grad.item()&#125;</span>&quot;</span>) <br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;∂e/∂a = <span class="hljs-subst">&#123;a.grad.item()&#125;</span>&quot;</span>)  <span class="hljs-comment"># 应该是 d = b + 1 = 3</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;∂e/∂b = <span class="hljs-subst">&#123;b.grad.item()&#125;</span>&quot;</span>)  <span class="hljs-comment"># 应该是 (∂e/∂c)*(∂c/∂b) + (∂e/∂d)*(∂d/∂b) = d + c = 3 + 5 = 8</span><br><br><span class="hljs-comment"># 输出</span><br><span class="hljs-comment"># e = 15.0</span><br><span class="hljs-comment"># ∂e/∂e = 1.0</span><br><span class="hljs-comment"># ∂e/∂c = 3.0</span><br><span class="hljs-comment"># ∂e/∂d = 5.0</span><br><span class="hljs-comment"># ∂e/∂a = 3.0</span><br><span class="hljs-comment"># ∂e/∂b = 8.0</span><br></code></pre></td></tr></table></figure><h1 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h1><p>上述我们知道了如何计算梯度，但是在实际的深度学习中，我们往往还需要构建模型，然后利用优化器对模型参数进行更新。</p><p>所以进一步，我们需要：</p><ol><li><p>把 <code>b</code> 设为“模型参数”（可优化的变量）；</p></li><li><p>将 <code>e = (a + b)(b + 1)</code> 作为模型的“前向计算”；</p></li><li><p>定义损失函数（比如 <code>loss = e ** 2</code>，希望 <code>e</code> 趋近于 0）；</p></li><li><p>使用 <code>optimizer</code> 优化变量 <code>b</code>。</p></li></ol><p>完整代码及输出如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br><span class="hljs-comment"># 假设 a 是常数，b 是我们要学习的参数</span><br>a = torch.tensor(<span class="hljs-number">3.0</span>)<br><br><span class="hljs-comment"># 将 b 作为参数需要被优化，requires_grad=True</span><br>b = torch.tensor(<span class="hljs-number">2.0</span>, requires_grad=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 构建优化器，优化目标是 b</span><br>optimizer = optim.SGD([b], lr=<span class="hljs-number">0.01</span>)<br><br><span class="hljs-comment"># 训练目标：希望 e 趋近于 0，所以目标是最小化 loss = e^2</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):<br>    optimizer.zero_grad()  <span class="hljs-comment"># 清除旧梯度</span><br><br>    <span class="hljs-comment"># 前向计算</span><br>    c = a + b<br>    d = b + <span class="hljs-number">1</span><br>    e = c * d<br><br>    <span class="hljs-comment"># 构造损失函数（比如希望 e 接近 0）</span><br>    loss = e ** <span class="hljs-number">2</span>  <span class="hljs-comment"># 损失函数 L = e^2</span><br><br>    <span class="hljs-comment"># 反向传播</span><br>    loss.backward()<br><br>    <span class="hljs-comment"># 梯度更新</span><br>    optimizer.step()<br><br>    <span class="hljs-comment"># 打印当前值</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch+<span class="hljs-number">1</span>:2d&#125;</span>: ∂loss/∂b = <span class="hljs-subst">&#123;b.grad.item():<span class="hljs-number">.4</span>f&#125;</span>, b = <span class="hljs-subst">&#123;b.item():<span class="hljs-number">.4</span>f&#125;</span>, e = <span class="hljs-subst">&#123;e.item():<span class="hljs-number">.4</span>f&#125;</span>, loss = <span class="hljs-subst">&#123;loss.item():<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># Epoch  1: ∂loss/∂b = 240.0000, b = -0.4000, e = 15.0000, loss = 225.0000</span><br><span class="hljs-comment"># Epoch  2: ∂loss/∂b = 9.9840, b = -0.4998, e = 1.5600, loss = 2.4336</span><br><span class="hljs-comment"># Epoch  3: ∂loss/∂b = 7.5037, b = -0.5749, e = 1.2505, loss = 1.5637</span><br><span class="hljs-comment"># Epoch  4: ∂loss/∂b = 5.8771, b = -0.6336, e = 1.0310, loss = 1.0629</span><br><span class="hljs-comment"># Epoch  5: ∂loss/∂b = 4.7381, b = -0.6810, e = 0.8669, loss = 0.7515</span><br><span class="hljs-comment"># Epoch  6: ∂loss/∂b = 3.9025, b = -0.7201, e = 0.7397, loss = 0.5471</span><br><span class="hljs-comment"># Epoch  7: ∂loss/∂b = 3.2678, b = -0.7527, e = 0.6383, loss = 0.4074</span><br><span class="hljs-comment"># Epoch  8: ∂loss/∂b = 2.7723, b = -0.7805, e = 0.5557, loss = 0.3088</span><br><span class="hljs-comment"># Epoch  9: ∂loss/∂b = 2.3771, b = -0.8042, e = 0.4873, loss = 0.2375</span><br><span class="hljs-comment"># Epoch 10: ∂loss/∂b = 2.0561, b = -0.8248, e = 0.4299, loss = 0.1848</span><br><span class="hljs-comment"># Epoch 11: ∂loss/∂b = 1.7916, b = -0.8427, e = 0.3811, loss = 0.1453</span><br><span class="hljs-comment"># Epoch 12: ∂loss/∂b = 1.5709, b = -0.8584, e = 0.3393, loss = 0.1151</span><br><span class="hljs-comment"># Epoch 13: ∂loss/∂b = 1.3846, b = -0.8723, e = 0.3032, loss = 0.0919</span><br><span class="hljs-comment"># Epoch 14: ∂loss/∂b = 1.2261, b = -0.8845, e = 0.2718, loss = 0.0739</span><br><span class="hljs-comment"># Epoch 15: ∂loss/∂b = 1.0900, b = -0.8954, e = 0.2443, loss = 0.0597</span><br><span class="hljs-comment"># Epoch 16: ∂loss/∂b = 0.9725, b = -0.9051, e = 0.2201, loss = 0.0484</span><br><span class="hljs-comment"># Epoch 17: ∂loss/∂b = 0.8702, b = -0.9138, e = 0.1987, loss = 0.0395</span><br><span class="hljs-comment"># Epoch 18: ∂loss/∂b = 0.7809, b = -0.9217, e = 0.1797, loss = 0.0323</span><br><span class="hljs-comment"># Epoch 19: ∂loss/∂b = 0.7023, b = -0.9287, e = 0.1628, loss = 0.0265</span><br><span class="hljs-comment"># Epoch 20: ∂loss/∂b = 0.6331, b = -0.9350, e = 0.1477, loss = 0.0218</span><br></code></pre></td></tr></table></figure><p>注意我们这里定义了 $loss&#x3D;e^2$，所以在原计算图的最上面会多出一个节点，然后会多出一条边，这条边对应的偏导是 $\frac{\partial loss}{\partial e}&#x3D;2e$。</p><p>对于Epoch 1，手动计算其相对于 $b$的导数为:</p><p>$$2e*\frac{\partial e}{\partial b}=2*15*8=240$$</p><p>然后因为采取的是SGD且学习率为0.01，所以更新 $b &#x3D; 2 - 0.01*240&#x3D; -0.4$。</p><p>而一般来说，我们会将其封装成模型进行使用，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, init_b</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.b = nn.Parameter(torch.tensor(init_b))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, a</span>):<br>        c = a + self.b<br>        d = self.b + <span class="hljs-number">1</span><br>        e = c * d<br>        <span class="hljs-keyword">return</span> e<br><br>a = torch.tensor(<span class="hljs-number">3.0</span>)<br>model = SimpleModel(init_b=<span class="hljs-number">2.0</span>)<br>optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.01</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):<br>    optimizer.zero_grad()<br>    e = model(a)<br>    loss = e ** <span class="hljs-number">2</span><br>    loss.backward()<br>    optimizer.step()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;epoch+<span class="hljs-number">1</span>:2d&#125;</span>: ∂loss/∂b = <span class="hljs-subst">&#123;model.b.grad.item():<span class="hljs-number">.4</span>f&#125;</span>, b = <span class="hljs-subst">&#123;model.b.item():<span class="hljs-number">.4</span>f&#125;</span>, e = <span class="hljs-subst">&#123;e.item():<span class="hljs-number">.4</span>f&#125;</span>, loss = <span class="hljs-subst">&#123;loss.item():<span class="hljs-number">.4</span>f&#125;</span>&quot;</span>)<br><br><span class="hljs-comment"># 输出结果：</span><br><span class="hljs-comment"># Epoch  1: ∂loss/∂b = 240.0000, b = -0.4000, e = 15.0000, loss = 225.0000</span><br><span class="hljs-comment"># Epoch  2: ∂loss/∂b = 9.9840, b = -0.4998, e = 1.5600, loss = 2.4336</span><br><span class="hljs-comment"># Epoch  3: ∂loss/∂b = 7.5037, b = -0.5749, e = 1.2505, loss = 1.5637</span><br><span class="hljs-comment"># Epoch  4: ∂loss/∂b = 5.8771, b = -0.6336, e = 1.0310, loss = 1.0629</span><br><span class="hljs-comment"># Epoch  5: ∂loss/∂b = 4.7381, b = -0.6810, e = 0.8669, loss = 0.7515</span><br><span class="hljs-comment"># Epoch  6: ∂loss/∂b = 3.9025, b = -0.7201, e = 0.7397, loss = 0.5471</span><br><span class="hljs-comment"># Epoch  7: ∂loss/∂b = 3.2678, b = -0.7527, e = 0.6383, loss = 0.4074</span><br><span class="hljs-comment"># Epoch  8: ∂loss/∂b = 2.7723, b = -0.7805, e = 0.5557, loss = 0.3088</span><br><span class="hljs-comment"># Epoch  9: ∂loss/∂b = 2.3771, b = -0.8042, e = 0.4873, loss = 0.2375</span><br><span class="hljs-comment"># Epoch 10: ∂loss/∂b = 2.0561, b = -0.8248, e = 0.4299, loss = 0.1848</span><br><span class="hljs-comment"># Epoch 11: ∂loss/∂b = 1.7916, b = -0.8427, e = 0.3811, loss = 0.1453</span><br><span class="hljs-comment"># Epoch 12: ∂loss/∂b = 1.5709, b = -0.8584, e = 0.3393, loss = 0.1151</span><br><span class="hljs-comment"># Epoch 13: ∂loss/∂b = 1.3846, b = -0.8723, e = 0.3032, loss = 0.0919</span><br><span class="hljs-comment"># Epoch 14: ∂loss/∂b = 1.2261, b = -0.8845, e = 0.2718, loss = 0.0739</span><br><span class="hljs-comment"># Epoch 15: ∂loss/∂b = 1.0900, b = -0.8954, e = 0.2443, loss = 0.0597</span><br><span class="hljs-comment"># Epoch 16: ∂loss/∂b = 0.9725, b = -0.9051, e = 0.2201, loss = 0.0484</span><br><span class="hljs-comment"># Epoch 17: ∂loss/∂b = 0.8702, b = -0.9138, e = 0.1987, loss = 0.0395</span><br><span class="hljs-comment"># Epoch 18: ∂loss/∂b = 0.7809, b = -0.9217, e = 0.1797, loss = 0.0323</span><br><span class="hljs-comment"># Epoch 19: ∂loss/∂b = 0.7023, b = -0.9287, e = 0.1628, loss = 0.0265</span><br><span class="hljs-comment"># Epoch 20: ∂loss/∂b = 0.6331, b = -0.9350, e = 0.1477, loss = 0.0218</span><br></code></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><p><a href="https://blog.csdn.net/weixin/_43314579/article/details/88937475">https://blog.csdn.net/weixin\_43314579/article/details/88937475</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/85147199">https://zhuanlan.zhihu.com/p/85147199</a></p></li><li><p><a href="https://wuli.wiki/online/PChain.html">https://wuli.wiki/online/PChain.html</a></p></li><li><p><a href="https://zh.wikipedia.org/wiki/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99">https://zh.wikipedia.org/wiki/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/111402123">https://zhuanlan.zhihu.com/p/111402123</a></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch torch.distributed 及NCCL初探</title>
    <link href="/2025/05/25/Pytorch%20torch.distributed%20%E5%8F%8ANCCL%E5%88%9D%E6%8E%A2/"/>
    <url>/2025/05/25/Pytorch%20torch.distributed%20%E5%8F%8ANCCL%E5%88%9D%E6%8E%A2/</url>
    
    <content type="html"><![CDATA[<h1 id="Pytorch-torch-distributed-举例学习"><a href="#Pytorch-torch-distributed-举例学习" class="headerlink" title="Pytorch torch.distributed 举例学习"></a>Pytorch torch.distributed 举例学习</h1><h2 id="单机通信"><a href="#单机通信" class="headerlink" title="单机通信"></a>单机通信</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.distributed <span class="hljs-keyword">as</span> dist<br><span class="hljs-keyword">import</span> torch.multiprocessing <span class="hljs-keyword">as</span> mp<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">setup</span>(<span class="hljs-params">rank, world_size</span>):<br>    <span class="hljs-comment"># 设置环境变量</span><br>    os.environ[<span class="hljs-string">&#x27;MASTER_ADDR&#x27;</span>] = <span class="hljs-string">&#x27;127.0.0.1&#x27;</span><br>    os.environ[<span class="hljs-string">&#x27;MASTER_PORT&#x27;</span>] = <span class="hljs-string">&#x27;29500&#x27;</span><br><br>    <span class="hljs-comment"># 初始化进程组</span><br>    dist.init_process_group(backend=<span class="hljs-string">&#x27;nccl&#x27;</span>, rank=rank, world_size=world_size)<br>    torch.cuda.set_device(rank)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cleanup</span>():<br>    dist.destroy_process_group()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">demo_all_reduce</span>(<span class="hljs-params">rank, world_size</span>):<br>    setup(rank, world_size)<br><br>    <span class="hljs-comment"># 每个进程创建一个张量</span><br>    tensor = torch.ones(<span class="hljs-number">1</span>).to(rank) * rank<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[<span class="hljs-subst">&#123;rank&#125;</span>] Before all_reduce: <span class="hljs-subst">&#123;tensor.item()&#125;</span>&quot;</span>)<br><br>    <span class="hljs-comment"># 对所有进程张量做 all_reduce（求和）</span><br>    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[<span class="hljs-subst">&#123;rank&#125;</span>] After all_reduce: <span class="hljs-subst">&#123;tensor.item()&#125;</span>&quot;</span>)<br><br>    cleanup()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    world_size = <span class="hljs-number">4</span><br>    mp.spawn(demo_all_reduce, args=(world_size,), nprocs=world_size, join=<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br></code></pre></td></tr></table></figure><p>一些注意事项：</p><ul><li><p><code>mp.spawn</code>可以同时启动多个训练进程，并传入不同的rank参数</p></li><li><p><code> dist.init_process_group(backend=&#39;nccl&#39;, rank=rank, world_size=world_size)</code>是初始化一个默认的通信进程组。</p><ul><li><p>初始化时需要指定使用什么通信<code>backend</code>，这里选择的是经典的nccl</p></li><li><p>然后还需给出当前的进程的<code>rank</code>，相当于是在告诉后端什么<code>rank</code>加入到了这个进程组</p></li><li><p>然后还需要指定<code>world_size</code>，即有多少个进程参与了通信。注意只有<code>world_size</code>个进程都完成了通信，一个通信操作才算完成。</p></li><li><p>还可以指定<code>init_method</code>指定分布式各进程之间如何发现彼此，完成初始化同步。<strong>常见取值</strong>：</p></li></ul></li></ul><table><thead><tr><th><strong>类型</strong></th><th><strong>示例</strong></th><th><strong>含义</strong></th></tr></thead><tbody><tr><td>TCP 地址</td><td>‘tcp:&#x2F;&#x2F;127.0.0.1:29500’</td><td>所有进程通过该地址建立连接</td></tr><tr><td>环境变量</td><td>‘env:&#x2F;&#x2F;‘</td><td>读取 MASTER_ADDR, MASTER_PORT 等环境变量</td></tr><tr><td>文件系统</td><td>‘file:&#x2F;&#x2F;&#x2F;tmp&#x2F;shared_init’</td><td>所有进程通过同一个文件进行同步（通常用于单机多进程）</td></tr></tbody></table><p><strong>默认值</strong>：如果你设置了 <code>MASTER_ADDR</code> 和 <code>MASTER_PORT</code>，可以使用 <code>&#39;env://&#39;</code> 来简洁初始化。</p><ul><li><p>还可以指定<code>timeout</code>，即表示连接超时的时间。</p></li><li><p><code>dist.all_reduce(tensor, op=dist.ReduceOp.SUM)</code>是在执行all_reduce操作，这里没有显示地调用group参数，所以默认是在默认通信组里进行通信</p></li></ul><h3 id="创建多个通信组"><a href="#创建多个通信组" class="headerlink" title="创建多个通信组"></a>创建多个通信组</h3><p>注意上面相当于是在创建一个默认的通信组，如果有创建多个通信组的需求，必须在创建默认的通信组后再自行创建，主要有两种方法：</p><ul><li>一种是直接通过<code>dist.new_group</code>来创建一个group进行通信。如下所示：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.distributed <span class="hljs-keyword">as</span> dist<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">setup</span>(<span class="hljs-params">rank, world_size</span>):<br>    os.environ[<span class="hljs-string">&#x27;MASTER_ADDR&#x27;</span>] = <span class="hljs-string">&#x27;127.0.0.1&#x27;</span><br>    os.environ[<span class="hljs-string">&#x27;MASTER_PORT&#x27;</span>] = <span class="hljs-string">&#x27;29500&#x27;</span><br>    dist.init_process_group(<span class="hljs-string">&#x27;nccl&#x27;</span>, rank=rank, world_size=world_size)<br><br>    <span class="hljs-comment"># 默认通信组初始化后，才能创建自定义通信组</span><br>    <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]:<br>        group = dist.new_group(ranks=[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>    <span class="hljs-keyword">else</span>:<br>        group = dist.new_group(ranks=[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br><br>    <span class="hljs-comment"># 举例：在各自组里 all_reduce</span><br>    tensor = torch.tensor([rank], dtype=torch.float32).cuda()<br>    dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Rank <span class="hljs-subst">&#123;rank&#125;</span>] tensor after all_reduce in group: <span class="hljs-subst">&#123;tensor.item()&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><ul><li>另一种是是通过<code>dist.new_subgroups_by_enumeration</code>来批量创建多个group进行通信，这适合需要创建多个group的场景，更加高效，其输入的参数类型为<code>list[list[int]]</code>。如下所示：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.distributed <span class="hljs-keyword">as</span> dist<br><span class="hljs-keyword">from</span> torch.multiprocessing <span class="hljs-keyword">import</span> spawn<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_distributed</span>(<span class="hljs-params">rank, world_size</span>):<br>    os.environ[<span class="hljs-string">&#x27;MASTER_ADDR&#x27;</span>] = <span class="hljs-string">&#x27;localhost&#x27;</span><br>    os.environ[<span class="hljs-string">&#x27;MASTER_PORT&#x27;</span>] = <span class="hljs-string">&#x27;29500&#x27;</span><br>    os.environ[<span class="hljs-string">&#x27;RANK&#x27;</span>] = <span class="hljs-built_in">str</span>(rank)<br>    os.environ[<span class="hljs-string">&#x27;WORLD_SIZE&#x27;</span>] = <span class="hljs-built_in">str</span>(world_size)<br>    dist.init_process_group(backend=<span class="hljs-string">&#x27;nccl&#x27;</span>, init_method=<span class="hljs-string">&#x27;env://&#x27;</span>, rank=rank, world_size=world_size)<br>    torch.cuda.set_device(rank % torch.cuda.device_count())<br><br>    <span class="hljs-comment"># 创建网格：DP=2, PP=2, TP=2</span><br>    dp, pp, tp = <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span><br>    <span class="hljs-keyword">assert</span> world_size == dp * pp * tp<br>    grid = torch.arange(world_size).view(dp, pp, tp)<br><br>    <span class="hljs-comment"># 当前 rank 在网格中的坐标</span><br>    dp_rank, pp_rank, tp_rank = (grid == rank).nonzero().flatten().tolist()<br><br>    <span class="hljs-comment"># 创建通信组</span><br>    tp_groups = [grid[d, p, :].tolist() <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(dp) <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(pp)]<br>    pp_groups = [grid[d, :, t].tolist() <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(dp) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tp)]<br>    dp_groups = [grid[:, p, t].tolist() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(pp) <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tp)]<br><br>    tp_group_list = dist.new_subgroups_by_enumeration(tp_groups)<br>    pp_group_list = dist.new_subgroups_by_enumeration(pp_groups)<br>    dp_group_list = dist.new_subgroups_by_enumeration(dp_groups)<br><br>    <span class="hljs-comment"># 当前进程的通信组</span><br>    tp_group = <span class="hljs-built_in">next</span>(g <span class="hljs-keyword">for</span> g <span class="hljs-keyword">in</span> tp_group_list <span class="hljs-keyword">if</span> g <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>)<br>    pp_group = <span class="hljs-built_in">next</span>(g <span class="hljs-keyword">for</span> g <span class="hljs-keyword">in</span> pp_group_list <span class="hljs-keyword">if</span> g <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>)<br>    dp_group = <span class="hljs-built_in">next</span>(g <span class="hljs-keyword">for</span> g <span class="hljs-keyword">in</span> dp_group_list <span class="hljs-keyword">if</span> g <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Rank <span class="hljs-subst">&#123;rank&#125;</span>] dp=<span class="hljs-subst">&#123;dp_rank&#125;</span>, pp=<span class="hljs-subst">&#123;pp_rank&#125;</span>, tp=<span class="hljs-subst">&#123;tp_rank&#125;</span> | &quot;</span><br>          <span class="hljs-string">f&quot;TP group: <span class="hljs-subst">&#123;tp_groups[dp_rank * pp + pp_rank]&#125;</span> | &quot;</span><br>          <span class="hljs-string">f&quot;PP group: <span class="hljs-subst">&#123;pp_groups[dp_rank * tp + tp_rank]&#125;</span> | &quot;</span><br>          <span class="hljs-string">f&quot;DP group: <span class="hljs-subst">&#123;dp_groups[pp_rank * tp + tp_rank]&#125;</span>&quot;</span>)<br><br>    dist.destroy_process_group()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    world_size = <span class="hljs-number">8</span><br>    spawn(init_distributed, args=(world_size,), nprocs=world_size)<br><br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs yaml">[<span class="hljs-string">Rank</span> <span class="hljs-number">0</span>] <span class="hljs-string">dp=0,</span> <span class="hljs-string">pp=0,</span> <span class="hljs-string">tp=0</span> <span class="hljs-string">|</span> <span class="hljs-attr">TP group:</span> [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>] <span class="hljs-string">|</span> <span class="hljs-attr">PP group:</span> [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>] <span class="hljs-string">|</span> <span class="hljs-attr">DP group:</span> [<span class="hljs-number">0</span>, <span class="hljs-number">4</span>]<br>[<span class="hljs-string">Rank</span> <span class="hljs-number">1</span>] <span class="hljs-string">dp=0,</span> <span class="hljs-string">pp=0,</span> <span class="hljs-string">tp=1</span> <span class="hljs-string">|</span> <span class="hljs-attr">TP group:</span> [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>] <span class="hljs-string">|</span> <span class="hljs-attr">PP group:</span> [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>] <span class="hljs-string">|</span> <span class="hljs-attr">DP group:</span> [<span class="hljs-number">1</span>, <span class="hljs-number">5</span>]<br>[<span class="hljs-string">Rank</span> <span class="hljs-number">2</span>] <span class="hljs-string">dp=0,</span> <span class="hljs-string">pp=1,</span> <span class="hljs-string">tp=0</span> <span class="hljs-string">|</span> <span class="hljs-attr">TP group:</span> [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>] <span class="hljs-string">|</span> <span class="hljs-attr">PP group:</span> [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>] <span class="hljs-string">|</span> <span class="hljs-attr">DP group:</span> [<span class="hljs-number">2</span>, <span class="hljs-number">6</span>]<br><span class="hljs-string">...</span><br></code></pre></td></tr></table></figure><h2 id="多机通信"><a href="#多机通信" class="headerlink" title="多机通信"></a>多机通信</h2><p>多机通信与单机通信在使用上还是比较类似的，因为在初始化时，其视野里都是各个进程来进行连接，所以只需要网络是互通的，本地连接和远程连接的区别并没有那么大。</p><p>如下所示的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># train.py</span><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.distributed <span class="hljs-keyword">as</span> dist<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> torch.nn.parallel <span class="hljs-keyword">import</span> DistributedDataParallel <span class="hljs-keyword">as</span> DDP<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">setup</span>():<br>    dist.init_process_group(backend=<span class="hljs-string">&#x27;nccl&#x27;</span>, init_method=<span class="hljs-string">&#x27;env://&#x27;</span>)<br>    torch.cuda.set_device(<span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;LOCAL_RANK&quot;</span>]))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cleanup</span>():<br>    dist.destroy_process_group()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">demo_basic</span>(<span class="hljs-params">rank, world_size</span>):<br>    setup()<br><br>    <span class="hljs-comment"># 每个进程只用一张卡</span><br>    device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>, <span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;LOCAL_RANK&quot;</span>]))<br><br>    <span class="hljs-comment"># 构造简单模型</span><br>    model = nn.Linear(<span class="hljs-number">10</span>, <span class="hljs-number">1</span>).to(device)<br>    ddp_model = DDP(model, device_ids=[<span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;LOCAL_RANK&quot;</span>])])<br><br>    <span class="hljs-comment"># 构造优化器和数据</span><br>    optimizer = optim.SGD(ddp_model.parameters(), lr=<span class="hljs-number">0.001</span>)<br>    loss_fn = nn.MSELoss()<br><br>    <span class="hljs-comment"># 伪造数据</span><br>    x = torch.randn(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>).to(device)<br>    y = torch.randn(<span class="hljs-number">64</span>, <span class="hljs-number">1</span>).to(device)<br><br>    <span class="hljs-comment"># 训练一步</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>        optimizer.zero_grad()<br>        outputs = ddp_model(x)<br>        loss = loss_fn(outputs, y)<br>        loss.backward()<br>        optimizer.step()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Rank <span class="hljs-subst">&#123;dist.get_rank()&#125;</span>] Epoch <span class="hljs-subst">&#123;epoch&#125;</span> Loss: <span class="hljs-subst">&#123;loss.item()&#125;</span>&quot;</span>)<br><br>    cleanup()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    demo_basic(rank=<span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;RANK&quot;</span>]), world_size=<span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;WORLD_SIZE&quot;</span>]))<br>    <br></code></pre></td></tr></table></figure><p>假设现在需要在两台机器上运行那么就需要指定其中一台主机为master节点，然后分别执行如下的启动命令：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">// 主机<span class="hljs-number">0</span>（master节点）<br>torchrun \<br>  --nnodes=<span class="hljs-number">2</span> \<br>  --node_rank=<span class="hljs-number">0</span> \<br>  --nproc_per_node=<span class="hljs-number">4</span> \<br>  --master_addr=<span class="hljs-number">26.104</span><span class="hljs-number">.249</span><span class="hljs-number">.162</span> \<br>  --master_port=<span class="hljs-number">29500</span> \<br>  train.py<br><br>// 主机<span class="hljs-number">1</span> <br>torchrun \<br>  --nnodes=<span class="hljs-number">2</span> \<br>  --node_rank=<span class="hljs-number">1</span> \<br>  --nproc_per_node=<span class="hljs-number">4</span> \<br>  --master_addr=<span class="hljs-number">26.104</span><span class="hljs-number">.249</span><span class="hljs-number">.162</span> \<br>  --master_port=<span class="hljs-number">29500</span> \<br>  train.py<br></code></pre></td></tr></table></figure><blockquote><p><strong>还可以设置如下的环境变量来进行相关的控制：</strong></p><ul><li><p><strong>NCCL_SOCKET_IFNAME：</strong>&#x6307;定使用的网卡，默认会自己找到，但是如果找不对可以自己设置，例如 <code>export NCCL_SOCKET_IFNAME=eth0</code></p></li><li><p><strong>NCCL_DEBUG：</strong>&#x53EF;以设置<code>NCCL_DEBUG=INFO</code> 来打印明确的警告消息以及基本的 NCCL 初始化信息。</p></li></ul></blockquote><h2 id="支持的通信操作"><a href="#支持的通信操作" class="headerlink" title="支持的通信操作"></a>支持的通信操作</h2><p>下面是参考官方文档，列举的<code>torch.distributed</code>模块实现的集合通讯操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 从 rank 0 广播 tensor 到所有进程</span><br><span class="hljs-comment"># tensor: 要广播的张量, src: 源进程的 rank</span><br>dist.broadcast(tensor, src=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 从 rank 0 广播 obj_list 到所有进程</span><br><span class="hljs-comment"># obj_list: 要广播的 Python 对象列表, src: 源进程的 rank</span><br>dist.broadcast_object_list(obj_list, src=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 对所有进程的 tensor 进行归约操作（如求和）</span><br><span class="hljs-comment"># tensor: 输入/输出张量, op: 归约操作类型（如 dist.ReduceOp.SUM）</span><br>dist.all_reduce(tensor, op=dist.ReduceOp.SUM)<br><br><span class="hljs-comment"># 将所有进程的 tensor 归约到目标进程</span><br><span class="hljs-comment"># tensor: 输入/输出张量, dst: 目标进程的 rank, op: 归约操作类型</span><br>dist.reduce(tensor, dst=<span class="hljs-number">0</span>, op=dist.ReduceOp.SUM)<br><br><span class="hljs-comment"># 将所有进程的 tensor 收集到所有进程</span><br><span class="hljs-comment"># gather_list: 输出列表，存储所有进程的 tensor, tensor: 当前进程的输入张量</span><br>dist.all_gather(gather_list, tensor)<br><br><span class="hljs-comment"># 将所有进程的 tensor 收集到一个大张量中</span><br><span class="hljs-comment"># output_tensor: 输出张量, tensor: 当前进程的输入张量</span><br>dist.all_gather_into_tensor(output_tensor, tensor)<br><br><span class="hljs-comment"># 将所有进程的 Python 对象收集到所有进程</span><br><span class="hljs-comment"># gather_list: 输出列表，存储所有进程的对象, obj: 当前进程的输入对象</span><br>dist.all_gather_object(gather_list, obj)<br><br><span class="hljs-comment"># 将所有进程的 tensor 收集到目标进程</span><br><span class="hljs-comment"># tensor: 当前进程的输入张量, gather_list: 目标进程的输出列表, dst: 目标进程的 rank</span><br>dist.gather(tensor, gather_list, dst=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 将所有进程的 Python 对象收集到目标进程</span><br><span class="hljs-comment"># obj: 当前进程的输入对象, gather_list: 目标进程的输出列表, dst: 目标进程的 rank</span><br>dist.gather_object(obj, gather_list, dst=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 将张量列表从源进程分发到所有进程</span><br><span class="hljs-comment"># tensor: 当前进程的输出张量, scatter_list: 源进程的输入列表, src: 源进程的 rank</span><br>dist.scatter(tensor, scatter_list, src=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 将 Python 对象列表从源进程分发到所有进程</span><br><span class="hljs-comment"># obj: 当前进程的输出对象, scatter_list: 源进程的输入列表, src: 源进程的 rank</span><br>dist.scatter_object_list(obj, scatter_list, src=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 将张量归约并分发到所有进程</span><br><span class="hljs-comment"># output_tensor: 输出张量, input_tensor: 输入张量列表, op: 归约操作类型</span><br>dist.reduce_scatter(output_tensor, [input_tensor], op=dist.ReduceOp.SUM)<br><br><span class="hljs-comment"># 将张量归约并分发到所有进程</span><br><span class="hljs-comment"># output_tensor: 输出张量, input_tensor: 输入张量, op: 归约操作类型</span><br>dist.reduce_scatter_tensor(output_tensor, input_tensor, op=dist.ReduceOp.SUM)<br><br><span class="hljs-comment"># 在所有进程之间交换张量的切片</span><br><span class="hljs-comment"># output_tensor: 输出张量, input_tensor: 输入张量</span><br>dist.all_to_all_single(output_tensor, input_tensor)<br><br><span class="hljs-comment"># 在所有进程之间交换张量列表</span><br><span class="hljs-comment"># output_tensor_list: 输出张量列表, input_tensor_list: 输入张量列表</span><br>dist.all_to_all(output_tensor_list, input_tensor_list)<br><br><span class="hljs-comment"># 同步所有进程，确保所有进程都到达此点后再继续执行</span><br>dist.barrier()<br><br><span class="hljs-comment"># 同步所有进程，并提供超时功能</span><br><span class="hljs-comment"># timeout: 超时时间（秒）</span><br>dist.monitored_barrier(timeout=<span class="hljs-number">10</span>)<br><br><span class="hljs-comment"># 异步归约操作</span><br><span class="hljs-comment"># tensor: 输入/输出张量, op: 归约操作类型, async_op: 是否异步执行</span><br>work = dist.all_reduce(tensor, op=dist.ReduceOp.SUM, async_op=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 等待异步操作完成</span><br>work.wait()<br><br><span class="hljs-comment"># 定义归约操作类型（如求和）</span><br>op = dist.ReduceOp.SUM<br></code></pre></td></tr></table></figure><h1 id="NCCL原理浅析"><a href="#NCCL原理浅析" class="headerlink" title="NCCL原理浅析"></a>NCCL原理浅析</h1><p>一般GPU间的通信采取的都是NCCL</p><h2 id="Ring-base-collectives"><a href="#Ring-base-collectives" class="headerlink" title="Ring-base collectives"></a>Ring-base collectives</h2><p>一开始NCCL采取的是ring-base collectives，即将所有的通信节点通过首尾连接形成一个单向环，数据在环上依次传输。以broadcast为例， 假设有4个GPU，GPU0为sender将信息发送给剩下的GPU，按照环的方式依次传输，GPU0–&gt;GPU1–&gt;GPU2–&gt;GPU3，若数据量为N，带宽为B，整个传输时间为（K-1）N&#x2F;B。时间随着节点数线性增长，不是很高效。</p><p><img src="/2025/05/25/Pytorch%20torch.distributed%20%E5%8F%8ANCCL%E5%88%9D%E6%8E%A2/image-2.png"></p><p>下面把要传输的数据分成S份，每次只传N&#x2F;S的数据量，传输过程如下所示：</p><p><img src="/2025/05/25/Pytorch%20torch.distributed%20%E5%8F%8ANCCL%E5%88%9D%E6%8E%A2/image.png"></p><p>GPU1接收到GPU0的一份数据后，也接着传到环的下个节点，这样以此类推，最后花的时间为</p><p>S*(N&#x2F;S&#x2F;B) + (k-2)*(N&#x2F;S&#x2F;B) &#x3D; N(S+K-2)&#x2F;(SB) –&gt; N&#x2F;B，条件是S远大于K，即数据的份数大于节点数，这个很容易满足。<strong>所以通信时间不随节点数的增加而增加，只和数据总量以及带宽有关</strong>。其它通信操作比如reduce、gather以此类推。</p><p>那么在以GPU为通信节点的场景下，怎么构建通信环呢？如下图所示：</p><p>单机4卡通过同一个PCIe switch挂载在一棵CPU的场景：</p><p><img src="/2025/05/25/Pytorch%20torch.distributed%20%E5%8F%8ANCCL%E5%88%9D%E6%8E%A2/S0P0b1JWGozbZtxjlUacSCQinNg.png"></p><p>单机8卡通过两个CPU下不同的PCIe switch挂载的场景：</p><p><img src="/2025/05/25/Pytorch%20torch.distributed%20%E5%8F%8ANCCL%E5%88%9D%E6%8E%A2/KFBQbL9M4o36pIxmmx7cKu7hnre.png"></p><p><img src="/2025/05/25/Pytorch%20torch.distributed%20%E5%8F%8ANCCL%E5%88%9D%E6%8E%A2/KSyWbToYxoYzJsxjXOgcMsiznie.webp"></p><h3 id="DGX-1-V100"><a href="#DGX-1-V100" class="headerlink" title="DGX-1 V100"></a>DGX-1 V100</h3><p>在实际设计中，往往会采取将各GPU形成多个环的方式来进行通信加速。DGX-1的NVLink链接拓扑如下图所示，可以看到其形成了一个复杂的拓扑结构。</p><p><img src="/2025/05/25/Pytorch%20torch.distributed%20%E5%8F%8ANCCL%E5%88%9D%E6%8E%A2/image-4.png"></p><p>一般而言一个NVLink的带宽速度为25GB&#x2F;s。注意上图中蓝色代表的是用一根NVLink相连，黄色代表的是用2根NVLinks相连，NVLink数量翻番往往也意味着其带宽也翻番了。例如在实际测试中，GPU0～GPU1之间的双向带宽为48GB&#x2F;s，约为25GB&#x2F;s * 2。GPU0和3之间的约为96GB&#x2F;s，约为50GB&#x2F;s *2。</p><p>NCCL在初始化的时候，会检查系统中的链路拓扑，并创建若干个环路，以达到最优的性能。以上图中的拓扑为例，NCCL会创建4种环路，分别是下面四种。这四种环相互不会影响，都是独立的链路带宽（不同方向，或者不同链路）。</p><table><thead><tr><th>Ring Channel</th><th>#NVLinks</th><th>Bus Bandwidth</th></tr></thead><tbody><tr><td>0-&gt;4-&gt;7-&gt;6-&gt;5-&gt;1-&gt;2-&gt;3</td><td>2</td><td>50 GB&#x2F;s</td></tr><tr><td>0&lt;-4&lt;-7&lt;-6&lt;-5&lt;-1&lt;-2&lt;-3</td><td>2</td><td>50 GB&#x2F;s</td></tr><tr><td>0-&gt;1-&gt;3-&gt;7-&gt;5-&gt;4-&gt;6-&gt;2</td><td>1</td><td>25 GB&#x2F;s</td></tr><tr><td>0&lt;-1&lt;-3&lt;-7&lt;-5&lt;-4&lt;-6&lt;-2</td><td>1</td><td>25 GB&#x2F;s</td></tr></tbody></table><p>接下来，NCCL会把需要通信的数据进行切片，每一个Channel负责通信部分的切片数据。这么一来，就同时有多个环在工作。我们知道一块V100可以插6根NVLink，NCCL这么一做，直接把6根NVLink的双向带宽全部拉满了，这理论的通信速度就达到了150GB&#x2F;s。</p><p>在实际测试中，allreduce的性能最高达到了130GB&#x2F;s，已经是一个很不错的结果了。</p><p>但是注意也正是这种多环路的设计使得在不是整机的时候会使得通信下降。</p><ul><li><p>例如对于2卡，以GPU0和GPU1为例，其单向连接反而只有一条NVLink了，其速度降为25GB&#x2F;s</p></li><li><p>例如对于4卡，以0,1,2,3 4张卡举例，可以形成4种回路，如下。此时双NVLink连接也会被降级为单NVlink连接的速度。整体的理想速度也只有了100GB&#x2F;s</p></li></ul><table><thead><tr><th>Ring Channel</th><th>#NVLinks</th><th>Bus Bandwidth</th></tr></thead><tbody><tr><td>0-&gt;1-&gt;2-&gt;3</td><td>1</td><td>25GB&#x2F;s</td></tr><tr><td>0&lt;-1&lt;-2&lt;-3</td><td>1</td><td>25GB&#x2F;s</td></tr><tr><td>0-&gt;3-&gt;1-&gt;2</td><td>1</td><td>25GB&#x2F;s</td></tr><tr><td>0&lt;-3&lt;-1&lt;-2</td><td>1</td><td>25GB&#x2F;s</td></tr></tbody></table><h1 id="NVSwitch"><a href="#NVSwitch" class="headerlink" title="NVSwitch"></a>NVSwitch</h1><p>随着技术的演进，为了更加全面的多卡之间的互联，MVSwitch应运而生。以下是各代GPU中的网络架构。可以看到从V系列开始，NVSwitch就被引入了进来。</p><p><img src="/2025/05/25/Pytorch%20torch.distributed%20%E5%8F%8ANCCL%E5%88%9D%E6%8E%A2/image-1.png"></p><ul><li><p>在 DGX-1 P100 中有 8 张 GPU 卡，每张 GPU 卡支持 4 条 NVLink 链路，这些链路允许 GPU 之间进行高速通信。在 DGX-1 P100 中，GPU 卡被组织成两个 cube mesh，每个 cube 包含 4 个 GPU（GPU 0~3 和 GPU 4~7）。在每个 cube 内部，GPU 之间可以直接通过 NVLink 或通过 PCIe Switch 进行通信。然而，跨 cube 的通信（例如 GPU 0 和 GPU 4）需要通过其他 GPU 间接进行。</p></li><li><p>DGX-2 引入了英伟达的第一代 NVSwitch 技术，这是一个重要的进步，因为它允许更高效的 GPU 间通信。在 Volta 架构中，每张 GPU 卡支持 6 条 NVLink 链路，而不再是 4 条。此外，通过引入 6 个 NVSwitch，NVSwitch 能够将服务器中的所有 GPU 卡全部互联起来，并且支持 8 对 GPU 同时通信，不再需要任何中间 GPU 跳数，实现直接高速通信，这大大提高了数据传输的效率和整体计算性能。</p></li><li><p>DGX-A100 使用的是第二代 NVSwitch 技术。相比于第一代，第二代 NVSwitch 提供了更高的通信带宽和更低的通信延迟。在 A100 架构中，每张 GPU 卡支持 12 条 NVLink（第三代）链路，并通过 6 个 NVSwitch 实现了全连接的网络拓扑。虽然标准的 DGX A100 配置仅包含 8 块 GPU 卡，但该系统可以扩展，支持更多的 A100 GPU 卡和 NVSwitch，以构建更大规模的超级计算机。</p></li><li><p>DGX-H100 使用的是第三代 NVSwitch 和第四代 NVLink 技术，其中每一个 GPU 卡支持 18 条 NVLink 链路。在 H100 架构中，通过引入了 4 个 NV Switch，采用了分层拓扑的方式，每张卡向第一个 NV Switch 接入 5 条链路，第二个 NV Switch 接入 4 条链路，第三个 NV Switch 接入 4 条链路，第四个 NV Switch 接入 5 条链路，总共 72 个 NVLink 提供 3.6 TB&#x2F;s 全双工 NVLink 网络带宽，比上一代提高 1.5 倍。</p></li></ul><p>第一代 NVSwitch 支持 18 路接口，NVSwitch 能够支持多达 16 个 GPU 的全互联，实现高效的数据共享和通信。</p><p><img src="/2025/05/25/Pytorch%20torch.distributed%20%E5%8F%8ANCCL%E5%88%9D%E6%8E%A2/image-3.png"></p><p>第一代的 NVSwitch 支持的 NVLink 2.0 技术，每个接口能够提供双通道，高达 50GB&#x2F;s 的带宽。这意味着通过 NVSwitch，整个系统能够实现总计 900GB&#x2F;s 的惊人带宽，极大地提升了数据传输速率和计算效率。</p><p>其次，NVSwitch 基于台积电的 12nm FinFET FFN 工艺制造，这种先进的工艺技术使得 NVSwitch 能够在 100W 的功率下运行，同时集成了高达 2 亿个晶体管。</p><p>在 NVSwitch 架构中，任意一对 GPU 都可以直接互联，且只要不超过六个 NVLink 的总带宽，单个 GPU 的流量就可以实现非阻塞传输。这也就意味着，NVSwitch 支持的全互联架构意味着系统可以轻松扩展，以支持更多的 GPU，而不会牺牲性能。每个 GPU 都能利用 NVLink 提供的高带宽，实现快速的数据交换。</p><p>NVSwitch 在解决多 GPU 间的互联有以下优势和特性：</p><ol><li><p>扩展性与可伸缩性：NVSwitch 的引入为 GPU 集群的扩展性提供了强大的支持。通过简单地添加更多的 NVSwitch，系统可以轻松地支持更多的 GPU，从而扩展计算能力。</p></li><li><p>高效的系统构建：例如，8个 GPU 可以通过三个 NVSwitch 构建成一个高效的互连网络(8个GPU对应8*6&#x3D;48个接口，所以需要48&#x2F;16&#x3D;3个NVSwitch)。这种设计允许数据在所有 GPU 链路之间自由交互，最大化了数据流通的灵活性和效率。</p></li></ol><ol start="3"><li><p>全双向带宽利用：在这种配置下，任意一对 GPU 都能够利用完整的 300 GBps 双向带宽进行通信。这意味着每个 GPU 对都能实现高速、低延迟的数据传输，极大地提升了计算任务的处理速度。</p></li><li><p>无阻塞通信：NVSwitch 中的交叉开关（XBAR）为数据传输提供了从点 A 到点 B 的唯一路径。这种设计确保了通信过程中的无阻塞和无干扰，进一步提升了数据传输的可靠性和系统的整体性能。</p></li></ol><ol start="5"><li>优化的网络拓扑：NVSwitch 支持的网络拓扑结构为构建大型 GPU 集群提供了优化的解决方案。它允许系统设计者根据具体的计算需求，灵活地配置 GPU 之间的连接方式。</li></ol><p>这里贴一个8卡A100的测试结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python">nvidia-smi topo -m<br>        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    CPU Affinity    NUMA Affinity   GPU NUMA ID<br>GPU0     X      NV12    NV12    NV12    NV12    NV12    NV12    NV12    PXB     NODE    SYS     SYS     <span class="hljs-number">0</span>-<span class="hljs-number">31</span>,<span class="hljs-number">64</span>-<span class="hljs-number">95</span>      <span class="hljs-number">0</span>               N/A<br>GPU1    NV12     X      NV12    NV12    NV12    NV12    NV12    NV12    PXB     NODE    SYS     SYS     <span class="hljs-number">0</span>-<span class="hljs-number">31</span>,<span class="hljs-number">64</span>-<span class="hljs-number">95</span>      <span class="hljs-number">0</span>               N/A<br>GPU2    NV12    NV12     X      NV12    NV12    NV12    NV12    NV12    NODE    PXB     SYS     SYS     <span class="hljs-number">0</span>-<span class="hljs-number">31</span>,<span class="hljs-number">64</span>-<span class="hljs-number">95</span>      <span class="hljs-number">0</span>               N/A<br>GPU3    NV12    NV12    NV12     X      NV12    NV12    NV12    NV12    NODE    PXB     SYS     SYS     <span class="hljs-number">0</span>-<span class="hljs-number">31</span>,<span class="hljs-number">64</span>-<span class="hljs-number">95</span>      <span class="hljs-number">0</span>               N/A<br>GPU4    NV12    NV12    NV12    NV12     X      NV12    NV12    NV12    SYS     SYS     PXB     NODE    <span class="hljs-number">32</span>-<span class="hljs-number">63</span>,<span class="hljs-number">96</span>-<span class="hljs-number">127</span>    <span class="hljs-number">1</span>               N/A<br>GPU5    NV12    NV12    NV12    NV12    NV12     X      NV12    NV12    SYS     SYS     PXB     NODE    <span class="hljs-number">32</span>-<span class="hljs-number">63</span>,<span class="hljs-number">96</span>-<span class="hljs-number">127</span>    <span class="hljs-number">1</span>               N/A<br>GPU6    NV12    NV12    NV12    NV12    NV12    NV12     X      NV12    SYS     SYS     NODE    PXB     <span class="hljs-number">32</span>-<span class="hljs-number">63</span>,<span class="hljs-number">96</span>-<span class="hljs-number">127</span>    <span class="hljs-number">1</span>               N/A<br>GPU7    NV12    NV12    NV12    NV12    NV12    NV12    NV12     X      SYS     SYS     NODE    PXB     <span class="hljs-number">32</span>-<span class="hljs-number">63</span>,<span class="hljs-number">96</span>-<span class="hljs-number">127</span>    <span class="hljs-number">1</span>               N/A<br>NIC0    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS      X      NODE    SYS     SYS<br>NIC1    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     NODE     X      SYS     SYS<br>NIC2    SYS     SYS     SYS     SYS     PXB     PXB     NODE    NODE    SYS     SYS      X      NODE<br>NIC3    SYS     SYS     SYS     SYS     NODE    NODE    PXB     PXB     SYS     SYS     NODE     X <br><br>Legend:<br><br>  X    = Self<br>  SYS  = Connection traversing PCIe <span class="hljs-keyword">as</span> well <span class="hljs-keyword">as</span> the SMP interconnect between NUMA nodes (e.g., QPI/UPI)<br>  NODE = Connection traversing PCIe <span class="hljs-keyword">as</span> well <span class="hljs-keyword">as</span> the interconnect between PCIe Host Bridges within a NUMA node<br>  PHB  = Connection traversing PCIe <span class="hljs-keyword">as</span> well <span class="hljs-keyword">as</span> a PCIe Host Bridge (typically the CPU)<br>  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)<br>  PIX  = Connection traversing at most a single PCIe bridge<br>  NV<span class="hljs-comment">#  = Connection traversing a bonded set of # NVLinks</span><br><br>NIC Legend:<br><br>  NIC0: mlx5_0<br>  NIC1: mlx5_1<br>  NIC2: mlx5_2<br>  NIC3: mlx5_3<br></code></pre></td></tr></table></figure><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><p><a href="https://pytorch.ac.cn/docs/stable/distributed.html">https://pytorch.ac.cn/docs/stable/distributed.html</a></p></li><li><p><a href="https://www.zhihu.com/question/63219175/answer/2768301153">https://www.zhihu.com/question/63219175/answer/2768301153</a></p></li><li><p><a href="https://www.zhihu.com/question/63219175/answer/206697974">https://www.zhihu.com/question/63219175/answer/206697974</a></p></li><li><p><a href="https://chenzomi12.github.io/02Hardware04NVIDIA/06DeepNvswitch.html">https://chenzomi12.github.io/02Hardware04NVIDIA/06DeepNvswitch.html</a></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GPU架构概览</title>
    <link href="/2025/05/10/GPU%E6%9E%B6%E6%9E%84%E7%AE%80%E4%BB%8B/"/>
    <url>/2025/05/10/GPU%E6%9E%B6%E6%9E%84%E7%AE%80%E4%BB%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="GPU架构概览"><a href="#GPU架构概览" class="headerlink" title="GPU架构概览"></a>GPU架构概览</h1><h2 id="物理体系架构"><a href="#物理体系架构" class="headerlink" title="物理体系架构"></a>物理体系架构</h2><p>下图是A100的物理体系架构：</p><ul><li>绿色部分是计算核心</li></ul><blockquote><p>在 NVidia 的 GPU 里，最基本的处理单元是SP（Streaming Processor），A100中，64SP会组成一个SM（streaming Multiprocessor），SM是GPU中调度的基础单元，A100中总共具有108个SM，所以得到共有108*64&#x3D;6192个计算核心。</p></blockquote><ul><li><p>中间蓝色部分是L2缓存</p></li><li><p>最上面是PCIE层，通过PCIE接口以外设的方式集成到服务器上。</p></li><li><p>最下面是NVLink，它是多个GPU间进行通信的组件，会对GPU之间的通信做些优化</p></li><li><p>两侧的HBM2就是显存，目前的A100的显存有两种40G and 80G</p></li></ul><p><img src="/2025/05/10/GPU%E6%9E%B6%E6%9E%84%E7%AE%80%E4%BB%8B/image-5.png"></p><p>SM是核心的计算元件，下图是A100的SM的结构，可以看到：</p><ul><li><p>存在一个192KB的L1级的Cache</p></li><li><p>各计算单元SP&#x2F;CUDA Core存在一个L0级的指令Cache</p></li></ul><blockquote><p>在 Fermi 架构之后，SP 被改称为 CUDA Core，通过 CUDA 来控制具体的指令执行。</p></blockquote><ul><li><p>计算单元存在适配 FP16、BF16、TF32、FP64、INT8、INT4 和 Binary 等各类数据类型运算的<strong>向量</strong>运行单元</p></li><li><p>计算单元存在一个额外的Tensor core去进行<strong>张量</strong>计算</p></li></ul><blockquote><p>每个TensorCore提供一个4x4x4矩阵处理数组，它执行操作D&#x3D;A*B+C，其中A、B、C和D是4×4矩阵。每个TensorCore每个时钟周期可以执行64个浮点FMA混合精度操作，而在一个SM中有多个TensorCore。</p><p>张量核心与普通的 CUDA 核心其实有很大的区别，<strong>CUDA 核心在每个时钟周期都可以准确的执行一次整数或者浮点数的运算</strong>，时钟的速度和核心的数量都会影响整体性能。<strong>张量核心通过牺牲一定的精度可以在每个时钟计算执行一次 4 x 4 的矩阵运算</strong>。</p></blockquote><ul><li><p>计算单元还存在一个Warp调度器负责调度执行可运行的Warp</p></li><li><p>计算单元有专门的特殊函数的计算单元（Special Functions Unit、SPU），（超越函数和数学函数,反平方根、正余弦啥的）</p></li><li><p>计算单元还存在Dispatch Unit作为指令分发单元</p></li></ul><p><img src="/2025/05/10/GPU%E6%9E%B6%E6%9E%84%E7%AE%80%E4%BB%8B/image-4.png"></p><p>其内存架构如下图所示：</p><ul><li><p>最外层是HBM内存，大小为80GB</p></li><li><p>然后还有一个SM共享的L2 Cache，大小为40MB</p></li><li><p>每个SM有一个L1级的Cache，大小为192KB</p></li><li><p>每个SM的寄存器也可以视作一个缓存，大小为256KB</p></li><li><p>GPU与CPU之间通过PCIe进行通信</p></li></ul><p><img src="/2025/05/10/GPU%E6%9E%B6%E6%9E%84%E7%AE%80%E4%BB%8B/image-3.png"></p><h2 id="CUDA编程模型下的逻辑体系结构"><a href="#CUDA编程模型下的逻辑体系结构" class="headerlink" title="CUDA编程模型下的逻辑体系结构"></a>CUDA编程模型下的逻辑体系结构</h2><h3 id="CUDA简介"><a href="#CUDA简介" class="headerlink" title="CUDA简介"></a>CUDA简介</h3><p>一个CUDA程序的可以分为两个部分: 在CPU上运行的<code>Host</code>程序；在GPU上运行的<code>Device</code>程序。两者拥有各自的存储器。GPU上运行的函数又被叫做<code>kernel</code>函数，通过<code>global</code>关键字声名，例如：</p><p>可以看到在程序中我们需要声明用多少个block，每个block中使用多少线程。线程可以使用1维、2维、3维的线程索引(thread index)来标识，block也可以使用1维、2维、3维的索引。例如在上面的启动命令<code>kernel&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;()</code>中就分别制定了block的数量以及每个block中threads的数量。然后在具体的kernel计算就可以根据相关id计算出要取第几位的数。</p><h3 id="计算线程模型简介"><a href="#计算线程模型简介" class="headerlink" title="计算线程模型简介"></a>计算线程模型简介</h3><p>在CUDA架构下，线程的组织结构可以统一成如下所示：</p><p><img src="/2025/05/10/GPU%E6%9E%B6%E6%9E%84%E7%AE%80%E4%BB%8B/image-2.png"></p><p>总的来说，CUDA 可以分为 Grid，Block 和 Thread 三个层次结构：</p><ul><li><p>线程层次结构Ⅰ-Grid：Kernel 在 device 上执行时，实际上是启动很多线程，一个 Kernel 所启动的所有线程称为一个网格（grid），同一个网格上的线程共享相同的全局内存空间，grid 是线程结构的第一层次。</p></li><li><p>线程层次结构Ⅱ-Block：Grid 分为多个线程块（block），一个 block 里面包含很多线程，Block 之间并行执行，并且无法通信，也没有执行顺序，每个 block 包含共享内存（shared memory），可以共享里面的 Thread。</p></li><li><p>线程层次结Ⅲ-Thread：CUDA 并行程序实际上会被多个 threads 执行，多个 threads 会被群组成一个线程 block，同一个 block 中 threads 可以同步，也可以通过 shared memory 通信。</p></li></ul><p><strong>将这些线程与硬件进行对应的关系如下所示：</strong></p><table><thead><tr><th><strong>软件</strong></th><th><strong>硬件</strong></th></tr></thead><tbody><tr><td>Thread</td><td>SP&#x2F;CUDA Core</td></tr><tr><td>Block</td><td>SM</td></tr><tr><td>Grid</td><td>一组SM</td></tr></tbody></table><p>注意往往32个Thread会组成一个Warp，Warp的调度流程如下：</p><ol><li><p>线程启动后，被分组成若干个 warp（每 32 个线程为一组）</p></li><li><p>每个 warp 被分配到某个 SM 上执行</p></li><li><p>SM 的 warp scheduler 轮询多个 warp，选择 ready warp 发射指令</p></li></ol><blockquote><p>同一个warp中的thread可以以任意顺序执行，active warps被SM资源限制。当一个warp空闲时，SM就可以调度驻留在该SM中另一个可用warp。在并发的warp之间切换是没什么消耗的，因为硬件资源早就被分配到所有thread和block，所以新调度的warp的状态已经存储在SM中了。</p><p>GPU 的线程切换只是切换了寄存器组（一个 SM 中有高达 64k 个寄存器），延迟超级低，几乎没有成本。一个 CUDA Core 可以随时在八个线程之间反复横跳，哪个线程数据准备好了就执行哪个。&#x20;</p></blockquote><ul><li>warp 内所有线程执行同一条指令，但使用不同数据（即 data-parallel）</li></ul><blockquote><p>GPU 控制部件面积比较小，为了节约控制器，<strong>一个 Warp 内部的所有 CUDA Core 的 PC（程序计数器）一直是同步的，但是访存地址是可以不同的，每个核心还可以有自己独立的寄存器组，它们使用不同的数据执行相同的命令</strong>，这种执行方式叫做 SIMT（Single Instruction Multi Trhead）。</p></blockquote><ul><li>遇到分支时，warp divergence 会导致串行化执行不同路径</li></ul><blockquote><p>极端情况下，每一个Core的指令流都不一样，那么甚至还可能导致一个 Warp 中仅有一个 Core 在工作，效率降低为 1&#x2F;32.</p><p><img src="/2025/05/10/GPU%E6%9E%B6%E6%9E%84%E7%AE%80%E4%BB%8B/image.png"></p></blockquote><h3 id="内存模型简介"><a href="#内存模型简介" class="headerlink" title="内存模型简介"></a>内存模型简介</h3><p><code>CUDA threads</code>在执行时，可以访问多个<code>memory spaces</code>，每个线程有自己的私有的<code>local memory</code>。每个block有一个<code>shared memory</code>，block的所有线程都可以访问。最后，所有线程都可以访问<code>global memory</code>。</p><p><img src="/2025/05/10/GPU%E6%9E%B6%E6%9E%84%E7%AE%80%E4%BB%8B/image-1.png"></p><h1 id="历史架构演进"><a href="#历史架构演进" class="headerlink" title="历史架构演进"></a>历史架构演进</h1><table><thead><tr><th>架构名称</th><th>中文名字</th><th>发布时间</th><th>核心参数</th><th>特点&amp;优势</th><th>纳米制程</th><th>代表型号</th></tr></thead><tbody><tr><td>Fermi</td><td>费米</td><td>2010</td><td>16 个 SM，每个 SM 包含 32 个 CUDA Cores，一共 512 CUDA Cores</td><td>首个完整 GPU 计算架构，支持与共享存储结合的 Cache 层次 GPU 架构，支持 ECC GPU 架构</td><td>40&#x2F;28nm, 30 亿晶体管</td><td>Quadro 7000</td></tr><tr><td>Kepler</td><td>开普勒</td><td>2012</td><td>15 个 SMX，每个 SMX 包括 192 个 FP32+64 个 FP64 CUDA Cores</td><td>游戏性能大幅提升，首次支持 GPU Direct 技术</td><td>28nm, 71 亿晶体管</td><td>K80, K40M</td></tr><tr><td>Maxwell</td><td>麦克斯韦</td><td>2014</td><td>16 个 SM，每个 SM 包括 4 个处理块，每个处理块包括 32 个 CUDA Cores+8 个 LD&#x2F;ST Unit + 8 SFU</td><td>每组 SM 单元从 192 个减少到每组 128 个，每个 SMM 单元拥有更多逻辑控制电路</td><td>28nm, 80 亿晶体管</td><td>M5000, M4000GTX 9XX 系列</td></tr><tr><td>Pascal</td><td>帕斯卡</td><td>2016</td><td>GP100 有 60 个 SM，每个 SM 包括 64 个 CUDA Cores，32 个 DP Cores</td><td>NVLink 第一代，双向互联带宽 160GB&#x2F;s，P100 拥有 56 个 SM HBM</td><td>16nm, 153 亿晶体管</td><td>P100, P6000, TTX1080</td></tr><tr><td>Volta</td><td>伏特</td><td>2017</td><td>80 个 SM，每个 SM 包括 32 个 FP64+64 Int32+64 FP32+8 个 Tensor Cores</td><td>NVLink2.0，Tensor Cores 第一代，支持 AI 运算，NVSwitch1.0</td><td>12nm, 211 亿晶体管</td><td>V100, TiTan V</td></tr><tr><td>Turing</td><td>图灵</td><td>2018</td><td>102 核心 92 个 SM，SM 重新设计，每个 SM 包含 64 个 Int32+64 个 FP32+8 个 Tensor Cores</td><td>Tensor Core2.0，RT Core 第一代</td><td>12nm, 186 亿晶体管</td><td>T4，2080TI, RTX 5000</td></tr><tr><td>Ampere</td><td>安培</td><td>2020</td><td>108 个 SM，每个 SM 包含 64 个 FP32+64 个 INT32+32 个 FP64+4 个 Tensor Cores</td><td>Tensor Core3.0，RT Core2.0，NVLink3.0，结构稀疏性矩阵 MIG1.0</td><td>7nm, 283 亿晶体管</td><td>A100, A30 系列</td></tr><tr><td>Hopper</td><td>赫柏</td><td>2022</td><td>132 个 SM，每个 SM 包含 128 个 FP32+64 个 INT32+64 个 FP64+4 个 Tensor Cores</td><td>Tensor Core4.0，NVLink4.0，结构稀疏性矩阵 MIG2.0</td><td>4nm, 800 亿晶体管</td><td>H100</td></tr><tr><td>Blackwell</td><td>布莱克韦尔</td><td>2024</td><td>-</td><td>Tensor Core5.0，NVLink5.0, 第二代 Transformer 引擎，支持 RAS</td><td>4NP, 2080 亿晶体管</td><td></td></tr></tbody></table><p>具体可以看这个：</p><p><a href="https://github.com/chenzomi12/AISystem/blob/main/02Hardware/03GPUBase/04History.md">https://github.com/chenzomi12/AISystem/blob/main/02Hardware/03GPUBase/04History.md</a></p><ul><li><p><strong>京东算法中台：</strong>&#x6536;推广、大模型、多模态。计算引擎、算力、调度、容错。团队规模60、70。</p></li><li><p>通用模型</p></li><li><p>数据、数据模型框架衔接，样本中心。</p></li><li><p>算法数据。批流一体，数据存储，数据使用</p></li><li><p>算力引擎：100B-》1000B</p></li><li><p>稀疏参数：行为流特征，从100GB到万GB。</p></li><li><p>模型断点续训练，数据怎么断点，如何细粒度恢复。向量化表示。</p></li><li><p><strong>京东上班节奏：10点～晚8点。绝大多数不加班。</strong></p></li></ul><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><p><a href="https://hustcat.github.io/gpu-architecture/">https://hustcat.github.io/gpu-architecture/</a></p></li><li><p><a href="https://www.zhihu.com/question/319355296/answer/2193938981">https://www.zhihu.com/question/319355296/answer/2193938981</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/8435687930">https://zhuanlan.zhihu.com/p/8435687930</a></p></li><li><p><a href="https://github.com/chenzomi12/AISystem/blob/main/02Hardware/03GPUBase">https://github.com/chenzomi12/AISystem/blob/main/02Hardware/03GPUBase</a></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【从零构建大模型】四、对模型进行无监督训练</title>
    <link href="/2025/05/04/LLMFromScratch4/"/>
    <url>/2025/05/04/LLMFromScratch4/</url>
    
    <content type="html"><![CDATA[<h1 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h1><p>构建大模型的全景图如下，本文介绍了如何训练大模型以及如何重加载已有的预训练过的大模型参数。</p><p><img src="/2025/05/04/LLMFromScratch4/image-4.png"></p><p>介绍的脉络如下：</p><p><img src="/2025/05/04/LLMFromScratch4/image-3.png"></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><h2 id="Evaluating-generative-text-models"><a href="#Evaluating-generative-text-models" class="headerlink" title="Evaluating generative text models"></a>Evaluating generative text models</h2><h3 id="Using-GPT-to-generate-text"><a href="#Using-GPT-to-generate-text" class="headerlink" title="Using GPT to generate text"></a>Using GPT to generate text</h3><p>简单回顾一下GPT模型，其结构的关键参数如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> previous_chapters <span class="hljs-keyword">import</span> GPTModel<br><span class="hljs-comment"># If the `previous_chapters.py` file is not available locally,</span><br><span class="hljs-comment"># you can import it from the `llms-from-scratch` PyPI package.</span><br><span class="hljs-comment"># For details, see: https://github.com/rasbt/LLMs-from-scratch/tree/main/pkg</span><br><span class="hljs-comment"># E.g.,</span><br><span class="hljs-comment"># from llms_from_scratch.ch04 import GPTModel</span><br><br>GPT_CONFIG_124M = &#123;<br>    <span class="hljs-string">&quot;vocab_size&quot;</span>: <span class="hljs-number">50257</span>,   <span class="hljs-comment"># Vocabulary size</span><br>    <span class="hljs-string">&quot;context_length&quot;</span>: <span class="hljs-number">256</span>, <span class="hljs-comment"># Shortened context length (orig: 1024)</span><br>    <span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">768</span>,        <span class="hljs-comment"># Embedding dimension</span><br>    <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">12</span>,         <span class="hljs-comment"># Number of attention heads</span><br>    <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">12</span>,        <span class="hljs-comment"># Number of layers</span><br>    <span class="hljs-string">&quot;drop_rate&quot;</span>: <span class="hljs-number">0.1</span>,      <span class="hljs-comment"># Dropout rate</span><br>    <span class="hljs-string">&quot;qkv_bias&quot;</span>: <span class="hljs-literal">False</span>      <span class="hljs-comment"># Query-key-value bias</span><br>&#125;<br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>model = GPTModel(GPT_CONFIG_124M)<br>model.<span class="hljs-built_in">eval</span>();  <span class="hljs-comment"># Disable dropout during inference</span><br></code></pre></td></tr></table></figure><blockquote><p>注意在现在流行的LLM中往往已经不给<code>nn.Linear</code>加入bias层。为了降低计算开销把上下文长度从1024缩减到了256。</p></blockquote><p>一个简单的对输入输出进行解码器的函数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tiktoken<br><span class="hljs-keyword">from</span> previous_chapters <span class="hljs-keyword">import</span> generate_text_simple<br><br><span class="hljs-comment"># Alternatively:</span><br><span class="hljs-comment"># from llms_from_scratch.ch04 import generate_text_simple</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">text_to_token_ids</span>(<span class="hljs-params">text, tokenizer</span>):<br>    encoded = tokenizer.encode(text, allowed_special=&#123;<span class="hljs-string">&#x27;&lt;|endoftext|&gt;&#x27;</span>&#125;)<br>    encoded_tensor = torch.tensor(encoded).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># add batch dimension</span><br>    <span class="hljs-keyword">return</span> encoded_tensor<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">token_ids_to_text</span>(<span class="hljs-params">token_ids, tokenizer</span>):<br>    flat = token_ids.squeeze(<span class="hljs-number">0</span>) <span class="hljs-comment"># remove batch dimension</span><br>    <span class="hljs-keyword">return</span> tokenizer.decode(flat.tolist())<br><br>start_context = <span class="hljs-string">&quot;Every effort moves you&quot;</span><br>tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br><br>token_ids = generate_text_simple(<br>    model=model,<br>    idx=text_to_token_ids(start_context, tokenizer),<br>    max_new_tokens=<span class="hljs-number">10</span>,<br>    context_size=GPT_CONFIG_124M[<span class="hljs-string">&quot;context_length&quot;</span>]<br>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output text:\n&quot;</span>, token_ids_to_text(token_ids, tokenizer))<br><span class="hljs-comment"># Output text:</span><br><span class="hljs-comment">#  Every effort moves you rentingetic wasnم refres RexMeCHicular stren</span><br></code></pre></td></tr></table></figure><p>可以看到现在没有被训了的模型的输出还是很混乱的。</p><h3 id="Calculating-the-text-generation-loss-cross-entropy-and-perplexity"><a href="#Calculating-the-text-generation-loss-cross-entropy-and-perplexity" class="headerlink" title="Calculating the text generation loss: cross-entropy and perplexity"></a>Calculating the text generation loss: cross-entropy and perplexity</h3><p>假设现在一个batch中有两个训练实例，那么其target就是相应的右移一位的结果，如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs = torch.tensor([[<span class="hljs-number">16833</span>, <span class="hljs-number">3626</span>, <span class="hljs-number">6100</span>],   <span class="hljs-comment"># [&quot;every effort moves&quot;,</span><br>                       [<span class="hljs-number">40</span>,    <span class="hljs-number">1107</span>, <span class="hljs-number">588</span>]])   <span class="hljs-comment">#  &quot;I really like&quot;]</span><br><br>targets = torch.tensor([[<span class="hljs-number">3626</span>, <span class="hljs-number">6100</span>, <span class="hljs-number">345</span>  ],  <span class="hljs-comment"># [&quot; effort moves you&quot;,</span><br>                        [<span class="hljs-number">1107</span>,  <span class="hljs-number">588</span>, <span class="hljs-number">11311</span>]]) <span class="hljs-comment">#  &quot; really like chocolate&quot;]</span><br></code></pre></td></tr></table></figure><p>最后在经过模型前向传播后我们会得到每一个batch中各个前缀的推理结果，这个结果的维度是字典中各个token的概览，简单地通过softmax获取最高可能性的token就可以得到推理的结果，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> torch.no_grad():<br>    logits = model(inputs)<br><br>probas = torch.softmax(logits, dim=-<span class="hljs-number">1</span>) <span class="hljs-comment"># Probability of each token in vocabulary</span><br><span class="hljs-built_in">print</span>(probas.shape) <span class="hljs-comment"># Shape: (batch_size, num_tokens, vocab_size)</span><br><span class="hljs-comment"># 输出</span><br><span class="hljs-comment"># torch.Size([2, 3, 50257])</span><br><br>token_ids = torch.argmax(probas, dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Token IDs:\n&quot;</span>, token_ids)<br><span class="hljs-comment"># 输出</span><br><span class="hljs-comment"># Token IDs:</span><br><span class="hljs-comment">#  tensor([[[16657],</span><br><span class="hljs-comment">#          [  339],</span><br><span class="hljs-comment">#          [42826]],</span><br><span class="hljs-comment"># </span><br><span class="hljs-comment">#         [[49906],</span><br><span class="hljs-comment">#          [29669],</span><br><span class="hljs-comment">#          [41751]]])</span><br>         <br>         <br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Targets batch 1: <span class="hljs-subst">&#123;token_ids_to_text(targets[<span class="hljs-number">0</span>], tokenizer)&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Outputs batch 1: <span class="hljs-subst">&#123;token_ids_to_text(token_ids[<span class="hljs-number">0</span>].flatten(), tokenizer)&#125;</span>&quot;</span>)<br><span class="hljs-comment"># Targets batch 1:  effort moves you</span><br><span class="hljs-comment"># Outputs batch 1:  Armed heNetflix</span><br></code></pre></td></tr></table></figure><p>现在的输出是混乱的，我们需要计算其与我们想要的target之间的距离，直接去看我们的target的token在输出中预测的概率即可，因为我们最后是希望其对应的概率是1。</p><p>而由于在训练过程中优化概率的对数比优化概率本身更加容易，所以我们会对概率取一个对数，又因为我们一般都是说最小化某个值，所以我们再取一个负数修改目标为最小化。而这也叫做<code>cross_entropy</code>（交叉熵）。</p><blockquote><p>更通用的交叉墒的公式为：</p><p>$$H(y, \hat{y}) &#x3D; -\sum_{i&#x3D;1}^{C} y_i \log(\hat{y}_i)$$</p><p>由于我们这目标是唯一的一个target，而其他token的目标概率都为0，所以就可以只关注目标y这一个。</p></blockquote><p>纯手写的计算方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">text_idx = <span class="hljs-number">0</span><br>target_probas_1 = probas[text_idx, [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>], targets[text_idx]]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Text 1:&quot;</span>, target_probas_1)<br><span class="hljs-comment"># Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])</span><br><br>text_idx = <span class="hljs-number">1</span><br>target_probas_2 = probas[text_idx, [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>], targets[text_idx]]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Text 2:&quot;</span>, target_probas_2)<br><span class="hljs-comment"># Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])</span><br><br><span class="hljs-comment"># Compute logarithm of all token probabilities</span><br>log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))<br><span class="hljs-built_in">print</span>(log_probas)<br><span class="hljs-comment"># tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])</span><br><br><span class="hljs-comment"># Calculate the average probability for each token</span><br>avg_log_probas = torch.mean(log_probas)<br>neg_avg_log_probas = avg_log_probas * -<span class="hljs-number">1</span><br><span class="hljs-built_in">print</span>(neg_avg_log_probas)<br><span class="hljs-comment"># tensor(10.7940)</span><br></code></pre></td></tr></table></figure><p>直接调用pytorch的写法如下，稍微不同的一点在于我们需要先将其展开，消去batch维度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">logits_flat = logits.flatten(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>targets_flat = targets.flatten()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Flattened logits:&quot;</span>, logits_flat.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Flattened targets:&quot;</span>, targets_flat.shape)<br><br><span class="hljs-comment"># Flattened logits: torch.Size([6, 50257])</span><br><span class="hljs-comment"># Flattened targets: torch.Size([6])</span><br><br>loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)<br><span class="hljs-built_in">print</span>(loss)<br><span class="hljs-comment"># tensor(10.7940)</span><br></code></pre></td></tr></table></figure><p>此外直接对交叉墒求指数就可以得到困惑度，它代表了大模型对输出的不确定性，越低的困惑度就更接近真实的分布，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">perplexity = torch.exp(loss)<br><span class="hljs-built_in">print</span>(perplexity)<br><span class="hljs-comment"># tensor(48725.8203)</span><br></code></pre></td></tr></table></figure><h3 id="Calculating-the-training-and-validation-set-losses"><a href="#Calculating-the-training-and-validation-set-losses" class="headerlink" title="Calculating the training and validation set losses"></a>Calculating the training and validation set losses</h3><p>在训练过程中会将数据分为训练集和测试集，最简单的分法就是直接将文本按比例进行划分，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> previous_chapters <span class="hljs-keyword">import</span> create_dataloader_v1<br><span class="hljs-comment"># Alternatively:</span><br><span class="hljs-comment"># from llms_from_scratch.ch02 import create_dataloader_v1</span><br><br><span class="hljs-comment"># Train/validation ratio</span><br>train_ratio = <span class="hljs-number">0.90</span><br>split_idx = <span class="hljs-built_in">int</span>(train_ratio * <span class="hljs-built_in">len</span>(text_data))<br>train_data = text_data[:split_idx]<br>val_data = text_data[split_idx:]<br><br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br><br>train_loader = create_dataloader_v1(<br>    train_data,<br>    batch_size=<span class="hljs-number">2</span>,<br>    max_length=GPT_CONFIG_124M[<span class="hljs-string">&quot;context_length&quot;</span>],<br>    stride=GPT_CONFIG_124M[<span class="hljs-string">&quot;context_length&quot;</span>],<br>    drop_last=<span class="hljs-literal">True</span>,<br>    shuffle=<span class="hljs-literal">True</span>,<br>    num_workers=<span class="hljs-number">0</span><br>)<br><br>val_loader = create_dataloader_v1(<br>    val_data,<br>    batch_size=<span class="hljs-number">2</span>,<br>    max_length=GPT_CONFIG_124M[<span class="hljs-string">&quot;context_length&quot;</span>],<br>    stride=GPT_CONFIG_124M[<span class="hljs-string">&quot;context_length&quot;</span>],<br>    drop_last=<span class="hljs-literal">False</span>,<br>    shuffle=<span class="hljs-literal">False</span>,<br>    num_workers=<span class="hljs-number">0</span><br>)<br></code></pre></td></tr></table></figure><p>然后计算每一个batch的的损失方法以及一个数据集中指定batch数量的损失的方法如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">calc_loss_batch</span>(<span class="hljs-params">input_batch, target_batch, model, device</span>):<br>    input_batch, target_batch = input_batch.to(device), target_batch.to(device)<br>    logits = model(input_batch)<br>    loss = torch.nn.functional.cross_entropy(logits.flatten(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>), target_batch.flatten())<br>    <span class="hljs-keyword">return</span> loss<br>    <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">calc_loss_loader</span>(<span class="hljs-params">data_loader, model, device, num_batches=<span class="hljs-literal">None</span></span>):<br>    total_loss = <span class="hljs-number">0.</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(data_loader) == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;nan&quot;</span>)<br>    <span class="hljs-keyword">elif</span> num_batches <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        num_batches = <span class="hljs-built_in">len</span>(data_loader)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># Reduce the number of batches to match the total number of batches in the data loader</span><br>        <span class="hljs-comment"># if num_batches exceeds the number of batches in the data loader</span><br>        num_batches = <span class="hljs-built_in">min</span>(num_batches, <span class="hljs-built_in">len</span>(data_loader))<br>    <span class="hljs-keyword">for</span> i, (input_batch, target_batch) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_loader):<br>        <span class="hljs-keyword">if</span> i &lt; num_batches:<br>            loss = calc_loss_batch(input_batch, target_batch, model, device)<br>            total_loss += loss.item()<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-keyword">return</span> total_loss / num_batches<br></code></pre></td></tr></table></figure><p>此时直接计算整个训练集和测试集的损失如下所示，可以看到都是比较大的损失：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> torch.no_grad(): <span class="hljs-comment"># Disable gradient tracking for efficiency because we are not training, yet</span><br>    train_loss = calc_loss_loader(train_loader, model, device)<br>    val_loss = calc_loss_loader(val_loader, model, device)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training loss:&quot;</span>, train_loss)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Validation loss:&quot;</span>, val_loss)<br><span class="hljs-comment"># Training loss: 10.987583372328016</span><br><span class="hljs-comment"># Validation loss: 10.98110580444336</span><br></code></pre></td></tr></table></figure><h2 id="Training-an-LLM"><a href="#Training-an-LLM" class="headerlink" title="Training an LLM"></a>Training an LLM</h2><p>暂时不考虑一些高阶的大模型训练方法，其整体的训练流程如下：</p><p><img src="/2025/05/04/LLMFromScratch4/image-1.png"></p><p>训练代码如下：</p><ul><li><p>与之前相比新加的内容为添加了loss.backward()进行反向传播获得梯度</p></li><li><p>添加了optimizer.step() 进行参数更新</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_model_simple</span>(<span class="hljs-params">model, train_loader, val_loader, optimizer, device, num_epochs,</span><br><span class="hljs-params">                       eval_freq, eval_iter, start_context, tokenizer</span>):<br>    <span class="hljs-comment"># Initialize lists to track losses and tokens seen</span><br>    train_losses, val_losses, track_tokens_seen = [], [], []<br>    tokens_seen, global_step = <span class="hljs-number">0</span>, -<span class="hljs-number">1</span><br><br>    <span class="hljs-comment"># Main training loop</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        model.train()  <span class="hljs-comment"># Set model to training mode</span><br>        <br>        <span class="hljs-keyword">for</span> input_batch, target_batch <span class="hljs-keyword">in</span> train_loader:<br>            optimizer.zero_grad() <span class="hljs-comment"># Reset loss gradients from previous batch iteration</span><br>            loss = calc_loss_batch(input_batch, target_batch, model, device)<br>            loss.backward() <span class="hljs-comment"># Calculate loss gradients</span><br>            optimizer.step() <span class="hljs-comment"># Update model weights using loss gradients</span><br>            tokens_seen += input_batch.numel()<br>            global_step += <span class="hljs-number">1</span><br><br>            <span class="hljs-comment"># Optional evaluation step</span><br>            <span class="hljs-keyword">if</span> global_step % eval_freq == <span class="hljs-number">0</span>:<br>                train_loss, val_loss = evaluate_model(<br>                    model, train_loader, val_loader, device, eval_iter)<br>                train_losses.append(train_loss)<br>                val_losses.append(val_loss)<br>                track_tokens_seen.append(tokens_seen)<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Ep <span class="hljs-subst">&#123;epoch+<span class="hljs-number">1</span>&#125;</span> (Step <span class="hljs-subst">&#123;global_step:06d&#125;</span>): &quot;</span><br>                      <span class="hljs-string">f&quot;Train loss <span class="hljs-subst">&#123;train_loss:<span class="hljs-number">.3</span>f&#125;</span>, Val loss <span class="hljs-subst">&#123;val_loss:<span class="hljs-number">.3</span>f&#125;</span>&quot;</span>)<br><br>        <span class="hljs-comment"># Print a sample text after each epoch</span><br>        generate_and_print_sample(<br>            model, tokenizer, device, start_context<br>        )<br><br>    <span class="hljs-keyword">return</span> train_losses, val_losses, track_tokens_seen<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_model</span>(<span class="hljs-params">model, train_loader, val_loader, device, eval_iter</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)<br>        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)<br>    model.train()<br>    <span class="hljs-keyword">return</span> train_loss, val_loss<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_and_print_sample</span>(<span class="hljs-params">model, tokenizer, device, start_context</span>):<br>    model.<span class="hljs-built_in">eval</span>()<br>    context_size = model.pos_emb.weight.shape[<span class="hljs-number">0</span>]<br>    encoded = text_to_token_ids(start_context, tokenizer).to(device)<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        token_ids = generate_text_simple(<br>            model=model, idx=encoded,<br>            max_new_tokens=<span class="hljs-number">50</span>, context_size=context_size<br>        )<br>    decoded_text = token_ids_to_text(token_ids, tokenizer)<br>    <span class="hljs-built_in">print</span>(decoded_text.replace(<span class="hljs-string">&quot;\n&quot;</span>, <span class="hljs-string">&quot; &quot;</span>))  <span class="hljs-comment"># Compact print format</span><br>    model.train()<br>    <br></code></pre></td></tr></table></figure><p>我们这里采用经典的AdamW优化器，调用上述代码进行训练的代码如下：</p><ul><li>可以看到随着训练的持续，模型可以输出一些更加通顺的句子</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-comment"># Note:</span><br><span class="hljs-comment"># Uncomment the following code to calculate the execution time</span><br><span class="hljs-keyword">import</span> time<br>start_time = time.time()<br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>model = GPTModel(GPT_CONFIG_124M)<br>model.to(device)<br>optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="hljs-number">0.0004</span>, weight_decay=<span class="hljs-number">0.1</span>)<br><br>num_epochs = <span class="hljs-number">10</span><br>train_losses, val_losses, tokens_seen = train_model_simple(<br>    model, train_loader, val_loader, optimizer, device,<br>    num_epochs=num_epochs, eval_freq=<span class="hljs-number">5</span>, eval_iter=<span class="hljs-number">5</span>,<br>    start_context=<span class="hljs-string">&quot;Every effort moves you&quot;</span>, tokenizer=tokenizer<br>)<br><br><span class="hljs-comment"># Note:</span><br><span class="hljs-comment"># Uncomment the following code to show the execution time</span><br>end_time = time.time()<br>execution_time_minutes = (end_time - start_time) / <span class="hljs-number">60</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Training completed in <span class="hljs-subst">&#123;execution_time_minutes:<span class="hljs-number">.2</span>f&#125;</span> minutes.&quot;</span>)<br><br><span class="hljs-comment"># 输出</span><br>Ep <span class="hljs-number">1</span> (Step <span class="hljs-number">000000</span>): Train loss <span class="hljs-number">9.783</span>, Val loss <span class="hljs-number">9.927</span><br>Ep <span class="hljs-number">1</span> (Step 000005): Train loss <span class="hljs-number">7.985</span>, Val loss <span class="hljs-number">8.335</span><br>Every effort moves you,,,,,,,,,,,,.                                     <br>Ep <span class="hljs-number">2</span> (Step <span class="hljs-number">0000</span>10): Train loss <span class="hljs-number">6.753</span>, Val loss <span class="hljs-number">7.048</span><br>Ep <span class="hljs-number">2</span> (Step 000015): Train loss <span class="hljs-number">6.114</span>, Val loss <span class="hljs-number">6.573</span><br>Every effort moves you, <span class="hljs-keyword">and</span>,, <span class="hljs-keyword">and</span>, <span class="hljs-keyword">and</span>,,,,, <span class="hljs-keyword">and</span>, <span class="hljs-keyword">and</span>,,,,,,,,,,,,,, <span class="hljs-keyword">and</span>,,,, <span class="hljs-keyword">and</span>,, <span class="hljs-keyword">and</span>,,,,, <span class="hljs-keyword">and</span>,,,,,,<br>Ep <span class="hljs-number">3</span> (Step 000020): Train loss <span class="hljs-number">5.525</span>, Val loss <span class="hljs-number">6.490</span><br>Ep <span class="hljs-number">3</span> (Step 000025): Train loss <span class="hljs-number">5.324</span>, Val loss <span class="hljs-number">6.387</span><br>Every effort moves you, <span class="hljs-keyword">and</span> to the picture.                      <span class="hljs-string">&quot;I, and the of the of the&#x27;s the honour, and, and I had been, and I</span><br><span class="hljs-string">Ep 4 (Step 000030): Train loss 4.761, Val loss 6.360</span><br><span class="hljs-string">Ep 4 (Step 000035): Train loss 4.461, Val loss 6.258</span><br><span class="hljs-string">Every effort moves you of the to the picture--as of the picture--as I had been &quot;</span> it was his <span class="hljs-string">&quot; I was the     &quot;</span>I was his I had been the his pictures--<span class="hljs-keyword">and</span> it the picture <span class="hljs-keyword">and</span> I had been the picture of<br>Ep <span class="hljs-number">5</span> (Step 000040): Train loss <span class="hljs-number">3.833</span>, Val loss <span class="hljs-number">6.196</span><br>Every effort moves you know the <span class="hljs-string">&quot;Oh, and he was not the fact by his last word.         &quot;</span>I was.      <span class="hljs-string">&quot;Oh, I felt a little a little the    </span><br><span class="hljs-string">Ep 6 (Step 000045): Train loss 3.352, Val loss 6.139</span><br><span class="hljs-string">Ep 6 (Step 000050): Train loss 2.861, Val loss 6.112</span><br><span class="hljs-string">Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.                       </span><br><span class="hljs-string">Ep 7 (Step 000055): Train loss 2.347, Val loss 6.138</span><br><span class="hljs-string">Ep 7 (Step 000060): Train loss 2.084, Val loss 6.179</span><br><span class="hljs-string">Every effort moves you know,&quot;</span> was one of the picture <span class="hljs-keyword">for</span> nothing--I told Mrs.  <span class="hljs-string">&quot;I looked--as of the fact, and I felt him--his back his head to the donkey. &quot;</span>Oh, and_--because he had always _<br>Ep <span class="hljs-number">8</span> (Step 000065): Train loss <span class="hljs-number">1.521</span>, Val loss <span class="hljs-number">6.176</span><br>Ep <span class="hljs-number">8</span> (Step 000070): Train loss <span class="hljs-number">1.272</span>, Val loss <span class="hljs-number">6.178</span><br>Every effort moves you?<span class="hljs-string">&quot; &quot;</span>I didn<span class="hljs-string">&#x27;t bear the picture--I told me.  &quot;I looked up, and went on groping and Mrs. I was back the head to look up at the honour being _mine_--because he was when I</span><br><span class="hljs-string">Ep 9 (Step 000075): Train loss 1.000, Val loss 6.277</span><br><span class="hljs-string">Ep 9 (Step 000080): Train loss 0.718, Val loss 6.281</span><br><span class="hljs-string">Every effort moves you?&quot;  &quot;Yes--quite insensible to the irony. She wanted him vindicated--and by me!&quot;  He laughed again, and threw back his head to look up at the sketch of the donkey. &quot;There were days when I</span><br><span class="hljs-string">Ep 10 (Step 000085): Train loss 0.506, Val loss 6.325</span><br><span class="hljs-string">Every effort moves you?&quot;  &quot;Yes--quite insensible to the irony. She wanted him vindicated--and by me!&quot;  He laughed again, and threw back his head to the donkey again. I saw that, and down the room, when I</span><br><span class="hljs-string">Training completed in 4.03 minutes.</span><br></code></pre></td></tr></table></figure><p>绘制训练集的loss与验证集的loss如下，可以发现整个模型训练过程中训练集的loss在一直快速下降，但是验证集的loss后面就基本保持不变了，说明其实是有发生过拟合的。</p><p><img src="/2025/05/04/LLMFromScratch4/image-2.png"></p><h2 id="Decoding-strategies-to-control-randomness"><a href="#Decoding-strategies-to-control-randomness" class="headerlink" title="Decoding strategies to control randomness"></a>Decoding strategies to control randomness</h2><p>这里介绍一下如何控制输出随机性的内容。上面介绍的输出实际上对于相同的输入一直都是相同的输出，我们希望能够有更加多样化的输出内容。</p><h3 id="Temperature-scaling"><a href="#Temperature-scaling" class="headerlink" title="Temperature scaling"></a>Temperature scaling</h3><p>一个方法就是通过对概率采样来得到输出，因为我们现在最后已经有了各个token的概率，这样概率大的自然采用得到的概率也就更大，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python">vocab = &#123; <br>    <span class="hljs-string">&quot;closer&quot;</span>: <span class="hljs-number">0</span>,<br>    <span class="hljs-string">&quot;every&quot;</span>: <span class="hljs-number">1</span>, <br>    <span class="hljs-string">&quot;effort&quot;</span>: <span class="hljs-number">2</span>, <br>    <span class="hljs-string">&quot;forward&quot;</span>: <span class="hljs-number">3</span>,<br>    <span class="hljs-string">&quot;inches&quot;</span>: <span class="hljs-number">4</span>,<br>    <span class="hljs-string">&quot;moves&quot;</span>: <span class="hljs-number">5</span>, <br>    <span class="hljs-string">&quot;pizza&quot;</span>: <span class="hljs-number">6</span>,<br>    <span class="hljs-string">&quot;toward&quot;</span>: <span class="hljs-number">7</span>,<br>    <span class="hljs-string">&quot;you&quot;</span>: <span class="hljs-number">8</span>,<br>&#125; <br><br>inverse_vocab = &#123;v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> vocab.items()&#125;<br><br><span class="hljs-comment"># Suppose input is &quot;every effort moves you&quot;, and the LLM</span><br><span class="hljs-comment"># returns the following logits for the next token:</span><br>next_token_logits = torch.tensor(<br>    [<span class="hljs-number">4.51</span>, <span class="hljs-number">0.89</span>, -<span class="hljs-number">1.90</span>, <span class="hljs-number">6.75</span>, <span class="hljs-number">1.63</span>, -<span class="hljs-number">1.62</span>, -<span class="hljs-number">1.89</span>, <span class="hljs-number">6.28</span>, <span class="hljs-number">1.79</span>]<br>)<br>probas = torch.softmax(next_token_logits, dim=<span class="hljs-number">0</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">print_sampled_tokens</span>(<span class="hljs-params">probas</span>):<br>    torch.manual_seed(<span class="hljs-number">123</span>) <span class="hljs-comment"># Manual seed for reproducibility</span><br>    sample = [torch.multinomial(probas, num_samples=<span class="hljs-number">1</span>).item() <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1_000</span>)]<br>    sampled_ids = torch.bincount(torch.tensor(sample), minlength=<span class="hljs-built_in">len</span>(probas))<br>    <span class="hljs-keyword">for</span> i, freq <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sampled_ids):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;freq&#125;</span> x <span class="hljs-subst">&#123;inverse_vocab[i]&#125;</span>&quot;</span>)<br><br>print_sampled_tokens(probas)<br><span class="hljs-comment"># 输出</span><br><span class="hljs-number">71</span> x closer<br><span class="hljs-number">2</span> x every<br><span class="hljs-number">0</span> x effort<br><span class="hljs-number">544</span> x forward<br><span class="hljs-number">2</span> x inches<br><span class="hljs-number">1</span> x moves<br><span class="hljs-number">0</span> x pizza<br><span class="hljs-number">376</span> x toward<br><span class="hljs-number">4</span> x you<br></code></pre></td></tr></table></figure><p>此外我们可以控制Temperature scaling（温度缩放）来进一步控制概率分布的整体情况，其做法就是在输出的概率进行softmax前除以一个temperature。</p><ul><li><p>如果temperature大于1那么就会使得各个概率被缩小，整体分布比较平缓</p></li><li><p>如果temperature小于1那么就会使得各个概率被放大，整体分布比较尖锐</p></li></ul><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_with_temperature</span>(<span class="hljs-params">logits, temperature</span>):<br>    scaled_logits = logits / temperature<br>    <span class="hljs-keyword">return</span> torch.softmax(scaled_logits, dim=<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># Temperature values</span><br>temperatures = [<span class="hljs-number">1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">5</span>]  <span class="hljs-comment"># Original, higher confidence, and lower confidence</span><br><br><span class="hljs-comment"># Calculate scaled probabilities</span><br>scaled_probas = [softmax_with_temperature(next_token_logits, T) <span class="hljs-keyword">for</span> T <span class="hljs-keyword">in</span> temperatures]<br></code></pre></td></tr></table></figure><p>同一个概率分布下不同的temperature控制下的概率分布情况如下所示：</p><p><img src="/2025/05/04/LLMFromScratch4/image.png"></p><h3 id="Top-k-sampling"><a href="#Top-k-sampling" class="headerlink" title="Top-k sampling"></a>Top-k sampling</h3><p>通过采样得到输出有可能得到概率较低的输出，这实际上可能确实是错误的输出，所以我们可以将top-k以外的输出概率都置为0来避免这种情况，其实际在处理时也就是在softmax之前将top-k以外的概率置为-inf，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">top_k = <span class="hljs-number">3</span><br>top_logits, top_pos = torch.topk(next_token_logits, top_k)<br><br>new_logits = torch.where(<br>    condition=next_token_logits &lt; top_logits[-<span class="hljs-number">1</span>],<br>    <span class="hljs-built_in">input</span>=torch.tensor(<span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;-inf&quot;</span>)), <br>    other=next_token_logits<br>)<br><br><span class="hljs-built_in">print</span>(new_logits)<br><span class="hljs-comment"># tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])</span><br><br>topk_probas = torch.softmax(new_logits, dim=<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(topk_probas)<br><span class="hljs-comment"># tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])</span><br></code></pre></td></tr></table></figure><h3 id="Modifying-the-text-generation-function"><a href="#Modifying-the-text-generation-function" class="headerlink" title="Modifying the text generation function"></a>Modifying the text generation function</h3><p>通过结合上述两项技术就可以得到一个新的generate函数，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate</span>(<span class="hljs-params">model, idx, max_new_tokens, context_size, temperature=<span class="hljs-number">0.0</span>, top_k=<span class="hljs-literal">None</span>, eos_id=<span class="hljs-literal">None</span></span>):<br><br>    <span class="hljs-comment"># For-loop is the same as before: Get logits, and only focus on last time step</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_new_tokens):<br>        idx_cond = idx[:, -context_size:]<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            logits = model(idx_cond)<br>        logits = logits[:, -<span class="hljs-number">1</span>, :]<br><br>        <span class="hljs-comment"># New: Filter logits with top_k sampling</span><br>        <span class="hljs-keyword">if</span> top_k <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># Keep only top_k values</span><br>            top_logits, _ = torch.topk(logits, top_k)<br>            min_val = top_logits[:, -<span class="hljs-number">1</span>]<br>            logits = torch.where(logits &lt; min_val, torch.tensor(<span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;-inf&quot;</span>)).to(logits.device), logits)<br><br>        <span class="hljs-comment"># New: Apply temperature scaling</span><br>        <span class="hljs-keyword">if</span> temperature &gt; <span class="hljs-number">0.0</span>:<br>            logits = logits / temperature<br><br>            <span class="hljs-comment"># Apply softmax to get probabilities</span><br>            probs = torch.softmax(logits, dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># (batch_size, context_len)</span><br><br>            <span class="hljs-comment"># Sample from the distribution</span><br>            idx_next = torch.multinomial(probs, num_samples=<span class="hljs-number">1</span>)  <span class="hljs-comment"># (batch_size, 1)</span><br><br>        <span class="hljs-comment"># Otherwise same as before: get idx of the vocab entry with the highest logits value</span><br>        <span class="hljs-keyword">else</span>:<br>            idx_next = torch.argmax(logits, dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># (batch_size, 1)</span><br><br>        <span class="hljs-keyword">if</span> idx_next == eos_id:  <span class="hljs-comment"># Stop generating early if end-of-sequence token is encountered and eos_id is specified</span><br>            <span class="hljs-keyword">break</span><br><br>        <span class="hljs-comment"># Same as before: append sampled index to the running sequence</span><br>        idx = torch.cat((idx, idx_next), dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># (batch_size, num_tokens+1)</span><br><br>    <span class="hljs-keyword">return</span> idx<br>    <br>torch.manual_seed(<span class="hljs-number">123</span>)<br><br>token_ids = generate(<br>    model=model,<br>    idx=text_to_token_ids(<span class="hljs-string">&quot;Every effort moves you&quot;</span>, tokenizer),<br>    max_new_tokens=<span class="hljs-number">15</span>,<br>    context_size=GPT_CONFIG_124M[<span class="hljs-string">&quot;context_length&quot;</span>],<br>    top_k=<span class="hljs-number">25</span>,<br>    temperature=<span class="hljs-number">1.4</span><br>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output text:\n&quot;</span>, token_ids_to_text(token_ids, tokenizer))<br><span class="hljs-comment"># Output text:</span><br><span class="hljs-comment">#  Every effort moves you know began to my surprise, a little it was the</span><br><span class="hljs-comment"># &quot;Ah enough</span><br></code></pre></td></tr></table></figure><h2 id="x20-Loading-and-saving-model-weights-in-PyTorch"><a href="#x20-Loading-and-saving-model-weights-in-PyTorch" class="headerlink" title="&#x20;Loading and saving model weights in PyTorch"></a>&#x20;Loading and saving model weights in PyTorch</h2><p>我们可以在训练过程中保存检查点，然后再有需要的时候再进行加载，从而支持断点续训的功能。</p><p>注意为了能够继续训练，我们往往不止需要保持模型的权重还需要保存优化器中的权重。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(&#123;<br>    <span class="hljs-string">&quot;model_state_dict&quot;</span>: model.state_dict(),<br>    <span class="hljs-string">&quot;optimizer_state_dict&quot;</span>: optimizer.state_dict(),<br>    &#125;, <br>    <span class="hljs-string">&quot;model_and_optimizer.pth&quot;</span><br>)<br><br>checkpoint = torch.load(<span class="hljs-string">&quot;model_and_optimizer.pth&quot;</span>, weights_only=<span class="hljs-literal">True</span>)<br><br>model = GPTModel(GPT_CONFIG_124M)<br>model.load_state_dict(checkpoint[<span class="hljs-string">&quot;model_state_dict&quot;</span>])<br><br>optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="hljs-number">0.0005</span>, weight_decay=<span class="hljs-number">0.1</span>)<br>optimizer.load_state_dict(checkpoint[<span class="hljs-string">&quot;optimizer_state_dict&quot;</span>])<br>model.train();<br></code></pre></td></tr></table></figure><h2 id="Loading-pretrained-weights-from-OpenAI"><a href="#Loading-pretrained-weights-from-OpenAI" class="headerlink" title="Loading pretrained weights from OpenAI"></a>Loading pretrained weights from OpenAI</h2><p>我们可以直接通过gpt_download库来下载gpt-2的权重信息，下载方法如下（注意这里已经下好了）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Relative import from the gpt_download.py contained in this folder</span><br><br><span class="hljs-keyword">from</span> gpt_download <span class="hljs-keyword">import</span> download_and_load_gpt2<br><span class="hljs-comment"># Alternatively:</span><br><span class="hljs-comment"># from llms_from_scratch.ch05 import download_and_load_gpt2</span><br><br>settings, params = download_and_load_gpt2(model_size=<span class="hljs-string">&quot;124M&quot;</span>, models_dir=<span class="hljs-string">&quot;gpt2&quot;</span>)<br><br><span class="hljs-comment"># File already exists and is up-to-date: gpt2/124M/checkpoint</span><br><span class="hljs-comment"># File already exists and is up-to-date: gpt2/124M/encoder.json</span><br><span class="hljs-comment"># File already exists and is up-to-date: gpt2/124M/hparams.json</span><br><span class="hljs-comment"># File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001</span><br><span class="hljs-comment"># File already exists and is up-to-date: gpt2/124M/model.ckpt.index</span><br><span class="hljs-comment"># File already exists and is up-to-date: gpt2/124M/model.ckpt.meta</span><br><span class="hljs-comment"># File already exists and is up-to-date: gpt2/124M/vocab.bpe</span><br></code></pre></td></tr></table></figure><p>查看一些关键文件的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Settings:&quot;</span>, settings)<br><span class="hljs-comment"># Settings: &#123;&#x27;n_vocab&#x27;: 50257, &#x27;n_ctx&#x27;: 1024, &#x27;n_embd&#x27;: 768, &#x27;n_head&#x27;: 12, &#x27;n_layer&#x27;: 12&#125;</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Parameter dictionary keys:&quot;</span>, params.keys())<br><span class="hljs-comment"># Parameter dictionary keys: dict_keys([&#x27;blocks&#x27;, &#x27;b&#x27;, &#x27;g&#x27;, &#x27;wpe&#x27;, &#x27;wte&#x27;])</span><br><br><span class="hljs-built_in">print</span>(params[<span class="hljs-string">&quot;wte&quot;</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Token embedding weight tensor dimensions:&quot;</span>, params[<span class="hljs-string">&quot;wte&quot;</span>].shape)<br><br><span class="hljs-comment"># 输出</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208</span><br><span class="hljs-string">   0.04531523]</span><br><span class="hljs-string"> [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983</span><br><span class="hljs-string">   0.04318958]</span><br><span class="hljs-string"> [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379</span><br><span class="hljs-string">  -0.08785918]</span><br><span class="hljs-string"> ...</span><br><span class="hljs-string"> [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269</span><br><span class="hljs-string">  -0.06952604]</span><br><span class="hljs-string"> [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701</span><br><span class="hljs-string">  -0.02245961]</span><br><span class="hljs-string"> [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823</span><br><span class="hljs-string">   0.12067825]]</span><br><span class="hljs-string">Token embedding weight tensor dimensions: (50257, 768)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure><p>可以看到embedding层确实就是我们之前说的支持50257种token，每个token的维度为768。</p><p>我们可以直接将下载到的模型的权重加载进我们自定义的相同结构的模型中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Define model configurations in a dictionary for compactness</span><br>model_configs = &#123;<br>    <span class="hljs-string">&quot;gpt2-small (124M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">768</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">12</span>&#125;,<br>    <span class="hljs-string">&quot;gpt2-medium (355M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1024</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">24</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">16</span>&#125;,<br>    <span class="hljs-string">&quot;gpt2-large (774M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1280</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">36</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">20</span>&#125;,<br>    <span class="hljs-string">&quot;gpt2-xl (1558M)&quot;</span>: &#123;<span class="hljs-string">&quot;emb_dim&quot;</span>: <span class="hljs-number">1600</span>, <span class="hljs-string">&quot;n_layers&quot;</span>: <span class="hljs-number">48</span>, <span class="hljs-string">&quot;n_heads&quot;</span>: <span class="hljs-number">25</span>&#125;,<br>&#125;<br><br><span class="hljs-comment"># Copy the base configuration and update with specific model settings</span><br>model_name = <span class="hljs-string">&quot;gpt2-small (124M)&quot;</span>  <span class="hljs-comment"># Example model name</span><br>NEW_CONFIG = GPT_CONFIG_124M.copy()<br>NEW_CONFIG.update(model_configs[model_name])<br>NEW_CONFIG.update(&#123;<span class="hljs-string">&quot;context_length&quot;</span>: <span class="hljs-number">1024</span>, <span class="hljs-string">&quot;qkv_bias&quot;</span>: <span class="hljs-literal">True</span>&#125;)<br><br>gpt = GPTModel(NEW_CONFIG)<br>gpt.<span class="hljs-built_in">eval</span>();<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">assign</span>(<span class="hljs-params">left, right</span>):<br>    <span class="hljs-keyword">if</span> left.shape != right.shape:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;Shape mismatch. Left: <span class="hljs-subst">&#123;left.shape&#125;</span>, Right: <span class="hljs-subst">&#123;right.shape&#125;</span>&quot;</span>)<br>    <span class="hljs-keyword">return</span> torch.nn.Parameter(torch.tensor(right))<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_weights_into_gpt</span>(<span class="hljs-params">gpt, params</span>):<br>    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[<span class="hljs-string">&#x27;wpe&#x27;</span>])<br>    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[<span class="hljs-string">&#x27;wte&#x27;</span>])<br>    <br>    <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(params[<span class="hljs-string">&quot;blocks&quot;</span>])):<br>        q_w, k_w, v_w = np.split(<br>            (params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;attn&quot;</span>][<span class="hljs-string">&quot;c_attn&quot;</span>])[<span class="hljs-string">&quot;w&quot;</span>], <span class="hljs-number">3</span>, axis=-<span class="hljs-number">1</span>)<br>        gpt.trf_blocks[b].att.W_query.weight = assign(<br>            gpt.trf_blocks[b].att.W_query.weight, q_w.T)<br>        gpt.trf_blocks[b].att.W_key.weight = assign(<br>            gpt.trf_blocks[b].att.W_key.weight, k_w.T)<br>        gpt.trf_blocks[b].att.W_value.weight = assign(<br>            gpt.trf_blocks[b].att.W_value.weight, v_w.T)<br><br>        q_b, k_b, v_b = np.split(<br>            (params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;attn&quot;</span>][<span class="hljs-string">&quot;c_attn&quot;</span>])[<span class="hljs-string">&quot;b&quot;</span>], <span class="hljs-number">3</span>, axis=-<span class="hljs-number">1</span>)<br>        gpt.trf_blocks[b].att.W_query.bias = assign(<br>            gpt.trf_blocks[b].att.W_query.bias, q_b)<br>        gpt.trf_blocks[b].att.W_key.bias = assign(<br>            gpt.trf_blocks[b].att.W_key.bias, k_b)<br>        gpt.trf_blocks[b].att.W_value.bias = assign(<br>            gpt.trf_blocks[b].att.W_value.bias, v_b)<br><br>        gpt.trf_blocks[b].att.out_proj.weight = assign(<br>            gpt.trf_blocks[b].att.out_proj.weight, <br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;attn&quot;</span>][<span class="hljs-string">&quot;c_proj&quot;</span>][<span class="hljs-string">&quot;w&quot;</span>].T)<br>        gpt.trf_blocks[b].att.out_proj.bias = assign(<br>            gpt.trf_blocks[b].att.out_proj.bias, <br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;attn&quot;</span>][<span class="hljs-string">&quot;c_proj&quot;</span>][<span class="hljs-string">&quot;b&quot;</span>])<br><br>        gpt.trf_blocks[b].ff.layers[<span class="hljs-number">0</span>].weight = assign(<br>            gpt.trf_blocks[b].ff.layers[<span class="hljs-number">0</span>].weight, <br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;mlp&quot;</span>][<span class="hljs-string">&quot;c_fc&quot;</span>][<span class="hljs-string">&quot;w&quot;</span>].T)<br>        gpt.trf_blocks[b].ff.layers[<span class="hljs-number">0</span>].bias = assign(<br>            gpt.trf_blocks[b].ff.layers[<span class="hljs-number">0</span>].bias, <br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;mlp&quot;</span>][<span class="hljs-string">&quot;c_fc&quot;</span>][<span class="hljs-string">&quot;b&quot;</span>])<br>        gpt.trf_blocks[b].ff.layers[<span class="hljs-number">2</span>].weight = assign(<br>            gpt.trf_blocks[b].ff.layers[<span class="hljs-number">2</span>].weight, <br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;mlp&quot;</span>][<span class="hljs-string">&quot;c_proj&quot;</span>][<span class="hljs-string">&quot;w&quot;</span>].T)<br>        gpt.trf_blocks[b].ff.layers[<span class="hljs-number">2</span>].bias = assign(<br>            gpt.trf_blocks[b].ff.layers[<span class="hljs-number">2</span>].bias, <br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;mlp&quot;</span>][<span class="hljs-string">&quot;c_proj&quot;</span>][<span class="hljs-string">&quot;b&quot;</span>])<br><br>        gpt.trf_blocks[b].norm1.scale = assign(<br>            gpt.trf_blocks[b].norm1.scale, <br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;ln_1&quot;</span>][<span class="hljs-string">&quot;g&quot;</span>])<br>        gpt.trf_blocks[b].norm1.shift = assign(<br>            gpt.trf_blocks[b].norm1.shift, <br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;ln_1&quot;</span>][<span class="hljs-string">&quot;b&quot;</span>])<br>        gpt.trf_blocks[b].norm2.scale = assign(<br>            gpt.trf_blocks[b].norm2.scale, <br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;ln_2&quot;</span>][<span class="hljs-string">&quot;g&quot;</span>])<br>        gpt.trf_blocks[b].norm2.shift = assign(<br>            gpt.trf_blocks[b].norm2.shift, <br>            params[<span class="hljs-string">&quot;blocks&quot;</span>][b][<span class="hljs-string">&quot;ln_2&quot;</span>][<span class="hljs-string">&quot;b&quot;</span>])<br><br>    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[<span class="hljs-string">&quot;g&quot;</span>])<br>    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[<span class="hljs-string">&quot;b&quot;</span>])<br>    gpt.out_head.weight = assign(gpt.out_head.weight, params[<span class="hljs-string">&quot;wte&quot;</span>])<br>    <br>    <br>load_weights_into_gpt(gpt, params)<br>gpt.to(device);<br></code></pre></td></tr></table></figure><p>然后进行调用和输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(<span class="hljs-number">123</span>)<br><br>token_ids = generate(<br>    model=gpt,<br>    idx=text_to_token_ids(<span class="hljs-string">&quot;Every effort moves you&quot;</span>, tokenizer).to(device),<br>    max_new_tokens=<span class="hljs-number">25</span>,<br>    context_size=NEW_CONFIG[<span class="hljs-string">&quot;context_length&quot;</span>],<br>    top_k=<span class="hljs-number">50</span>,<br>    temperature=<span class="hljs-number">1.5</span><br>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output text:\n&quot;</span>, token_ids_to_text(token_ids, tokenizer))<br><span class="hljs-comment">#  Every effort moves you toward an equal share for each vote plus half. Inequality is often not an accurate representation of human worth; to know the</span><br></code></pre></td></tr></table></figure><p>实际上还有许多其他的模型权重的加载方法，这里先不再赘述。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://knowledge.zhaoweiguo.com/build/html/x-learning/books/ais/2024/build/_llm/_from/_scratch#understanding-llm">https://knowledge.zhaoweiguo.com/build/html/x-learning/books/ais/2024/build\_llm\_from\_scratch#understanding-llm</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>Build a Large Language Model (From Scratch)</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【从零构建大模型】三、从零实现一个 GPT 模型以生成文本</title>
    <link href="/2025/05/03/LLMFromScratch3/"/>
    <url>/2025/05/03/LLMFromScratch3/</url>
    
    <content type="html"><![CDATA[<h1 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h1><p>构建大模型的全景图如下，本文介绍了基础GPT-2系列的模型架构。</p><p><img src="/2025/05/03/LLMFromScratch3/image-8.png"></p><p>介绍的脉络如下：</p><p><img src="/2025/05/03/LLMFromScratch3/image-5.png"></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><h2 id="Coding-an-LLM-architecture"><a href="#Coding-an-LLM-architecture" class="headerlink" title="Coding an LLM architecture"></a>Coding an LLM architecture</h2><p>一个参数量为124 million的GPT-2模型包括了以下的定义参数：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">GPT_CONFIG_124M</span> <span class="hljs-string">=</span> &#123;<br>    <span class="hljs-attr">&quot;vocab_size&quot;:</span> <span class="hljs-number">50257</span>,    <span class="hljs-comment"># Vocabulary size</span><br>    <span class="hljs-attr">&quot;context_length&quot;:</span> <span class="hljs-number">1024</span>, <span class="hljs-comment"># Context length</span><br>    <span class="hljs-attr">&quot;emb_dim&quot;:</span> <span class="hljs-number">768</span>,         <span class="hljs-comment"># Embedding dimension</span><br>    <span class="hljs-attr">&quot;n_heads&quot;:</span> <span class="hljs-number">12</span>,          <span class="hljs-comment"># Number of attention heads</span><br>    <span class="hljs-attr">&quot;n_layers&quot;:</span> <span class="hljs-number">12</span>,         <span class="hljs-comment"># Number of layers</span><br>    <span class="hljs-attr">&quot;drop_rate&quot;:</span> <span class="hljs-number">0.1</span>,       <span class="hljs-comment"># Dropout rate</span><br>    <span class="hljs-attr">&quot;qkv_bias&quot;:</span> <span class="hljs-literal">False</span>       <span class="hljs-comment"># Query-Key-Value bias</span><br>&#125;<br></code></pre></td></tr></table></figure><ul><li><p>vocab_size：单词数量，也就是BPE解码器所支持的单词数量</p></li><li><p>context_length：最长允许输入的token数量，也就是常说的上下文长度</p></li><li><p>emb_dim：Embedding层的维度</p></li><li><p>n_heads：多头注意力中注意力头的数量</p></li><li><p>n_layers：transformer块的数量</p></li><li><p>drop_rate：为了防止过拟合所采用的丢弃率，0.1意味着丢弃10%</p></li><li><p>qkv_bias：Liner层是否再加一个bias层</p></li></ul><p>这些参数在初始化GPT模型的时候采用如下的使用方法（一些层还没有介绍，先留白）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DummyGPTModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.tok_emb = nn.Embedding(cfg[<span class="hljs-string">&quot;vocab_size&quot;</span>], cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        self.pos_emb = nn.Embedding(cfg[<span class="hljs-string">&quot;context_length&quot;</span>], cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        self.drop_emb = nn.Dropout(cfg[<span class="hljs-string">&quot;drop_rate&quot;</span>])<br>        <br>        <span class="hljs-comment"># Use a placeholder for TransformerBlock</span><br>        self.trf_blocks = nn.Sequential(<br>            *[DummyTransformerBlock(cfg) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(cfg[<span class="hljs-string">&quot;n_layers&quot;</span>])])<br>        <br>        <span class="hljs-comment"># Use a placeholder for LayerNorm</span><br>        self.final_norm = DummyLayerNorm(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        self.out_head = nn.Linear(<br>            cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>], cfg[<span class="hljs-string">&quot;vocab_size&quot;</span>], bias=<span class="hljs-literal">False</span><br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, in_idx</span>):<br>        batch_size, seq_len = in_idx.shape<br>        tok_embeds = self.tok_emb(in_idx)<br>        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))<br>        x = tok_embeds + pos_embeds<br>        x = self.drop_emb(x)<br>        x = self.trf_blocks(x)<br>        x = self.final_norm(x)<br>        logits = self.out_head(x)<br>        <span class="hljs-keyword">return</span> logits<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DummyTransformerBlock</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># A simple placeholder</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># This block does nothing and just returns its input.</span><br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DummyLayerNorm</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, normalized_shape, eps=<span class="hljs-number">1e-5</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># The parameters here are just to mimic the LayerNorm interface.</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># This layer does nothing and just returns its input.</span><br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="Normalizing-activations-with-layer-normalization"><a href="#Normalizing-activations-with-layer-normalization" class="headerlink" title="Normalizing activations with layer normalization"></a>Normalizing activations with layer normalization</h2><p>LayerNorm归一化层的作用是将某一个维度中的参数都均值化到0，同时将方差归为1。其处理方法如下：</p><ul><li><p>$$\mu &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} x_i$$：均值</p></li><li><p>$$\sigma^2 &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} (x_i - \mu)^2$$：方差</p></li><li><p>$$\varepsilon$$：小值，防止除以0</p></li></ul><p><img src="/2025/05/03/LLMFromScratch3/image-3.png"></p><p>而更灵活一点的实现会再额外添加了一个scale变量以控制各变量x进行缩放，还有shift变量来控制变量x进行平移。</p><p>简单使用代码实现，如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LayerNorm</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, emb_dim</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.eps = <span class="hljs-number">1e-5</span><br>        self.scale = nn.Parameter(torch.ones(emb_dim))<br>        self.shift = nn.Parameter(torch.zeros(emb_dim))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        mean = x.mean(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        var = x.var(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>, unbiased=<span class="hljs-literal">False</span>)<br>        norm_x = (x - mean) / torch.sqrt(var + self.eps)<br>        <span class="hljs-keyword">return</span> self.scale * norm_x + self.shift<br></code></pre></td></tr></table></figure><blockquote><p>需要注意的是我们这里的x其实就是样本，所以 $$\mu$$实际上并不是标准的均值，故理论上在计算方差时应该除以N-1，不然就是有偏的。不过由于GPT-2的结构是这样的，所以我们仿照它的做法，此外由于embedding层的维数N一般都比较大，所以N和N-1也差别不大。</p></blockquote><h2 id="Implementing-a-feed-forward-network-with-GELU-activations"><a href="#Implementing-a-feed-forward-network-with-GELU-activations" class="headerlink" title="Implementing a feed forward network with GELU activations"></a>Implementing a feed forward network with GELU activations</h2><p>一个前馈网络的结构如下：</p><p><img src="/2025/05/03/LLMFromScratch3/image-6.png"></p><p>注意这里采用的是GELU激活函数，其表达式如下：</p><p>$$\text{GELU}(x) \approx 0.5 \cdot x \cdot \left(1 + \tanh\left[\sqrt{\frac{2}{\pi}} \cdot \left(x + 0.044715 \cdot x^3\right)\right]\right)$$</p><p>相比于RELU激活函数，GELU是一个平滑的非线性函数，近似于ReLU，但负值具有非零梯度（约-0.75除外）。</p><p><img src="/2025/05/03/LLMFromScratch3/image-7.png"></p><p>代码实现前馈网络如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeedForward</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.layers = nn.Sequential(<br>            nn.Linear(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>], <span class="hljs-number">4</span> * cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>]),<br>            GELU(),<br>            nn.Linear(<span class="hljs-number">4</span> * cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>], cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>]),<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> self.layers(x)<br></code></pre></td></tr></table></figure><h2 id="Adding-shortcut-connections"><a href="#Adding-shortcut-connections" class="headerlink" title="Adding shortcut connections"></a>Adding shortcut connections</h2><p>shortcut连接主要是为了解决梯度消息的问题，它将之前网络的输出与现在网络的输入相加后再进行传递，如下所示：</p><p><img src="/2025/05/03/LLMFromScratch3/image-4.png"></p><p>该机制的代码实现如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ExampleDeepNeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, layer_sizes, use_shortcut</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.use_shortcut = use_shortcut<br>        self.layers = nn.ModuleList([<br>            nn.Sequential(nn.Linear(layer_sizes[<span class="hljs-number">0</span>], layer_sizes[<span class="hljs-number">1</span>]), GELU()),<br>            nn.Sequential(nn.Linear(layer_sizes[<span class="hljs-number">1</span>], layer_sizes[<span class="hljs-number">2</span>]), GELU()),<br>            nn.Sequential(nn.Linear(layer_sizes[<span class="hljs-number">2</span>], layer_sizes[<span class="hljs-number">3</span>]), GELU()),<br>            nn.Sequential(nn.Linear(layer_sizes[<span class="hljs-number">3</span>], layer_sizes[<span class="hljs-number">4</span>]), GELU()),<br>            nn.Sequential(nn.Linear(layer_sizes[<span class="hljs-number">4</span>], layer_sizes[<span class="hljs-number">5</span>]), GELU())<br>        ])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers:<br>            <span class="hljs-comment"># Compute the output of the current layer</span><br>            layer_output = layer(x)<br>            <span class="hljs-comment"># Check if shortcut can be applied</span><br>            <span class="hljs-keyword">if</span> self.use_shortcut <span class="hljs-keyword">and</span> x.shape == layer_output.shape:<br>                x = x + layer_output<br>            <span class="hljs-keyword">else</span>:<br>                x = layer_output<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">print_gradients</span>(<span class="hljs-params">model, x</span>):<br>    <span class="hljs-comment"># Forward pass</span><br>    output = model(x)<br>    target = torch.tensor([[<span class="hljs-number">0.</span>]])<br><br>    <span class="hljs-comment"># Calculate loss based on how close the target</span><br>    <span class="hljs-comment"># and output are</span><br>    loss = nn.MSELoss()<br>    loss = loss(output, target)<br>    <br>    <span class="hljs-comment"># Backward pass to calculate the gradients</span><br>    loss.backward()<br><br>    <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;weight&#x27;</span> <span class="hljs-keyword">in</span> name:<br>            <span class="hljs-comment"># Print the mean absolute gradient of the weights</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;name&#125;</span> has gradient mean of <span class="hljs-subst">&#123;param.grad.<span class="hljs-built_in">abs</span>().mean().item()&#125;</span>&quot;</span>)<br>    <br>layer_sizes = [<span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>]  <br><br>sample_input = torch.tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, -<span class="hljs-number">1.</span>]])<br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>model_without_shortcut = ExampleDeepNeuralNetwork(<br>    layer_sizes, use_shortcut=<span class="hljs-literal">False</span><br>)<br>print_gradients(model_without_shortcut, sample_input)        <br><br><span class="hljs-comment"># 没有shortcut连接</span><br><span class="hljs-comment"># layers.0.0.weight has gradient mean of 0.00020173584925942123</span><br><span class="hljs-comment"># layers.1.0.weight has gradient mean of 0.00012011159560643137</span><br><span class="hljs-comment"># layers.2.0.weight has gradient mean of 0.0007152040489017963</span><br><span class="hljs-comment"># layers.3.0.weight has gradient mean of 0.0013988736318424344</span><br><span class="hljs-comment"># layers.4.0.weight has gradient mean of 0.005049645435065031</span><br><br><span class="hljs-comment"># 有shortcut连接</span><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>model_with_shortcut = ExampleDeepNeuralNetwork(<br>    layer_sizes, use_shortcut=<span class="hljs-literal">True</span><br>)<br>print_gradients(model_with_shortcut, sample_input)<br><span class="hljs-comment"># layers.0.0.weight has gradient mean of 0.22169792652130127</span><br><span class="hljs-comment"># layers.1.0.weight has gradient mean of 0.20694108307361603</span><br><span class="hljs-comment"># layers.2.0.weight has gradient mean of 0.3289699852466583</span><br><span class="hljs-comment"># layers.3.0.weight has gradient mean of 0.2665732204914093</span><br><span class="hljs-comment"># layers.4.0.weight has gradient mean of 1.3258541822433472</span><br></code></pre></td></tr></table></figure><h2 id="Connecting-attention-and-linear-layers-in-a-transformer-block"><a href="#Connecting-attention-and-linear-layers-in-a-transformer-block" class="headerlink" title="Connecting attention and linear layers in a transformer block"></a>Connecting attention and linear layers in a transformer block</h2><p>一个transformer块的结构如下所示：</p><p><img src="/2025/05/03/LLMFromScratch3/image-1.png"></p><p>简单的代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> previous_chapters <span class="hljs-keyword">import</span> MultiHeadAttention<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerBlock</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.att = MultiHeadAttention(<br>            d_in=cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>],<br>            d_out=cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>],<br>            context_length=cfg[<span class="hljs-string">&quot;context_length&quot;</span>],<br>            num_heads=cfg[<span class="hljs-string">&quot;n_heads&quot;</span>], <br>            dropout=cfg[<span class="hljs-string">&quot;drop_rate&quot;</span>],<br>            qkv_bias=cfg[<span class="hljs-string">&quot;qkv_bias&quot;</span>])<br>        self.ff = FeedForward(cfg)<br>        self.norm1 = LayerNorm(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        self.norm2 = LayerNorm(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        self.drop_shortcut = nn.Dropout(cfg[<span class="hljs-string">&quot;drop_rate&quot;</span>])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># Shortcut connection for attention block</span><br>        shortcut = x<br>        x = self.norm1(x)<br>        x = self.att(x)  <span class="hljs-comment"># Shape [batch_size, num_tokens, emb_size]</span><br>        x = self.drop_shortcut(x)<br>        x = x + shortcut  <span class="hljs-comment"># Add the original input back</span><br><br>        <span class="hljs-comment"># Shortcut connection for feed forward block</span><br>        shortcut = x<br>        x = self.norm2(x)<br>        x = self.ff(x)<br>        x = self.drop_shortcut(x)<br>        x = x + shortcut  <span class="hljs-comment"># Add the original input back</span><br><br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>transformer块采用这种结构的好处可以如下理解：</p><ul><li><p>多头注意力机制可以识别并分析出序列中各元属之间的关系</p></li><li><p>前馈网络强化了局部的信息</p></li><li><p>对每个位置进行特定的非线性变换，提升其独立特征表达</p></li><li><p>协同效果：这种组合让模型既能捕捉全局模式，又能处理局部细节，从而在面对复杂数据模式时表现出更强的处理能力。</p></li><li><p>【例】句子翻译任务：自注意力机制帮助模型理解句子结构和词语之间的关系；前馈网络对每个单词的特定信息进行调整和优化，从而生成更准确的翻译。</p></li></ul><p>注意对于整一个Transformer结构，其输入和输出的形状最后是相同的。这种输入输出相同的设计更加方便各层之间的叠加，输入可以直接当做输出叠加上去。</p><p>例如下面的这个示例，我们采用batch size&#x3D;2，上下文长度为4，每个embedding层的维度为768，最后得到的结果也是相同的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(<span class="hljs-number">123</span>)<br><br>x = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">768</span>)  <span class="hljs-comment"># Shape: [batch_size, num_tokens, emb_dim]</span><br>block = TransformerBlock(GPT_CONFIG_124M)<br>output = block(x)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input shape:&quot;</span>, x.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output shape:&quot;</span>, output.shape)<br><br><span class="hljs-comment"># Input shape: torch.Size([2, 4, 768])</span><br><span class="hljs-comment"># Output shape: torch.Size([2, 4, 768])</span><br></code></pre></td></tr></table></figure><h2 id="Coding-the-GPT-model"><a href="#Coding-the-GPT-model" class="headerlink" title="Coding the GPT model"></a>Coding the GPT model</h2><p>一个GPT模型的结构概览如下图所示。</p><p><img src="/2025/05/03/LLMFromScratch3/image-2.png"></p><p>对于一个124 million参数的GPT-2模型，其使用了12个transformer块，对于最大的 GPT-2 模型，其参数有 1.542 billion，这个 Transformer 块重复了 36 次。</p><p>叠加起来的代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GPTModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, cfg</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.tok_emb = nn.Embedding(cfg[<span class="hljs-string">&quot;vocab_size&quot;</span>], cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        self.pos_emb = nn.Embedding(cfg[<span class="hljs-string">&quot;context_length&quot;</span>], cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        self.drop_emb = nn.Dropout(cfg[<span class="hljs-string">&quot;drop_rate&quot;</span>])<br><br>        self.trf_blocks = nn.Sequential(<br>            *[TransformerBlock(cfg) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(cfg[<span class="hljs-string">&quot;n_layers&quot;</span>])])<br><br>        self.final_norm = LayerNorm(cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>])<br>        self.out_head = nn.Linear(<br>            cfg[<span class="hljs-string">&quot;emb_dim&quot;</span>], cfg[<span class="hljs-string">&quot;vocab_size&quot;</span>], bias=<span class="hljs-literal">False</span><br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, in_idx</span>):<br>        batch_size, seq_len = in_idx.shape<br>        tok_embeds = self.tok_emb(in_idx)<br>        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))<br>        x = tok_embeds + pos_embeds  <span class="hljs-comment"># Shape [batch_size, num_tokens, emb_size]</span><br>        x = self.drop_emb(x)<br>        x = self.trf_blocks(x)<br>        x = self.final_norm(x)<br>        logits = self.out_head(x)<br>        <span class="hljs-keyword">return</span> logits<br></code></pre></td></tr></table></figure><p>注意模型最后的输出的结果是后续接着各个词的对应id的概率，其维度为字典的大小。注意其实际上生成了这个上下文中各个前缀生成的可能的词的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(<span class="hljs-number">123</span>)<br>model = GPTModel(GPT_CONFIG_124M)<br><br>out = model(batch)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input batch:\n&quot;</span>, batch)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nOutput shape:&quot;</span>, out.shape)<br><span class="hljs-built_in">print</span>(out)<br><br><span class="hljs-comment"># 输出</span><br>Input batch:<br> tensor([[<span class="hljs-number">6109</span>, <span class="hljs-number">3626</span>, <span class="hljs-number">6100</span>,  <span class="hljs-number">345</span>],<br>        [<span class="hljs-number">6109</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">6622</span>,  <span class="hljs-number">257</span>]])<br><br>Output shape: torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">50257</span>])<br>tensor([[[ <span class="hljs-number">0.1381</span>,  <span class="hljs-number">0.0077</span>, -<span class="hljs-number">0.1963</span>,  ..., -<span class="hljs-number">0.0222</span>, -<span class="hljs-number">0.1060</span>,  <span class="hljs-number">0.1717</span>],<br>         [ <span class="hljs-number">0.3865</span>, -<span class="hljs-number">0.8408</span>, -<span class="hljs-number">0.6564</span>,  ..., -<span class="hljs-number">0.5163</span>,  <span class="hljs-number">0.2369</span>, -<span class="hljs-number">0.3357</span>],<br>         [ <span class="hljs-number">0.6989</span>, -<span class="hljs-number">0.1829</span>, -<span class="hljs-number">0.1631</span>,  ...,  <span class="hljs-number">0.1472</span>, -<span class="hljs-number">0.6504</span>, -<span class="hljs-number">0.0056</span>],<br>         [-<span class="hljs-number">0.4290</span>,  <span class="hljs-number">0.1669</span>, -<span class="hljs-number">0.1258</span>,  ...,  <span class="hljs-number">1.1579</span>,  <span class="hljs-number">0.5303</span>, -<span class="hljs-number">0.5549</span>]],<br><br>        [[ <span class="hljs-number">0.1094</span>, -<span class="hljs-number">0.2894</span>, -<span class="hljs-number">0.1467</span>,  ..., -<span class="hljs-number">0.0557</span>,  <span class="hljs-number">0.2911</span>, -<span class="hljs-number">0.2824</span>],<br>         [ <span class="hljs-number">0.0882</span>, -<span class="hljs-number">0.3552</span>, -<span class="hljs-number">0.3527</span>,  ...,  <span class="hljs-number">1.2930</span>,  <span class="hljs-number">0.0053</span>,  <span class="hljs-number">0.1898</span>],<br>         [ <span class="hljs-number">0.6091</span>,  <span class="hljs-number">0.4702</span>, -<span class="hljs-number">0.4094</span>,  ...,  <span class="hljs-number">0.7688</span>,  <span class="hljs-number">0.3787</span>, -<span class="hljs-number">0.1974</span>],<br>         [-<span class="hljs-number">0.0612</span>, -<span class="hljs-number">0.0737</span>,  <span class="hljs-number">0.4751</span>,  ...,  <span class="hljs-number">1.2463</span>, -<span class="hljs-number">0.3834</span>,  <span class="hljs-number">0.0609</span>]]],<br>       grad_fn=&lt;UnsafeViewBackward0&gt;)<br></code></pre></td></tr></table></figure><h4 id="参数量分析"><a href="#参数量分析" class="headerlink" title="参数量分析"></a>参数量分析</h4><p>GPT2各个size的参数如下：</p><ul><li><p><strong>GPT2-small</strong> (the 124M configuration we already implemented):</p><ul><li><p>“emb_dim” &#x3D; 768</p></li><li><p>“n_layers” &#x3D; 12</p></li><li><p>“n_heads” &#x3D; 12</p></li></ul></li><li><p><strong>GPT2-medium:</strong></p><ul><li><p>“emb_dim” &#x3D; 1024</p></li><li><p>“n_layers” &#x3D; 24</p></li><li><p>“n_heads” &#x3D; 16</p></li></ul></li><li><p><strong>GPT2-large:</strong></p><ul><li><p>“emb_dim” &#x3D; 1280</p></li><li><p>“n_layers” &#x3D; 36</p></li><li><p>“n_heads” &#x3D; 20</p></li></ul></li><li><p><strong>GPT2-XL:</strong></p><ul><li><p>“emb_dim” &#x3D; 1600</p></li><li><p>“n_layers” &#x3D; 48</p></li><li><p>“n_heads” &#x3D; 25</p></li></ul></li></ul><p>我们代码实现的是GPT2-small，但是如果我们直接将所有的参数量都统计出来会发现其值并不是124M，而是163M：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">model = GPTModel(GPT_CONFIG_124M)<br>total_params = <span class="hljs-built_in">sum</span>(p.numel() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters())<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Total number of parameters: <span class="hljs-subst">&#123;total_params:,&#125;</span>&quot;</span>)<br><span class="hljs-comment"># Total number of parameters: 163,009,536</span><br></code></pre></td></tr></table></figure><p>这是由于在最初的GPT-2模型中使用了模型参数绑定，也就是<code>self.out_head.weight</code> &#x3D;<code> self.tok_emb.weight</code>。因为tok_emb负责将id转化为对应的embedding，其行数是50257，列数是768维，而out_head负责将embedding再转为字典中词数量的维度，故可以复用。</p><p>减去out_head这一层的参数量后也可以看到确实就是124M的参数量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">total_params_gpt2 =  total_params - <span class="hljs-built_in">sum</span>(p.numel() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.out_head.parameters())<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Number of trainable parameters considering weight tying: <span class="hljs-subst">&#123;total_params_gpt2:,&#125;</span>&quot;</span>)<br><span class="hljs-comment"># Number of trainable parameters considering weight tying: 124,412,160</span><br></code></pre></td></tr></table></figure><h2 id="Generating-text"><a href="#Generating-text" class="headerlink" title="Generating text"></a>Generating text</h2><p>下图展示了生成文本的一个经典过程，即每次生成一个新的token，然后再将这个token拼接起来继续生成后一个token。</p><p><img src="/2025/05/03/LLMFromScratch3/image.png"></p><p>一个简单的生成文本的代码实现如下所示，在这里我们将只取输出的n_token那一层的最后一维，代表利用前面全部的信息所得到的后一个词的预测结果，然后再简单地进行softmax选取概览最高的那一个词作为输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_text_simple</span>(<span class="hljs-params">model, idx, max_new_tokens, context_size</span>):<br>    <span class="hljs-comment"># idx is (batch, n_tokens) array of indices in the current context</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(max_new_tokens):<br>        <br>        <span class="hljs-comment"># Crop current context if it exceeds the supported context size</span><br>        <span class="hljs-comment"># E.g., if LLM supports only 5 tokens, and the context size is 10</span><br>        <span class="hljs-comment"># then only the last 5 tokens are used as context</span><br>        idx_cond = idx[:, -context_size:]<br>        <br>        <span class="hljs-comment"># Get the predictions</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            logits = model(idx_cond)<br>        <br>        <span class="hljs-comment"># Focus only on the last time step</span><br>        <span class="hljs-comment"># (batch, n_tokens, vocab_size) becomes (batch, vocab_size)</span><br>        logits = logits[:, -<span class="hljs-number">1</span>, :]  <br><br>        <span class="hljs-comment"># Apply softmax to get probabilities</span><br>        probas = torch.softmax(logits, dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># (batch, vocab_size)</span><br><br>        <span class="hljs-comment"># Get the idx of the vocab entry with the highest probability value</span><br>        idx_next = torch.argmax(probas, dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)  <span class="hljs-comment"># (batch, 1)</span><br><br>        <span class="hljs-comment"># Append sampled index to the running sequence</span><br>        idx = torch.cat((idx, idx_next), dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># (batch, n_tokens+1)</span><br><br>    <span class="hljs-keyword">return</span> idx<br></code></pre></td></tr></table></figure><p>注意这时我们还没有对模型训练，所以模型前向传播得到的都是一些混乱的单词</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">start_context = <span class="hljs-string">&quot;Hello, I am&quot;</span><br><br>encoded = tokenizer.encode(start_context)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;encoded:&quot;</span>, encoded)<br><span class="hljs-comment"># encoded: [15496, 11, 314, 716]</span><br><br>encoded_tensor = torch.tensor(encoded).unsqueeze(<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;encoded_tensor.shape:&quot;</span>, encoded_tensor.shape)<br><span class="hljs-comment"># encoded_tensor.shape: torch.Size([1, 4])</span><br><br>model.<span class="hljs-built_in">eval</span>() <span class="hljs-comment"># disable dropout</span><br><br>out = generate_text_simple(<br>    model=model,<br>    idx=encoded_tensor, <br>    max_new_tokens=<span class="hljs-number">6</span>, <br>    context_size=GPT_CONFIG_124M[<span class="hljs-string">&quot;context_length&quot;</span>]<br>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output:&quot;</span>, out)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Output length:&quot;</span>, <span class="hljs-built_in">len</span>(out[<span class="hljs-number">0</span>]))<br><span class="hljs-comment"># Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])</span><br><span class="hljs-comment"># Output length: 10</span><br><br>decoded_text = tokenizer.decode(out.squeeze(<span class="hljs-number">0</span>).tolist())<br><span class="hljs-built_in">print</span>(decoded_text)<br><span class="hljs-comment"># Hello, I am Featureiman Byeswickattribute argue</span><br></code></pre></td></tr></table></figure><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://knowledge.zhaoweiguo.com/build/html/x-learning/books/ais/2024/build/_llm/_from/_scratch#understanding-llm">https://knowledge.zhaoweiguo.com/build/html/x-learning/books/ais/2024/build\_llm\_from\_scratch#understanding-llm</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>Build a Large Language Model (From Scratch)</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【从零构建大模型】二、编码Attention机制</title>
    <link href="/2025/05/02/LLMFromScratch2/"/>
    <url>/2025/05/02/LLMFromScratch2/</url>
    
    <content type="html"><![CDATA[<h1 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h1><p>构建大模型的全景图如下，本文介绍了基础的attention处理。</p><p><img src="/2025/05/02/LLMFromScratch2/image-13.png"></p><p>介绍的脉络如下：</p><p><img src="/2025/05/02/LLMFromScratch2/image-12.png"></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><h2 id="The-problem-with-modeling-long-sequences"><a href="#The-problem-with-modeling-long-sequences" class="headerlink" title="The problem with modeling long sequences"></a>The problem with modeling long sequences</h2><p>对于类似翻译的任务，由于不同语言的语法问题，所以难以做到一对一的逐字翻译，需要提前对原本的字符串进行encoder提取信息，然后使用decoder模块进行翻译。</p><p>而传统的encoder-decoder RNNs方法在encoder阶段无法从编码器访问先前的隐藏状态。因此，它只能依赖于当前隐藏状态，而当前隐藏状态包含了所有相关信息。这可能会导致上下文丢失，尤其是在依赖关系可能跨越很长距离的复杂句子中。</p><p>所以提出了注意力机制来更好地捕获原本的信息。</p><p><img src="/2025/05/02/LLMFromScratch2/image-11.png"></p><h2 id="Capturing-data-dependencies-with-attention-mechanisms"><a href="#Capturing-data-dependencies-with-attention-mechanisms" class="headerlink" title="Capturing data dependencies with attention mechanisms"></a>Capturing data dependencies with attention mechanisms</h2><p>在注意力机制中，encoder可以选择性地访问所有输入的tokens，并自行判断哪些tokens更加重要，而这部分判断就是依靠<code>attention weights</code>。</p><p><img src="/2025/05/02/LLMFromScratch2/image-10.png"></p><p>Self attention是transfomer中的关键的一种机制，它允许输入序列中的每个位置在计算序列的表示时关注同一序列中的所有位置。自注意力机制是基于 Transformer 架构的当代LLM的关键组成部分，例如 GPT 系列。下面我们将从头开始编写这种自注意力机制的代码</p><h2 id="Attending-to-different-parts-of-the-input-with-self-attention"><a href="#Attending-to-different-parts-of-the-input-with-self-attention" class="headerlink" title="Attending to different parts of the input with self-attention"></a>Attending to different parts of the input with self-attention</h2><h3 id="A-simple-self-attention-mechanism-without-trainable-weights"><a href="#A-simple-self-attention-mechanism-without-trainable-weights" class="headerlink" title="A simple self-attention mechanism without trainable weights"></a>A simple self-attention mechanism without trainable weights</h3><p>自注意力机制的目标是为每个输入元素计算一个上下文向量，该向量结合了所有其他输入元素的信息。</p><p>下图表示了一种简化后的attention机制，我们想要计算得到第2个字符的上下文向量就只需要用<code>attention weights</code>与原序列中的各个token相乘即可。</p><p><img src="/2025/05/02/LLMFromScratch2/image-9.png"></p><p>而一个简单的计算两者<code>attention weights</code>的方法就是直接将两者的token embedding进行点积。</p><blockquote><p>点积还是相似性的度量，因为它量化了两个向量的对齐程度：更高的点积表示更大程度的对齐或相似性向量之间。在自注意力机制的背景下，点积决定了序列中元素相互关注的程度：点积越高，两个元素之间的相似度和注意力分数就越高。</p></blockquote><p>通过点积得到<code>attention weights</code>后往往还会进行归一化，一般采用softmax函数。</p><p>最后通过各个token的embeddings与<code>attention weights</code>的加权相乘就可以得到需要的上下文向量。</p><p><img src="/2025/05/02/LLMFromScratch2/image.png"></p><p>简单的代码实现如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>inputs = torch.tensor(<br>  [[<span class="hljs-number">0.43</span>, <span class="hljs-number">0.15</span>, <span class="hljs-number">0.89</span>], <span class="hljs-comment"># Your     (x^1)</span><br>   [<span class="hljs-number">0.55</span>, <span class="hljs-number">0.87</span>, <span class="hljs-number">0.66</span>], <span class="hljs-comment"># journey  (x^2)</span><br>   [<span class="hljs-number">0.57</span>, <span class="hljs-number">0.85</span>, <span class="hljs-number">0.64</span>], <span class="hljs-comment"># starts   (x^3)</span><br>   [<span class="hljs-number">0.22</span>, <span class="hljs-number">0.58</span>, <span class="hljs-number">0.33</span>], <span class="hljs-comment"># with     (x^4)</span><br>   [<span class="hljs-number">0.77</span>, <span class="hljs-number">0.25</span>, <span class="hljs-number">0.10</span>], <span class="hljs-comment"># one      (x^5)</span><br>   [<span class="hljs-number">0.05</span>, <span class="hljs-number">0.80</span>, <span class="hljs-number">0.55</span>]] <span class="hljs-comment"># step     (x^6)</span><br>)<br><br>query = inputs[<span class="hljs-number">1</span>]  <span class="hljs-comment"># 2nd input token is the query</span><br><br>attn_scores_2 = torch.empty(inputs.shape[<span class="hljs-number">0</span>])<br><span class="hljs-keyword">for</span> i, x_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(inputs):<br>    attn_scores_2[i] = torch.dot(x_i, query) <span class="hljs-comment"># dot product (transpose not necessary here since they are 1-dim vectors)</span><br><br><span class="hljs-built_in">print</span>(attn_scores_2)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax_naive</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> torch.exp(x) / torch.exp(x).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">0</span>)<br><br>attn_weights_2_naive = softmax_naive(attn_scores_2)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Attention weights:&quot;</span>, attn_weights_2_naive)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Sum:&quot;</span>, attn_weights_2_naive.<span class="hljs-built_in">sum</span>())<br><br>query = inputs[<span class="hljs-number">1</span>] <span class="hljs-comment"># 2nd input token is the query</span><br><br>context_vec_2 = torch.zeros(query.shape)<br><span class="hljs-keyword">for</span> i,x_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(inputs):<br>    context_vec_2 += attn_weights_2[i]*x_i<br><br><span class="hljs-built_in">print</span>(context_vec_2)<br></code></pre></td></tr></table></figure><h3 id="Computing-attention-weights-for-all-input-tokens"><a href="#Computing-attention-weights-for-all-input-tokens" class="headerlink" title="Computing attention weights for all input tokens"></a>Computing attention weights for all input tokens</h3><p>前面说的是单个token如何计算上下文向量, 而在实际过程中可以简单的使用向量处理的方向一次性得到所有的<code>attention weight</code>和上下文向量，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">attn_scores = inputs @ inputs.T<br><span class="hljs-built_in">print</span>(attn_scores)<br><br>attn_weights = torch.softmax(attn_scores, dim=-<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(attn_weights)<br><br>all_context_vecs = attn_weights @ inputs<br><span class="hljs-built_in">print</span>(all_context_vecs)<br></code></pre></td></tr></table></figure><h2 id="Implementing-self-attention-with-trainable-weights"><a href="#Implementing-self-attention-with-trainable-weights" class="headerlink" title="Implementing self-attention with trainable weights"></a>Implementing self-attention with trainable weights</h2><h3 id="Computing-the-attention-weights-step-by-step"><a href="#Computing-the-attention-weights-step-by-step" class="headerlink" title="Computing the attention weights step by step"></a>Computing the attention weights step by step</h3><p>在实际的self-attention中会三个可训练的权重矩阵$$W_q$$，$$W_k$$，$$W_v$$，其分别用于与原token embedding相乘计算<strong>Q</strong>uery、<strong>K</strong>ey、<strong>V</strong>alue。下面以d_in&#x3D;3维度的token以及d_out&#x3D;2维度的输出来演示如何计算第二token的上下文。</p><blockquote><p>请注意，在类似 GPT 的模型中，输入和输出维度通常相同，但为了便于说明，为了更好地跟踪计算，我们在此选择不同的输入（d_in&#x3D;3）和输出（d_out&#x3D;2）维度。</p></blockquote><p>整体流程如下图所示：</p><ol><li><p>各个token与$$W_q$$，$$W_k$$，$$W_v$$相乘的到q、k、v</p></li><li><p>第二个token的q与各个k相乘的到attention</p></li><li><p>对attention进行softmax</p></li><li><p>对attention进行缩放</p></li><li><p>各个attention与v加权相乘得到上下文向量</p></li></ol><blockquote><p>为什么在自注意力机制中会用嵌入维度的平方根来缩放点积？</p><ul><li><p>缩放的目的：用嵌入维度的平方根来缩放，是为了避免训练过程中出现过小的梯度。若不做缩放，训练时可能会遇到梯度非常小的情况，导致模型学习变慢，甚至陷入停滞。</p></li><li><p>出现梯度变小的原因：1、当嵌入维度（即向量的维度）增加时，两个向量的点积值会变大。在GPT等大型语言模型（LLM）中，嵌入维度往往很高，可能达到上千，因此点积也变得很大。 2、在点积结果上应用softmax 函数时，如果数值较大，softmax 输出的概率分布会变得很尖锐，近似于阶跃函数。此时，大部分概率集中在几个值上，导致其他部分的梯度几乎为零。这样就会导致模型训练时更新不充分。</p></li><li><p>缩放的效果：通过用嵌入维度的平方根缩放点积的大小，可以让点积的数值控制在合理范围，使得softmax 函数的输出更加平滑，从而使得梯度较大，模型可以更有效地学习。这种缩放的自注意力机制因此被称为“缩放点积注意力” (scaled-dot product attention)。</p></li></ul></blockquote><p><img src="/2025/05/02/LLMFromScratch2/image-1.png"></p><p>整体的代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br>inputs = torch.tensor(<br>  [[<span class="hljs-number">0.43</span>, <span class="hljs-number">0.15</span>, <span class="hljs-number">0.89</span>], <span class="hljs-comment"># Your     (x^1)</span><br>    [<span class="hljs-number">0.55</span>, <span class="hljs-number">0.87</span>, <span class="hljs-number">0.66</span>], <span class="hljs-comment"># journey  (x^2)</span><br>    [<span class="hljs-number">0.57</span>, <span class="hljs-number">0.85</span>, <span class="hljs-number">0.64</span>], <span class="hljs-comment"># starts    (x^3)</span><br>    [<span class="hljs-number">0.22</span>, <span class="hljs-number">0.58</span>, <span class="hljs-number">0.33</span>], <span class="hljs-comment"># with      (x^4)</span><br>    [<span class="hljs-number">0.77</span>, <span class="hljs-number">0.25</span>, <span class="hljs-number">0.10</span>], <span class="hljs-comment"># one       (x^5)</span><br>    [<span class="hljs-number">0.05</span>, <span class="hljs-number">0.80</span>, <span class="hljs-number">0.55</span>]] <span class="hljs-comment"># step      (x^6)</span><br>)<br><br>x_2 = inputs[<span class="hljs-number">1</span>] <span class="hljs-comment">#A</span><br>d_in = inputs.shape[<span class="hljs-number">1</span>] <span class="hljs-comment">#B  = 3</span><br>d_out = <span class="hljs-number">2</span> <span class="hljs-comment">#C</span><br><br><span class="hljs-comment"># Initialize the three weight matrices Wq, Wk, and Wv</span><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=<span class="hljs-literal">False</span>)<br>W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=<span class="hljs-literal">False</span>)<br>W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 示例结构: shape=(3,2)</span><br><span class="hljs-comment"># tensor([[0.3821, 0.6605],</span><br><span class="hljs-comment">#         [0.8536, 0.5932],</span><br><span class="hljs-comment">#         [0.6367, 0.9826]])</span><br><span class="hljs-comment"># 说明</span><br><span class="hljs-comment"># setting requires_grad=False to reduce clutter in the outputs for illustration purposes</span><br><span class="hljs-comment"># 正式使用时需要设置 requires_grad=True</span><br><br>query_2 = x_2 @ W_query<br><br><span class="hljs-comment"># 虽然这儿只计算context vector`Z^(2)`，但仍然需要所有输入元素的键和值向量</span><br><span class="hljs-comment"># obtain all keys and values via matrix multiplication</span><br>keys = inputs @ W_key<br>values = inputs @ W_value<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;keys.shape:&quot;</span>, keys.shape)<br><span class="hljs-comment"># 输出</span><br><span class="hljs-comment"># keys.shape: torch.Size([6, 2])</span><br><br>attn_scores_2 = query_2 @ keys.T<br><span class="hljs-comment"># All attention scores for given queryprint(attn_scores_2)</span><br><span class="hljs-comment"># 输出</span><br><span class="hljs-comment"># tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])</span><br><br>d_k = keys.shape[-<span class="hljs-number">1</span>]attn_weights_2 = torch.softmax(attn_scores_2 / d_k**<span class="hljs-number">0.5</span>, dim=-<span class="hljs-number">1</span>)<span class="hljs-built_in">print</span>(attn_weights_2)<br><br><span class="hljs-comment"># 输出</span><br><span class="hljs-comment"># tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])</span><br>context_vec_2 = attn_weights_2 @ valuesprint(context_vec_2)<span class="hljs-comment"># 输出# tensor([0.3061, 0.8210])</span><br></code></pre></td></tr></table></figure><h3 id="Implementing-a-compact-SelfAttention-class"><a href="#Implementing-a-compact-SelfAttention-class" class="headerlink" title="Implementing a compact SelfAttention class"></a>Implementing a compact SelfAttention class</h3><p>类似的在实际计算所有的上下文向量的时候都是通过矩阵相乘的方法来做的，如下图所示：</p><p><img src="/2025/05/02/LLMFromScratch2/image-2.png"></p><p>代码如下所示，注意这里使用了nn.Linear，这有助于更稳定、有效的模型训练。：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SelfAttention_v2</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_in, d_out, qkv_bias=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.d_out = d_out<br>        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        keys = self.W_key(x)<br>        queries = self.W_query(x)<br>        values = self.W_value(x)<br><br>        attn_scores = queries @ keys.T<br><br>        attn_weights = torch.softmax(attn_scores / keys.shape[-<span class="hljs-number">1</span>]**<span class="hljs-number">0.5</span>, dim)<br><br>        context_vec = attn_weights @ values<br>        <span class="hljs-keyword">return</span> context_vec<br><br>torch.manual_seed(<span class="hljs-number">789</span>)<br>sa_v2 = SelfAttention_v2(d_in, d_out)<br><span class="hljs-built_in">print</span>(sa_v2(inputs))<br><span class="hljs-comment"># 输出</span><br>tensor([[-<span class="hljs-number">0.0739</span>,  <span class="hljs-number">0.0713</span>],<br>        [-<span class="hljs-number">0.0748</span>,  <span class="hljs-number">0.0703</span>],<br>        [-<span class="hljs-number">0.0749</span>,  <span class="hljs-number">0.0702</span>],<br>        [-<span class="hljs-number">0.0760</span>,  <span class="hljs-number">0.0685</span>],<br>        [-<span class="hljs-number">0.0763</span>,  <span class="hljs-number">0.0679</span>],<br>        [-<span class="hljs-number">0.0754</span>,  <span class="hljs-number">0.0693</span>]], grad_fn=&lt;MmBackward0&gt;)<br></code></pre></td></tr></table></figure><h2 id="Hiding-future-words-with-causal-attention"><a href="#Hiding-future-words-with-causal-attention" class="headerlink" title="Hiding future words with causal attention"></a>Hiding future words with causal attention</h2><p>在decoder生成阶段时我们只能看到要生成的token的前面的token，所以需要对生成的attention weight进行mask，这也叫做Causal attention。</p><p><img src="/2025/05/02/LLMFromScratch2/image-3.png"></p><h3 id="Applying-a-causal-attention-mask"><a href="#Applying-a-causal-attention-mask" class="headerlink" title="Applying a causal attention mask"></a>Applying a causal attention mask</h3><p>简单的mask就是在生成了attention weight之后在进行mask，然后再进行归一化。</p><p><img src="/2025/05/02/LLMFromScratch2/image-4.png"></p><p>而更常见的方法是利用softmax的数学特性，在q@k得到attention之后直接给要mask的attention weights记为-inf，这样softmax之后其值为0。</p><p><img src="/2025/05/02/LLMFromScratch2/image-5.png"></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">mask = torch.triu(torch.ones(context_length, context_length), diagonal=<span class="hljs-number">1</span>)<br>masked = attn_scores.masked_fill(mask.<span class="hljs-built_in">bool</span>(), -torch.inf)<br><span class="hljs-built_in">print</span>(masked)<br><span class="hljs-comment"># 输出</span><br>tensor([[<span class="hljs-number">0.2899</span>,   -inf,   -inf,   -inf,   -inf,   -inf],<br>        [<span class="hljs-number">0.4656</span>, <span class="hljs-number">0.1723</span>,   -inf,   -inf,   -inf,   -inf],<br>        [<span class="hljs-number">0.4594</span>, <span class="hljs-number">0.1703</span>, <span class="hljs-number">0.1731</span>,   -inf,   -inf,<br>        [<span class="hljs-number">0.2642</span>, <span class="hljs-number">0.1024</span>, <span class="hljs-number">0.1036</span>, <span class="hljs-number">0.0186</span>,   -inf,<br>        [<span class="hljs-number">0.2183</span>, <span class="hljs-number">0.0874</span>, <span class="hljs-number">0.0882</span>, <span class="hljs-number">0.0177</span>, <span class="hljs-number">0.0786</span>,<br>        [<span class="hljs-number">0.3408</span>, <span class="hljs-number">0.1270</span>, <span class="hljs-number">0.1290</span>, <span class="hljs-number">0.0198</span>, <span class="hljs-number">0.1290</span>, <span class="hljs-number">0.0078</span>]],<br>       grad_fn=&lt;MaskedFillBackward0&gt;)<br><br><br>attn_weights = torch.softmax(masked / keys.shape[-<span class="hljs-number">1</span>]**<span class="hljs-number">0.5</span>, dim=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(attn_weights)<br><span class="hljs-comment"># 输出(已经规范化，不用再额外操作了，节省了操作)</span><br>tensor([[<span class="hljs-number">1.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>],<br>        [<span class="hljs-number">0.5517</span>, <span class="hljs-number">0.4483</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>],<br>        [<span class="hljs-number">0.3800</span>, <span class="hljs-number">0.3097</span>, <span class="hljs-number">0.3103</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>],<br>        [<span class="hljs-number">0.2758</span>, <span class="hljs-number">0.2460</span>, <span class="hljs-number">0.2462</span>, <span class="hljs-number">0.2319</span>, <span class="hljs-number">0.0000</span>, <span class="hljs-number">0.0000</span>],<br>        [<span class="hljs-number">0.2175</span>, <span class="hljs-number">0.1983</span>, <span class="hljs-number">0.1984</span>, <span class="hljs-number">0.1888</span>, <span class="hljs-number">0.1971</span>, <span class="hljs-number">0.0000</span>],<br>        [<span class="hljs-number">0.1935</span>, <span class="hljs-number">0.1663</span>, <span class="hljs-number">0.1666</span>, <span class="hljs-number">0.1542</span>, <span class="hljs-number">0.1666</span>, <span class="hljs-number">0.1529</span>]],<br>       grad_fn=&lt;SoftmaxBackward0&gt;)<br></code></pre></td></tr></table></figure><h3 id="Masking-additional-attention-weights-with-dropout"><a href="#Masking-additional-attention-weights-with-dropout" class="headerlink" title="Masking additional attention weights with dropout"></a>Masking additional attention weights with dropout</h3><p>此外，我们还应用dropout来减少训练过程中的过拟合，确保模型不会过度依赖任何特定的隐藏层单元集。需要强调的是，Dropout 仅在训练期间使用，训练结束后将被禁用。</p><p>可以有两处dropout的地方</p><ol><li><p>在计算完attention weight之后</p></li><li><p>在attention weight与values相乘之后</p></li></ol><p>一般第一种更加普遍，下图我们以50%的dropout 比例为例来介绍，在实际如GPT模型中往往只会采取10%、20%的比例。</p><p><img src="/2025/05/02/LLMFromScratch2/image-6.png"></p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.manual_seed(<span class="hljs-number">123</span>)<br>dropout = torch.nn.Dropout(<span class="hljs-number">0.5</span>) <span class="hljs-comment">#A</span><br>example = torch.ones(<span class="hljs-number">6</span>, <span class="hljs-number">6</span>) <span class="hljs-comment">#B</span><br><span class="hljs-built_in">print</span>(dropout(example))<br><span class="hljs-comment"># 输出(有近一半是0)</span><br>tensor([[<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">0.</span>],<br>        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>],<br>        [<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>],<br>        [<span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>],<br>        [<span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>],<br>        [<span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">0.</span>]])<br></code></pre></td></tr></table></figure><h3 id="Implementing-a-compact-causal-self-attention-class"><a href="#Implementing-a-compact-causal-self-attention-class" class="headerlink" title="Implementing a compact causal self-attention class"></a>Implementing a compact causal self-attention class</h3><p>将mask和dropout的特性加上后的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CausalAttention</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_in, d_out, context_length,</span><br><span class="hljs-params">                 dropout, qkv_bias=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.d_out = d_out<br>        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        self.dropout = nn.Dropout(dropout) <span class="hljs-comment"># A</span><br>        self.register_buffer(<br>            <span class="hljs-string">&#x27;mask&#x27;</span>,<br>            torch.triu(<br>                torch.ones(context_length, context_length),<br>                diagonal=<span class="hljs-number">1</span><br>            )<br>        ) <span class="hljs-comment">#B</span><br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        b, num_tokens, d_in = x.shape  <span class="hljs-comment">#C</span><br>        keys = self.W_key(x)<br>        queries = self.W_query(x)<br>        values = self.W_value(x)<br><br>        attn_scores = queries @ keys.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment">#C</span><br>        attn_scores.masked_fill_(  <span class="hljs-comment">#D</span><br>            self.mask.<span class="hljs-built_in">bool</span>()[:num_tokens, :num_tokens], -torch.inf)<br>        attn_weights = torch.softmax(attn_scores / keys.shape[-<span class="hljs-number">1</span>]**<span class="hljs-number">0.5</span>, dim=-<span class="hljs-number">1</span>)<br>        attn_weights = self.dropout(attn_weights)<br><br>        context_vec = attn_weights @ values<br>        <span class="hljs-keyword">return</span> context_vec<br><br><br><span class="hljs-keyword">import</span> torch<br>inputs = torch.tensor(<br>  [[<span class="hljs-number">0.43</span>, <span class="hljs-number">0.15</span>, <span class="hljs-number">0.89</span>], <span class="hljs-comment"># Your     (x^1)</span><br>   [<span class="hljs-number">0.55</span>, <span class="hljs-number">0.87</span>, <span class="hljs-number">0.66</span>], <span class="hljs-comment"># journey  (x^2)</span><br>   [<span class="hljs-number">0.57</span>, <span class="hljs-number">0.85</span>, <span class="hljs-number">0.64</span>], <span class="hljs-comment"># starts   (x^3)</span><br>   [<span class="hljs-number">0.22</span>, <span class="hljs-number">0.58</span>, <span class="hljs-number">0.33</span>], <span class="hljs-comment"># with     (x^4)</span><br>   [<span class="hljs-number">0.77</span>, <span class="hljs-number">0.25</span>, <span class="hljs-number">0.10</span>], <span class="hljs-comment"># one      (x^5)</span><br>   [<span class="hljs-number">0.05</span>, <span class="hljs-number">0.80</span>, <span class="hljs-number">0.55</span>]] <span class="hljs-comment"># step     (x^6)</span><br>)<br>batch = torch.stack((inputs, inputs), dim=<span class="hljs-number">0</span>)<br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>context_length = batch.shape[<span class="hljs-number">1</span>]<br>ca = CausalAttention(d_in, d_out, context_length, <span class="hljs-number">0.0</span>)<br>context_vecs = ca(batch)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;context_vecs.shape:&quot;</span>, context_vecs.shape)<br><span class="hljs-comment"># 输出</span><br><span class="hljs-comment"># context_vecs.shape: torch.Size([2, 6, 2])</span><br></code></pre></td></tr></table></figure><h2 id="Extending-single-head-attention-to-multi-head-attention"><a href="#Extending-single-head-attention-to-multi-head-attention" class="headerlink" title="Extending single-head attention to multi-head attention"></a>Extending single-head attention to multi-head attention</h2><h3 id="Stacking-multiple-single-head-attention-layers"><a href="#Stacking-multiple-single-head-attention-layers" class="headerlink" title="Stacking multiple single-head attention layers"></a>Stacking multiple single-head attention layers</h3><p>实际往往会采取多头注意力机制，在分别得到对应的上下文后会将其直接进行拼接，然后一般会再与一个全联接层进行相乘</p><p><img src="/2025/05/02/LLMFromScratch2/image-7.png"></p><p>简单地将上述的代码进行包装就可以得到多头注意力的版本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttentionWrapper</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.heads = nn.ModuleList(<br>            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) <br>             <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_heads)]<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> torch.cat([head(x) <span class="hljs-keyword">for</span> head <span class="hljs-keyword">in</span> self.heads], dim=-<span class="hljs-number">1</span>)<br><br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br><br>context_length = batch.shape[<span class="hljs-number">1</span>] <span class="hljs-comment"># This is the number of tokens</span><br>d_in, d_out = <span class="hljs-number">3</span>, <span class="hljs-number">2</span><br>mha = MultiHeadAttentionWrapper(<br>    d_in, d_out, context_length, <span class="hljs-number">0.0</span>, num_heads=<span class="hljs-number">2</span><br>)<br><br>context_vecs = mha(batch)<br><br><span class="hljs-built_in">print</span>(context_vecs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;context_vecs.shape:&quot;</span>, context_vecs.shape)<br><br><span class="hljs-comment"># 输出</span><br>tensor([[[-<span class="hljs-number">0.4519</span>,  <span class="hljs-number">0.2216</span>,  <span class="hljs-number">0.4772</span>,  <span class="hljs-number">0.1063</span>],<br>         [-<span class="hljs-number">0.5874</span>,  <span class="hljs-number">0.0058</span>,  <span class="hljs-number">0.5891</span>,  <span class="hljs-number">0.3257</span>],<br>         [-<span class="hljs-number">0.6300</span>, -<span class="hljs-number">0.0632</span>,  <span class="hljs-number">0.6202</span>,  <span class="hljs-number">0.3860</span>],<br>         [-<span class="hljs-number">0.5675</span>, -<span class="hljs-number">0.0843</span>,  <span class="hljs-number">0.5478</span>,  <span class="hljs-number">0.3589</span>],<br>         [-<span class="hljs-number">0.5526</span>, -<span class="hljs-number">0.0981</span>,  <span class="hljs-number">0.5321</span>,  <span class="hljs-number">0.3428</span>],<br>         [-<span class="hljs-number">0.5299</span>, -<span class="hljs-number">0.1081</span>,  <span class="hljs-number">0.5077</span>,  <span class="hljs-number">0.3493</span>]],<br><br>        [[-<span class="hljs-number">0.4519</span>,  <span class="hljs-number">0.2216</span>,  <span class="hljs-number">0.4772</span>,  <span class="hljs-number">0.1063</span>],<br>         [-<span class="hljs-number">0.5874</span>,  <span class="hljs-number">0.0058</span>,  <span class="hljs-number">0.5891</span>,  <span class="hljs-number">0.3257</span>],<br>         [-<span class="hljs-number">0.6300</span>, -<span class="hljs-number">0.0632</span>,  <span class="hljs-number">0.6202</span>,  <span class="hljs-number">0.3860</span>],<br>         [-<span class="hljs-number">0.5675</span>, -<span class="hljs-number">0.0843</span>,  <span class="hljs-number">0.5478</span>,  <span class="hljs-number">0.3589</span>],<br>         [-<span class="hljs-number">0.5526</span>, -<span class="hljs-number">0.0981</span>,  <span class="hljs-number">0.5321</span>,  <span class="hljs-number">0.3428</span>],<br>         [-<span class="hljs-number">0.5299</span>, -<span class="hljs-number">0.1081</span>,  <span class="hljs-number">0.5077</span>,  <span class="hljs-number">0.3493</span>]]], grad_fn=&lt;CatBackward0&gt;)<br>context_vecs.shape: torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">4</span>])<br></code></pre></td></tr></table></figure><h3 id="Implementing-multi-head-attention-with-weight-splits"><a href="#Implementing-multi-head-attention-with-weight-splits" class="headerlink" title="Implementing multi-head attention with weight splits"></a>Implementing multi-head attention with weight splits</h3><p>在实际的处理中，为了更好的并行化，其实会作为一个大矩阵来计算多头注意力。例如我们会一次性初始化一个大的Wq，然后通过一次矩阵运算得到Q后再对其进行形状的转化，分割成多个self-attention中的Q。再计算得到attention，再计算得到上下文，最后又通过形状的变化得到拼接后的输出。</p><p><img src="/2025/05/02/LLMFromScratch2/image-8.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-keyword">assert</span> (d_out % num_heads == <span class="hljs-number">0</span>), \<br>            <span class="hljs-string">&quot;d_out must be divisible by num_heads&quot;</span><br><br>        self.d_out = d_out<br>        self.num_heads = num_heads<br>        self.head_dim = d_out // num_heads <span class="hljs-comment"># Reduce the projection dim to match desired output dim</span><br><br>        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)<br>        self.out_proj = nn.Linear(d_out, d_out)  <span class="hljs-comment"># Linear layer to combine head outputs</span><br>        self.dropout = nn.Dropout(dropout)<br>        self.register_buffer(<br>            <span class="hljs-string">&quot;mask&quot;</span>,<br>            torch.triu(torch.ones(context_length, context_length),<br>                       diagonal=<span class="hljs-number">1</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        b, num_tokens, d_in = x.shape<br>        <span class="hljs-comment"># As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, </span><br>        <span class="hljs-comment"># this will result in errors in the mask creation further below. </span><br>        <span class="hljs-comment"># In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  </span><br>        <span class="hljs-comment"># do not exceed `context_length` before reaching this forwar</span><br><br>        keys = self.W_key(x) <span class="hljs-comment"># Shape: (b, num_tokens, d_out)</span><br>        queries = self.W_query(x)<br>        values = self.W_value(x)<br><br>        <span class="hljs-comment"># We implicitly split the matrix by adding a `num_heads` dimension</span><br>        <span class="hljs-comment"># Unroll last dim: (b, num_tokens, d_out) -&gt; (b, num_tokens, num_heads, head_dim)</span><br>        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) <br>        values = values.view(b, num_tokens, self.num_heads, self.head_dim)<br>        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)<br><br>        <span class="hljs-comment"># Transpose: (b, num_tokens, num_heads, head_dim) -&gt; (b, num_heads, num_tokens, head_dim)</span><br>        keys = keys.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        queries = queries.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        values = values.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br><br>        <span class="hljs-comment"># Compute scaled dot-product attention (aka self-attention) with a causal mask</span><br>        <span class="hljs-comment"># attn_scores Shape: (b, num_heads, num_tokens, num_tokens)</span><br>        attn_scores = queries @ keys.transpose(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>)  <span class="hljs-comment"># Dot product for each head</span><br><br>        <span class="hljs-comment"># Original mask truncated to the number of tokens and converted to boolean</span><br>        mask_bool = self.mask.<span class="hljs-built_in">bool</span>()[:num_tokens, :num_tokens]<br><br>        <span class="hljs-comment"># Use the mask to fill attention scores</span><br>        attn_scores.masked_fill_(mask_bool, -torch.inf)<br>        <br>        attn_weights = torch.softmax(attn_scores / keys.shape[-<span class="hljs-number">1</span>]**<span class="hljs-number">0.5</span>, dim=-<span class="hljs-number">1</span>)<br>        attn_weights = self.dropout(attn_weights)<br><br>        <span class="hljs-comment"># (b, num_heads, num_tokens, num_tokens) @ (b, num_heads, num_tokens, head_dim) = (b, num_heads, num_tokens, head_dim)</span><br>        <span class="hljs-comment"># Shape: (b, num_tokens, num_heads, head_dim)</span><br>        context_vec = (attn_weights @ values).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <br>        <br>        <span class="hljs-comment"># Combine heads, where self.d_out = self.num_heads * self.head_dim</span><br>        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)<br>        context_vec = self.out_proj(context_vec) <span class="hljs-comment"># optional projection</span><br><br>        <span class="hljs-keyword">return</span> context_vec<br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br><br>batch_size, context_length, d_in = batch.shape<br>d_out = <span class="hljs-number">4</span><br>mha = MultiHeadAttention(d_in, d_out, context_length, <span class="hljs-number">0.0</span>, num_heads=<span class="hljs-number">2</span>)<br><br>context_vecs = mha(batch)<br><br><span class="hljs-built_in">print</span>(context_vecs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;context_vecs.shape:&quot;</span>, context_vecs.shape)<br><br><span class="hljs-comment"># 输出</span><br>tensor([[[ <span class="hljs-number">0.1184</span>,  <span class="hljs-number">0.3120</span>, -<span class="hljs-number">0.0847</span>, -<span class="hljs-number">0.5774</span>],<br>         [ <span class="hljs-number">0.0178</span>,  <span class="hljs-number">0.3221</span>, -<span class="hljs-number">0.0763</span>, -<span class="hljs-number">0.4225</span>],<br>         [-<span class="hljs-number">0.0147</span>,  <span class="hljs-number">0.3259</span>, -<span class="hljs-number">0.0734</span>, -<span class="hljs-number">0.3721</span>],<br>         [-<span class="hljs-number">0.0116</span>,  <span class="hljs-number">0.3138</span>, -<span class="hljs-number">0.0708</span>, -<span class="hljs-number">0.3624</span>],<br>         [-<span class="hljs-number">0.0117</span>,  <span class="hljs-number">0.2973</span>, -<span class="hljs-number">0.0698</span>, -<span class="hljs-number">0.3543</span>],<br>         [-<span class="hljs-number">0.0132</span>,  <span class="hljs-number">0.2990</span>, -<span class="hljs-number">0.0689</span>, -<span class="hljs-number">0.3490</span>]],<br><br>        [[ <span class="hljs-number">0.1184</span>,  <span class="hljs-number">0.3120</span>, -<span class="hljs-number">0.0847</span>, -<span class="hljs-number">0.5774</span>],<br>         [ <span class="hljs-number">0.0178</span>,  <span class="hljs-number">0.3221</span>, -<span class="hljs-number">0.0763</span>, -<span class="hljs-number">0.4225</span>],<br>         [-<span class="hljs-number">0.0147</span>,  <span class="hljs-number">0.3259</span>, -<span class="hljs-number">0.0734</span>, -<span class="hljs-number">0.3721</span>],<br>         [-<span class="hljs-number">0.0116</span>,  <span class="hljs-number">0.3138</span>, -<span class="hljs-number">0.0708</span>, -<span class="hljs-number">0.3624</span>],<br>         [-<span class="hljs-number">0.0117</span>,  <span class="hljs-number">0.2973</span>, -<span class="hljs-number">0.0698</span>, -<span class="hljs-number">0.3543</span>],<br>         [-<span class="hljs-number">0.0132</span>,  <span class="hljs-number">0.2990</span>, -<span class="hljs-number">0.0689</span>, -<span class="hljs-number">0.3490</span>]]], grad_fn=&lt;ViewBackward0&gt;)<br>context_vecs.shape: torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">4</span>])<br></code></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li><p>自注意力机制将上下文向量表示计算为输入的加权和。</p></li><li><p>在简化的注意力机制中，注意力权重是通过点积计算的。</p></li><li><p>在LLM中使用的自注意力机制（也称为缩放点积注意力机制）中，我们引入了可训练的权重矩阵来计算输入的中间变换：查询、值和键。当使用从左到右读取和生成文本的LLM时，我们添加了因果注意力掩码，以防止LLM访问未来的token。</p></li><li><p>除了因果注意力掩码将注意力权重归零之外，我们还可以添加 dropout mask 来减少 LLM 中的过度拟合。</p></li><li><p>基于 Transformer 的 LLM 中的注意力模块涉及因果注意力的多个实例，这称为多头注意力。</p></li><li><p>我们可以通过堆叠多个因果注意模块实例来创建多头注意模块。</p></li><li><p>创建多头注意力模块的更有效方法涉及分批矩阵乘法。</p></li></ul><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://knowledge.zhaoweiguo.com/build/html/x-learning/books/ais/2024/build/_llm/_from/_scratch#understanding-llm">https://knowledge.zhaoweiguo.com/build/html/x-learning/books/ais/2024/build\_llm\_from\_scratch#understanding-llm</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>Build a Large Language Model (From Scratch)</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【从零构建大模型】一、文本数据处理</title>
    <link href="/2025/05/01/LLMFromScratch1/"/>
    <url>/2025/05/01/LLMFromScratch1/</url>
    
    <content type="html"><![CDATA[<h1 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h1><p>构建大模型的全景图如下，本文介绍了最开始的数据处理。</p><p><img src="/2025/05/01/LLMFromScratch1/image-9.png"></p><p>数据处理的全景图如下所示，大致流程为：</p><ol><li><p>将原文本，一般为一个string，进行分割。</p></li><li><p>对分割后的词转化为id。</p></li><li><p>生成一个embeddings层，然后id作为序号去embeddings层中取对应的行作为自己的表征。</p></li></ol><p><img src="/2025/05/01/LLMFromScratch1/image-8.png"></p><p>如此转化后就将原本深度学习模型不能处理的原始数据转化为了可以处理的矩阵，同时我们希望这最后表示原始数据的矩阵中也能表示词之间的关系，例如词意相似的单词在可以空间上较为接近。</p><p><img src="/2025/05/01/LLMFromScratch1/image-4.png"></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><h2 id="Tokenizing-text"><a href="#Tokenizing-text" class="headerlink" title="Tokenizing text"></a>Tokenizing text</h2><p><img src="/2025/05/01/LLMFromScratch1/image-3.png"></p><p>拆分的规则可以有多种，最简单的就是直接按单词粒度进行拆分，例如直接以空格作为分隔符，但是需要注意拆分过程中需要注意标点符合与单词之间没有空格。</p><p>简单的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">text = <span class="hljs-string">&quot;Hello, world. Is this-- a test?&quot;</span><br><br>result = re.split(<span class="hljs-string">r&#x27;([,.:;?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, text)<br>result = [item.strip() <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> result <span class="hljs-keyword">if</span> item.strip()]<br><span class="hljs-built_in">print</span>(result)<br></code></pre></td></tr></table></figure><h2 id="Converting-tokens-into-token-IDs"><a href="#Converting-tokens-into-token-IDs" class="headerlink" title="Converting tokens into token IDs"></a>Converting tokens into token IDs</h2><p>在进行拆分后需要对所有的单词、符合进行去重，建立一张可以单词与id互相映射的词汇表，最简单的方法就是直接按照字母顺序对其进行排序。然后这张词汇表在代码中的显示就是一个支持encode和decode的类，如下所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">all_words = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">set</span>(preprocessed))<br>vocab = &#123;token:integer <span class="hljs-keyword">for</span> integer,token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(all_words)&#125;<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleTokenizerV1</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab</span>):<br>        self.str_to_int = vocab<br>        self.int_to_str = &#123;i:s <span class="hljs-keyword">for</span> s,i <span class="hljs-keyword">in</span> vocab.items()&#125;<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, text</span>):<br>        preprocessed = re.split(<span class="hljs-string">r&#x27;([,.:;?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, text)<br>                                <br>        preprocessed = [<br>            item.strip() <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> preprocessed <span class="hljs-keyword">if</span> item.strip()<br>        ]<br>        ids = [self.str_to_int[s] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> preprocessed]<br>        <span class="hljs-keyword">return</span> ids<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">self, ids</span>):<br>        text = <span class="hljs-string">&quot; &quot;</span>.join([self.int_to_str[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ids])<br>        <span class="hljs-comment"># Replace spaces before the specified punctuations</span><br>        text = re.sub(<span class="hljs-string">r&#x27;\s+([,.?!&quot;()\&#x27;])&#x27;</span>, <span class="hljs-string">r&#x27;\1&#x27;</span>, text)<br>        <span class="hljs-keyword">return</span> text<br></code></pre></td></tr></table></figure><h2 id="Adding-special-context-tokens"><a href="#Adding-special-context-tokens" class="headerlink" title="Adding special context tokens"></a>Adding special context tokens</h2><p>常见的特殊标识符有：</p><ul><li><p><code>[BOS]</code> 一个文本序列的起始标识</p></li><li><p><code>[EOS]</code> 一个文本序列的结束标识</p></li><li><p><code>[PAD]</code> 如果batch size大于1，那么就需要用它来填充那些较短的文本，以做到长度统一</p></li><li><p><code>[UNK]</code> 用于表示在词汇表之外的单词</p></li></ul><blockquote><p>GPT、GPT-2 只使用了<code>&lt;|endoftext|&gt;</code> ，当做结束的标识符号，也当做填充的标识符。GPT-2不需要<code>[UNK]</code> ，因为它使用 byte-pair encoding (BPE)来编码</p></blockquote><p>加入<code>[UNK]</code> 后的词汇表如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleTokenizerV2</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab</span>):<br>        self.str_to_int = vocab<br>        self.int_to_str = &#123; i:s <span class="hljs-keyword">for</span> s,i <span class="hljs-keyword">in</span> vocab.items()&#125;<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, text</span>):<br>        preprocessed = re.split(<span class="hljs-string">r&#x27;([,.:;?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, text)<br>        preprocessed = [item.strip() <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> preprocessed <span class="hljs-keyword">if</span> item.strip()]<br>        preprocessed = [<br>            item <span class="hljs-keyword">if</span> item <span class="hljs-keyword">in</span> self.str_to_int <br>            <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;&lt;|unk|&gt;&quot;</span> <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> preprocessed<br>        ]<br><br>        ids = [self.str_to_int[s] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> preprocessed]<br>        <span class="hljs-keyword">return</span> ids<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">self, ids</span>):<br>        text = <span class="hljs-string">&quot; &quot;</span>.join([self.int_to_str[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ids])<br>        <span class="hljs-comment"># Replace spaces before the specified punctuations</span><br>        text = re.sub(<span class="hljs-string">r&#x27;\s+([,.:;?!&quot;()\&#x27;])&#x27;</span>, <span class="hljs-string">r&#x27;\1&#x27;</span>, text)<br>        <span class="hljs-keyword">return</span> text<br></code></pre></td></tr></table></figure><h2 id="BytePair-encoding-BPE"><a href="#BytePair-encoding-BPE" class="headerlink" title="BytePair encoding(BPE)"></a>BytePair encoding(BPE)</h2><p>BPE分词器可以将句子分解并转化为id，对于未知词，可以将其分解为子词和单个字符，这样它就可以解析任何单词。</p><ul><li>OpenAI 开源 [tiktoken]( <a href="https://github.com/openai/tiktoken">https://github.com/openai/tiktoken</a> ) 库中的 BPE 标记器，其核心算法以 Rust 实现，以提高计算性能</li></ul><p><img src="/2025/05/01/LLMFromScratch1/image-6.png"></p><p>简单使用如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> importlib<br><span class="hljs-keyword">import</span> tiktoken<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;tiktoken version:&quot;</span>, importlib.metadata.version(<span class="hljs-string">&quot;tiktoken&quot;</span>))<br>tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br><br>text = (<br>    <span class="hljs-string">&quot;Hello, do you like tea? &lt;|endoftext|&gt; In the sunlit terraces&quot;</span><br>     <span class="hljs-string">&quot;of someunknownPlace.&quot;</span><br>)<br><br>integers = tokenizer.encode(text, allowed_special=&#123;<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>&#125;)<br><br><span class="hljs-built_in">print</span>(integers)<br></code></pre></td></tr></table></figure><h2 id="Data-sampling-with-a-sliding-window"><a href="#Data-sampling-with-a-sliding-window" class="headerlink" title="Data sampling with a sliding window"></a>Data sampling with a sliding window</h2><p>在训练过程中往往会将文本划分为多个块，一个模型进行训练的预测时只会基于这个块中前面的部分词来预测后一个词，简单的表示如下：</p><p><img src="/2025/05/01/LLMFromScratch1/image-7.png"></p><p>因为是基于已有的输入预测后一个，所以对于正确的output实际上就是input往后移一位，如下：</p><p><img src="/2025/05/01/LLMFromScratch1/image-5.png"></p><p>上面这个是一个batch，为了产生多组batch可以使用滑动窗口的思想，通过控制步长来生成多个batch：</p><p><img src="/2025/05/01/LLMFromScratch1/image-1.png"></p><p>一个简单的生成数据集的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GPTDatasetV1</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, txt, tokenizer, max_length, stride</span>):<br>        self.input_ids = []<br>        self.target_ids = []<br><br>        <span class="hljs-comment"># Tokenize the entire text</span><br>        token_ids = tokenizer.encode(txt, allowed_special=&#123;<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>&#125;)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(token_ids) &gt; max_length, <span class="hljs-string">&quot;Number of tokenized inputs must at least be equal to max_length+1&quot;</span><br><br>        <span class="hljs-comment"># Use a sliding window to chunk the book into overlapping sequences of max_length</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(token_ids) - max_length, stride):<br>            input_chunk = token_ids[i:i + max_length]<br>            target_chunk = token_ids[i + <span class="hljs-number">1</span>: i + max_length + <span class="hljs-number">1</span>]<br>            self.input_ids.append(torch.tensor(input_chunk))<br>            self.target_ids.append(torch.tensor(target_chunk))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.input_ids)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> self.input_ids[idx], self.target_ids[idx]<br>        <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_dataloader_v1</span>(<span class="hljs-params">txt, batch_size=<span class="hljs-number">4</span>, max_length=<span class="hljs-number">256</span>, </span><br><span class="hljs-params">                         stride=<span class="hljs-number">128</span>, shuffle=<span class="hljs-literal">True</span>, drop_last=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">                         num_workers=<span class="hljs-number">0</span></span>):<br><br>    <span class="hljs-comment"># Initialize the tokenizer</span><br>    tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br><br>    <span class="hljs-comment"># Create dataset</span><br>    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)<br><br>    <span class="hljs-comment"># Create dataloader</span><br>    dataloader = DataLoader(<br>        dataset,<br>        batch_size=batch_size,<br>        shuffle=shuffle,<br>        drop_last=drop_last,<br>        num_workers=num_workers<br>    )<br><br>    <span class="hljs-keyword">return</span> dataloader<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;the-verdict.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    raw_text = f.read()<br>dataloader = create_dataloader_v1(<br>    raw_text, batch_size=<span class="hljs-number">1</span>, max_length=<span class="hljs-number">4</span>, stride=<span class="hljs-number">1</span>, shuffle=<span class="hljs-literal">False</span><br>)<br><br>data_iter = <span class="hljs-built_in">iter</span>(dataloader)<br>first_batch = <span class="hljs-built_in">next</span>(data_iter)<br><span class="hljs-built_in">print</span>(first_batch)<br><span class="hljs-comment"># [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]</span><br></code></pre></td></tr></table></figure><h2 id="Creating-token-embeddings"><a href="#Creating-token-embeddings" class="headerlink" title="Creating token embeddings"></a>Creating token embeddings</h2><p>得到标记 ID 后需要将其转换为连续的向量表示，即所谓的token embeddings。</p><p>其首先需要初始化一个embeddings层，其行数为id的数量，列数为用来表示一个token的信息量，也就是输出的维度。</p><p>然后根据当前token的id就直接去对应的行取出那一行作为其token的表示，如下：</p><p><img src="/2025/05/01/LLMFromScratch1/image-2.png"></p><p>简单的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">vocab_size = <span class="hljs-number">6</span><br>output_dim = <span class="hljs-number">3</span><br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>embedding_layer = torch.nn.Embedding(vocab_size, output_dim)<br><br><span class="hljs-built_in">print</span>(embedding_layer.weight)<br><span class="hljs-built_in">print</span>(embedding_layer(torch.tensor([<span class="hljs-number">3</span>])))<br><span class="hljs-comment"># tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=&lt;EmbeddingBackward0&gt;)</span><br></code></pre></td></tr></table></figure><h2 id="Encoding-word-positions"><a href="#Encoding-word-positions" class="headerlink" title="Encoding word positions"></a>Encoding word positions</h2><p>一个词在不同的位置其表示的含义可能会有所不同，所以还需要有位置编码的信息。position-aware embeddings有两类：</p><ul><li><p>相对位置嵌入的重点并非关注标记的绝对位置，而是标记之间的相对位置或距离。这意味着模型学习的是“相距多远”而不是“具体在哪个位置”的关系。这样做的好处是，即使在训练过程中没有遇到过这种长度的序列，模型也能更好地泛化到不同长度的序列。</p></li><li><p>OpenAI 的 GPT 模型使用绝对位置嵌入，这些嵌入在训练过程中进行优化，而不是像原始 Transformer 模型中的位置编码那样固定或预定义。此优化过程是模型训练本身的一部分。</p></li></ul><p>简单的位置嵌入编码就是直接生成一个行数为最大位置数的embeddings层，然后各个词按照序列id去取对应的embddings来得到位置嵌入编码，之后需要将Token embeddings和Positional embeddings进行相加最后得到一个完整的Iput embeddings。</p><p><img src="/2025/05/01/LLMFromScratch1/image.png"></p><p>简单的实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">context_length = max_length<br>pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)<br><br>pos_embeddings = pos_embedding_layer(torch.arange(max_length))<br><br>input_embeddings = token_embeddings + pos_embeddings<br><span class="hljs-built_in">print</span>(input_embeddings.shape)<br></code></pre></td></tr></table></figure><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://knowledge.zhaoweiguo.com/build/html/x-learning/books/ais/2024/build/_llm/_from/_scratch#understanding-llm">https://knowledge.zhaoweiguo.com/build/html/x-learning/books/ais/2024/build\_llm\_from\_scratch#understanding-llm</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>Build a Large Language Model (From Scratch)</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型显存占用浅析</title>
    <link href="/2025/04/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8/"/>
    <url>/2025/04/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="浮点数"><a href="#浮点数" class="headerlink" title="浮点数"></a>浮点数</h2><p>对于大模型常说的1B、7B中的B指的是Billion，即十亿参数，然后还需考虑模型采用什么位数来存储，常见的表示类型如下：</p><p><img src="/2025/04/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8/image-4.png"></p><p>可以非常直观地看到，浮点数主要是由符号位（sign）、指数位（exponent）和小数位（mantissa）三部分组成。 符号位都是1位（0表示正，1表示负），指数位影响浮点数范围，小数位影响精度。 其中TF32并不是有32bit，只有19bit不要记错了。BF16指的是Brain Float 16，由Google Brain团队提出。</p><p>而对于上述的计数方式，以BF16为例，从下面这一个例子来表示具体是如何计数的：</p><p><img src="/2025/04/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8/image-2.png"></p><p>其计算规则如下：</p><p><img src="/2025/04/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8/image-1.png"></p><p>符号位Sign &#x3D; 1，代表是负数</p><p>指数位Exponent &#x3D; 17，中间一坨是 2^(−110)</p><p>小数位Mantissa &#x3D; 3，后面那一坨是 1+3&#x2F;128</p><p>最终结果，三个部分乘起来就是最终结果 -8.004646331359449e-34</p><p>1Byte&#x3D;8bit，以fp32为例，1个fp32的参数就是32&#x2F;8&#x3D;4Bytes，故而对于1B的fp32的模型，其占用的显存为4 Billion bytes ，而1GB约等于10^9，即1 Billion，故模型的显存占用约为4GB。</p><h2 id="显存占用类型"><a href="#显存占用类型" class="headerlink" title="显存占用类型"></a>显存占用类型</h2><p>而在大模型训练过程中，显存主要被以下几个部分占用：</p><ol><li><p>模型权重</p></li><li><p>优化器状态</p></li><li><p>梯度</p></li><li><p>激活值</p></li><li><p>临时缓冲区</p></li></ol><p>不同的训练阶段（如SFT、RLHF）对显存的需求也有所不同。</p><h1 id="混合精度训练"><a href="#混合精度训练" class="headerlink" title="混合精度训练"></a>混合精度训练</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在进行大模型训练时，往往采取的是混合精度训练（<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1710.03740">MIXED PRECISION TRAINING</a>）</p><p>混合精度训练是在尽可能减少精度损失的情况下利用半精度浮点数加速训练。它使用FP16即半精度浮点数存储权重和梯度。在减少占用内存的同时起到了加速训练的效果。</p><p>整体过程如下所示：</p><p><img src="/2025/04/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8/image-3.png"></p><p>从图中可以看出，在计算过程中所产生的权重，激活值，梯度等均使用 FP16 来进行存储和计算，其中权重使用FP32额外进行备份。这样做的原因是，在更新权重公式为如下形式。</p><p>对于优化器和权重采用fp32是因为在深度模型中，学习率×梯度的参数值可能会非常小，如果利用FP16来进行相加的话，则很可能会出现舍入误差问题，导致更新无效。因此通过将权重拷贝成FP32格式，并且确保整个更新过程是在FP32格式下进行的。</p><p>注意虽然这样子会导致我们需要一个额外的FP16的模型权重，但是由实际上额外拷贝一份权重只增加了训练时候静态内存的占用。而在训练过程中内存中分为动态内存和静态内容，其中动态内存是静态内存的3-4倍，主要是中间变量值和激活值。只要动态内存的值基本都是使用FP16来进行存储，则最终模型与整网使用FP32进行训练相比起来，内存占用也基本能够减半。</p><h2 id="Loss-scaling"><a href="#Loss-scaling" class="headerlink" title="Loss-scaling"></a>Loss-scaling</h2><p>得到FP32的loss后，放大并保存为FP16格式，进行反向传播，更新时转为FP32缩放回来。下图可以看到，很多激活值比较小，无法用FP16表示。因此在前向传播后对loss进行扩大（固定值或动态值），这样在反响传播时所有的值也都扩大了相同的倍数。在更新FP32的权重之前unscale回去。</p><p><img src="/2025/04/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8/image.png"></p><h2 id="显存计算"><a href="#显存计算" class="headerlink" title="显存计算"></a>显存计算</h2><p>对于llama3.1 8B模型，FP32和BF16混合精度训练，用的是<a href="https://zhida.zhihu.com/search?content_id=246631720&content_type=Article&match_order=1&q=AdamW%E4%BC%98%E5%8C%96%E5%99%A8&zhida_source=entity">AdamW优化器</a>，请问模型训练时占用显存大概为多少？</p><p>解：</p><p>模型参数：16（BF16） + 32（PF32）&#x3D; 48G</p><p>梯度参数：16（BF16）&#x3D; 16G</p><p>Adam优化器参数：32（PF32） + 32（PF32）&#x3D; 64G</p><p>不考虑激活值的情况下，总显存大约占用 （48 + 16 + 64） &#x3D; 128G</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><p>一文讲明白大模型显存占用（只考虑单卡）：<a href="https://zhuanlan.zhihu.com/p/713256008">https://zhuanlan.zhihu.com/p/713256008</a></p></li><li><p>【通俗易读】LLM训练-从显存占用分析到DeepSpeed ZeRO 三阶段解读<a href="https://zhuanlan.zhihu.com/p/694880795">https://zhuanlan.zhihu.com/p/694880795</a></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer-KV cache浅析</title>
    <link href="/2025/04/25/Transformer-KV%20cache%E6%B5%85%E6%9E%90/"/>
    <url>/2025/04/25/Transformer-KV%20cache%E6%B5%85%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h1 id="推理回顾"><a href="#推理回顾" class="headerlink" title="推理回顾"></a>推理回顾</h1><p>假设模型最终生成了“遥遥领先”4个字。</p><p>当模型生成第一个“遥”字时，input&#x3D;”&lt;s&gt;”, “&lt;s&gt;”是起始字符。<a href="https://zhida.zhihu.com/search?content_id=235350632&content_type=Article&match_order=1&q=Attention&zhida_source=entity">Attention</a>的计算如下：</p><p><img src="/2025/04/25/Transformer-KV%20cache%E6%B5%85%E6%9E%90/image-3.png"></p><p>为了看上去方便，我们暂时忽略scale项根号d， 但是要注意这个scale面试时经常考。</p><p>如上图所示，最终Attention的计算公式如下，（softmaxed 表示已经按行进行了softmax）:</p><p><img src="/2025/04/25/Transformer-KV%20cache%E6%B5%85%E6%9E%90/image-2.png"></p><p>以此类推生成第四个字的时候如下：</p><p><img src="/2025/04/25/Transformer-KV%20cache%E6%B5%85%E6%9E%90/image-4.png"></p><p>公式为：</p><p><img src="/2025/04/25/Transformer-KV%20cache%E6%B5%85%E6%9E%90/image.png"></p><blockquote><p>需要注意的一点在于QK^T这个矩阵的长宽都等于历史字符的数量，随着对话轮数的增加，这部分空间暂用是十分恐怖的</p></blockquote><h1 id="KV-Cache"><a href="#KV-Cache" class="headerlink" title="KV Cache"></a>KV Cache</h1><p>这里有一个特点，如果是对于一般的自回归系统，我们在生成下一个字的时候往往需要参考过去所有的字，但是实际上因为mask的存在，我们的Attention_4只需要拿当前新增字符的Q与过去生成的字符（也包括自己）的K、V相乘就可得到，所以我们可以将过去的K、V进行缓存以进行更高速的计算</p><p>下图展示了使用KV Cache和不使用的对比</p><p><img src="/2025/04/25/Transformer-KV%20cache%E6%B5%85%E6%9E%90/image-1.png"></p><p>而在实际实现时的做法就较为简单了，直接和缓存起来的过去key和value与新生成的字符的key和value拼接就可以得到新的key和value。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> layer_past <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        past_key, past_value = layer_past<br>        key = torch.cat((past_key, key), dim=-<span class="hljs-number">2</span>)<br>        value = torch.cat((past_value, value), dim=-<span class="hljs-number">2</span>)<br>    <br>    <span class="hljs-keyword">if</span> use_cache <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span>:<br>        present = (key, value)<br>    <span class="hljs-keyword">else</span>:<br>        present = <span class="hljs-literal">None</span><br>    <br>    <span class="hljs-keyword">if</span> self.reorder_and_upcast_attn:<br>        attn_output, attn_weights = self._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)<br>    <span class="hljs-keyword">else</span>:<br>        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)<br></code></pre></td></tr></table></figure><h1 id="显存代价"><a href="#显存代价" class="headerlink" title="显存代价"></a>显存代价</h1><p>KV cache实际上可以认为是一种以空间换时间的操作，其内存消耗为：</p><p>$$2 \times batch \times context_length \times n_layers \times n_head \times d_heads \times Pa$$</p><ul><li><p><code>2</code>代表是k与v两类缓存</p></li><li><p>**<code>batch</code>**：批量大小（batch size），表示一次训练或推理过程中输入的样本数量。</p></li><li><p>**<code>context_length</code>**：上下文长度，也就是输入序列的长度。在语言模型中，通常对应于文本的词数或标记的数量。</p></li><li><p>**<code>n_layers</code>**：模型的层数，即 Transformer 模型中的层数，各个层有自己独立的K、V。例如，BERT 或 GPT 中的层数。</p></li><li><p>**<code>n_heads</code>**：注意力头的数量。在多头注意力机制中，模型会将注意力计算分为多个头，每个头独立地进行计算，最后将结果合并。</p></li><li><p>**<code>d_heads</code>**：每个注意力头的维度。在多头注意力中，每个头计算的 <code>key</code> 和 <code>value</code> 的维度，通常是整个模型的嵌入维度（如 <code>hidden_size</code>）除以 <code>n_heads</code>。</p></li><li><p>**<code>Pa</code>**：一个常数，用于表示每个 <code>key</code> 和 <code>value</code> 向量的每个元素占用的内存量（如浮点数的字节数）。通常，如果是 32 位浮点数，则为 4 字节；如果是 16 位浮点数，则为 2 字节。</p></li></ul><p>以一个batch_size&#x3D;32, context_length&#x3D;2048,  n_layer&#x3D;32, n_head&#x3D;32, d_head&#x3D;128, float32类型，则需要占用的显存为: 2 * 32 * 2048 * 32 * 32  * 4096 * 4 &#x2F; 1024&#x2F;1024&#x2F;1024 &#x3D; 64G。</p><h1 id="优化速度"><a href="#优化速度" class="headerlink" title="优化速度"></a>优化速度</h1><p>有过一些实验，对于hugging face等推理库：</p><ul><li><p>使用kvcache耗时11s</p></li><li><p>不使用kvcache耗时56s</p></li></ul><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><p>大模型推理加速：看图学KV Cache：<a href="https://zhuanlan.zhihu.com/p/662498827">https://zhuanlan.zhihu.com/p/662498827</a></p></li><li><p>大模型推理性能优化之KV Cache解读: <a href="https://zhuanlan.zhihu.com/p/630832593">https://zhuanlan.zhihu.com/p/630832593</a></p></li><li><p>【8】KV Cache 原理讲解: <a href="https://www.bilibili.com/video/BV17CPkeEEzk/?spm%5C_id%5C_from=333.337.search-card.all.click%5C&vd%5C_source=cd12a18b61f61365725f1704677a6b74">https://www.bilibili.com/video/BV17CPkeEEzk/?spm\_id\_from=333.337.search-card.all.click\&amp;vd\_source=cd12a18b61f61365725f1704677a6b74</a></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer 中Decoder-only、Encoder-only、Decoder-encoder架构区别</title>
    <link href="/2025/04/23/Transformer%20%E4%B8%ADDecoder-only%E3%80%81Encoder-only%E3%80%81Decoder-encoder%E6%9E%B6%E6%9E%84%E5%8C%BA%E5%88%AB/"/>
    <url>/2025/04/23/Transformer%20%E4%B8%ADDecoder-only%E3%80%81Encoder-only%E3%80%81Decoder-encoder%E6%9E%B6%E6%9E%84%E5%8C%BA%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<h1 id="不同架构的特点"><a href="#不同架构的特点" class="headerlink" title="不同架构的特点"></a>不同架构的特点</h1><h2 id="Encoder-Decoder-模型-x20"><a href="#Encoder-Decoder-模型-x20" class="headerlink" title="Encoder-Decoder 模型&#x20;"></a><strong>Encoder-Decoder 模型&#x20;</strong></h2><ul><li><p><strong>特点</strong>:包含编码器和解码器两个部分。编码器处理输入序列，生成上下文向量;解码器则根据编码器的输出生成目标序列。这个结构能够<strong>同时处理输入和输出序列的关联。</strong></p></li><li><p><strong>典型模型：T5(Text-to-Text Transfer Transformer) 、</strong>&#x42;ART</p></li><li><p><strong>&#x20;应用</strong>:序列到序列任务，如机器翻译、摘要生成。&#x20;</p></li></ul><h2 id="Encoder-only-模型-x20"><a href="#Encoder-only-模型-x20" class="headerlink" title="Encoder-only 模型&#x20;"></a><strong>Encoder-only 模型&#x20;</strong></h2><ul><li><p><strong>特点</strong>: Encoder-only 模型只使用编码器部分。编码器的核心是处理输入序列，并生成该序列的上下文向量(即隐藏状态)，它能够很好地捕捉输入序列的全局信&#x606F;<strong>，对输入进行深度理解。&#x20;</strong></p></li><li><p><strong>典型模型：BERT</strong></p></li><li><p><strong>应用</strong>:自然语言理解(NLU)任务，如文本分类、命名实体识别。</p></li><li><p>&#x20;<strong>局限性</strong>:无法直接用于生成任务。&#x20;</p></li></ul><h2 id="Decoder-only-模型-x20"><a href="#Decoder-only-模型-x20" class="headerlink" title="Decoder-only 模型&#x20;"></a><strong>Decoder-only 模型&#x20;</strong></h2><ul><li><p><strong>特点</strong>:<strong>以自回归方式，基于先前生成的词预测下一个词。&#x20;</strong></p></li><li><p><strong>典型模型：GPT</strong></p></li><li><p><strong>应用</strong>:自然语言生成(NLG)任务，如文本生成、对话系统。&#x20;</p></li><li><p><strong>优势</strong>:能够同时处理理解和生成任务。&#x20;</p></li></ul><h1 id="为什么现在的LLM都是Decoder-only的架构？"><a href="#为什么现在的LLM都是Decoder-only的架构？" class="headerlink" title="为什么现在的LLM都是Decoder only的架构？"></a>为什么现在的LLM都是Decoder only的架构？</h1><p>目前主要的几种架构有：</p><ul><li><p>以BERT为代表的encoder-only</p></li><li><p>以T5和BART为代表的encoder-decoder</p></li><li><p>以GPT为代表的decoder-only</p></li><li><p>还有以UNILM为代表的PrefixLM（相比于GPT只改了attention mask，前缀部分是双向，后面要生成的部分是单向的causal mask），可以用这张图辅助记忆：</p></li></ul><p><img src="/2025/04/23/Transformer%20%E4%B8%ADDecoder-only%E3%80%81Encoder-only%E3%80%81Decoder-encoder%E6%9E%B6%E6%9E%84%E5%8C%BA%E5%88%AB/image.png"></p><h2 id="Encoder-only的缺点"><a href="#Encoder-only的缺点" class="headerlink" title="Encoder-only的缺点"></a>Encoder-only的缺点</h2><p>首先淘汰掉BERT这种<strong>encoder-only</strong>，因为它用<strong>masked language modeling</strong>预训练，不擅长做生成任务，做NLU一般也需要有监督的下游数据微调；相比之下，decoder-only的模型用next token prediction预训练，兼顾理解和生成，在各种下游任务上的zero-shot和few-shot泛化性能都很好。</p><h2 id="引入了部分双向attention的encoder-decoder和Prefix-LM的相比于Decoder-only的缺点"><a href="#引入了部分双向attention的encoder-decoder和Prefix-LM的相比于Decoder-only的缺点" class="headerlink" title="引入了部分双向attention的encoder-decoder和Prefix-LM的相比于Decoder-only的缺点"></a>引入了部分双向attention的encoder-decoder和Prefix-LM的相比于Decoder-only的缺点</h2><p>虽然它们也能兼顾理解和生成，泛化性能也不错，但是这部分设计往往没有被大部分大模型工作采用。而主要是采用了decoder-only设计。原因在于</p><ol><li><p><strong>Decoder-only泛化性能更好。</strong>&#x8FC7;去有实验表明用next token prediction预训练的decoder-only模型在各种下游任务上zero-shot泛化性能最好；另外，许多工作表明decoder-only模型的few-shot（也就是上下文学习，in-context learning）泛化能力更强。</p></li><li><p>Decoder-only<strong>泛化性能更好的原因：</strong></p><ol><li><p><strong>注意力满秩问题</strong>：双向attention的注意力矩阵容易退化为低秩状态，而causal attention的注意力矩阵是下三角矩阵，必然是满秩的，建模能力更强；</p></li><li><p><strong>预训练难度问题：</strong>&#x7EAF;粹的decoder-only架构+next token predicition预训练，每个位置所能接触的信息比其他架构少，要预测下一个token难度更高，当模型足够大，数据足够多的时候，decoder-only模型学习通用表征的上限更高；</p></li><li><p><strong>上下文学习为decoder-only架构带来的更好的few-shot性能：</strong>&#x70;rompt和demonstration的信息可以视为对模型参数的隐式微调[2]，decoder-only的架构相比encoder-decoder在in-context learning上会更有优势，因为prompt可以更加直接地作用于decoder每一层的参数，微调的信号更强；</p></li><li><p><strong>隐式位置编码：</strong>&#x63;ausal attention （就是decoder-only的单向attention）具有隐式的位置编码功能 [3]，它打破了transformer的位置不变性，而带有双向attention的模型，如果不带位置编码，双向attention的部分token可以对换也不改变表示，对语序的区分能力天生较弱。</p></li></ol></li><li><p><strong>效率问题：</strong>&#x64;ecoder-only支持一直复用KV-Cache，对多轮对话更友好，因为每个token的表示只和它之前的输入有关，而encoder-decoder和PrefixLM就难以做到；</p></li><li><p><strong>轨迹依赖问题</strong>：OpenAI作为开拓者勇于挖坑踩坑，以decoder-only架构为基础摸索出了一套行之有效的训练方法和Scaling Law，后来者鉴于时间和计算成本，自然不愿意做太多结构上的大改动，继续沿用decoder-only架构。在工程生态上，decoder-only架构也形成了先发优势，Megatron和flash attention等重要工具对causal attention的支持更好。</p></li></ol><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>为什么现在的LLM都是Decoder only的架构？<a href="https://www.zhihu.com/question/588325646">https://www.zhihu.com/question/588325646</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer详解</title>
    <link href="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/"/>
    <url>/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/</url>
    
    <content type="html"><![CDATA[<h1 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h1><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-14.png"></p><p>Transformer由两个部分组成，包括Decoder和Encoder两个部分</p><h1 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h1><p>对于输入X，我们有对应的WQ、WK、WV矩阵，可以计算得到，Q、K、V</p><ul><li><p>Q代表<strong>查询向量</strong>（Query）</p></li><li><p>K代表<strong>键向量</strong>（Key），KQ其实就代表了对于各个字符的注意力</p></li><li><p>V代表<strong>值向量</strong>（Value），需要与注意力相乘得倒最后的值</p></li></ul><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-13.png"></p><p>然后依据下面的公式可以计算出对应的Attention。公式中计算矩阵Q和K每一行向量的内积，为了防止内积过大，因此除以 dk 的平方根</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-12.png"></p><p>注意softmax的公式为：</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-10.png"></p><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-8.png"></p><p>下图是 h&#x3D;8 时候的情况，此时会得到 8 个输出矩阵Z。</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-11.png"></p><p>得到 8 个输出矩阵 Z1 到 Z8 之后，Multi-Head Attention 将它们拼接在一起 (Concat)，然后传入一个Linear层，得到 Multi-Head Attention 最终的输出Z。</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-7.png"></p><p>可以看到 Multi-Head Attention 输出的矩阵Z与其输入的矩阵X的维度是一样的。</p><h1 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h1><p>Transformer中attention在对字符串对处理过程中是并行的，直接相乘即可，所以没有像RNN那样有位置信息，所以需要添加位置信息，在transormer原文中采用了<strong>固定位置编码（Sinusoidal Positional Encoding）</strong>&#x7684;方式，其计算公式如下：</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-5.png"></p><p>其中：</p><ul><li><p>pospospos 是序列中某个位置的索引（从 0 开始）。</p></li><li><p>i 是位置编码的维度索引（从 0 开始）。</p></li><li><p>d 是位置编码的总维度，通常与词嵌入维度相同。</p></li></ul><p>公式中的10000^{2i&#x2F;d}是一个缩放因子，控制不同维度的周期性，使得较小的维度具有较高的频率，较大的维度则具有较低的频率。</p><p>使用正弦和余弦函数是为了保证每个位置的编码是周期性的，同时具有较高的区分度，能够区分不同位置之间的关系。</p><h1 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h1><p>下图左边是Encoder架构，它由多个block组成：</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-9.png"></p><p>上图红色部分是 Transformer 的 Encoder block 结构，可以看到是由 Multi-Head Attention, Add &amp; Norm, Feed Forward, Add &amp; Norm 组成的。刚刚已经了解了 Multi-Head Attention 的计算过程，现在了解一下 Add &amp; Norm 和 Feed Forward 部分。</p><h2 id="x20-Add-Norm"><a href="#x20-Add-Norm" class="headerlink" title="&#x20;Add &amp; Norm"></a>&#x20;Add &amp; Norm</h2><p>该部分的计算公式如下</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-6.png"></p><p>两个向量相加其实也被叫为<strong>残差连接</strong>，在深度神经网络中，随着网络的加深，可能会遇到 <strong>梯度消失</strong> 或 <strong>梯度爆炸</strong> 问题，使得训练变得困难。为了避免这种情况，<strong>残差连接</strong> 提供了一条直接的路径，使得网络中的信息可以更容易地向前传播（即通过每一层），同时也更容易向后传播（即通过反向传播更新梯度）。</p><p>通过加上 X（即原始输入），模型不仅可以学习到来自当前层的信息，还可以保留和传递输入层的部分信息。这有助于解决深层网络的训练问题，并且使得模型能够更快地学习。</p><p><strong>Layer Normalization</strong>的公式如下，它会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-4.png"></p><p>其中：</p><ul><li><p>μ是输入向量 x的均值。</p></li><li><p>σ^2是输入向量的方差。</p></li><li><p>ϵ是一个小常数，用于防止除零错误，通常设定为10^{-6}。</p></li><li><p>\hat{x}_i是归一化后的每个元素。</p></li></ul><p>它有以下的作用：</p><ul><li><p><strong>梯度消失和梯度爆炸问题</strong>：在训练深层神经网络时，由于网络层数很深，梯度在反向传播过程中可能会消失或爆炸，导致训练不稳定。Layer Normalization 有助于缓解这个问题，通过对每个样本进行归一化来保证每一层的激活分布稳定，从而改善训练稳定性。</p></li><li><p><strong>加速收敛</strong>：归一化可以使得网络的训练更加平稳，避免了不同层次的激活值在训练中波动过大，从而加速了训练过程。</p></li><li><p><strong>避免内部协变量偏移（Internal Covariate Shift）</strong>：在神经网络训练过程中，由于层之间参数的变化，会导致每一层的输入分布发生变化，导致训练变得不稳定。Layer Normalization 通过归一化层输入，减少了这种变化，从而使训练过程更加稳定。</p></li></ul><h2 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed Forward"></a>Feed Forward</h2><p>Feed Forward 层比较简单，是一个两层的全连接层，第一层的激活函数为 Relu，第二层不使用激活函数，对应的公式如下。</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-2.png"></p><h2 id="组成Encorder"><a href="#组成Encorder" class="headerlink" title="组成Encorder"></a>组成Encorder</h2><p>通过上面描述的 Multi-Head Attention, Feed Forward, Add &amp; Norm 就可以构造出一个 Encoder block，多个Encoder block 叠加就可以组成 Encoder。</p><p>第一个 Encoder block 的输入为句子单词的表示向量矩阵，后续 Encoder block 的输入是前一个 Encoder block 的输出，最后一个 Encoder block 输出的矩阵就是编码信息矩阵 C，这一矩阵后续会用到 Decoder 中。</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-3.png"></p><h1 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h1><p>图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：</p><ul><li><p>包含两个 Multi-Head Attention 层。</p></li><li><p>第一个 Multi-Head Attention 层采用了 Masked 操作。</p></li><li><p>第二个 Multi-Head Attention 层的K, V矩阵使用 Encoder 的编码信息矩阵C进行计算，而Q使用上一个 Decoder block 的输出计算。</p></li><li><p>最后有一个 Softmax 层计算下一个翻译单词的概率。</p></li></ul><p><strong>注意：Self-attention</strong> 中的 W_Q, W_K, W_V 矩阵 <strong>是共享的</strong>，即每个 attention 头在每个解码器子层都使用相同的矩阵。</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-1.png"></p><h2 id="Masked-Multi-Head-Attention"><a href="#Masked-Multi-Head-Attention" class="headerlink" title="Masked Multi-Head Attention"></a>Masked Multi-Head Attention</h2><p>其主要是为了将输入盖住，防止在训练过程中模型提前看到后面要预测的单词</p><p><strong>第一步：</strong>&#x662F; Decoder 的输入矩阵和 Mask 矩阵</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image.png"></p><p><strong>第二步：</strong>&#x63A5;下来的操作和之前的 Self-Attention 一样，通过输入矩阵X计算得到<strong>Q,K,V</strong>矩阵。然后计算Q和 K^T 的乘积 <strong>QK^T</strong> 。</p><p><strong>第三步：</strong>&#x5728;得到 QKT 之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用Mask矩阵遮挡住每一个单词之后的信息，遮挡操作其实就是直接和Mask 矩阵直接按位相乘：</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-17.png"></p><p>得到 Mask QK^T 之后在 Mask QK^T上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。</p><p><strong>第四步：</strong>&#x4F7F;用 Mask QK^T与矩阵 V相乘，得到输出 Z，则单词 1 的输出向量 Z1 是只包含单词 1 信息的。</p><p><strong>第五步：</strong>&#x901A;过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 Zi ，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出Zi 然后计算得到第一个 Multi-Head Attention 的输出Z，Z与输入X维度一样。</p><h2 id="第二个Multi-Head-Attention"><a href="#第二个Multi-Head-Attention" class="headerlink" title="第二个Multi-Head Attention"></a>第二个Multi-Head Attention</h2><p>其主要的区别在于其中 Self-Attention 的 <strong>K, V矩阵</strong>不是使用 上一个 Decoder block 的输出计算的，而是<strong>使用</strong> <strong>Encoder 的编码信息矩阵 C&#x20;</strong>&#x8BA1;算的。</p><p>根据 Encoder 的输出 C计算得到 K, V，根据**上一个 Decoder block 的输出 Z 计算 Q (如果是第一个 Decoder block 则使用输入矩阵 X 进行计算)**，后续的计算方法与之前描述的一致。</p><p>这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 Mask)。</p><h2 id="Softmax输出"><a href="#Softmax输出" class="headerlink" title="Softmax输出"></a>Softmax输出</h2><p>经过多层的Decoder block 后，会利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 Z，因为 Mask 的存在，使得单词 0 的输出 Z0 只包含单词 0 的信息，如下：</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-16.png"></p><p>Softmax 根据输出矩阵的每一行预测下一个单词：</p><p><img src="/2025/04/22/Transformer%E8%AF%A6%E8%A7%A3/image-15.png"></p><h1 id="为什么Decoder-only更为流行"><a href="#为什么Decoder-only更为流行" class="headerlink" title="为什么Decoder-only更为流行"></a>为什么Decoder-only更为流行</h1><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><p><a href="https://zhuanlan.zhihu.com/p/338817680">https://zhuanlan.zhihu.com/p/338817680</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/713256008">https://zhuanlan.zhihu.com/p/713256008</a></p></li><li><p><a href="https://www.bilibili.com/video/BV1YsqSY8EiW/?spm%5C_id%5C_from=333.337.search-card.all.click%5C&vd%5C_source=cd12a18b61f61365725f1704677a6b74">https://www.bilibili.com/video/BV1YsqSY8EiW/?spm\_id\_from=333.337.search-card.all.click\&amp;vd\_source=cd12a18b61f61365725f1704677a6b74</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/625184011">https://zhuanlan.zhihu.com/p/625184011</a></p></li><li><p><a href="https://www.zhihu.com/question/588325646">https://www.zhihu.com/question/588325646</a></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型训练-优化器整理</title>
    <link href="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/"/>
    <url>/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<p>优化器指的是对于我们的一堆模型参数 θ ，以及一个损失函数 Lθ) ，如何找到找到 L(θ) 的最小值。</p><h1 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h1><p>随机梯度下降 Stochastic Gradient Descent SGD 是最为基础的梯度下降方法。</p><p>其核心思路是：在每一步迭代中，用当前的梯度，按着负梯度方向把参数“推”一点，直到 loss 变得更小。</p><p>对于一个模型参数 θ，目标是最小化损失函数 L(θ)，SGD 每一步的更新公式是：</p><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image-14.png"></p><ul><li><p>θ：模型参数</p></li><li><p>η：学习率（step size）</p></li><li><p>$$∇_θL$$：当前梯度</p></li></ul><p>依据更新时使用的样本数的不同，它可以分为多种类型：</p><ul><li><p>如果用 <strong>整个训练集</strong> 来计算一次梯度，叫 <strong>Batch Gradient Descent</strong></p></li><li><p>如果用 <strong>一个样本</strong> 来计算梯度，叫 <strong>True SGD</strong></p></li><li><p>如果用 <strong>一个小批量样本（如32、64、128）</strong>，叫 <strong>Mini-batch SGD</strong>（最常用）</p></li></ul><p><strong>Mini-batch SGD</strong> 就是现在主流训练方法——又能并行训练，又比逐样本快。</p><p>SGD足够简单和直观，但是SGD也有一些明显的缺点：</p><h1 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h1><p><strong>Momentum（动量法）</strong> 是在 SGD 基础上的进行了优化，它引入了动量机制，它用过去的“梯度方向”给当前的更新加“惯性”！像小球在山谷中滑行，会积蓄动能，不会被小坑困住。</p><p><strong>Momentum</strong>的更新公式如下：</p><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image-12.png"></p><ul><li><p>γ：动量系数（一般取 0.9 或 0.99）</p></li><li><p>v_t：累计的更新速度（类似速度向量）</p></li><li><p>η：学习率</p></li><li><p>∇θL(θt)：当前梯度</p></li></ul><p>v_t累积了过去的动量，使得在进行梯度更新的时候具有了惯性。</p><p>但是它也存在明显的缺点：</p><h1 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h1><p>SGD的一个问题在于所有参数都共用同一个学习率，而这会导致：</p><ul><li><p>大梯度的参数震荡很厉害</p></li><li><p>小梯度的参数几乎不更新</p></li></ul><p>为此AdaGrad提出依据历史的梯度信息来调整学习率。其参数更新的计算公式如下：</p><p>首先记录每个参数过去所有的梯度平方的“累计值”：</p><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image-13.png"></p><p>然后更新参数的时候，把学习率除以这个值的平方根：</p><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image-11.png"></p><ul><li><p>θt：第 t 次迭代的参数</p></li><li><p>η：初始学习率</p></li><li><p>g_t：当前梯度</p></li><li><p>G_t：当前参数的梯度平方和</p></li><li><p>ϵ：防止除以 0 的小常数（如 1e−8）</p></li></ul><p>在这种方法下就很好的解决了之前的问题：</p><ul><li><p>如果某个参数的历史梯度<strong>一直很大</strong>，那它的 GtG_tGt 就大，学习率就变小 → 避免剧烈震荡</p></li><li><p>如果某个参数的历史梯度<strong>一直很小</strong>，学习率就不怎么缩 → 它还能继续学</p></li></ul><p>但是它也存在一个明显的缺点，那就是因为G_t是不断累积的，所以到了后期梯度会变得很小，导致模型参数基本不更新。</p><h1 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h1><p>为了解决AdaGrad随着时间推移，模型参数基本不更新的问题，RMSProp被提出。</p><p>它用<strong>指数加权移动平均（EMA）</strong>&#x6765;更新梯度的平方，公式如下，即 $$E[g^2]_t<br>$$中越往前的梯度f对于当前的 $$E[g^2]_t$$的影响就越小,从而使得学习率最后不会降为0，而是一直保持一定的平衡：</p><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image-10.png"></p><p>然后更新参数时使用：</p><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image-9.png"></p><ul><li><p>γ：衰减系数（典型值 0.9）</p></li><li><p>η：学习率</p></li><li><p>g_t：当前梯度</p></li><li><p>E[g^2]_t：当前参数历史梯度平方的加权平均</p></li><li><p>ϵ：小常数，防止除以 0</p></li></ul><p>类比理解一下：</p><ul><li><p><strong>AdaGrad</strong>：你每次都把经验“记下来”，越记越多，最后太谨慎，啥都不敢动了 🙈</p></li><li><p><strong>RMSProp</strong>：你只记“最近几次”的经验 → 有记忆，但也保持灵活，适应当前环境 🧠✨</p></li></ul><h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>总的来说，<strong>Adam &#x3D; Momentum + RMSProp + 偏差校正</strong>，它是一种带动量的、带自适应学习率的优化器。</p><p>它吸收了前人经验，<strong>既有方向记忆（动量）</strong>，又能<strong>差异化学习率（RMSProp思想）</strong>，而且还做了聪明的<strong>偏差修正</strong>，让前期更新不“失控”。</p><p>Adam 给每个参数记录了两个“动量”：</p><ul><li>&#x20;<strong>一阶矩（梯度的滑动平均）</strong>：</li></ul><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image-8.png"></p><p>像 Momentum：记录梯度方向的平均值</p><ul><li><strong>二阶矩（梯度平方的滑动平均）</strong>：</li></ul><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image-7.png"></p><p>像 RMSProp：记录梯度的幅度变化（用于调整每个参数学习率）</p><p>此外由于在训练初期， $$m_t$$和$$v_t$$会偏向于 0（因为一开始动量积累值太小）</p><p>Adam 加了修正项：</p><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image-4.png"></p><p>最终参数更新的公式如下：</p><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image-6.png"></p><p>可视化理解一下的话，Adam 就像一个聪明的登山者：</p><ul><li><p>手里有指南针（动量）🧭</p></li><li><p>脚底有自适应的避震鞋（学习率调整）👟</p></li><li><p>前期还会做“热身”（偏差校正）🔥<br>&#x20;→ 一路平稳往谷底走，不会乱跳也不会卡住</p></li></ul><p>和前面的优化器对比一下，得到如下的结果：</p><h1 id="AdamW"><a href="#AdamW" class="headerlink" title="AdamW"></a>AdamW</h1><h2 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h2><p>在介绍AdamW之前，我们需要先理解一下权重衰减的概念。</p><p>权重衰减指的是在训练过程中，让<strong>模型参数自动变小</strong>，从而<strong>防止过拟合，需要权重衰减主要</strong>有两个原因：</p><ol><li><p><strong>防止过拟合：</strong>&#x53C2;数太大可能在训练集上学得很好，但泛化能力差。衰减可以让它更“温和”地拟合数据。</p></li><li><p><strong>提升数值稳定性：</strong>&#x8FC7;大的参数可能导致不稳定，比如梯度爆炸或输出范围奇怪（尤其在 softmax&#x2F;logits 中）</p></li></ol><p>一般而言，在训练模型时，对于Loss我们有：</p><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image-5.png"></p><p>比如交叉熵损失、均方误差之类的。</p><p>为了进行权重衰减，我们可以在计算Loss时加入一个L2正则项：</p><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image-3.png"></p><ul><li><p>L(θ)是原本的损失（衡量模型对训练集的拟合）</p></li><li><p>||θ||^2 是参数向量的平方和（越大说明参数越激进）</p></li><li><p>λ是一个超参数，控制“惩罚力度”</p></li></ul><p>对于SGD，在加入了L2正则项后，损失函数再对θ求导可以得到：</p><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image.png"></p><p>再把 2λ 合并写成 λ，并将参数写全就可以得到：</p><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image-1.png"></p><p>再整理一下得到：</p><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image-2.png"></p><p>而在SGD加L2正则可以做到权重衰减，但是对于Adam却有了问题，因为Adam 中每个参数的更新方向会被“除以一个平方根的梯度平方平均”，也就是：</p><p><img src="/2025/04/12/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83-%E4%BC%98%E5%8C%96%E5%99%A8/image-15.png"></p><p>如果把 λ⋅θ加到这里就会导致梯度方向被缩放，权重衰减也被缩放！从而导致衰减效果不稳定，容易<strong>衰减不到位或者过头</strong>。</p><h2 id="AdamW核心修改"><a href="#AdamW核心修改" class="headerlink" title="AdamW核心修改"></a>AdamW核心修改</h2><p>为了解决Adam中权重衰减的问题，AdamW的做法是<strong>不把 λ⋅θ 当做“梯度的一部分”加进去，而是直接减掉，</strong></p><p>AdamW下，参数更新的公式变成了：</p><p><strong>θ &#x3D; θ - η * ∇L - η * λ * θ&#x20;</strong></p><p>即直接减掉，不去受到学习率更新的影响。</p>]]></content>
    
    
    <categories>
      
      <category>LLM</category>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes Informer介绍</title>
    <link href="/2025/04/09/K8s%20Informer%E4%BB%8B%E7%BB%8D/"/>
    <url>/2025/04/09/K8s%20Informer%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>之前面试多次被问到过k8s的informer中如果请求丢失了会发生什么，感觉这应该是Operator中的经典面试题。正好也整理一下这方面的内容。</p><p>这个问题类似于：</p><ul><li>在 Kubernetes 系统中，组件之间的通过 HTTP 协议进行通信，在不依赖任何中间件的情况下需要保证消息的实时性、可靠性、顺序性等。那么 Kubernetes 是如何做到的呢？答案就是 Informer 机制，尤其是其中定期发送的resync机制</li></ul><h1 id="框架概览"><a href="#框架概览" class="headerlink" title="框架概览"></a>框架概览</h1><p><img src="/2025/04/09/K8s%20Informer%E4%BB%8B%E7%BB%8D/image.png"></p><p>日常开发是只需要关注绿色的部分，即：</p><ol><li><p>调用<code>AddEventHandler</code>，添加相应的逻辑处理<code>AddFunc</code>、<code>DeleteFunc</code>、<code>UpdateFunc</code></p></li><li><p>实现 worker 逻辑从 workqueue 中消费 obj-key 即可。</p></li></ol><p>其余的部分都是由client-go完成，主要关注SharedIndexInformer。</p><h1 id="SharedIndexInformer"><a href="#SharedIndexInformer" class="headerlink" title="SharedIndexInformer"></a>SharedIndexInformer</h1><p>为了避免多个Listener去监控同一个资源的变化，从而加大APIServer的压力，主要的设计思路是使用单例模式，一个资源只实例化一个Informer，后续所有的 Listener 都共享这一个 Informer 实例即可。</p><p>从下面的源代码可以看出所有的 Informer 都通过同一个工厂<code>SharedInformerFactory</code>来生成：</p><ul><li><p>其内部存在一个 map，名为<code>informers</code>来存储所有当前已经实例化的所有 informer。</p></li><li><p>通过<code>InformerFor</code>这个方法来实现共享机制，也就是 Singleton 模式，具体见下述代码和注解。</p></li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> sharedInformerFactory <span class="hljs-keyword">struct</span> &#123;<br>        ...<span class="hljs-comment">// 工厂级别(所有 informer)默认的 resync 时间defaultResync    time.Duration// 每个 informer 具体的 resync 时间customResync     map[reflect.Type]time.Duration// informer 实例的 mapinformers map[reflect.Type]cache.SharedIndexInformer...</span><br>&#125;<br><br><span class="hljs-comment">// 共享机制 通过 InformerFor 来完成func (f *sharedInformerFactory) InformerFor(</span><br>        obj runtime.Object, <br>        newFunc internalinterfaces.NewInformerFunc,<br>) cache.SharedIndexInformer &#123;<br>        ...informerType := reflect.TypeOf(obj)<br>        <span class="hljs-comment">// 如果已经有 informer 实例 就直接返回该实例informer, exists := f.informers[informerType]</span><br>        <span class="hljs-keyword">if</span> exists &#123;<br>                <span class="hljs-keyword">return</span> informer<br>        &#125;<br>        <span class="hljs-comment">// 如果不存在该类型的 informer// 1. 设置 informer 的 resync 时间resyncPeriod, exists := f.customResync[informerType]</span><br>        <span class="hljs-keyword">if</span> !exists &#123;<br>                resyncPeriod = f.defaultResync<br>        &#125;<br>        <span class="hljs-comment">// 2. 实例化该 informerinformer = newFunc(f.client, resyncPeriod)</span><br>        <span class="hljs-comment">// 3. 在 map 中记录该 informerf.informers[informerType] = informerreturn informer</span><br>&#125;<br></code></pre></td></tr></table></figure><p>一个Informer中包含了以下几个组件：</p><ul><li><p><strong>Reflector：</strong>&#x4E00;个 informer 就有一个 Reflector。 Reflector 负责监控对应的资源，其中包含 ListerWatcher、store(DeltaFIFO)、lastSyncResourceVersion、resyncPeriod 等信息， 当资源发生变化时，会触发相应 obj 的变更事件，并将该 obj 的 delta 放入 DeltaFIFO 中。 DeltaFIFO会被消费然后用来更新本地的状态。</p></li><li><p><strong>Indexer：</strong>&#x5B83;是 Informer 机制中本地最全的数据存储，其会通过 DeltaFIFO 中最新的 Delta 不停的更新自身信息，并对外提供状态信息，例如支持通过get获取。</p></li><li><p><strong>Listerner：</strong>&#x49;nformer中包含了多个由<code>AddEventHandler</code>或<code>AddEventHandlerWithResyncPeriod</code>向 informer 注册新的 Listener。当 HandleDeltas 处理 DeltaFIFO 中的 Delta 时，会将这些更新事件派发给注册的 Listener。当然这里具体派发给哪些 Listener 有一定的规则，具体如下：</p><ul><li><p>派发给<code>listeners</code>：DeltaType 为<code>Added</code>、<code>Updated</code>、<code>Deleted</code>、新旧资源版本号不一致的<code>Replaced</code></p></li><li><p>派发给<code>syncingListeners</code>：DeltaType 为<code>Sync</code>、新旧资源版本号一致的<code>Replaced</code></p></li></ul></li></ul><h1 id="数据同步流"><a href="#数据同步流" class="headerlink" title="数据同步流"></a>数据同步流</h1><p>有四类数据存储需要同步：API Server、DeltaFIFO、Indexer、Listener。对于这四部分，可以简单理解：<strong>API Server 侧为最权威的数据、DeltaFIFO 为本地最新的数据、Indexer 为本地最全的数据、Listener 为用户侧做逻辑用的数据。</strong></p><h3 id="远端通路：远端-API-Server-⇔-本地-DeltaFIFO、Indexer、Listener"><a href="#远端通路：远端-API-Server-⇔-本地-DeltaFIFO、Indexer、Listener" class="headerlink" title="远端通路：远端(API Server) ⇔ 本地(DeltaFIFO、Indexer、Listener)"></a><strong>远端通路：远端(API Server) ⇔ 本地(DeltaFIFO、Indexer、Listener)</strong></h3><p>负责将APIServer侧的obj同步到本地，主要同步类型有两类：</p><ul><li><p>通过<code>List</code>行为产生的同步行为，这类 event 的 DeltaType 为<code>Replaced</code>，同时只有在 Reflector 初始启动时才会产生。</p></li><li><p>通过<code>Watch</code>行为产生的同步行为，对于 watch 到的<code>Added、Modified、Deleted</code>类型的 event，对应的 DeltaType 为<code>Added、Updated、Deleted</code>。</p></li></ul><h3 id="本地通路：本地-DeltaFIFO、Indexer、SyncingListener）之间同步"><a href="#本地通路：本地-DeltaFIFO、Indexer、SyncingListener）之间同步" class="headerlink" title="本地通路：本地(DeltaFIFO、Indexer、SyncingListener）之间同步"></a><strong>本地通路：本地(DeltaFIFO、Indexer、SyncingListener）之间同步</strong></h3><p>本地通路是通过 Reflector 的<code>ListAndWatch</code>方法中运行一个 goroutine 来执行定期的<code>Resync</code>操作。它会从 Indxer 拉一遍对应到所有 objs 的Delta 到 DeltaFIFO 中(list)，其中的 Delta 为<code>Sync</code>状态。之后 handleDeltas 就会同步 DeltaFIFO 中的 Sync Delta 给 syncingListeners 和 Indexer。</p><h1 id="关键源码解析"><a href="#关键源码解析" class="headerlink" title="关键源码解析"></a>关键源码解析</h1><h2 id="ListAndWatch-方法"><a href="#ListAndWatch-方法" class="headerlink" title="ListAndWatch 方法"></a><strong>ListAndWatch 方法</strong></h2><p>简化后的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *Reflector)</span></span> ListAndWatch(stopCh &lt;-<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;) <span class="hljs-type">error</span> &#123; <br>        <span class="hljs-comment">// list...// resync...// watch...</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *Reflector)</span></span> Run(stopCh &lt;-<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;) &#123;<br>        ...wait.BackoffUntil(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>                <span class="hljs-keyword">if</span> err := r.ListAndWatch(stopCh); err != <span class="hljs-literal">nil</span> &#123;<br>                        r.watchErrorHandler(r, err)<br>                &#125;<br>        &#125;, r.backoffManager, <span class="hljs-literal">true</span>, stopCh)<br>        ...<br>&#125;<br></code></pre></td></tr></table></figure><p>BackoffUntil包装了ListAndWatch，用来在ListAndWatch退出的时候重新启动它，只有stopChan发来停止消息的时候才会真正的停止。</p><p>其内部做了三件事：<strong>list-&gt;resync-&gt;watch</strong>，主要介绍如下。</p><h3 id="List"><a href="#List" class="headerlink" title="List"></a>List</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *Reflector)</span></span> ListAndWatch(stopCh &lt;-<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;) <span class="hljs-type">error</span> &#123; <br>        <span class="hljs-comment">// listif err := func() error &#123;</span><br>                ...<span class="hljs-comment">// 1. 开启 goroutine 执行 listgo func() &#123;</span><br>                        <span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>                                <span class="hljs-keyword">if</span> r := <span class="hljs-built_in">recover</span>(); r != <span class="hljs-literal">nil</span> &#123;<br>                                        <span class="hljs-comment">// list 失败，向 panicCh 发送信号panicCh &lt;- r</span><br>                                &#125;<br>                        &#125;()<br>                        <span class="hljs-comment">// 执行 list 操作，从 API Server 侧获取所有 obj 集合...// 成功完成 listclose(listCh)</span><br>                &#125;()<br>                <span class="hljs-comment">// 2. 等待执行 list 操作的 goroutine 结束，或者 stopCh、panicCh 终止select &#123;</span><br>                <span class="hljs-keyword">case</span> &lt;-stopCh:<br>                        <span class="hljs-keyword">return</span> nilcase r := &lt;-panicCh:<br>                        <span class="hljs-built_in">panic</span>(r)<br>                <span class="hljs-keyword">case</span> &lt;-listCh:<br>                &#125;<br>                ...<span class="hljs-comment">// 待研究 watch cache 是什么？if options.ResourceVersion == &quot;0&quot; &amp;&amp; paginatedResult &#123;</span><br>                        r.paginatedResult = <span class="hljs-literal">true</span><br>                &#125;<br><br>                r.setIsLastSyncResourceVersionUnavailable(<span class="hljs-literal">false</span>) <span class="hljs-comment">// list was successful...listMetaInterface, err := meta.ListAccessor(list)</span><br>                ...resourceVersion = listMetaInterface.GetResourceVersion()<br>                <span class="hljs-comment">// 从 list 中整理所有 obj 为一个数组items, err := meta.ExtractList(list)</span><br><br>                <span class="hljs-comment">// 3. 将 API Server 侧的最新 Obj 集合同步到 DeltaFIFO 中 最终调用 DeltaFIFO 的 Replace 方法if err := r.syncWith(items, resourceVersion); err != nil &#123;</span><br>                        <span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;unable to sync list result: %v&quot;</span>, err)<br>                &#125;<br>                r.setLastSyncResourceVersion(resourceVersion)<br>                ...<span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>        &#125;(); err != <span class="hljs-literal">nil</span> &#123;<br>                <span class="hljs-keyword">return</span> err<br>        &#125;<br>        <span class="hljs-comment">// resync...// watch...</span><br>&#125;<br></code></pre></td></tr></table></figure><p>list 操作在 ListAndWatch 中只会运行一次，简单来说，也可看作三个步骤：</p><ol><li><p>派发 goroutine 去 API Server 拉取最新的 Obj 集合</p></li><li><p>等待 goroutine 结束，<code>listCh</code>接收到信号，表示 list 完成。或者<code>stopCh</code>、<code>panicCh</code>发来信号。其中 stopCh 表示调用者需要停止，panicCh 表示 goroutine 的 list 过程出错了</p></li><li><p>整理 API Server 侧拉取到的最新 obj 集合，同时<code>syncWith</code>到 DeltaFIFO 中（最终调用 DeltaFIFO 的 Replace 方法）。</p></li></ol><h3 id="Resync"><a href="#Resync" class="headerlink" title="Resync"></a>Resync</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *Reflector)</span></span> ListAndWatch(stopCh &lt;-<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;) <span class="hljs-type">error</span> &#123; <br>        <span class="hljs-comment">// list</span><br>        ...<br>        <span class="hljs-comment">// resync</span><br>        <span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>                resyncCh, cleanup := r.resyncChan()<br>                <span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>                        cleanup() <span class="hljs-comment">// Call the last one written into cleanup</span><br>                &#125;()<br>                <span class="hljs-keyword">for</span> &#123;<br>                        <span class="hljs-keyword">select</span> &#123;<br>                        <span class="hljs-keyword">case</span> &lt;-resyncCh:<br>                        <span class="hljs-keyword">case</span> &lt;-stopCh:<br>                                <span class="hljs-keyword">return</span><br>                        <span class="hljs-keyword">case</span> &lt;-cancelCh:<br>                                <span class="hljs-keyword">return</span><br>                        &#125;<br>                        <span class="hljs-keyword">if</span> r.ShouldResync == <span class="hljs-literal">nil</span> || r.ShouldResync() &#123;<br>                                klog.V(<span class="hljs-number">4</span>).Infof(<span class="hljs-string">&quot;%s: forcing resync&quot;</span>, r.name)<br>                                <span class="hljs-keyword">if</span> err := r.store.Resync(); err != <span class="hljs-literal">nil</span> &#123;<br>                                        resyncerrc &lt;- err<br>                                        <span class="hljs-keyword">return</span><br>                                &#125;<br>                        &#125;<br>                        cleanup()<br>                        resyncCh, cleanup = r.resyncChan()<br>                &#125;<br>        &#125;()<br>        <span class="hljs-comment">// watch</span><br>        ...<br>&#125;<br></code></pre></td></tr></table></figure><p>这部分是通过派发 goroutine 来完成的，内部通过 for 死循环来定期执行<code>Resync</code>操作，<code>resyncChan()</code>会定期向<code>resyncCh</code>发来信号，定期的时间由 resyncPeriod 属性来设置。 整个过程直到<code>cancelCh</code>或者<code>stopCh</code>发来停止信号，其中 cancelCh 表示本次 ListAndWatch 结束了，stopCh 表示上层(调用者)发来停止信号。 在每次的<code>Resync</code>操作操作中：</p><ol><li>首先调用<code>ShouldResync</code>函数，其具体实现在 sharedProcessor 中，其会根据每一个 Listener 的同步时间来选出当前期待&#x2F;需要进行 Resync 的 Listener 放入<code>syncingListeners</code>中。</li></ol><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *sharedProcessor)</span></span> shouldResync() <span class="hljs-type">bool</span> &#123;<br>        p.listenersLock.Lock()<br>        <span class="hljs-keyword">defer</span> p.listenersLock.Unlock()<br><br>        p.syncingListeners = []*processorListener&#123;&#125;<br><br>        resyncNeeded := <span class="hljs-literal">false</span><br>        now := p.clock.Now()<br>        <span class="hljs-comment">// 遍历所有的 Listener，将同步时间已经到了的</span><br>        <span class="hljs-comment">// Listener 加入 syncingListeners</span><br>        <span class="hljs-keyword">for</span> _, listener := <span class="hljs-keyword">range</span> p.listeners &#123;<br>                <span class="hljs-keyword">if</span> listener.shouldResync(now) &#123;<br>                        resyncNeeded = <span class="hljs-literal">true</span><br>                        p.syncingListeners = <span class="hljs-built_in">append</span>(p.syncingListeners, listener)<br>                        listener.determineNextResync(now)<br>                &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> resyncNeeded<br>&#125;<br></code></pre></td></tr></table></figure><ul><li>调用 store.Resync()，具体由 DeltaFIFO 中的 Resync 来实现，想要完成将 Indexer 中的 obj 全部刷到 DeltaFIFO 中（list）。 需要注意，在这个过程中，如果 DeltaFIFO 的 items 中已经存在该 obj，就不需要放了。因为我们的目的就是同步本地之间的 obj 信息， 既然在 items 中已经存在了该信息，并且该信息一定是本地最新的，未来也会被处理同步到本地所有存储中，因此这里就不需要再添加了。 具体处理细节看下面代码注解。</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *DeltaFIFO)</span></span> Resync() <span class="hljs-type">error</span> &#123;<br>        f.lock.Lock()<br>        <span class="hljs-keyword">defer</span> f.lock.Unlock()<br>        <span class="hljs-comment">// knownObjects 可以理解为 Indexer</span><br>        <span class="hljs-keyword">if</span> f.knownObjects == <span class="hljs-literal">nil</span> &#123;<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>        &#125;<br>        <span class="hljs-comment">// 将 Indexer 中所有的 obj 刷到 DeltaFIFO 中</span><br>        keys := f.knownObjects.ListKeys()<br>        <span class="hljs-keyword">for</span> _, k := <span class="hljs-keyword">range</span> keys &#123;<br>                <span class="hljs-keyword">if</span> err := f.syncKeyLocked(k); err != <span class="hljs-literal">nil</span> &#123;<br>                        <span class="hljs-keyword">return</span> err<br>                &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *DeltaFIFO)</span></span> syncKeyLocked(key <span class="hljs-type">string</span>) <span class="hljs-type">error</span> &#123;<br>        <span class="hljs-comment">// 通过 key 在 Indexer 中获得 obj</span><br>        obj, exists, err := f.knownObjects.GetByKey(key)<br>        ...<br>        <span class="hljs-comment">// 计算 DeltaFIFO 中 Obj 的 key</span><br>        id, err := f.KeyOf(obj)<br>        ...<br>        <span class="hljs-comment">// 如果在 items 中已经存在该 obj，就不需要再添加了</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(f.items[id]) &gt; <span class="hljs-number">0</span> &#123;<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>        &#125;<br>        <span class="hljs-comment">// 如果在 items 中没有该 obj，就添加 Sync 类型的 Deltas</span><br>        <span class="hljs-keyword">if</span> err := f.queueActionLocked(Sync, obj); err != <span class="hljs-literal">nil</span> &#123;<br>                <span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;couldn&#x27;t queue object: %v&quot;</span>, err)<br>        &#125;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p><strong>注意Resync也就是保证丢失的信息可以再次被处理的关键。</strong></p><p>如果是从 Resync 重新同步到 Delta FIFO 队列的事件，会分发到 updateNotification 中触发 onUpdate 的回调</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// k8s.io/client-go/tools/cache/shared_informer.gofunc (s *sharedIndexInformer) HandleDeltas(obj interface&#123;&#125;) error &#123;</span><br>        s.blockDeltas.Lock()<br>        <span class="hljs-keyword">defer</span> s.blockDeltas.Unlock()<br><br>        <span class="hljs-comment">// from oldest to newestfor _, d := range obj.(Deltas) &#123;</span><br>                <span class="hljs-comment">// 判断事件类型，看事件是通过新增、更新、替换、删除还是 Resync 重新同步产生的switch d.Type &#123;</span><br>                <span class="hljs-keyword">case</span> Sync, Replaced, Added, Updated:<br>                        s.cacheMutationDetector.AddObject(d.Object)<br>                        <span class="hljs-keyword">if</span> old, exists, err := s.indexer.Get(d.Object); err == <span class="hljs-literal">nil</span> &amp;&amp; exists &#123;<br>                                <span class="hljs-keyword">if</span> err := s.indexer.Update(d.Object); err != <span class="hljs-literal">nil</span> &#123;<br>                                        <span class="hljs-keyword">return</span> err<br>                                &#125;<br>                                <br>                                isSync := falseswitch &#123;<br>                                <span class="hljs-keyword">case</span> d.Type == Sync:<br>                                        <span class="hljs-comment">// 如果是通过 Resync 重新同步得到的事件则做个标记isSync = truecase d.Type == Replaced:</span><br>                                        ...<br>                                &#125;<br>                                <span class="hljs-comment">// 如果是通过 Resync 重新同步得到的事件，则触发 onUpdate 回调s.processor.distribute(updateNotification&#123;oldObj: old, newObj: d.Object&#125;, isSync)</span><br>                        &#125; <span class="hljs-keyword">else</span> &#123;<br>                                <span class="hljs-keyword">if</span> err := s.indexer.Add(d.Object); err != <span class="hljs-literal">nil</span> &#123;<br>                                        <span class="hljs-keyword">return</span> err<br>                                &#125;<br>                                s.processor.distribute(addNotification&#123;newObj: d.Object&#125;, <span class="hljs-literal">false</span>)<br>                        &#125;<br>                <span class="hljs-keyword">case</span> Deleted:<br>                        <span class="hljs-keyword">if</span> err := s.indexer.Delete(d.Object); err != <span class="hljs-literal">nil</span> &#123;<br>                                <span class="hljs-keyword">return</span> err<br>                        &#125;<br>                        s.processor.distribute(deleteNotification&#123;oldObj: d.Object&#125;, <span class="hljs-literal">false</span>)<br>                &#125;<br>        &#125;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="Watch"><a href="#Watch" class="headerlink" title="Watch"></a>Watch</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(r *Reflector)</span></span> ListAndWatch(stopCh &lt;-<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;) <span class="hljs-type">error</span> &#123; <br>        <span class="hljs-comment">// list</span><br>        ...<br>        <span class="hljs-comment">// resync</span><br>        ...<br>        <span class="hljs-comment">// watch</span><br>                <span class="hljs-keyword">for</span> &#123;<br>            ...<br>                w, err := r.listerWatcher.Watch(options)<br>                ...<br>                <span class="hljs-comment">// 开始 watch</span><br>                <span class="hljs-keyword">if</span> err := r.watchHandler(start, w, &amp;resourceVersion, resyncerrc, stopCh); err != <span class="hljs-literal">nil</span> &#123;<br>                        <span class="hljs-comment">// 如果不是 stopCh 发来的主动停止，就记录日志</span><br>                        <span class="hljs-keyword">if</span> err != errorStopRequested &#123;<br>                                ...<br>                        &#125;<br>                        <span class="hljs-comment">// 注意这里返回的为 nil，结合 BackoffUntil 函数看</span><br>                        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>                &#125;<br>        &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>整个 watch 包在一个 for 死循环中，具体的 watch 行为通过<code>watchHandler</code>函数来实现，其内部循环监听 watch 对象（由 listerWatcher.Watch 产生）的<code>ResultChan</code>。 如果发来 evenet，并且没有出错，就按照四种类型进行处理： 分别为<strong>Added、Modified、Deleted、Bookmark</strong>，表示有 Obj 被：添加、修改、删除，以及版本更新。之后对于前三种类型， 分别调用 store(DeltaFIFO)的<code>Add、Update、Delete</code>方法， 向 DeltaFIFO 中添加 DeltaType 为<code>Added、Updated、Deleted</code>的 Delta。后续通过 Pop 函数中的 HandleDeltas 消费这些 Deltas。</p><h2 id="HandleDeltas-方法"><a href="#HandleDeltas-方法" class="headerlink" title="HandleDeltas 方法"></a><strong>HandleDeltas 方法</strong></h2><p>该方法的功能就是循环处理 item(Deltas)中的 Delta，对于每一个 Delta：按照操作类型分类，<code>Deleted</code>为一类，剩余操作<code>Sync, Replaced, Added, Updated</code>归为另一类：</p><ol><li><p>对于<code>Deleted</code>：首先调用 indexer 的<strong>Delete</strong>方法，在本地存储中删除该 Obj。之后调用 distribute 方法，对所有的 Listener 进行<strong>deleteNotification</strong>通知删除 Obj 消息；</p></li><li><p>对于<code>Sync, Replaced, Added, Updated</code>：首先查看在 indexer 中是否能够 get 到该 Obj：</p><ul><li><p>如果可以 get：调用 indexer 的<strong>Update</strong>方法，更新本地存储的 Obj，之后调用 distribute 方法，对所有的 Listener 进行<strong>updateNotification</strong>通知更新 Obj 消息；（<strong>注意</strong>：这部分的 distribute 针对 Sync 和部分 Replaced(见下述说明)只需要通知<code>syncingListeners</code>，而不是所有的 listeners。通过 distribute 方法最后的 bool 参数来设定，大部分情况设定为 false，说明通知所有的 listeners）</p></li><li><p>如果 get 不到：调用 indexer 的<strong>Add</strong>方法，在本地存储添加该 Obj，之后调用 distribute 方法，对所有的 Listener 进行<strong>addNotification</strong>通知添加 Obj 消息；</p></li></ul></li></ol><h2 id="AddEventHandler"><a href="#AddEventHandler" class="headerlink" title="AddEventHandler"></a>AddEventHandler</h2><p>下面这部分代码负责将各资源的informer的eventHandler注册进来，有几个注意点点：</p><ul><li><p>如果s.defaultEventHandlerResyncPeriod不大于0，那么就不会尝试更新resyncPeriod</p></li><li><p>如果s没有启动，那么就不会只是单纯的注册进去，否则还会遍历所有的indexer，为每个给listener触发一个addNotification</p></li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(s *sharedIndexInformer)</span></span> AddEventHandler(handler ResourceEventHandler) &#123;<br>    s.AddEventHandlerWithResyncPeriod(handler, s.defaultEventHandlerResyncPeriod)<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(s *sharedIndexInformer)</span></span> AddEventHandlerWithResyncPeriod(handler ResourceEventHandler, resyncPeriod time.Duration) &#123;<br>    s.startedLock.Lock()<br>    <span class="hljs-keyword">defer</span> s.startedLock.Unlock()<br><br>    <span class="hljs-keyword">if</span> s.stopped &#123;<br>       klog.V(<span class="hljs-number">2</span>).Infof(<span class="hljs-string">&quot;Handler %v was not added to shared informer because it has stopped already&quot;</span>, handler)<br>       <span class="hljs-keyword">return</span><br>    &#125;<br><br>    <span class="hljs-keyword">if</span> resyncPeriod &gt; <span class="hljs-number">0</span> &#123;<br>       <span class="hljs-keyword">if</span> resyncPeriod &lt; minimumResyncPeriod &#123;<br>          klog.Warningf(<span class="hljs-string">&quot;resyncPeriod %d is too small. Changing it to the minimum allowed value of %d&quot;</span>, resyncPeriod, minimumResyncPeriod)<br>          resyncPeriod = minimumResyncPeriod<br>       &#125;<br><br>       <span class="hljs-keyword">if</span> resyncPeriod &lt; s.resyncCheckPeriod &#123;<br>          <span class="hljs-keyword">if</span> s.started &#123;<br>             klog.Warningf(<span class="hljs-string">&quot;resyncPeriod %d is smaller than resyncCheckPeriod %d and the informer has already started. Changing it to %d&quot;</span>, resyncPeriod, s.resyncCheckPeriod, s.resyncCheckPeriod)<br>             resyncPeriod = s.resyncCheckPeriod<br>          &#125; <span class="hljs-keyword">else</span> &#123;<br>             <span class="hljs-comment">// if the event handler&#x27;s resyncPeriod is smaller than the current resyncCheckPeriod, update</span><br>             <span class="hljs-comment">// resyncCheckPeriod to match resyncPeriod and adjust the resync periods of all the listeners</span><br>             <span class="hljs-comment">// accordingly</span><br>             s.resyncCheckPeriod = resyncPeriod<br>             s.processor.resyncCheckPeriodChanged(resyncPeriod)<br>          &#125;<br>       &#125;<br>    &#125;<br><br>    listener := newProcessListener(handler, resyncPeriod, determineResyncPeriod(resyncPeriod, s.resyncCheckPeriod), s.clock.Now(), initialBufferSize)<br><br>    <span class="hljs-keyword">if</span> !s.started &#123;<br>       s.processor.addListener(listener)<br>       <span class="hljs-keyword">return</span><br>    &#125;<br><br>    <span class="hljs-comment">// in order to safely join, we have to</span><br>    <span class="hljs-comment">// 1. stop sending add/update/delete notifications</span><br>    <span class="hljs-comment">// 2. do a list against the store</span><br>    <span class="hljs-comment">// 3. send synthetic &quot;Add&quot; events to the new handler</span><br>    <span class="hljs-comment">// 4. unblock</span><br>    s.blockDeltas.Lock()<br>    <span class="hljs-keyword">defer</span> s.blockDeltas.Unlock()<br><br>    s.processor.addListener(listener)<br>    <span class="hljs-keyword">for</span> _, item := <span class="hljs-keyword">range</span> s.indexer.List() &#123;<br>       listener.add(addNotification&#123;newObj: item&#125;)<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><p><a href="https://blog.imoe.tech/2023/02/15/kubernetes-informer-mechanism/#%E6%B6%88%E8%B4%B9%E8%80%85%E9%80%BB%E8%BE%91">https://blog.imoe.tech/2023/02/15/kubernetes-informer-mechanism/#%E6%B6%88%E8%B4%B9%E8%80%85%E9%80%BB%E8%BE%91</a></p></li><li><p><a href="https://cloudnativecn.com/blog/client-go-informer-source-code/">https://cloudnativecn.com/blog/client-go-informer-source-code/</a></p></li><li><p><a href="https://github.com/k8s-club/k8s-club/blob/main/articles/Informer%E6%9C%BA%E5%88%B6%20-%20%E6%A6%82%E8%BF%B0.md">https://github.com/k8s-club/k8s-club/blob/main/articles/Informer%E6%9C%BA%E5%88%B6%20-%20%E6%A6%82%E8%BF%B0.md</a></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>k8s</category>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubeflow Trainer梳理</title>
    <link href="/2025/04/01/Kubeflow%20Trainer%E6%A2%B3%E7%90%86/"/>
    <url>/2025/04/01/Kubeflow%20Trainer%E6%A2%B3%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>仓库链接：<a href="https://github.com/kubeflow/trainer">https://github.com/kubeflow/trainer</a></p><h1 id="主要的结构体介绍"><a href="#主要的结构体介绍" class="headerlink" title="主要的结构体介绍"></a>主要的结构体介绍</h1><h2 id="TrainJob"><a href="#TrainJob" class="headerlink" title="TrainJob"></a>TrainJob</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// TrainJob represents configuration of a training job.</span><br><span class="hljs-keyword">type</span> TrainJob <span class="hljs-keyword">struct</span> &#123;<br>    metav1.TypeMeta <span class="hljs-string">`json:&quot;,inline&quot;`</span><br><br>    <span class="hljs-comment">// Standard object&#x27;s metadata.</span><br>    metav1.ObjectMeta <span class="hljs-string">`json:&quot;metadata,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// Specification of the desired TrainJob.</span><br>    Spec TrainJobSpec <span class="hljs-string">`json:&quot;spec,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// Current status of TrainJob.</span><br>    Status TrainJobStatus <span class="hljs-string">`json:&quot;status,omitempty&quot;`</span><br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// TrainJobSpec represents specification of the desired TrainJob.</span><br><span class="hljs-keyword">type</span> TrainJobSpec <span class="hljs-keyword">struct</span> &#123;<br>    <span class="hljs-comment">// Reference to the training runtime.</span><br>    <span class="hljs-comment">// The field is immutable.</span><br>    <span class="hljs-comment">// +kubebuilder:validation:XValidation:rule=&quot;self == oldSelf&quot;, message=&quot;runtimeRef is immutable&quot;</span><br>    RuntimeRef RuntimeRef <span class="hljs-string">`json:&quot;runtimeRef&quot;`</span><br><br>    <span class="hljs-comment">// Configuration of the desired trainer.</span><br>    Trainer *Trainer <span class="hljs-string">`json:&quot;trainer,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// Configuration of the training dataset.</span><br>    DatasetConfig *DatasetConfig <span class="hljs-string">`json:&quot;datasetConfig,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// Configuration of the pre-trained and trained model.</span><br>    ModelConfig *ModelConfig <span class="hljs-string">`json:&quot;modelConfig,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// Labels to apply for the derivative JobSet and Jobs.</span><br>    <span class="hljs-comment">// They will be merged with the TrainingRuntime values.</span><br>    Labels <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-type">string</span> <span class="hljs-string">`json:&quot;labels,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// Annotations to apply for the derivative JobSet and Jobs.</span><br>    <span class="hljs-comment">// They will be merged with the TrainingRuntime values.</span><br>    Annotations <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-type">string</span> <span class="hljs-string">`json:&quot;annotations,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// Custom overrides for the training runtime.</span><br>    <span class="hljs-comment">// +listType=atomic</span><br>    PodSpecOverrides []PodSpecOverride <span class="hljs-string">`json:&quot;podSpecOverrides,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// Whether the controller should suspend the running TrainJob.</span><br>    <span class="hljs-comment">// Defaults to false.</span><br>    <span class="hljs-comment">// +kubebuilder:default=false</span><br>    Suspend *<span class="hljs-type">bool</span> <span class="hljs-string">`json:&quot;suspend,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// ManagedBy is used to indicate the controller or entity that manages a TrainJob.</span><br>    <span class="hljs-comment">// The value must be either an empty, `trainer.kubeflow.org/trainjob-controller` or</span><br>    <span class="hljs-comment">// `kueue.x-k8s.io/multikueue`. The built-in TrainJob controller reconciles TrainJob which</span><br>    <span class="hljs-comment">// don&#x27;t have this field at all or the field value is the reserved string</span><br>    <span class="hljs-comment">// `trainer.kubeflow.org/trainjob-controller`, but delegates reconciling TrainJobs</span><br>    <span class="hljs-comment">// with a &#x27;kueue.x-k8s.io/multikueue&#x27; to the Kueue. The field is immutable.</span><br>    <span class="hljs-comment">// Defaults to `trainer.kubeflow.org/trainjob-controller`</span><br>    <span class="hljs-comment">// +kubebuilder:default=&quot;trainer.kubeflow.org/trainjob-controller&quot;</span><br>    <span class="hljs-comment">// +kubebuilder:validation:XValidation:rule=&quot;self in [&#x27;trainer.kubeflow.org/trainjob-controller&#x27;, &#x27;kueue.x-k8s.io/multikueue&#x27;]&quot;, message=&quot;ManagedBy must be trainer.kubeflow.org/trainjob-controller or kueue.x-k8s.io/multikueue if set&quot;</span><br>    <span class="hljs-comment">// +kubebuilder:validation:XValidation:rule=&quot;self == oldSelf&quot;, message=&quot;ManagedBy value is immutable&quot;</span><br>    ManagedBy *<span class="hljs-type">string</span> <span class="hljs-string">`json:&quot;managedBy,omitempty&quot;`</span><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="TrainingRuntimeList"><a href="#TrainingRuntimeList" class="headerlink" title="TrainingRuntimeList"></a>TrainingRuntimeList</h2><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// TrainingRuntimeList is a collection of training runtimes.</span><br><span class="hljs-keyword">type</span> TrainingRuntimeList <span class="hljs-keyword">struct</span> &#123;<br>    metav1.TypeMeta <span class="hljs-string">`json:&quot;,inline&quot;`</span><br><br>    <span class="hljs-comment">// Standard list metadata.</span><br>    metav1.ListMeta <span class="hljs-string">`json:&quot;metadata,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// List of TrainingRuntimes.</span><br>    Items []TrainingRuntime <span class="hljs-string">`json:&quot;items&quot;`</span><br>&#125;<br><br><span class="hljs-comment">// TrainingRuntime represents a training runtime which can be referenced as part of</span><br><span class="hljs-comment">// `runtimeRef` API in TrainJob. This resource is a namespaced-scoped and can be referenced</span><br><span class="hljs-comment">// by TrainJob that created in the *same* namespace as the TrainingRuntime.</span><br><span class="hljs-keyword">type</span> TrainingRuntime <span class="hljs-keyword">struct</span> &#123;<br>    metav1.TypeMeta <span class="hljs-string">`json:&quot;,inline&quot;`</span><br><br>    <span class="hljs-comment">// Standard object&#x27;s metadata.</span><br>    metav1.ObjectMeta <span class="hljs-string">`json:&quot;metadata,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// Specification of the desired TrainingRuntime.</span><br>    Spec TrainingRuntimeSpec <span class="hljs-string">`json:&quot;spec,omitempty&quot;`</span><br>&#125;<br><br><span class="hljs-comment">// TrainingRuntimeSpec represents a specification of the desired training runtime.</span><br><span class="hljs-keyword">type</span> TrainingRuntimeSpec <span class="hljs-keyword">struct</span> &#123;<br>    <span class="hljs-comment">// Configuration for the model training with ML-specific parameters.</span><br>    MLPolicy *MLPolicy <span class="hljs-string">`json:&quot;mlPolicy,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// Configuration for the PodGroup to enable gang-scheduling via supported plugins.</span><br>    PodGroupPolicy *PodGroupPolicy <span class="hljs-string">`json:&quot;podGroupPolicy,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// JobSet template which will be used by TrainJob.</span><br>    Template JobSetTemplateSpec <span class="hljs-string">`json:&quot;template&quot;`</span><br>&#125;<br><br><span class="hljs-comment">// MLPolicy represents configuration for the model trining with ML-specific parameters.</span><br><span class="hljs-comment">// +kubebuilder:validation:XValidation:rule=&quot;!(has(self.numNodes) &amp;&amp; (has(self.torch) &amp;&amp; has(self.torch.elasticPolicy)))&quot;, message=&quot;numNodes should not be set if torch.elasticPolicy is configured&quot;</span><br><span class="hljs-comment">// +kubebuilder:validation:XValidation:rule=&quot;!(has(self.torch) &amp;&amp; has(self.mpi))&quot;, message=&quot;Only one of the policy can be configured&quot;</span><br><span class="hljs-keyword">type</span> MLPolicy <span class="hljs-keyword">struct</span> &#123;<br>    <span class="hljs-comment">// Number of training nodes.</span><br>    <span class="hljs-comment">// Defaults to 1.</span><br>    NumNodes *<span class="hljs-type">int32</span> <span class="hljs-string">`json:&quot;numNodes,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// Configuration for the runtime-specific parameters, such as Torch or MPI.</span><br>    <span class="hljs-comment">// Only one of its members may be specified.</span><br>    MLPolicySource <span class="hljs-string">`json:&quot;,inline&quot;`</span><br>&#125;<br><br><span class="hljs-comment">// MLPolicySource represents the runtime-specific configuration for various technologies.</span><br><span class="hljs-comment">// One of the following specs can be set.</span><br><span class="hljs-keyword">type</span> MLPolicySource <span class="hljs-keyword">struct</span> &#123;<br>    <span class="hljs-comment">// Configuration for the PyTorch runtime.</span><br>    Torch *TorchMLPolicySource <span class="hljs-string">`json:&quot;torch,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// Configuration for the MPI Runtime.</span><br>    MPI *MPIMLPolicySource <span class="hljs-string">`json:&quot;mpi,omitempty&quot;`</span><br>&#125;<br><br><span class="hljs-comment">// PodGroupPolicy represents a PodGroup configuration for gang-scheduling.</span><br><span class="hljs-keyword">type</span> PodGroupPolicy <span class="hljs-keyword">struct</span> &#123;<br>    <span class="hljs-comment">// Configuration for gang-scheduling using various plugins.</span><br>    PodGroupPolicySource <span class="hljs-string">`json:&quot;,inline&quot;`</span><br>&#125;<br><br><span class="hljs-comment">// PodGroupPolicySource represents supported plugins for gang-scheduling.</span><br><span class="hljs-comment">// Only one of its members may be specified.</span><br><span class="hljs-keyword">type</span> PodGroupPolicySource <span class="hljs-keyword">struct</span> &#123;<br>    <span class="hljs-comment">// Coscheduling plugin from the Kubernetes scheduler-plugins for gang-scheduling.</span><br>    Coscheduling *CoschedulingPodGroupPolicySource <span class="hljs-string">`json:&quot;coscheduling,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// TODO (andreyvelich): Add support for Volcano gang-scheduler.</span><br>&#125;<br><br><span class="hljs-comment">// JobSetTemplateSpec represents a template of the desired JobSet.</span><br><span class="hljs-keyword">type</span> JobSetTemplateSpec <span class="hljs-keyword">struct</span> &#123;<br>    <span class="hljs-comment">// Metadata for custom JobSet&#x27;s labels and annotations.</span><br>    <span class="hljs-comment">// JobSet name and namespace is equal to the TrainJob&#x27;s name and namespace.</span><br>    metav1.ObjectMeta <span class="hljs-string">`json:&quot;metadata,omitempty&quot;`</span><br><br>    <span class="hljs-comment">// Specification of the desired JobSet which will be created from TrainJob.</span><br>    Spec jobsetv1alpha2.JobSetSpec <span class="hljs-string">`json:&quot;spec,omitempty&quot;`</span><br>&#125;<br><br></code></pre></td></tr></table></figure><h1 id="设计巧思"><a href="#设计巧思" class="headerlink" title="设计巧思"></a>设计巧思</h1><h2 id="TrainJob-TrainJobStatus"><a href="#TrainJob-TrainJobStatus" class="headerlink" title="TrainJob.TrainJobStatus"></a>TrainJob.TrainJobStatus</h2><p>结构：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// TrainJobStatus represents the current status of TrainJob.</span><br>type TrainJobStatus <span class="hljs-keyword">struct</span> &#123;<br>    <span class="hljs-comment">// Conditions for the TrainJob.</span><br>    <span class="hljs-comment">//</span><br>    <span class="hljs-comment">// +optional</span><br>    <span class="hljs-comment">// +listType=map</span><br>    <span class="hljs-comment">// +listMapKey=type</span><br>    <span class="hljs-comment">// +patchStrategy=merge</span><br>    <span class="hljs-comment">// +patchMergeKey=type</span><br>    Conditions []metav1.Condition `json:<span class="hljs-string">&quot;conditions,omitempty&quot;</span> patchStrategy:<span class="hljs-string">&quot;merge&quot;</span> patchMergeKey:<span class="hljs-string">&quot;type&quot;</span>`<br><br>    <span class="hljs-comment">// JobsStatus tracks the child Jobs in TrainJob.</span><br>    <span class="hljs-comment">// +listType=map</span><br>    <span class="hljs-comment">// +listMapKey=name</span><br>    JobsStatus []JobStatus `json:<span class="hljs-string">&quot;jobsStatus,omitempty&quot;</span>`<br>&#125;<br><span class="hljs-comment">// Condition contains details for one aspect of the current state of this API Resource.</span><br><span class="hljs-comment">// ---</span><br><span class="hljs-comment">// This struct is intended for direct use as an array at the field path .status.conditions.  For example,</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">//  type FooStatus struct&#123;</span><br><span class="hljs-comment">//      // Represents the observations of a foo&#x27;s current state.</span><br><span class="hljs-comment">//      // Known .status.conditions.type are: &quot;Available&quot;, &quot;Progressing&quot;, and &quot;Degraded&quot;</span><br><span class="hljs-comment">//      // +patchMergeKey=type</span><br><span class="hljs-comment">//      // +patchStrategy=merge</span><br><span class="hljs-comment">//      // +listType=map</span><br><span class="hljs-comment">//      // +listMapKey=type</span><br><span class="hljs-comment">//      Conditions []metav1.Condition `json:&quot;conditions,omitempty&quot; patchStrategy:&quot;merge&quot; patchMergeKey:&quot;type&quot; protobuf:&quot;bytes,1,rep,name=conditions&quot;`</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">//      // other fields</span><br><span class="hljs-comment">//  &#125;</span><br>type Condition <span class="hljs-keyword">struct</span> &#123;<br>    <span class="hljs-comment">// type of condition in CamelCase or in foo.example.com/CamelCase.</span><br>    <span class="hljs-comment">// ---</span><br>    <span class="hljs-comment">// Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be</span><br>    <span class="hljs-comment">// useful (see .node.status.conditions), the ability to deconflict is important.</span><br>    <span class="hljs-comment">// The regex it matches is (dns1123SubdomainFmt/)?(qualifiedNameFmt)</span><br>    <span class="hljs-comment">// +required</span><br>    <span class="hljs-comment">// +kubebuilder:validation:Required</span><br>    <span class="hljs-comment">// +kubebuilder:validation:Pattern=`^([a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*/)?(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])$`</span><br>    <span class="hljs-comment">// +kubebuilder:validation:MaxLength=316</span><br>    Type string `json:<span class="hljs-string">&quot;type&quot;</span> protobuf:<span class="hljs-string">&quot;bytes,1,opt,name=type&quot;</span>`<br>    <span class="hljs-comment">// status of the condition, one of True, False, Unknown.</span><br>    <span class="hljs-comment">// +required</span><br>    <span class="hljs-comment">// +kubebuilder:validation:Required</span><br>    <span class="hljs-comment">// +kubebuilder:validation:Enum=True;False;Unknown</span><br>    Status ConditionStatus `json:<span class="hljs-string">&quot;status&quot;</span> protobuf:<span class="hljs-string">&quot;bytes,2,opt,name=status&quot;</span>`<br>    <span class="hljs-comment">// observedGeneration represents the .metadata.generation that the condition was set based upon.</span><br>    <span class="hljs-comment">// For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date</span><br>    <span class="hljs-comment">// with respect to the current state of the instance.</span><br>    <span class="hljs-comment">// +optional</span><br>    <span class="hljs-comment">// +kubebuilder:validation:Minimum=0</span><br>    ObservedGeneration int64 `json:<span class="hljs-string">&quot;observedGeneration,omitempty&quot;</span> protobuf:<span class="hljs-string">&quot;varint,3,opt,name=observedGeneration&quot;</span>`<br>    <span class="hljs-comment">// lastTransitionTime is the last time the condition transitioned from one status to another.</span><br>    <span class="hljs-comment">// This should be when the underlying condition changed.  If that is not known, then using the time when the API field changed is acceptable.</span><br>    <span class="hljs-comment">// +required</span><br>    <span class="hljs-comment">// +kubebuilder:validation:Required</span><br>    <span class="hljs-comment">// +kubebuilder:validation:Type=string</span><br>    <span class="hljs-comment">// +kubebuilder:validation:Format=date-time</span><br>    LastTransitionTime Time `json:<span class="hljs-string">&quot;lastTransitionTime&quot;</span> protobuf:<span class="hljs-string">&quot;bytes,4,opt,name=lastTransitionTime&quot;</span>`<br>    <span class="hljs-comment">// reason contains a programmatic identifier indicating the reason for the condition&#x27;s last transition.</span><br>    <span class="hljs-comment">// Producers of specific condition types may define expected values and meanings for this field,</span><br>    <span class="hljs-comment">// and whether the values are considered a guaranteed API.</span><br>    <span class="hljs-comment">// The value should be a CamelCase string.</span><br>    <span class="hljs-comment">// This field may not be empty.</span><br>    <span class="hljs-comment">// +required</span><br>    <span class="hljs-comment">// +kubebuilder:validation:Required</span><br>    <span class="hljs-comment">// +kubebuilder:validation:MaxLength=1024</span><br>    <span class="hljs-comment">// +kubebuilder:validation:MinLength=1</span><br>    <span class="hljs-comment">// +kubebuilder:validation:Pattern=`^[A-Za-z]([A-Za-z0-9_,:]*[A-Za-z0-9_])?$`</span><br>    Reason string `json:<span class="hljs-string">&quot;reason&quot;</span> protobuf:<span class="hljs-string">&quot;bytes,5,opt,name=reason&quot;</span>`<br>    <span class="hljs-comment">// message is a human readable message indicating details about the transition.</span><br>    <span class="hljs-comment">// This may be an empty string.</span><br>    <span class="hljs-comment">// +required</span><br>    <span class="hljs-comment">// +kubebuilder:validation:Required</span><br>    <span class="hljs-comment">// +kubebuilder:validation:MaxLength=32768</span><br>    Message string `json:<span class="hljs-string">&quot;message&quot;</span> protobuf:<span class="hljs-string">&quot;bytes,6,opt,name=message&quot;</span>`<br>&#125;<br><br><br>type JobStatus <span class="hljs-keyword">struct</span> &#123;<br>    <span class="hljs-comment">// Name of the child Job.</span><br>    Name string `json:<span class="hljs-string">&quot;name&quot;</span>`<br><br>    <span class="hljs-comment">// Ready is the number of child Jobs where the number of ready pods and completed pods</span><br>    <span class="hljs-comment">// is greater than or equal to the total expected pod count for the child Job.</span><br>    Ready int32 `json:<span class="hljs-string">&quot;ready&quot;</span>`<br><br>    <span class="hljs-comment">// Succeeded is the number of successfully completed child Jobs.</span><br>    Succeeded int32 `json:<span class="hljs-string">&quot;succeeded&quot;</span>`<br><br>    <span class="hljs-comment">// Failed is the number of failed child Jobs.</span><br>    Failed int32 `json:<span class="hljs-string">&quot;failed&quot;</span>`<br><br>    <span class="hljs-comment">// Active is the number of child Jobs with at least 1 pod in a running or pending state</span><br>    <span class="hljs-comment">// which are not marked for deletion.</span><br>    Active int32 `json:<span class="hljs-string">&quot;active&quot;</span>`<br><br>    <span class="hljs-comment">// Suspended is the number of child Jobs which are in a suspended state.</span><br>    Suspended int32 `json:<span class="hljs-string">&quot;suspended&quot;</span>`<br>&#125;<br></code></pre></td></tr></table></figure><p>一个结束的trainJob的example：</p><p><img src="/images/image.png"></p><p>如何看一个trainJob是否结束了：</p><ul><li>直接遍历status.Conditions，比较是否有type为<em>TrainJobComplete或TrainJobFailed</em>，并且比较其status值是否为True</li></ul><p><strong>优点：</strong>&#x4F7F;用condition可以清楚地记录所有发生过的事件的过程。</p><p>如何SetStatuCondition：</p><ul><li><p>以newCondition.Type为标准去查看之前是否已经有相关的Condition了</p></li><li><p>如果还没有这个事件，就加入</p></li><li><p>如果已经有了，就更新为现在这个Condition</p></li></ul><p><strong>优点：</strong>&#x63A7;制了Condition的数量，</p><h1 id="主要流程介绍"><a href="#主要流程介绍" class="headerlink" title="主要流程介绍"></a>主要流程介绍</h1><h2 id="x20-TrainJobReconciler-Reconcile"><a href="#x20-TrainJobReconciler-Reconcile" class="headerlink" title="&#x20;TrainJobReconciler.Reconcile"></a>&#x20;TrainJobReconciler.Reconcile</h2><p>这是主要的回调函数</p><ol><li><p>根据req的名字得到trainJob</p></li><li><p>如果trainJob已经结束了就直接返回</p></li><li><p>根据这个trainJob所属的runtime的APIGroup和Kind来得到这个类别的所有的runtime</p></li><li><p>然后调用reconcileObjects(ctx context.Context, runtime jobruntimes.Runtime, trainJob *trainer.TrainJob)</p></li><li><p>更新trainJob的Status</p><ol><li><p>查看trainJob.Spec.Suspend和trainJob.Status.Conditions，任意标记了suspended，就将一个新的suspended添加到Status.Conditions中</p></li><li><p>根据reconcileObjects的返回的opState来添加对应的<em>creationSucceeded、buildFailed、creationFailed、updateFailed</em>对应的Condition</p></li><li><p>设置TerminalCondition</p></li><li><p>如果在上述的Condition设置中发生了condition的变化，就进行Status的更新</p></li></ol></li></ol><h2 id="TrainJobReconciler-reconcileObjects"><a href="#TrainJobReconciler-reconcileObjects" class="headerlink" title="TrainJobReconciler.reconcileObjects"></a>TrainJobReconciler.reconcileObjects</h2><ol><li><p>让传进来的runtime根据trainJob.Spec.RuntimeRef.Name得到ClusterTrainingRuntime</p></li><li><p>调用ClusterTrainingRuntime来创建新的objects</p><ol><li><p>对于label和annotation会在template的基础上添加上trainJob的</p></li><li><p>最后会根据TemplateSpec生成出带label、annotation、mlPolicy、podGroupPolicy、PodSpecReplicas的infoOption，并根据infoOption来生成Info</p></li><li><p>根据f.enforceMLPlugins来依据MPI、Torch、PlainML中的设置更新info.RuntimePolicy.MLPolicy、info.RuntimePolicy.MLPolicy.Torch来更新info.Trainer中的一些信息，如NumNodes、Env、ContainerPort等</p></li><li><p>根据f.enforcePodGroupPolicyPlugins来依据CoScheduling的设置更新info.Scheduler中的信息，如info.Scheduler.PodLabels[schedulerpluginsv1alpha1.<em>PodGroupLabel</em>] &#x3D; trainJob.Name</p></li><li><p>根据f.componentBuilderPlugins来依据coScheduling、jobSet、MPI等使用info更新一下额外的components，如CoScheduling就需要更新schedulerpluginsv1alpha1ac.PodGroup</p></li></ol></li><li><p>遍历创建的objects进行处理</p><ol><li><p>对object进行类型转换，转为obj</p></li><li><p>调用patch更新obj</p></li></ol></li></ol><h1 id="JobSet介绍"><a href="#JobSet介绍" class="headerlink" title="JobSet介绍"></a>JobSet介绍</h1><p>JobSet是trainRuntime中的template，是创建pod的基本单元，一个jobset中有多个ReplicatedJob，每个ReplicatedJob代表的是多个一样的pod。</p><h1 id="Coscheduling所添加的内容"><a href="#Coscheduling所添加的内容" class="headerlink" title="Coscheduling所添加的内容"></a>Coscheduling所添加的内容</h1><p>原本Coscheduling所使用的podGroup的定义：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// PodGroupSpec represents the template of a pod group.</span><br><span class="hljs-keyword">type</span> PodGroupSpec <span class="hljs-keyword">struct</span> &#123;<br>        <span class="hljs-comment">// MinMember defines the minimal number of members/tasks to run the pod group;</span><br>        <span class="hljs-comment">// if there&#x27;s not enough resources to start all tasks, the scheduler</span><br>        <span class="hljs-comment">// will not start any.</span><br>        MinMember <span class="hljs-type">uint32</span> <span class="hljs-string">`json:&quot;minMember&quot;`</span><br><br>        <span class="hljs-comment">// MinResources defines the minimal resource of members/tasks to run the pod group;</span><br>        <span class="hljs-comment">// if there&#x27;s not enough resources to start all tasks, the scheduler</span><br>        <span class="hljs-comment">// will not start any.</span><br>        MinResources *v1.ResourceList <span class="hljs-string">`json:&quot;minResources,omitempty&quot;`</span><br><br>        <span class="hljs-comment">// ScheduleTimeoutSeconds defines the maximal time of members/tasks to wait before run the pod group;</span><br>        ScheduleTimeoutSeconds *<span class="hljs-type">int32</span> <span class="hljs-string">`json:&quot;scheduleTimeoutSeconds,omitempty&quot;`</span><br>&#125;<br><br><span class="hljs-comment">// PodGroupStatus represents the current state of a pod group.</span><br><span class="hljs-keyword">type</span> PodGroupStatus <span class="hljs-keyword">struct</span> &#123;<br>        <span class="hljs-comment">// Current phase of PodGroup.</span><br>        Phase PodGroupPhase <span class="hljs-string">`json:&quot;phase&quot;`</span><br><br>        <span class="hljs-comment">// OccupiedBy marks the workload (e.g., deployment, statefulset) UID that occupy the podgroup.</span><br>        <span class="hljs-comment">// It is empty if not initialized.</span><br>        OccupiedBy <span class="hljs-type">string</span> <span class="hljs-string">`json:&quot;occupiedBy,omitempty&quot;`</span><br><br>        <span class="hljs-comment">// The number of actively running pods.</span><br>        <span class="hljs-comment">// +optional</span><br>        Running <span class="hljs-type">uint32</span> <span class="hljs-string">`json:&quot;running&quot;`</span><br><br>        <span class="hljs-comment">// The number of pods which reached phase Succeeded.</span><br>        <span class="hljs-comment">// +optional</span><br>        Succeeded <span class="hljs-type">uint32</span> <span class="hljs-string">`json:&quot;succeeded&quot;`</span><br><br>        <span class="hljs-comment">// The number of pods which reached phase Failed.</span><br>        <span class="hljs-comment">// +optional</span><br>        Failed <span class="hljs-type">uint32</span> <span class="hljs-string">`json:&quot;failed&quot;`</span><br><br>        <span class="hljs-comment">// ScheduleStartTime of the group</span><br>        ScheduleStartTime metav1.Time <span class="hljs-string">`json:&quot;scheduleStartTime&quot;`</span><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="pkg-runtime-framework-plugins-coscheduling"><a href="#pkg-runtime-framework-plugins-coscheduling" class="headerlink" title="pkg&#x2F;runtime&#x2F;framework&#x2F;plugins&#x2F;coscheduling"></a>pkg&#x2F;runtime&#x2F;framework&#x2F;plugins&#x2F;coscheduling</h2><h4 id="对于CoScheduling"><a href="#对于CoScheduling" class="headerlink" title="对于CoScheduling"></a>对于<strong>CoScheduling</strong></h4><p>需要实现三大Plug：</p><ul><li><p>EnforcePodGroupPolicyPlugin：</p><ul><li><p>EnforcePodGroupPolicy函数：</p><ul><li>主要是加入了<code>info.Scheduler.PodLabels[schedulerpluginsv1alpha1.</code><em><code>PodGroupLabel</code></em><code>] = trainJob.Name</code></li></ul></li></ul></li><li><p>WatchExtensionPlugin</p><ul><li><p>ReconcilerBuilders函数：</p><ul><li>确保 PodGroup CRD 已安装，并配置控制器监听 PodGroup、LimitRange 和 RuntimeClass 资源的变化。当这些资源发生变化时，会触发相应的处理逻辑，以保证系统状态的一致性。</li></ul></li></ul></li><li><p>ComponentBuilderPlugin</p><ul><li><p>build函数：</p><ul><li><p>如果 info.RuntimePolicy.PodGroupPolicy.Coscheduling不为空，就创建schedulerpluginsv1alpha1ac.PodGroup(需要的内容，包括：</p></li><li><p>根据info.TotalRequests计算totalMembers、各个类型的资源的总的资源请求</p></li><li><p>构建出podGroup，主要是填充下面这部分内容：</p></li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// PodGroupSpecApplyConfiguration represents an declarative configuration of the PodGroupSpec type for use</span><br><span class="hljs-comment">// with apply.</span><br><span class="hljs-keyword">type</span> PodGroupSpecApplyConfiguration <span class="hljs-keyword">struct</span> &#123;<br>    MinMember              *<span class="hljs-type">int32</span>           <span class="hljs-string">`json:&quot;minMember,omitempty&quot;`</span><br>    MinResources           *v1.ResourceList <span class="hljs-string">`json:&quot;minResources,omitempty&quot;`</span><br>    ScheduleTimeoutSeconds *<span class="hljs-type">int32</span>           <span class="hljs-string">`json:&quot;scheduleTimeoutSeconds,omitempty&quot;`</span><br>&#125;<br></code></pre></td></tr></table></figure><ul><li>将podGroup的ownerReferences指向trainJob</li></ul></li></ul></li></ul><h4 id="对于PodGroupRuntimeClassHandler"><a href="#对于PodGroupRuntimeClassHandler" class="headerlink" title="对于PodGroupRuntimeClassHandler"></a>对于<strong>PodGroupRuntimeClassHandler</strong></h4><p>需要添加这几个plugin：</p><ul><li><p>TypedEventHandler，这个包含了对event增删查改以及Generic的操作</p></li><li><p>增删查改实际上都统一调用了<strong>queueSuspendedTrainJobs</strong>，具体代码执行如下：</p><ul><li><p>依据runtimeClass.Name获取到所有的trainingRuntimes和clusterTrainingRuntimes</p></li><li><p>依据trainingRuntime.Name和clusterTrainingRuntime.Name获取到对应的trainJob</p></li><li><p>遍历所有的trainJob，如果它已经suspend那么就将其作为一个request加入到处理队列中</p></li></ul></li></ul><h1 id="Volcano中相关内容"><a href="#Volcano中相关内容" class="headerlink" title="Volcano中相关内容"></a>Volcano中相关内容</h1><h2 id="PodGroup整理"><a href="#PodGroup整理" class="headerlink" title="PodGroup整理"></a>PodGroup整理</h2><p><strong>podGroup定义：</strong></p><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs protobuf"><span class="hljs-comment">// PodGroupSpec represents the template of a pod group.</span><br>type PodGroupSpec struct &#123;<br>    <span class="hljs-comment">// MinMember defines the minimal number of members/tasks to run the pod group;</span><br>    <span class="hljs-comment">// if there&#x27;s not enough resources to start all tasks, the scheduler</span><br>    <span class="hljs-comment">// will not start anyone.</span><br>    MinMember <span class="hljs-type">int32</span> `json:<span class="hljs-string">&quot;minMember,omitempty&quot;</span> protobuf:<span class="hljs-string">&quot;bytes,1,opt,name=minMember&quot;</span>`<br><br>    <span class="hljs-comment">// Queue defines the queue to allocate resource for PodGroup; if queue does not exist,</span><br>    <span class="hljs-comment">// the PodGroup will not be scheduled. Defaults to `default` Queue with the lowest weight.</span><br>    <span class="hljs-comment">// +optional</span><br>    Queue <span class="hljs-type">string</span> `json:<span class="hljs-string">&quot;queue,omitempty&quot;</span> protobuf:<span class="hljs-string">&quot;bytes,2,opt,name=queue&quot;</span>`<br><br>    <span class="hljs-comment">// If specified, indicates the PodGroup&#x27;s priority. &quot;system-node-critical&quot; and</span><br>    <span class="hljs-comment">// &quot;system-cluster-critical&quot; are two special keywords which indicate the</span><br>    <span class="hljs-comment">// highest priorities with the former being the highest priority. Any other</span><br>    <span class="hljs-comment">// name must be defined by creating a PriorityClass object with that name.</span><br>    <span class="hljs-comment">// If not specified, the PodGroup priority will be default or zero if there is no</span><br>    <span class="hljs-comment">// default.</span><br>    <span class="hljs-comment">// +optional</span><br>    PriorityClassName <span class="hljs-type">string</span> `json:<span class="hljs-string">&quot;priorityClassName,omitempty&quot;</span> protobuf:<span class="hljs-string">&quot;bytes,3,opt,name=priorityClassName&quot;</span>`<br><br>    <span class="hljs-comment">// MinResources defines the minimal resource of members/tasks to run the pod group;</span><br>    <span class="hljs-comment">// if there&#x27;s not enough resources to start all tasks, the scheduler</span><br>    <span class="hljs-comment">// will not start anyone.</span><br>    MinResources *v1.ResourceList `json:<span class="hljs-string">&quot;minResources,omitempty&quot;</span> protobuf:<span class="hljs-string">&quot;bytes,4,opt,name=minResources&quot;</span>`<br>&#125;<br><br><span class="hljs-comment">// PodGroupStatus represents the current state of a pod group.</span><br>type PodGroupStatus struct &#123;<br>    <span class="hljs-comment">// Current phase of PodGroup.</span><br>    Phase PodGroupPhase `json:<span class="hljs-string">&quot;phase,omitempty&quot;</span> protobuf:<span class="hljs-string">&quot;bytes,1,opt,name=phase&quot;</span>`<br><br>    <span class="hljs-comment">// The conditions of PodGroup.</span><br>    <span class="hljs-comment">// +optional</span><br>    Conditions []PodGroupCondition `json:<span class="hljs-string">&quot;conditions,omitempty&quot;</span> protobuf:<span class="hljs-string">&quot;bytes,2,opt,name=conditions&quot;</span>`<br><br>    <span class="hljs-comment">// The number of actively running pods.</span><br>    <span class="hljs-comment">// +optional</span><br>    Running <span class="hljs-type">int32</span> `json:<span class="hljs-string">&quot;running,omitempty&quot;</span> protobuf:<span class="hljs-string">&quot;bytes,3,opt,name=running&quot;</span>`<br><br>    <span class="hljs-comment">// The number of pods which reached phase Succeeded.</span><br>    <span class="hljs-comment">// +optional</span><br>    Succeeded <span class="hljs-type">int32</span> `json:<span class="hljs-string">&quot;succeeded,omitempty&quot;</span> protobuf:<span class="hljs-string">&quot;bytes,4,opt,name=succeeded&quot;</span>`<br><br>    <span class="hljs-comment">// The number of pods which reached phase Failed.</span><br>    <span class="hljs-comment">// +optional</span><br>    Failed <span class="hljs-type">int32</span> `json:<span class="hljs-string">&quot;failed,omitempty&quot;</span> protobuf:<span class="hljs-string">&quot;bytes,5,opt,name=failed&quot;</span>`<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="PodGroup-Controller如何管理Pod"><a href="#PodGroup-Controller如何管理Pod" class="headerlink" title="PodGroup Controller如何管理Pod"></a>PodGroup Controller如何管理Pod</h3><p>查看pkg&#x2F;controllers&#x2F;podgroup&#x2F;pg_controller.go相关内容：<br>哪些Pod会被其管理：</p><ul><li><p>&#x20;pod.Spec.SchedulerName符合Volcano对应的调度器</p></li><li><p>pod.Annotation中有scheduling.k8s.io&#x2F;group-name，且名字等于podGroup，如果没有就会创建一个对应的podGroup</p></li><li><p>可能还需要”volcano.sh&#x2F;task-spec”&#x3D;</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>k8s</category>
      
      <category>kubeflow</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2024开源之夏参与经验总结</title>
    <link href="/2024/12/02/OSPPExperience/"/>
    <url>/2024/12/02/OSPPExperience/</url>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>最近参与了2024年中科院举办的<a href="https://summer-ospp.ac.cn/">开源之夏活动</a>，完成了字节跳动开源的Godel Scheduler的”<a href="https://summer-ospp.ac.cn/org/prodetail/247d60192?lang=zh&list=pro">基于 Godel Scheduler 实现应用亲和&#x2F;反亲和以及均匀分布功能</a>“的项目。这里做一个经验贴，希望能帮助到后面想要参与到这个项目中的人来。</p><h1 id="项目报名"><a href="#项目报名" class="headerlink" title="项目报名"></a>项目报名</h1><p>下面这个是2024年整体的一个活动时间。</p><p><img src="/2024/12/02/OSPPExperience/timeLine.png" alt="活动时间线"></p><p>我印象中在4月29号之前就已经在陆陆续续放出会参与到开源之夏活动中的项目了，所以可以在此期间多多关注一下，然后提早做准备。</p><p>我个人是在5月初就基本决定了要做哪几个项目，然后优先选择了其中一个最想做的来做准备，选择的时候主要是参考哪一个与我现有的研究规划最为接近同时又是在力所能及的范围内的，我最后选择的godel-scheduler的项目虽然是一个比较新的开源项目，但是我之前就已经有读到过这个项目的论文，并且我个人也比较相信字节的背书（事实证明字节的人确实也很技术过硬，都很nice）。基本确定之后我就开始做了相关的准备工作，首先去恶补了一下k8s调度器的相关知识并开始阅读源码，并且正好也将其整理成了博客，然后尝试部署使用了godel-scheduler。然后我是5月15号就去主动发邮件联系了项目的导师，做了一番自我介绍以及对项目的青睐，不过其实当时导师也只是回了一下感谢关注，后面有信息会周知。据我所知有些项目的导师你在联系的时候会交流的比较热烈，会进一步确定双方的意向。可能还是主要看导师的习惯，如果导师回复的没有那么热烈也不一定是没机会，集中精力去准备就好了。</p><p>后面正式申请的时候需要按要求填写一份个人简历和项目申请书，项目申请书是会公开出去的，需要注意不要在里面写个人隐私。项目申请书的内容也是仅供参考，个人感觉把自己的预研工作写清楚，展现自己有完成这个项目的能力就行了，像里面写的时间规划后面也是可以灵活调整的。</p><p>本来想要申请两个项目的以保底，不过奈何精力不足，就直接all in到一个项目中了。</p><h1 id="项目开发"><a href="#项目开发" class="headerlink" title="项目开发"></a>项目开发</h1><p>开源之夏官方公告中有说这些项目是需要一整个暑假全勤去搞，但是实际上据我观察大部分任务可能并不需要这么多时间。像我当时暑假是实习+实验室项目与论文+开源之夏并行的，实际上多肝肝也还是来得及（就是后期可能会有点点赶）。</p><p>我是当时花了大概一个星期完成了一个初步版本，然后在8月初就提了一个pr，然后本来想着是等着有人在下面给我提改进意见，然后继续改的。但是实际上一直没人理，不过其实也有我当时因为比较忙所以想先缓缓的原因，然后后面到了9月初我去主动邮件联系了导师，然后导师帮我拉了一个飞书群，让另外一名正式员工来指导我。</p><p>当时其实本来感觉已经开发的差不多了，但是后面交流下来发现自己还是对很多地方理解的不是很彻底，没有把项目与原生k8s不同的地方兼顾到，所以后面又改了很多。</p><p>值得注意的是虽然上面写的项目开发的时间是在9月30号，9月30号也确实就需要上传结项项目书了，但是10月份其实还是可以继续在pr上commit，所以实际上开发是可以一直推进到10月底。我实际上也一直干到了那时候，也多亏后面不在实习了，不然肯定干不完了。所以个人感觉还是要去尽早联系导师，多多沟通，预留足够的时间，导师确实能够在开发过程中给予很多的指导。</p><p>还有一个需要提前注意的点，就是开发过程中的commit提交需要使用个人注册的学校邮箱，还需要把github的邮箱隐私模式关掉，我当时忘记关了，所以后面还发了份邮件过去说明这个事情。</p><h1 id="项目结项"><a href="#项目结项" class="headerlink" title="项目结项"></a>项目结项</h1><p>项目结项需要写一个结项报告，主要就是把完成了什么事情写出来，这个只要和导师那边协商好了就行不是太大的问题，主要还是看开发进度。我当时实际上最后还是差一个小功能没有彻底完成，所以就先合到了主项目的一个分支，然后后面11月才把功能彻底完成，提交了一个新的pr上去。</p><p>最后结项通过之后要记得去填写银行卡信息。</p><p>导师还可以在结项后去帮忙申请优秀学生，我是当时申请了最具潜力奖，祈愿一个能过~</p>]]></content>
    
    
    <categories>
      
      <category>开源之夏</category>
      
    </categories>
    
    
    <tags>
      
      <tag>开源之夏</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何在K8s集群中管理与使用GPU</title>
    <link href="/2024/11/19/k8sUseGPU/"/>
    <url>/2024/11/19/k8sUseGPU/</url>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>随着人工智能的兴起，GPU作为重要的智算算力类型愈发受到重视，而Kubernetes（k8s）作为业界主流的集群管理系统，如何方便管理、使用GPU也是其需要解决的一大问题，故此收集整理了K8s管理与使用GPU的相关资料以学习。</p><h1 id="物理机如何使用GPU"><a href="#物理机如何使用GPU" class="headerlink" title="物理机如何使用GPU"></a>物理机如何使用GPU</h1><p>如果给一台普通的物理机，例如我们日常用的笔记本电脑应该如何使用GPU呢。其主要涉及到两个插件的安装，分别是<strong>Nvidia Driver</strong>和<strong>CUDA Toolkit</strong>。</p><h2 id="Nvidia-Driver"><a href="#Nvidia-Driver" class="headerlink" title="Nvidia Driver"></a>Nvidia Driver</h2><p>Nvidia Driver就是GPU驱动，其与其他驱动类似，其主要作用是作为操作系统与GPU硬件之间沟通的桥梁，它需要负责将GPU复杂的硬件功能抽象为标准化接口，方便操作系统和软件调用，并能把GPU硬件的反馈结果传递给操作系统或应用程序。</p><h2 id="Cuda-Toolkit"><a href="#Cuda-Toolkit" class="headerlink" title="Cuda Toolkit"></a>Cuda Toolkit</h2><p>Cuda toolkit是NVIDIA提供的一个开发工具集，包含了一系列用于GPU编程的工具和库。其主要由以下组件组成：</p><ul><li><strong>Compiler</strong>: CUDA-C和CUDA-C++编译器NVCC位于bin&#x2F;目录中。它建立在NVVM优化器之上，而NVVM优化器本身构建在LLVM编译器基础结构之上。希望开发人员可以使用nvm&#x2F;目录下的Compiler SDK来直接针对NVVM进行开发。</li><li><strong>Tools</strong>: 提供一些像profiler,debuggers等工具，这些工具可以从bin&#x2F;目录中获取</li><li><strong>Libraries</strong>: 下面列出的部分科学库和实用程序库可以在lib&#x2F;目录中使用(Windows上的DLL位于bin&#x2F;中)，它们的接口在include&#x2F;目录中可获取。<ul><li>cudart: CUDA Runtime</li><li>cudadevrt: CUDA device runtime</li><li>cupti: CUDA profiling tools interface</li><li>nvml: NVIDIA management library</li><li>nvrtc: CUDA runtime compilation</li><li>cublas: BLAS (Basic Linear Algebra Subprograms，基础线性代数程序集)</li><li>cublas_device: BLAS kernel interface</li><li>…</li></ul></li><li><strong>Runtime Api</strong>：提供GPU访问的接口，包括：<ul><li>CUDA Runtime API： 提供简单易用的高层接口，简化GPU的初始化和资源管理。</li><li>CUDA Driver API： 更底层的接口，提供对GPU的精细控制，适合需要自定义优化的高级用户。</li></ul></li><li><strong>CUDA Samples</strong>: 演示如何使用各种CUDA和library API的代码示例。可在Linux和Mac上的samples&#x2F;目录中获得，Windows上的路径是<code>C：\ProgramData\NVIDIA Corporation\CUDA Samples</code>中。在Linux和Mac上，<code>samples/</code>目录是只读的，如果要对它们进行修改，则必须将这些示例复制到另一个位置。</li></ul><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>安装完以上插件后就可以使用GPU了，我们可以直接使用CUDA来编程也可以利用Pytorch、TensorFlow等机器学习库来间接使用GPU，在使用GPU时，其整体的调用链如下图所示：</p><p><img src="/2024/11/19/k8sUseGPU/gpuApi.png" alt="GPU调用链"></p><h1 id="Docker如何使用GPU"><a href="#Docker如何使用GPU" class="headerlink" title="Docker如何使用GPU"></a>Docker如何使用GPU</h1><h2 id="配置nvidia-container-runtime"><a href="#配置nvidia-container-runtime" class="headerlink" title="配置nvidia-container-runtime"></a>配置nvidia-container-runtime</h2><p>正常创建一个容器的流程是这样的:</p><p><img src="/2024/11/19/k8sUseGPU/dockerCreate.jfif" alt="docker创建容器流程"></p><p>简单来说主要有以下这些步骤：</p><ol><li><strong>用户命令传递</strong>： CLI 将用户命令解析并使用 HTTP 或 Unix Socket 与 dockerd 通信。</li><li><strong>调度与管理</strong>： dockerd 解析命令并检查、拉取镜像，再调用 containerd 创建一个新的容器任务，准备容器的元数据和配置（如挂载点、网络设置、环境变量等），并为每个任务创建一个 containerd-shim 进程</li><li><strong>隔离与启动</strong>： containerd-shim 启动并调用 runc 创建隔离环境，runc 从 containerd 提供的配置中读取容器规格，包括文件系统挂载、网络命名空间、cgroups 配置（限制 CPU、内存等资源），并配置PID、Network、Mount Namespace级别的隔离，再配置 Cgroups，限制资源使用，设置 rootfs，将镜像内容挂载为容器的根文件系统。</li><li><strong>容器运行</strong>： runc 启动用户指定的进程，容器进入运行状态。</li></ol><p>而为了能够让容器也能直接使用GPU，我们就需要修改创建容器的关键runtime为<strong>nvidia-container-runtime</strong>，而我们一般都通过<strong>NVIDIA Container Toolkit</strong>来安装nvidia-container-runtime。旧版本修改runtime为nvidia-container-runtime是需要手动在<code>etc/docker/daemon.json</code>中增加配置，指定使用 nvidia 的 runtime，如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-attr">&quot;runtimes&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;nvidia&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>        <span class="hljs-attr">&quot;args&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-attr">&quot;path&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;nvidia-container-runtime&quot;</span><br>    <span class="hljs-punctuation">&#125;</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>新版 toolkit 带了一个nvidia-ctk 工具，执行以下命令即可一键配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo nvidia-ctk runtime configure --runtime=docker<br></code></pre></td></tr></table></figure><p>然后重启 Docker 即可，再创建使用GPU的容器时，只需要加入<code>--gpu</code>参数即可，如<code>docker run --rm --gpus all  nvidia/cuda:12.0.1-runtime-ubuntu22.04 nvidia-smi</code>。</p><h2 id="说明-1"><a href="#说明-1" class="headerlink" title="说明"></a>说明</h2><p>修改runtime后，如下图所示，containerd-shim会调用指定的运行时nvidia-container-runtime。nvidia-container-runtime相比于默认的runc多实现了<strong>nvidia-container-runime-hook</strong>，该hook是在容器启动后（Namespace已创建完成），容器自定义命令(Entrypoint)启动前执行。当检测到<strong>NVIDIA_VISIBLE_DEVICES</strong>环境变量时，会<strong>调用libnvidia-container挂载GPU Device和CUDA Driver</strong>。如果没有检测到NVIDIA_VISIBLE_DEVICES就直接执行默认的runc。</p><p><img src="/2024/11/19/k8sUseGPU/dockerCreateWithGPU.png" alt="docker创建GPU容器流程"></p><p>在Docker 环境中的 CUDA 调用的整体层级如下图所示，NVIDIA 将原来 CUDA 应用依赖的API环境划分为两个部分：</p><ul><li><strong>驱动级API</strong>：由libcuda.so.major.minor动态库和内核module提供支持，图中表示为CUDA Driver，它必须在宿主机上就配置好，且只能有一个版本。</li><li><strong>非驱动级API</strong>：由动态库libcublas.so等用户空间级别的API（算是对驱动级API的一种更高级的封装）组成，图中表示为CUDA Toolkit，直接存在在各个容器中，各个容器中的CUDA Toolkit的版本也可以不同。</li></ul><p><img src="/2024/11/19/k8sUseGPU/dockerGPUApi.png" alt="docker中的CUDA调用"></p><h1 id="K8s如何使用GPU"><a href="#K8s如何使用GPU" class="headerlink" title="K8s如何使用GPU"></a>K8s如何使用GPU</h1><p>通过上述说明，我们可以手动在宿主机中起一个使用GPU的容器，但是对于k8s管理的大规模集群，我们还需要做到可以让k8s感知到有哪些GPU可以使用，可以通过k8s的pod来创建使用GPU的容器。</p><h2 id="手动配置"><a href="#手动配置" class="headerlink" title="手动配置"></a>手动配置</h2><p>在k8s中使用GPU资源涉及到的一个关键组件就是<strong>NVIDIA Device Plugin</strong>。</p><p>Device plugin是k8s 用于管理和调度容器中设备资源的一种插件机制，它可以将物理设备（如 GPU、FPGA 等）暴露给容器，从而提供更高级别的资源管理和调度能力。它由各个硬件对应的厂商提供，其主要是通过DeamonSet部署到各个主机上，然后上报给kubelet对应的硬件资源的情况，再上报给master。</p><p>当我们安装了NVDIA的device plugin后再次查看node的可分配资源就可以看到GPU相关的信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@<span class="hljs-built_in">test</span>:~<span class="hljs-comment"># k describe node test|grep Capacity -A7</span><br>Capacity:<br>  cpu:                48<br>  ephemeral-storage:  460364840Ki<br>  hugepages-1Gi:      0<br>  hugepages-2Mi:      0<br>  memory:             98260824Ki<br>  nvidia.com/gpu:     2<br>  pods:               110<br></code></pre></td></tr></table></figure><p>可以看到，除了常见的 cpu、memory 之外，还有<code>nvidia.com/gpu</code>, 这个就是GPU资源，数量为 2 说明我们有两张 GPU。当我们要为pod分配GPU资源的时候也比较简单，只需要在<code>resources.limits</code>中加入<code>nvidia.com/gpu: 1</code>，就可以为pod申请一块GPU资源了。</p><h3 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h3><p>而具体来说NVDIA device plugin主要是通过实现ListAndWatch 接口来上报节点上的GPU数量，实现Allocate接口， 支持分配GPU的行为。</p><p>这部分的关键源码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// ListAndWatch lists devices and update that list according to the health status</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(plugin *NvidiaDevicePlugin)</span></span> ListAndWatch(e *pluginapi.Empty, s pluginapi.DevicePlugin_ListAndWatchServer) <span class="hljs-type">error</span> &#123;<br>    s.Send(&amp;pluginapi.ListAndWatchResponse&#123;Devices: plugin.apiDevices()&#125;)<br><br>    <span class="hljs-keyword">for</span> &#123;<br>        <span class="hljs-keyword">select</span> &#123;<br>        <span class="hljs-keyword">case</span> &lt;-plugin.stop:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>        <span class="hljs-keyword">case</span> d := &lt;-plugin.health:<br>            <span class="hljs-comment">// 收到某个设备有健康问题，标志该设备不健康</span><br>            <span class="hljs-comment">// <span class="hljs-doctag">FIXME:</span> there is no way to recover from the Unhealthy state.</span><br>            d.Health = pluginapi.Unhealthy<br>            log.Printf(<span class="hljs-string">&quot;&#x27;%s&#x27; device marked unhealthy: %s&quot;</span>, plugin.rm.Resource(), d.ID)<br>            <span class="hljs-comment">// 重新发送新的可用的device列表</span><br>            s.Send(&amp;pluginapi.ListAndWatchResponse&#123;Devices: plugin.apiDevices()&#125;)<br>        &#125;<br>    &#125;<br>&#125;<br><br><span class="hljs-comment">// Allocat主要是分配显卡，给容器指定要附加的NVIDIA_VISIBLE_DEVICES环境变量</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(plugin *NvidiaDevicePlugin)</span></span> Allocate(ctx context.Context, reqs *pluginapi.AllocateRequest) (*pluginapi.AllocateResponse, <span class="hljs-type">error</span>) &#123;<br>    responses := pluginapi.AllocateResponse&#123;&#125;<br>    <span class="hljs-comment">// 为每个请求分配设备</span><br>    <span class="hljs-keyword">for</span> _, req := <span class="hljs-keyword">range</span> reqs.ContainerRequests &#123;<br> <br>        <span class="hljs-keyword">if</span> plugin.config.Sharing.TimeSlicing.FailRequestsGreaterThanOne &amp;&amp; rm.AnnotatedIDs(req.DevicesIDs).AnyHasAnnotations() &#123;<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(req.DevicesIDs) &gt; <span class="hljs-number">1</span> &#123;<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, fmt.Errorf(<span class="hljs-string">&quot;request for &#x27;%v: %v&#x27; too large: maximum request size for shared resources is 1&quot;</span>, plugin.rm.Resource(), <span class="hljs-built_in">len</span>(req.DevicesIDs))<br>            &#125;<br>        &#125;<br>        <span class="hljs-comment">// 判断一下申请的设备ID是不是自己所管理的，也就是所拥有的设备，也就是校验是不是自己注册的那些设备</span><br>        <span class="hljs-keyword">for</span> _, id := <span class="hljs-keyword">range</span> req.DevicesIDs &#123;<br>            <span class="hljs-keyword">if</span> !plugin.rm.Devices().Contains(id) &#123;<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, fmt.Errorf(<span class="hljs-string">&quot;invalid allocation request for &#x27;%s&#x27;: unknown device: %s&quot;</span>, plugin.rm.Resource(), id)<br>            &#125;<br>        &#125;<br><br>        response := pluginapi.ContainerAllocateResponse&#123;&#125;<br>        <span class="hljs-comment">// 将注册时的设备ID转换为具体的gpu id</span><br>        ids := req.DevicesIDs<br>        deviceIDs := plugin.deviceIDsFromAnnotatedDeviceIDs(ids)<br>        <span class="hljs-comment">// 将分配的设备信息保存到Env里面去，后续docker的runC将设备信息以环境变量的形式注入到容器</span><br>        <span class="hljs-keyword">if</span> *plugin.config.Flags.Plugin.DeviceListStrategy == spec.DeviceListStrategyEnvvar &#123;<br>            response.Envs = plugin.apiEnvs(plugin.deviceListEnvvar, deviceIDs)<br>        &#125;<br>        <span class="hljs-keyword">if</span> *plugin.config.Flags.Plugin.DeviceListStrategy == spec.DeviceListStrategyVolumeMounts &#123;<br>            response.Envs = plugin.apiEnvs(plugin.deviceListEnvvar, []<span class="hljs-type">string</span>&#123;deviceListAsVolumeMountsContainerPathRoot&#125;)<br>            response.Mounts = plugin.apiMounts(deviceIDs)<br>        &#125;<br>        <span class="hljs-keyword">if</span> *plugin.config.Flags.Plugin.PassDeviceSpecs &#123;<br>            response.Devices = plugin.apiDeviceSpecs(*plugin.config.Flags.NvidiaDriverRoot, ids)<br>        &#125;<br>        <span class="hljs-keyword">if</span> *plugin.config.Flags.GDSEnabled &#123;<br>            response.Envs[<span class="hljs-string">&quot;NVIDIA_GDS&quot;</span>] = <span class="hljs-string">&quot;enabled&quot;</span><br>        &#125;<br>        <span class="hljs-keyword">if</span> *plugin.config.Flags.MOFEDEnabled &#123;<br>            response.Envs[<span class="hljs-string">&quot;NVIDIA_MOFED&quot;</span>] = <span class="hljs-string">&quot;enabled&quot;</span><br>        &#125;<br><br>        responses.ContainerResponses = <span class="hljs-built_in">append</span>(responses.ContainerResponses, &amp;response)<br>    &#125;<br><br>    <span class="hljs-keyword">return</span> &amp;responses, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>整个Kubernetes调度GPU的过程如下：</p><ol><li>GPU Device plugin 部署到GPU节点上，通过<strong>ListAndWatch接口</strong>，上报注册节点的GPU信息和对应的DeviceID。</li><li>当有声明<code>nvidia.com/gpu</code>的GPU Pod创建出现，调度器会综合考虑GPU设备的空闲情况，将Pod调度到有充足GPU设备的节点上。</li><li>节点上的kubelet启动Pod时，根据request中的声明调用各个Device plugin的<strong>allocate接口</strong>，由于容器声明了GPU。kubelet根据之前ListAndWatch接口收到的Device信息，选取合适的设备，DeviceID作为参数，调用GPU DevicePlugin的allocate接口。</li><li>GPU device plugin接收到调用，将DeviceID转换为<strong>NVIDIA_VISIBLE_DEVICES环境变量</strong>，返回给kubelet</li><li>kubelet收到返回内容后，会自动将返回的环境变量注入到容器中，并开始创建容器。</li><li>容器创建时，<strong>nvidia-container-runtime</strong>调用gpu-containers-runtime-hook根据容器的NVIDIA_VISIBLE_DEVICES环境变量，来决定这个容器是否为GPU容器，并且可以使用哪些GPU设备。如果没有携带NVIDIA_VISIBLE_DEVICES这个环境变量，那么就会按照普通的docker启动方式来启动。</li></ol><h2 id="使用GPU-Operator安装"><a href="#使用GPU-Operator安装" class="headerlink" title="使用GPU Operator安装"></a>使用GPU Operator安装</h2><p>GPU Operator旨在简化在Kubernetes环境中使用GPU的过程，通过自动化的方式处理GPU驱动程序安装、Controller Toolkit、Device-Plugin 、监控等组件。<br>NVIDIA GPU Operator总共包含如下的几个组件：</p><ul><li><strong>NFD(Node Feature Discovery)</strong>: 用于给节点打上某些标签，这些标签包括 cpu id、内核版本、操作系统版本、是不是GPU节点等，其中需要关注的标签是<code>nvidia.com/gpu.present=true</code>，如果节点存在该标签，那么说明该节点是GPU节点。</li><li><strong>GFD(GPU Feature Discovery)</strong>: 用于收集节点的GPU设备属性（GPU驱动版本、GPU型号等），并将这些属性以节点标签的方式透出。在k8s集群中以DaemonSet方式部署，只有节点拥有标签<code>nvidia.com/gpu.present=true</code>时，DaemonSet控制的Pod才会在该节点上运行。<ul><li>新版本 GFD迁移到了<code>NVIDIA/k8s-device-plugin</code></li></ul></li><li><strong>NVIDIA Driver Installer</strong>：基于容器的方式在节点上安装 NVIDIA GPU驱动，在k8s集群中以DaemonSet 方式部署，只有节点拥有标签nvidia.com&#x2F;gpu.present&#x3D;true时，DaemonSet 控制的 Pod 才会在该节点上运行。</li><li><strong>NVIDIA Container Toolkit Installer</strong>：能够实现在容器中使用GPU设备，在k8s集群中以DaemonSet 方式部署，同样的，只有节点拥有标签<code>nvidia.com/gpu.present=true</code>时，DaemonSet 控制的 Pod 才会在该节点上运行。</li><li><strong>NVIDIA Device Plugin</strong>：NVIDIA Device Plugin 用于实现将GPU设备以 Kubernetes 扩展资源的方式供用户使用，在k8s集群中以DaemonSet 方式部署，只有节点拥有标签nvidia.com&#x2F;gpu.present&#x3D;true时，DaemonSet 控制的 Pod 才会在该节点上运行。</li><li><strong>DCGM Exporter</strong>：周期性的收集节点GPU设备的状态（当前温度、总的显存、已使用显存、使用率等）并暴露Metrics，结合Prometheus和Grafana使用。在k8s集群中以DaemonSet 方式部署，只有节点拥有标签nvidia.com&#x2F;gpu.present&#x3D;true时，DaemonSet 控制的Pod才会在该节点上运行。</li></ul><p>首先是 GFD、NFD，二者都是用于发现 Node 上的信息，并以label形式添加到k8snode对象上，特别是GFD会添加<code>nvidia.com/gpu.present=true</code>标签表示该节点有GPU，只有携带该标签的节点才会安装后续组件。<br>然后则是Driver Installer、Container Toolkit Installer用于安装GPU驱动和container toolkit。<br>接下来这是device-plugin让k8s能感知到GPU资源信息便于调度和管理。<br>最后的exporter则是采集GPU监控并以Prometheus Metrics格式暴露，用于做GPU监控。</p><p>这里着重提及一下<strong>NVIDIA Driver Installer</strong>和<strong>NVIDIA Container Toolkit Installer</strong>是如何通过容器的方式来给主机安装对应的内容的。<br>其安装的主要方法还是通过hostPath挂载的方式来将相关的目录挂载进容器中，然后控制容器将对应的内容添加进目录里。而如果要将相关内容卸载，也是只需要将对应的容器删除，容器就会自动移除相应安装的内容。</p><p>GPU Operator虽然方便了安装但是仍然存在一些缺点：</p><ul><li>Driver Installer 以DaemonSet 方式运行的，每个节点上运行的 Pod 都一样，但是镜像由 驱动版本+内核版本+操作系统版本拼接而成，因此<strong>需要集群中所有节点操作系统一致</strong>。</li><li>NVIDIA Container Toolkit Installer 同样是以DaemonSet 方式运行的，另外安装时需要指定 Runtime，这也造成了集群的节点<strong>必须安装相同的 Container Runtime</strong>。</li></ul><h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><p>目前学习下来的一大感受就是GPU管理的粒度很粗，都是以整卡为单位进行划分，这势必会造成资源的浪费，如何结合GPU卡虚拟化来进行细粒度的分配感觉是一个很大的问题。同时它也没有刻画GPU卡之间、CPU与GPU卡之间的拓扑关系，而当前对于大模型训练其网络管理与优化是非常重要的。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="https://www.lixueduan.com/posts/ai/01-how-to-use-gpu">https://www.lixueduan.com/posts/ai/01-how-to-use-gpu</a></li><li><a href="https://www.lixueduan.com/posts/ai/02-gpu-operator/">https://www.lixueduan.com/posts/ai/02-gpu-operator/</a></li><li><a href="https://www.aneasystone.com/archives/2023/12/scheduling-gpus-in-kubernetes.html">https://www.aneasystone.com/archives/2023/12/scheduling-gpus-in-kubernetes.html</a></li><li><a href="https://blog.csdn.net/qq_43684922/article/details/127024933">https://blog.csdn.net/qq_43684922/article/details/127024933</a></li><li><a href="https://kubernetes.io/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">https://kubernetes.io/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/</a></li><li><a href="https://kubernetes.io/zh-cn/docs/tasks/manage-gpus/scheduling-gpus/">https://kubernetes.io/zh-cn/docs/tasks/manage-gpus/scheduling-gpus/</a></li><li><a href="https://blog.csdn.net/qq_43684922/article/details/127025776">https://blog.csdn.net/qq_43684922/article/details/127025776</a></li><li><a href="https://www.lixueduan.com/posts/kubernetes/21-device-plugin/#">https://www.lixueduan.com/posts/kubernetes/21-device-plugin/#</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>k8s</category>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>GPU</tag>
      
      <tag>AI</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>InternLM 书生大模型实战营</title>
    <link href="/2024/09/13/InternLM/"/>
    <url>/2024/09/13/InternLM/</url>
    
    <content type="html"><![CDATA[<h1 id="1-入门岛"><a href="#1-入门岛" class="headerlink" title="1. 入门岛"></a>1. 入门岛</h1><h2 id="1-1-第一关-Linux-基础知识"><a href="#1-1-第一关-Linux-基础知识" class="headerlink" title="1.1 第一关 Linux 基础知识"></a>1.1 第一关 Linux 基础知识</h2><h3 id="闯关任务-完成SSH连接与端口映射并运行hello-world-py"><a href="#闯关任务-完成SSH连接与端口映射并运行hello-world-py" class="headerlink" title="闯关任务 完成SSH连接与端口映射并运行hello_world.py"></a>闯关任务 完成SSH连接与端口映射并运行hello_world.py</h3><p>hello_world运行截图：</p><p><img src="/2024/09/13/InternLM/helloWorld2.jpg" alt="运行hello_world.py"></p><p>访问截图：</p><p><img src="/2024/09/13/InternLM/helloWorld1.jpg" alt="访问hello_world.py"></p><h3 id="可选任务2-使用-VSCODE-远程连接开发机并创建一个conda环境"><a href="#可选任务2-使用-VSCODE-远程连接开发机并创建一个conda环境" class="headerlink" title="可选任务2 使用 VSCODE 远程连接开发机并创建一个conda环境"></a>可选任务2 使用 VSCODE 远程连接开发机并创建一个conda环境</h3><p>创建一个名为py310的conda环境，安装python3.10</p><p><img src="/2024/09/13/InternLM/conda.jpg" alt="conda创建"></p><h2 id="1-2-第二关-Python-基础知识"><a href="#1-2-第二关-Python-基础知识" class="headerlink" title="1.2 第二关 Python 基础知识"></a>1.2 第二关 Python 基础知识</h2><h3 id="闯关任务-Python实现wordcount"><a href="#闯关任务-Python实现wordcount" class="headerlink" title="闯关任务 Python实现wordcount"></a>闯关任务 Python实现wordcount</h3><p>wordcount.py代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><br>text = <span class="hljs-string">&quot;&quot;&quot;Hello world!  </span><br><span class="hljs-string">This is an example.  </span><br><span class="hljs-string">Word count is fun.  </span><br><span class="hljs-string">Is it fun to count words?  </span><br><span class="hljs-string">Yes, it is fun!&quot;&quot;&quot;</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">wordcount</span>(<span class="hljs-params">text</span>):<br>    <span class="hljs-comment"># 去除标点符号，只保留字母和空格</span><br>    text = re.sub(<span class="hljs-string">r&quot;[^\w\s]&quot;</span>, <span class="hljs-string">&quot;&quot;</span>, text)<br>    <br>    <span class="hljs-comment"># 将所有字母转换为小写</span><br>    text = text.lower()<br>    <br>    <span class="hljs-comment"># 以空格分割字符串，生成单词列表</span><br>    words = text.split()<br>    <br>    <span class="hljs-comment"># 创建字典统计每个单词出现的次数</span><br>    word_count = defaultdict(<span class="hljs-built_in">int</span>)<br>    <br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>        word_count[word] += <span class="hljs-number">1</span><br>    <br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">dict</span>(word_count)<br><br><span class="hljs-built_in">print</span>(wordcount(text))<br></code></pre></td></tr></table></figure><p>运行结果如下：</p><p><img src="/2024/09/13/InternLM/wordcount.jpg" alt="wordcount运行结果"></p><h3 id="闯关任务-Vscode连接InternStudio-debug笔记"><a href="#闯关任务-Vscode连接InternStudio-debug笔记" class="headerlink" title="闯关任务 Vscode连接InternStudio debug笔记"></a>闯关任务 Vscode连接InternStudio debug笔记</h3><p>debug记录截图如下：</p><p><img src="/2024/09/13/InternLM/wordcount2.jpg" alt="debug"></p><h1 id="2-基础岛"><a href="#2-基础岛" class="headerlink" title="2. 基础岛"></a>2. 基础岛</h1><h2 id="2-1-第1关-书生大模型全链路开源开放体系"><a href="#2-1-第1关-书生大模型全链路开源开放体系" class="headerlink" title="2.1 第1关 书生大模型全链路开源开放体系"></a>2.1 第1关 书生大模型全链路开源开放体系</h2><p>通过下图可以看到这最近一两年国产大模型的迅速成长的发展历史，目前已经逐渐追赶上了国际先进水平，未来可期！</p><p><img src="/2024/09/13/InternLM/interLM-gpt.jpg" alt="InterLM成长路径"></p><p>这里给出的全链路打通的体系视角对于了解大模型相关的技术和产品有很大的帮助。一个完整的大模型体系需要包括的板块有：</p><ul><li>数据</li><li>预训练</li><li>微调</li><li>部署</li><li>评测</li><li>应用</li></ul><p><img src="/2024/09/13/InternLM/puyu.jpg" alt="书生大模型全链路开源开放体系"></p><p>这里的LLM的思维链与最新的<a href="https://openai.com/o1/">OpenAIo1模型</a>具有很大的相似性，可见这确实是一种有效的优化LLM模型能力的方法。</p><p><img src="/2024/09/13/InternLM/interLMplan.jpg" alt="思维链"></p><p><img src="/2024/09/13/InternLM/interLMthink.jpg" alt="思维链2"></p><h2 id="2-2-第2关-8G-显存玩转书生大模型-Demo"><a href="#2-2-第2关-8G-显存玩转书生大模型-Demo" class="headerlink" title="2.2 第2关 8G 显存玩转书生大模型 Demo"></a>2.2 第2关 8G 显存玩转书生大模型 Demo</h2><h3 id="使用-Cli-Demo-完成-InternLM2-Chat-1-8B-模型的部署，并生成-300-字小故事"><a href="#使用-Cli-Demo-完成-InternLM2-Chat-1-8B-模型的部署，并生成-300-字小故事" class="headerlink" title="使用 Cli Demo 完成 InternLM2-Chat-1.8B 模型的部署，并生成 300 字小故事"></a>使用 Cli Demo 完成 InternLM2-Chat-1.8B 模型的部署，并生成 300 字小故事</h3><p><img src="/2024/09/13/InternLM/cli_demo.jpg" alt=" InternLM2-Chat-1.8B 模型 部署"></p><h3 id="使用-LMDeploy-完成-InternLM-XComposer2-VL-1-8B-的部署"><a href="#使用-LMDeploy-完成-InternLM-XComposer2-VL-1-8B-的部署" class="headerlink" title="使用 LMDeploy 完成 InternLM-XComposer2-VL-1.8B 的部署"></a>使用 LMDeploy 完成 InternLM-XComposer2-VL-1.8B 的部署</h3><p><img src="/2024/09/13/InternLM/picUnderstand.jpg" alt="InternLM-XComposer2-VL-1.8B 图文理解"></p><h3 id="使用-LMDeploy-完成-InternVL2-2B-的部署"><a href="#使用-LMDeploy-完成-InternVL2-2B-的部署" class="headerlink" title="使用 LMDeploy 完成 InternVL2-2B 的部署"></a>使用 LMDeploy 完成 InternVL2-2B 的部署</h3><p><img src="/2024/09/13/InternLM/Gradio.jpg" alt="使用 LMDeploy 完成 InternVL2-2B 的部署"></p><h2 id="2-3-第3关-浦语提示词工程实践"><a href="#2-3-第3关-浦语提示词工程实践" class="headerlink" title="2.3 第3关 浦语提示词工程实践"></a>2.3 第3关 浦语提示词工程实践</h2><h3 id="使用LangGPT生成提示词"><a href="#使用LangGPT生成提示词" class="headerlink" title="使用LangGPT生成提示词"></a>使用LangGPT生成提示词</h3><p>在ChatGPT的<a href="https://chatgpt.com/g/g-Apzuylaqk-langgpt-ti-shi-ci-zhuan-jia">LangGPT专家</a>生成提示词。</p><p>提问词如下：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">帮我生成一个两个小数比较专家的提示词，我希望能够准确的指明比较小数的步骤，即先比较小数点前的整数部分，再比较小数点后的小数部分，尤其注意比较小数的时候，需要按位一个一个数字比较。注意提示词应该是英文。<br></code></pre></td></tr></table></figure><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"># <span class="hljs-keyword">Role</span>: Assistant<br><br>## Profile<br>- author: LangGPT <br>- <span class="hljs-keyword">version</span>: <span class="hljs-number">1.0</span><br>- <span class="hljs-keyword">language</span>: English<br>- description: You are tasked <span class="hljs-keyword">with</span> comparing two <span class="hljs-type">decimal</span> numbers step <span class="hljs-keyword">by</span> step. First, compare the <span class="hljs-type">integer</span> parts <span class="hljs-keyword">of</span> the numbers. <span class="hljs-keyword">If</span> they are the same, <span class="hljs-keyword">move</span> <span class="hljs-keyword">on</span> <span class="hljs-keyword">to</span> compare the <span class="hljs-type">decimal</span> parts. <span class="hljs-keyword">When</span> comparing the <span class="hljs-type">decimal</span> parts, compare <span class="hljs-keyword">each</span> digit one <span class="hljs-keyword">by</span> one starting <span class="hljs-keyword">from</span> the first digit <span class="hljs-keyword">after</span> the <span class="hljs-type">decimal</span> <span class="hljs-type">point</span>.<br><br>## Skills<br><span class="hljs-number">1.</span> Ability <span class="hljs-keyword">to</span> compare numbers <span class="hljs-keyword">by</span> analyzing <span class="hljs-keyword">both</span> <span class="hljs-type">integer</span> <span class="hljs-keyword">and</span> <span class="hljs-type">decimal</span> parts.<br><span class="hljs-number">2.</span> <span class="hljs-type">Precision</span> <span class="hljs-keyword">in</span> digit-<span class="hljs-keyword">by</span>-digit comparison <span class="hljs-keyword">of</span> <span class="hljs-type">decimal</span> numbers.<br><span class="hljs-number">3.</span> Clear <span class="hljs-keyword">and</span> logical step-<span class="hljs-keyword">by</span>-step analysis.<br><br>## Rules<br><span class="hljs-number">1.</span> First, compare the <span class="hljs-type">integer</span> parts <span class="hljs-keyword">of</span> the two numbers. <br><span class="hljs-number">2.</span> <span class="hljs-keyword">If</span> the <span class="hljs-type">integer</span> parts are different, <span class="hljs-keyword">return</span> the result based <span class="hljs-keyword">on</span> which <span class="hljs-type">integer</span> <span class="hljs-keyword">is</span> larger <span class="hljs-keyword">or</span> smaller.<br><span class="hljs-number">3.</span> <span class="hljs-keyword">If</span> the <span class="hljs-type">integer</span> parts are the same, proceed <span class="hljs-keyword">to</span> compare the <span class="hljs-type">decimal</span> parts.<br><span class="hljs-number">4.</span> Compare <span class="hljs-keyword">each</span> digit <span class="hljs-keyword">after</span> the <span class="hljs-type">decimal</span> <span class="hljs-type">point</span> one <span class="hljs-keyword">by</span> one, starting <span class="hljs-keyword">from</span> the first <span class="hljs-type">decimal</span> digit.<br><span class="hljs-number">5.</span> <span class="hljs-keyword">If</span> one number has more <span class="hljs-type">decimal</span> digits but <span class="hljs-keyword">all</span> prior digits are the same, the number <span class="hljs-keyword">with</span> the additional non-zero digits <span class="hljs-keyword">is</span> considered larger.<br><span class="hljs-number">6.</span> Ensure the comparison follows a <span class="hljs-keyword">strict</span> left-<span class="hljs-keyword">to</span>-right <span class="hljs-keyword">order</span> <span class="hljs-keyword">when</span> comparing <span class="hljs-type">decimal</span> digits.<br><br>## Workflows<br><span class="hljs-number">1.</span> Receive two <span class="hljs-type">decimal</span> numbers <span class="hljs-keyword">as</span> <span class="hljs-keyword">input</span>.<br><span class="hljs-number">2.</span> Extract <span class="hljs-keyword">and</span> compare the <span class="hljs-type">integer</span> parts <span class="hljs-keyword">of</span> <span class="hljs-keyword">both</span> numbers.<br><span class="hljs-number">3.</span> <span class="hljs-keyword">If</span> the integers are equal, <span class="hljs-keyword">begin</span> comparing the <span class="hljs-type">decimal</span> parts digit <span class="hljs-keyword">by</span> digit.<br><span class="hljs-number">4.</span> <span class="hljs-keyword">Return</span> which number <span class="hljs-keyword">is</span> larger <span class="hljs-keyword">or</span> <span class="hljs-keyword">if</span> they are equal, based <span class="hljs-keyword">on</span> the comparison.<br></code></pre></td></tr></table></figure><h3 id="实验比较"><a href="#实验比较" class="headerlink" title="实验比较"></a>实验比较</h3><p>没有提示词下对比13.11和13.8的大小，其会认为13.11更大。</p><p><img src="/2024/09/13/InternLM/noPrompt.jpg" alt="无提示词下的输出"></p><p>有提示词下，它会一个步骤一个步骤的比较，最终得出13.8更大的正确结论。</p><p><img src="/2024/09/13/InternLM/prompt.jpg" alt="有提示词下的输出"></p><h2 id="2-4-第4关-浦InternLM-LlamaIndex-RAG-实践"><a href="#2-4-第4关-浦InternLM-LlamaIndex-RAG-实践" class="headerlink" title="2.4 第4关 浦InternLM + LlamaIndex RAG 实践"></a>2.4 第4关 浦InternLM + LlamaIndex RAG 实践</h2><h3 id="RAG前的问答"><a href="#RAG前的问答" class="headerlink" title="RAG前的问答"></a>RAG前的问答</h3><p>使用了第一部斗罗大陆的txt文本作为知识库，问答如下：</p><p>问题：斗罗大陆中唐三最后成了什么神?</p><p><img src="/2024/09/13/InternLM/normalAnswer.png" alt="RAG前的回答"></p><p>可见其并不知道唐三最后成了什么神。</p><h3 id="RAG后的问答"><a href="#RAG后的问答" class="headerlink" title="RAG后的问答"></a>RAG后的问答</h3><p><img src="/2024/09/13/InternLM/answerRAG.png" alt="RAG后的回答"></p><p>可以见到经过RAG调整后，模型使用《斗罗大陆》的知识库最后知道了唐三最后成为了海神。</p><h2 id="2-5-第5关-XTuner-微调个人小助手认知"><a href="#2-5-第5关-XTuner-微调个人小助手认知" class="headerlink" title="2.5 第5关 XTuner 微调个人小助手认知"></a>2.5 第5关 XTuner 微调个人小助手认知</h2><p>在XTuner微调之前询问相关问题得到的回答如下：</p><p><img src="/2024/09/13/InternLM/XTunerBefore.jpg" alt="XTuner微调前的回答"></p><p>使用XTuner利用如下的重复数据来进行微调：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;conversation&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;input&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;请介绍一下你自己&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;output&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;我是滑滑蛋同学的小助手，内在是上海AI实验室书生·浦语的1.8B大模型哦&quot;</span><br>        <span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;conversation&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;input&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;你在实战营做什么&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;output&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;我在这里帮助滑滑蛋同学完成XTuner微调个人小助手的任务&quot;</span><br>        <span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br></code></pre></td></tr></table></figure><p>微调后同样的问题的回答如下，可见微调成功：</p><p><img src="/2024/09/13/InternLM/XTunerAfter.jpg" alt="XTuner微调后的回答"></p><p>不过又试了其他的一些问题，发现微调完之后模型过拟合了，只会说这两个问题回答了。</p><h2 id="2-6-第6关-OpenCompass-评测-InternLM-1-8B-实践"><a href="#2-6-第6关-OpenCompass-评测-InternLM-1-8B-实践" class="headerlink" title="2.6 第6关 OpenCompass 评测 InternLM-1.8B 实践"></a>2.6 第6关 OpenCompass 评测 InternLM-1.8B 实践</h2><pre><code class="hljs">注意在安装环境时可能会报错，发现主要是由于按照了numpy的2.x的版本，我换成了1.26.4的版本后就正常了。</code></pre><p>安装好OpenCompass后，使用如下的命令，利用ceval_gen数据集评测InternLM-1.8B模型：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">python run.py<br>--datasets ceval_gen \ <span class="hljs-comment"># 数据集准备</span><br>--models hf_internlm2_chat_1_8b \  <span class="hljs-comment"># 模型准备</span><br>--debug<br></code></pre></td></tr></table></figure><p>评测结果如下：</p><p><img src="/2024/09/13/InternLM/test1.jpg" alt="ceval_gen数据集评测"></p><p>OpenCompass也支持通过代码来控制评测的内容，如下代码定义了通过ceval数据集评测InternLM-1.8B模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> mmengine.config <span class="hljs-keyword">import</span> read_base<br><br><span class="hljs-keyword">with</span> read_base():<br>    <span class="hljs-keyword">from</span> .datasets.ceval.ceval_gen <span class="hljs-keyword">import</span> ceval_datasets<br>    <span class="hljs-keyword">from</span> .models.hf_internlm.hf_internlm2_chat_1_8b <span class="hljs-keyword">import</span> models <span class="hljs-keyword">as</span> hf_internlm2_chat_1_8b_models<br><br>datasets = ceval_datasets<br>models = hf_internlm2_chat_1_8b_models<br></code></pre></td></tr></table></figure><p>使用命令<code>python run.py configs/eval_tutorial_demo.py --debug</code>运行，得到的评测结果与上面一致。</p><h1 id="3-进阶岛"><a href="#3-进阶岛" class="headerlink" title="3. 进阶岛"></a>3. 进阶岛</h1><h2 id="3-1-第1关-探索-InternLM-模型能力边界"><a href="#3-1-第1关-探索-InternLM-模型能力边界" class="headerlink" title="3.1 第1关 探索 InternLM 模型能力边界"></a>3.1 第1关 探索 InternLM 模型能力边界</h2><h2 id="3-2-第2关-Lagent-自定义你的-Agent-智能体"><a href="#3-2-第2关-Lagent-自定义你的-Agent-智能体" class="headerlink" title="3.2 第2关 Lagent 自定义你的 Agent 智能体"></a>3.2 第2关 Lagent 自定义你的 Agent 智能体</h2><p>首先使用默认配置来运行一个Agent，如下，确实可以通过ArxivSearch插件来搜索llama 3相关的论文。</p><p><img src="/2024/09/13/InternLM/langptSearch.jpg" alt="Arxiv搜索"></p><p>然后自定义一个Agent，这个Agent可以调用MagicMaker插件来生成相关的图画，这里是生成了一个国风的二次元美少女图画。</p><p><img src="/2024/09/13/InternLM/langptImage.jpg" alt="图画生成"></p>]]></content>
    
    
    <categories>
      
      <category>书生大模型实战营</category>
      
    </categories>
    
    
    <tags>
      
      <tag>InternLM</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读】{MegaScale}:Scaling Large Language Model Training to More Than 10,000 {GPUs}</title>
    <link href="/2024/08/18/MegaScalePapaerRead/"/>
    <url>/2024/08/18/MegaScalePapaerRead/</url>
    
    <content type="html"><![CDATA[<h1 id="论文基础信息"><a href="#论文基础信息" class="headerlink" title="论文基础信息"></a>论文基础信息</h1><p><strong>论文地址：</strong> <a href="https://www.usenix.org/conference/nsdi24/presentation/jiang-ziheng">{MegaScale}: Scaling Large Language Model Training to More Than 10,000 {GPUs}</a></p><p><strong>收录会议：</strong> 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24)(CCF-A，计算机网络顶级会议)</p><p><strong>作者机构：</strong> 字节跳动x北京大学</p><p><strong>部分组件的开源地址：</strong><a href="https://github.com/volcengine/veScale">veScale</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>MegaScale是字节跳动一个用于训练超过1万台GPU规模的LLM的生产系统。</p><p>为了提高大规模系统的<strong>效率</strong>，他们采用全栈方法，跨<strong>model block</strong>和<strong>优化器</strong>设计、<strong>计算和通信重叠</strong>、<strong>算子优化</strong>、<strong>数据管道</strong>和<strong>网络性能调整</strong>来共同设计算法和系统组件。</p><p>为了提高大规模系统的<strong>稳定性</strong>，他们<strong>加强</strong>了对系统的<strong>可观测性</strong>，开发了一套<strong>诊断工具</strong>来<strong>监控</strong>各个<strong>系统组件和事件</strong>，识别报错根本原因。</p><p>最终在 12,288 个 GPU 上训练 175B LLM 模型时，MegaScale 实现了 55.2% 的模型 FLOP 利用率 (MFU)，<strong>与 Megatron-LM 相比</strong>，<strong>MFU 提高了 1.34 倍</strong>。</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>LLM展示出来的巨大能力越来越被被重视。缩放定律显示了模型大小和训练数据大小是决定模型能力的关键因素。所以人们开始在数千亿乃至数万亿的token上训练具有数千亿甚至数万亿参数的大型模型。LLM领域的主要参与者构建了拥有数万个 GPU 的大规模 AI 集群来训练LLM。传统虽然也有管理过万台GPU，但是那都是面对多个小任务，而现在LLM一个任务就占据了数万台GPU，这带来了<strong>两个挑战</strong>：</p><ul><li><strong>如何提高训练效率</strong>。其中主要指标看的是FLOP模型利用率，即假设峰值 FLOP 为 100% 时观察到的吞吐量与理论最大吞吐量的比率。这挑战包括了通信、算子优化、数据预处理、GPU内存消耗等。</li><li><strong>如何提高稳定性</strong>。训练过程中故障和掉队是LLM训练的常态，但是其故障的代价也是非常高昂的，所以减少恢复时间至关重要；而掉队不仅影响自己的工作，还会减慢涉及数万个GPU的整个工作。</li></ul><p>为了解决这两个挑战，提出了<strong>两个系统设计原则</strong>：</p><ul><li><strong>算法-系统协同设计。</strong>其<ul><li>对<strong>模型架构</strong>进行了一些修改并融入了有效的优化技术，包括parallel transformer、slide window attention、LAMB optimizer。</li><li>利用<strong>混合并行</strong>策略，将<strong>数据并行</strong>、<strong>piple并行</strong>、<strong>张量并行</strong>和<strong>序列并行</strong>相结合。并根据每个并行策略的模式设计定制技术，以最大化通信和计算之间的重叠。</li><li><strong>应用预取</strong>和<strong>基于树的加载</strong>来<strong>优化数据管道</strong>。</li><li>我们利用<strong>非阻塞异步操作</strong>，消除<strong>大规模集体通信组初始化</strong>的全局障碍。</li><li>设计<strong>自定义网络拓扑</strong>，<strong>减少 ECMP 哈希冲突</strong>，<strong>自定义拥塞控制</strong>，并<strong>调整重传超时参数</strong>以获得高网络性能。</li></ul></li><li><strong>深度可观测性。</strong><ul><li>许多硬稳定性问题<strong>只会大规模出现</strong>，这可能源于堆栈深处的各种软件和硬件故障。考虑到系统的规模和复杂性，<strong>手动识别和解决每个问题是不可行的</strong>。</li><li>我们应用深度可观察性的原理<strong>构建了</strong>一套<strong>诊断工具</strong>。我们所说的“<strong>深度可观察</strong>性”是指一种全面的监控和可视化策略，它超越表面指标，收集系统堆栈每个组件的详细、精细的数据，旨在创建系统性能的多维视图。这套工具使我们能够通过揭示复杂的相互作用和依赖机制来诊断系统并确定根本原因。</li><li>我们开发了一个强大的训练框架来<strong>自动化故障定位和恢复</strong>。我们设计了<strong>封装</strong>各种形式信息的<strong>心跳消息</strong>，以方便实时异常检测并<strong>提供早期预警</strong>。我们实施了一<strong>套诊断测试</strong>来<strong>识别导致中断的节点</strong>。我们<strong>优化检查点和恢复程序</strong>以<strong>减少中断</strong>。为了解决<strong>掉队</strong>造成的细微差别，我们开发了一个<strong>性能分析工具</strong>来<strong>记录细粒度的 CUDA 事件</strong>并从分布式视图<strong>生成系统范围的热图和时间线跟踪</strong>，并开发了一个 <strong>3D 并行训练可视化工具</strong>来显示排名之间的数据依赖关系诊断。</li></ul></li></ul><h1 id="大规模高效训练"><a href="#大规模高效训练" class="headerlink" title="大规模高效训练"></a>大规模高效训练</h1><h2 id="模型算法优化"><a href="#模型算法优化" class="headerlink" title="模型算法优化"></a>模型算法优化</h2><p><strong>并行transformer</strong></p><p>采取之前使用过的并行transformer架构，如下的一个变化：</p><p><img src="/2024/08/18/MegaScalePapaerRead/transformer.png" alt="transformer"></p><p>这可以使得MLP块和Attention块并行进行。</p><p><strong>滑动窗口注意力（SWA）</strong></p><p>滑动窗口注意力是一种稀疏注意力机制，它采用围绕输入序列中每个标记的固定大小的窗口。计算复杂度为 O(s × w)，其中 s 是输入序列长度，w 是固定窗口大小。滑动窗口注意力比完全自注意力更有效，其计算复杂度为 O(s × s)，假设 w ≪ s。</p><p><strong>LAMB优化器</strong></p><p>为了提高批量大小从而进行大规模高效训练，我们采用LAMB 优化器，其已被证明可以在不影响准确性的情况下将 BERT 的训练批量大小扩展到 64K。在 LLM 设置中，我们的实验发现 LAMB 可以将批量大小扩展到 4 倍，而不会损失精度。</p><h2 id="3D并行中的通信折叠"><a href="#3D并行中的通信折叠" class="headerlink" title="3D并行中的通信折叠"></a>3D并行中的通信折叠</h2><p><strong>数据并行优化</strong></p><p>针对数据并行中从其他模型更新参数的前向传递的all-gather和在收集梯度的后向传递的reduce-scatter操作，由于all-gather 操作在模型块的前向传递之前触发，reduce-scatter 操作在其向后传递后开始。这导致了第一个全收集操作和最后一个reduce-scatter操作无法隐藏的挑战。受 PyTorch FSDP 的启发，初始all-gather 操作在每次迭代开始时预取，使其与数据加载操作重叠，有效地将通信时间减少 1&#x2F;(2 * vpp_size) 。我们还首先启动高优先级通信以最大化重叠。通信算子的优先级由依赖于通信结果的相应计算算子的顺序确定。</p><p><strong>pipline并行重叠优化</strong></p><p>对于warm-up阶段，我们将发送和接收解耦，使得操作可以和计算重叠，同时也避免了被较慢的发送和接收所阻塞。</p><p>对于cool-down阶段，作为warma-up的逆过程，我们逆向应用相关优化。</p><p>对于steady阶段，由于前向和后向计算都独立于相邻的通信操作，所以使发送和接收操作异步启动以与计算重叠</p><p><strong>张量&#x2F;序列并行性重叠</strong></p><p>张量并行通常用于计算密集型操作中的权重划分，而 LayerNorm 和 Dropout 等操作则沿序列维度进行划分以节省 GPU 内存。这就需要对 GPU 之间的输入收集和输出重新分配进行all-gather和reduce-scatter操作。</p><p>下图 a 显示了并行transformer架构中的这种通信模式。在这里，两个通信操作处于关键路径上。为了消除这种开销，我们选择将all-gather和reduce-scatter与 FFN 路径上的并行线性融合（下图 b）。由于FFN路径上的GEMM核较大，因此可以更好地隐藏通信。我们将 GEMM 内核分成小块，并通过通信管道执行（下图c）。该策略可以类似地应用于向后传递</p><p><img src="/2024/08/18/MegaScalePapaerRead/parallel.png" alt="并行优化"></p><h2 id="算子优化"><a href="#算子优化" class="headerlink" title="算子优化"></a>算子优化</h2><p>采用flash-attention2来通过改进不同线程块和线程束之间的工作划分对transformer的进行优化。</p><p>对于 LayerNorm 和 GeLU算子还进行了算子融合的优化，从而优化了内存的访问，实现了更好的性能。</p><h2 id="数据Pipeline优化"><a href="#数据Pipeline优化" class="headerlink" title="数据Pipeline优化"></a>数据Pipeline优化</h2><p>目标是隐藏数据预处理和数据加载的时间消耗，从而减少GPU的空闲。手段如下：</p><ul><li><strong>异步数据预处理：</strong>当 GPU 工作线程在每个训练步骤结束时同步梯度时，可以开始后续步骤的数据预处理，从而隐藏了预处理开销。</li><li><strong>减少冗余数据加载：</strong>传统分布式训练的典型数据加载阶段，每个GPU worker都有自己的数据加载器，通过将数据读取到CPU中再传递到GPU中，多个work的读取会导致磁盘带宽读取的瓶颈。考虑到一台机器上的GPU worker都在同一个张量并行组中，所以每次迭代输入的数据都相同，故在每台机器上使用单个专用的数据加载器将数据读取到共享内存中，然后GPU worker直接将必要的数据复制到自己的GPU内存中进行处理。</li></ul><h2 id="集体通讯组初始化"><a href="#集体通讯组初始化" class="headerlink" title="集体通讯组初始化"></a>集体通讯组初始化</h2><p>在初始化时，需要在各个GPU worker之间建立NVIDIA集体通信组（NCCL），默认实现在2048个GPU上的初始化需要1047秒，虽然这对于训练时间微不足道，但是却对日常测试开发造成了影响，还阻碍了快速重启和恢复机制，故对其进行了优化。</p><ul><li><strong>优化barrier operation：</strong>采用异步非阻塞的Redis替换单线程阻塞的TCPStore。这将2048个GPU上的初始化时间减少到了361秒。</li><li><strong>优化global barrier：</strong>通过设计通信组初始化的顺序，以尽量减少对global barrier的需要。这种方法将全局屏障的时间复杂度从 O(n2) 降低到 O(n)。通过这些优化，初始化时间在 2048 个 GPU 上减少到 5 秒以下，在超过 10,000 个 GPU 上减少到 30 秒以下。</li></ul><h2 id="网络性能调优"><a href="#网络性能调优" class="headerlink" title="网络性能调优"></a>网络性能调优</h2><p>主要优化的手段包括以下这些（不做详细介绍了）：</p><ul><li><strong>优化网络拓扑：</strong>数据中心网络采用基于 Broadcom Tomahawk 4 芯片的高性能交换机构建。</li><li><strong>减少 ECMP 哈希冲突。</strong>精心设计了网络拓扑并调度网络流量，以减少 ECMP 哈希冲突。</li><li><strong>拥塞控制：</strong>开发了一种结合了 Swift  和 DCQCN 原理的算法，将往返时间 (RTT) 的精确测量与显式拥塞通知 (ECN) 的快速拥塞响应功能集成在一起。</li><li><strong>重传超时设置：</strong>重新设置NCCL中的参数来控制重传定时器和重试计数，并在 NIC 上启用了 adap_retrans 功能。</li></ul><h1 id="大规模训练容错"><a href="#大规模训练容错" class="headerlink" title="大规模训练容错"></a>大规模训练容错</h1><h2 id="鲁棒性的训练工作流"><a href="#鲁棒性的训练工作流" class="headerlink" title="鲁棒性的训练工作流"></a>鲁棒性的训练工作流</h2><p>如下图所示，用户将训练任务提交给Driver后，他与k8s进行交互，为每个Executor分配资源并启动相应的pod，一个Exector关联一个node。Exector需要定期通过心跳给Driver上报信息，如果收到了异常信息或者超时未收到信息，那么就会停止所有Exector的任务执行，并启动自检程序，识别到有问题的节点后，将相关节点上报给k8s进行驱逐，并补充等量的通过了健康检查的节点。</p><p><img src="/2024/08/18/MegaScalePapaerRead/workflow.png" alt="容错工作流"></p><h2 id="监控数据收集和分析"><a href="#监控数据收集和分析" class="headerlink" title="监控数据收集和分析"></a>监控数据收集和分析</h2><p><strong>心跳信息</strong>包括：</p><ol><li>Exector基本信息</li><li>训练的日志信息，用来识别警告、错误等关键字</li><li>RDMA流量指标，有些异常发生后，训练还是能继续进行，这时就可以通过分析RDMA流量指标来分析，因为训练具有周期性，一般流量情况都相似，如果出现异常波动，就需要进行人工排除或启动自动恢复程序了。</li></ol><p>为了加强对<strong>训练稳定性和表现的监控</strong>，还开发了精度达到毫秒级的监控系统：</p><ol><li>二级监控通常用于评估整体健康状况并排除常见配置对训练的影响。</li><li>毫秒级的监控也用于判断网络是否拥塞，数据并行和管道并行的数据传输速度是否达到物理极限。</li></ol><h2 id="诊断测试"><a href="#诊断测试" class="headerlink" title="诊断测试"></a>诊断测试</h2><p>自检诊断的执行时间和准确性之间存在权衡。通过迭代实验和优化，我们部署了一套轻量级诊断测试，有效覆盖了实际训练过程中遇到的各种硬件和软件故障。</p><p><strong>主机内网络测试：</strong></p><ol><li>环回测试测量从所有 RDMA NIC (RNIC) 到各种主机内端点（包括内存节点和 GPU）的环回带宽。</li><li>RNICto-RNIC 测试检查同一主机上不同 RNIC 之间的连接和带宽性能。</li></ol><p><strong>NCCL 测试：</strong></p><ol><li>为了识别 GPU 通信中的潜在故障，我们在单个节点内的 GPU 之间运行全面测试，以观察带宽是否符合预期基准。</li><li>一旦主机内通信测试通过，每个节点还会与同一 ToR 交换机下的相邻机器进行 all-reduce 测试，以评估节点间 GPU 通信。</li></ol><h2 id="快速检测点和恢复"><a href="#快速检测点和恢复" class="headerlink" title="快速检测点和恢复"></a>快速检测点和恢复</h2><p>目的是为了确保最新的<strong>检查点尽可能接近故障发生时的训练进度状态</strong>，同时还希望<strong>减少检查点过程引入的延迟</strong>，特别是关键路径上阻碍训练进度的时间，从而阻碍整体系统吞吐量。</p><p>为了<strong>快速构建检查点</strong>，引入了一种优化的两阶段方法：</p><ol><li>第一阶段：每个<strong>GPU将其片上状态写入到主机内存中</strong>，然后继续训练，过程可缩短到几秒钟</li><li>第二阶段：<strong>后台进程接管</strong>，将主机内存异步<strong>传输到分布式文件系统</strong>（HDFS）中</li></ol><p>为了优化从<strong>检查点恢复</strong>的过程，由于其<strong>瓶颈</strong>主要在于<strong>HDFS的带宽</strong>，又意识到多个<strong>GPU worker</strong>通常都<strong>共享相同的状态分区</strong>，如同一个数据并行组的worker，为了<strong>避免重复读</strong>，一组中只有<strong>一个worker从HDFS读取</strong>，然后再将其<strong>状态广播</strong>给其他需要的GPU worker。</p><h1 id="大规模训练故障排除"><a href="#大规模训练故障排除" class="headerlink" title="大规模训练故障排除"></a>大规模训练故障排除</h1><p>尽管我们强大的训练框架可以自动发现、查明并解决大多数常见故障，但仍然存在<strong>某些硬件异常现象</strong>，这些异常现象以<strong>概率的形式出现</strong>，<strong>无法通过机器自检发现</strong>。一些异常可能会使系统看似正常运行，但<strong>会显着降低训练效率</strong>。为了解决这些细微差别的情况，我们<strong>实现了多种自定义监控和分析工具</strong>，旨在支持逐案异常检测。</p><h2 id="使用-CUDA-事件监视器进行性能诊断"><a href="#使用-CUDA-事件监视器进行性能诊断" class="headerlink" title="使用 CUDA 事件监视器进行性能诊断"></a>使用 CUDA 事件监视器进行性能诊断</h2><p>在大规模实验中观察到不同的running表现出不同的计算效率。即使使用相同的配置，这种不一致仍然存在，如下图所示。我们还观察到，训练任务的性能在此规模上并不一致。各种训练任务的 MFU 随着时间的推移逐渐下降。但是在单 GPU GEMM 微基准测试下没有检测到明显的差异。</p><p><img src="/2024/08/18/MegaScalePapaerRead/performance.png" alt="性能差异"></p><p>为了诊断这种性能问题，我们开发了一种性能分析工具，用于记录运行期间每个机器级别上关键代码段的执行时间。其基于CUDA事件方法对事件进行计时，以防止性能损失并提供两种可视化模式，可以从不同角度分析收集到的数据。</p><ul><li>第一种模式使用热图来显示<strong>不同维度的机器之间的时间消耗差异</strong>。由于训练性能主要由最慢的主机所决定，所以在观察到了<strong>性能表现较差的主机后可以将其驱逐出去</strong>。</li><li>第二种模式<strong>从不同的分布式视图（数据并行性、管道并行性、张量并行性）以跟踪格式显示机器上的事件时间线。</strong>通过将各种等级的跟踪跨度聚合到单个时间线上，我们获得了全面的视角，揭示了数据并行等级之间的整体执行顺序、管道气泡和同步特征。</li></ul><p>当计时器数据以逐行格式写入本地文件时，一个单独的流处理进程会将该日志文件与 Kafka 队列实时同步。分析数据库通过使用来自该 Kafka 队列的数据来保持更新，从而在不中断训练作业的情况下实现动态分析。</p><h2 id="3D-并行训练可视化"><a href="#3D-并行训练可视化" class="headerlink" title="3D 并行训练可视化"></a>3D 并行训练可视化</h2><p>由于3D并行技术的使用以及前述的优化技术的使用，<strong>数据流和任务排序变得极其复杂</strong>，各个GPU worker之间存在<strong>复杂依赖关系</strong>，这加大了故障检测的挑战：</p><p><strong>单个GPU worker故障</strong>，可能会导致<strong>整个集群的NCCL通信停滞</strong>，从而导致<strong>大量的超时的消息</strong>。<strong>为了快速排除</strong>，我们让每个GPU worker在通信<strong>超时的时候</strong>记<strong>录自己正在进行的事件</strong>。然后，使用这些日志根据 3D 并行设置中的逻辑拓扑<strong>构建数据依赖关系的可视化表示</strong>。从而支持<strong>快速查明有问题的节点，</strong>一旦确定，这些节点可以通过强大的训练框架手动隔离和标记以进行维护。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>这里指出MegaScale 构建于 <a href="https://github.com/NVIDIA/Megatron-LM/tree/main">Megatron-LM</a> 之上，通过各种实验展示了MegaScale的优越性。其中在 12,288 个 GPU 上训练 175B LLM 模型时，MegaScale 实现了 55.2% MFU，比 Megatron-LM 提高了 1.34 倍。</p>]]></content>
    
    
    <categories>
      
      <category>AI集群</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>分布式训练</tag>
      
      <tag>LLM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读】Fluid:Dataset Abstraction and Elastic Acceleration for Cloud-native Deep Learning Training Jobs</title>
    <link href="/2024/08/17/FluidPaper/"/>
    <url>/2024/08/17/FluidPaper/</url>
    
    <content type="html"><![CDATA[<h1 id="论文基础信息"><a href="#论文基础信息" class="headerlink" title="论文基础信息"></a>论文基础信息</h1><p><strong>论文地址：</strong> <a href="https://ieeexplore.ieee.org/document/9835158">Fluid: Dataset Abstraction and Elastic Acceleration for Cloud-native Deep Learning Training Jobs</a></p><p><strong>收录会议：</strong> 2022 IEEE 38th International Conference on Data Engineering (ICDE)(CCF-A，数据库领域顶会)</p><p><strong>作者机构：</strong> 阿里云X南京大学</p><p><strong>开源项目地址：</strong><a href="https://github.com/fluid-cloudnative/fluid">fluid</a></p><p><strong>其他：</strong> 期刊版的论文：<a href="https://ieeexplore.ieee.org/document/10249214">High-Level Data Abstraction and Elastic Data Caching for Data-Intensive AI Applications on Cloud-Native Platforms</a></p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>深度学习快速发展，其既需要计算也需要数据，同时越来越倾向采用容器化和云原生技术，但是这种环境下存在计算与存储分离的特点，而一般从远端读取数据过慢，会存在<strong>I&#x2F;O瓶颈</strong>。<br>现有机器学习框架有采用了I&#x2F;O与GPU计算重叠的方式，但是这不足以消除瓶颈，而其他的预读取、内存缓存、多层次缓存等方式更适用于传统集群而不是云原生环境，主要不足在于：</p><ol><li>云中具有不同访问协议的<strong>异构存储</strong>方法，其导致<strong>程序复杂</strong>。缓存系统<strong>配置复杂</strong>，鉴于各种云环境下数据集的不同及用户理解的困难，<strong>难以配置性能良好的缓存系统</strong>。</li><li>云中的<strong>计算资源</strong>具有高度<strong>灵活性</strong>和<strong>弹性，</strong>这要求<strong>缓存</strong>也应随其<strong>自动拓展</strong>，而如何及时<strong>准确估计缓存需求</strong>、<strong>高效动态拓展</strong>是一个难题。</li><li><strong>同一数据集</strong>训练<strong>多个模型</strong>很流行，如何在多个深度学习作业之间<strong>有效共享数据</strong>存在挑战，这要求<strong>降低共享数据缓存的大小</strong>，<strong>避免</strong>频繁的数据<strong>缓存抖动</strong>，从而提高整体训练性能。</li></ol><h1 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h1><ol><li>将<strong>数据集进行抽象</strong>（提供了Fluid Dataset的CRD），并以PVC的统一形式提供用户使用。同时每个Fluid Dataset都由自动调整的Cache Runtime（如使用开源的Alluxio系统）提供缓存支持。</li><li>设计了一种基于<strong>训练速度跟踪</strong>的<strong>缓存自动拓展机制</strong>，以自动调整缓存容量，匹配训练速度和计算资源。</li><li>提出了一种<strong>数据缓存和DL作业协同感知</strong>的<strong>调度策略</strong>，通过高效的缓存机制<strong>加速</strong>具有某些<strong>相同数据集</strong>的<strong>多个DL训练作业</strong>的整体性能。</li><li>实验结果表明Fluid具有优越的性能，将Fluid 与现有的广泛使用和前沿解决方案集成时<strong>可将性能提高约 2 倍</strong>。</li></ol><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="数据集抽象和管理"><a href="#数据集抽象和管理" class="headerlink" title="数据集抽象和管理"></a>数据集抽象和管理</h2><p>Fluid Dataset作为数据集的抽象，Cache Runtime作为Cache的抽象，<strong>一个Cache Runtime与Fluid Dataset一一对应</strong>。如此设计更加灵活，相比于统一的缓存方案，其<strong>优点</strong>在于：</p><ol><li>分解的缓存管理器<strong>不太可能成为瓶颈</strong>，尤其是当许多文件元数据操作处于<strong>高度并行时</strong>；</li><li>这允许我们<strong>根据不同数据集的特点调整</strong>分布式缓存系统的<strong>配置</strong>，从而达到最优的配置。</li></ol><h2 id="分布式缓存自动配置"><a href="#分布式缓存自动配置" class="headerlink" title="分布式缓存自动配置"></a>分布式缓存自动配置</h2><p><strong>系统配置</strong>（例如<strong>线程池大小</strong>和<strong>元数据缓存级别</strong>）对于分布式缓存系统<strong>非常重要</strong>，因为它们会极大地影响不同类型的文件操作的性能。所以这里将候选配置、Dataset特征输入到一个预训练好的<strong>模型</strong>进行<strong>打分</strong>，然后选择最高得分的配置。<br><img src="/2024/08/17/FluidPaper/config.png" alt="配置打分"></p><h2 id="动态缓存系统的自动拓展"><a href="#动态缓存系统的自动拓展" class="headerlink" title="动态缓存系统的自动拓展"></a>动态缓存系统的自动拓展</h2><p>为了实时测量深度学习作业的训练速度，协调器<strong>收集</strong>在时间窗口T内总共<strong>训练的样本数</strong>SampleNum，从而<strong>得到训练速度</strong>为SampleNum&#x2F;T<br>Fluid通过反复实验的方式拓展缓存容量，其算法如下，主要根据训练速度不断调整。<br><img src="/2024/08/17/FluidPaper/alg.png" alt="拓展算法"></p><h2 id="基于共享的调度策略"><a href="#基于共享的调度策略" class="headerlink" title="基于共享的调度策略"></a>基于共享的调度策略</h2><p>旨在<strong>优先运行已缓存的作业</strong>，同时通过考虑数据集的特征来推测DL作业的运行时间。<br>调度打分如下，选取得分最高的进行调度。<br><img src="/2024/08/17/FluidPaper/score.png" alt="调度打分"></p><h1 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h1><p><img src="/2024/08/17/FluidPaper/arc.png" alt="具体实现"></p><h2 id="Fluid-Controller"><a href="#Fluid-Controller" class="headerlink" title="Fluid Controller"></a>Fluid Controller</h2><p>将Fluid Dataset和Cache Runtime定义为CRD，再定义Fluid COntroller为operator，Fluid COntroller负责绑定两者，同时配置的ML模型也在其中，一旦缓存就位，就会创建相应的PV和PVC，用户通过引用PVC来访问数据文件。</p><h2 id="Fluid-Csi插件"><a href="#Fluid-Csi插件" class="headerlink" title="Fluid Csi插件"></a>Fluid Csi插件</h2><p>Kubernetes 提供了容器存储接口(CSI) 。当任何容器需要通过分布式缓存系统访问数据时，它会引用相应的 PVC，然后 Fluid CSI 插件负责将分布式缓存系统挂载到容器上。</p><h2 id="缓存系统自动缩放器"><a href="#缓存系统自动缩放器" class="headerlink" title="缓存系统自动缩放器"></a>缓存系统自动缩放器</h2><p>缩放器就是前诉的协调器，它从DL工作者那里收集实时训练速度，并指示分布式缓存系统动态扩展缓存容量，以容纳与 DL 作业训练速度相匹配的训练示例数量。此外，如果缓存容量不足以容纳所有预取的训练示例，缓存系统自动缩放器将添加更多缓存工作者，以动态增加分布式缓存系统的可用缓存容量。此外，我们还通过逐出已使用的训练示例来优化缓存使用率，因为它们不会在同一时期被使用。</p><h2 id="Fluid调度器"><a href="#Fluid调度器" class="headerlink" title="Fluid调度器"></a>Fluid调度器</h2><p>该调度器基于K8s的原生调度器容器实现，通过 Kubernetes APIServer，调度器定期监视所有节点的资源状态以及缓存系统的状态，当一些资源被释放且可以调度一批深度学习作业时，Fluid 调度器会根据依赖数据集等因素对所有待处理的深度学习作业进行评分，并将得分最高的深度学习作业提交给容器调度器。<br>使用实例<br>一个简单的使用实例如下：<br><img src="/2024/08/17/FluidPaper/example.png" alt="使用实例"></p>]]></content>
    
    
    <categories>
      
      <category>AI集群</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>集群缓存</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>设置Clash代理，彻底解决linux系统Docker pull的问题</title>
    <link href="/2024/06/23/ClashForDocker/"/>
    <url>/2024/06/23/ClashForDocker/</url>
    
    <content type="html"><![CDATA[<p>因为当前政策的收紧，很多docker镜像网站和加速器都不能使用了，虽然目前还有一些可以使用，但是打游击战终究是不可靠的，在可以预见的将来肯定会更加难以下载docker镜像，而我也厌烦了各种docker pull失败，所以这里直接使用clash设置代理，翻入墙外来彻底解决这个问题。</p><h1 id="安装配置Clash"><a href="#安装配置Clash" class="headerlink" title="安装配置Clash"></a>安装配置Clash</h1><h2 id="1-下载Clash"><a href="#1-下载Clash" class="headerlink" title="1. 下载Clash"></a>1. 下载Clash</h2><ul><li>由于原Clash已经删库跑路，所以可以使用<a href="https://drive.rocklinuxmirror.eu.org/index.php/s/5MndEnkeqzpiiS2/download?path=/&files=clash-linux-amd64-v1.18.0.gz">镜像地址</a>下载linux版本的clash。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@localhost ~]<span class="hljs-comment"># wget https://drive.rocklinuxmirror.eu.org/index.php/s/5MndEnkeqzpiiS2/download?path=%2F&amp;files=clash-linux-amd64-v1.18.0.gz</span><br></code></pre></td></tr></table></figure><h2 id="2-配置并运行clash"><a href="#2-配置并运行clash" class="headerlink" title="2. 配置并运行clash"></a>2. 配置并运行clash</h2><ul><li><p>获取订阅，如果没有clash的订阅的话 可以用我现在在用的这个<a href="https://xyz5.tigr05.xyz/#/register?code=HvGGi2Fu">tigr</a>，还是挺优惠的，也很稳定。</p><p>  <strong>注意：</strong>如果原本使用的是windows的Clash，也可以在Profiles中右键单击Edit获得。</p></li></ul><p><img src="/2024/06/23/ClashForDocker/clashConfig.png" alt="配置文件"></p><p>配置文件的内容大致如下，port、socks-port是相关流量转发的接口，external-controller是控制平面的接口，可以酌情修改，proxies下面的内容就是机场提供的接口了。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">port:</span> <span class="hljs-number">7890</span><br><span class="hljs-attr">socks-port:</span> <span class="hljs-number">7891</span><br><span class="hljs-attr">allow-lan:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">mode:</span> <span class="hljs-string">rule</span><br><span class="hljs-attr">log-level:</span> <span class="hljs-string">info</span><br><span class="hljs-attr">external-controller:</span> <span class="hljs-string">&#x27;0.0.0.0:9999&#x27;</span><br><span class="hljs-attr">experimental:</span><br>    <span class="hljs-attr">ignore-resolve-fail:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">dns:</span><br>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">ipv6:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">enhanced-mode:</span> <span class="hljs-string">fake-ip</span><br>    <span class="hljs-attr">nameserver:</span> [<span class="hljs-number">1.2</span><span class="hljs-number">.4</span><span class="hljs-number">.8</span>, <span class="hljs-number">223.5</span><span class="hljs-number">.5</span><span class="hljs-number">.5</span>]<br>    <span class="hljs-attr">fallback:</span> [<span class="hljs-number">1.2</span><span class="hljs-number">.4</span><span class="hljs-number">.8</span>, <span class="hljs-number">223.5</span><span class="hljs-number">.5</span><span class="hljs-number">.5</span>]<br><span class="hljs-attr">proxies:</span><br><span class="hljs-string">-...</span><br></code></pre></td></tr></table></figure><ul><li>命令执行过程（仅供参考,请按本地设置稍作调整）</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@localhost ~]<span class="hljs-comment"># su</span><br>[root@localhost ~]<span class="hljs-comment"># mkdir /opt/clash</span><br>[root@localhost ~]<span class="hljs-comment"># gunzip clash-linux-amd64-v1.18.0.gz</span><br>[root@localhost ~]<span class="hljs-comment"># mv Desktop/clash-linux-amd64-v1.18.0 /opt/clash/clash</span><br>[root@localhost ~]<span class="hljs-comment"># cd /opt/clash/</span><br>[root@localhost clash]<span class="hljs-comment"># wget -O config.yaml [订阅链接]</span><br>--2021-05-01 22:39:37--  [订阅链接]<br>Resolving www.sub-speeder.com (www.sub-speeder.com)... 104.21.18.176, 172.67.182.209, 2606:4700:3035::ac43:b6d1, ...<br>Connecting to www.sub-speeder.com (www.sub-speeder.com)|104.21.18.176|:443... connected.<br>HTTP request sent, awaiting response... 200 OK<br>Length: 44830 (44K) [application/octet-stream]<br>Saving to: ‘config.yaml’<br><br>100%[======================================&gt;] 44,830       287KB/s   <span class="hljs-keyword">in</span> 0.2s<br><br>2021-05-01 22:39:38 (287 KB/s) - ‘config.yaml’ saved [44830/44830]<br><br>[root@localhost clash]<span class="hljs-comment"># wget -O Country.mmdb &lt;https://cdn.jsdelivr.net/gh/Hackl0us/GeoIP2-CN@release/Country.mmdb&gt;</span><br>--2021-05-01 22:39:55--  &lt;https://www.sub-speeder.com/client-download/Country.mmdb&gt;<br>Resolving www.sub-speeder.com (www.sub-speeder.com)... 172.67.182.209, 104.21.18.176, 2606:4700:3035::ac43:b6d1, ...<br>Connecting to www.sub-speeder.com (www.sub-speeder.com)|172.67.182.209|:443... connected.<br>HTTP request sent, awaiting response... 200 OK<br>Length: 3878104 (3.7M) [application/octet-stream]<br>Saving to: ‘Country.mmdb’<br><br>100%[======================================&gt;] 3,878,104   2.86MB/s   <span class="hljs-keyword">in</span> 1.3s<br><br>2021-05-01 22:39:58 (2.86 MB/s) - ‘Country.mmdb’ saved [3878104/3878104]<br>[root@localhost clash]<span class="hljs-comment"># chmod +x clash</span><br>[root@localhost clash]<span class="hljs-comment"># ll</span><br>total 19832<br>-r-x--x--x. 1 root root 9326592 May  1 23:24 clash<br>-rw-r--r--. 1 root root   44830 May  1 22:39 config.yaml<br>-rw-r--r--. 1 root root 3878104 Oct  9  2020 Country.mmdb<br>[root@localhost clash]<span class="hljs-comment"># ./clash -d .</span><br>INFO[0000] Start initial compatible provider Proxy<br>INFO[0000] Start initial compatible provider Domestic<br>INFO[0000] Start initial compatible provider AsianTV<br>INFO[0000] Start initial compatible provider Others<br>INFO[0000] Start initial compatible provider GlobalTV<br><br></code></pre></td></tr></table></figure><h2 id="3-启用系统代理"><a href="#3-启用系统代理" class="headerlink" title="3. 启用系统代理"></a>3. 启用系统代理</h2><ul><li>命令行形式开启就是通过设置相关环境变量即可</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@localhost ~]<span class="hljs-comment"># export https_proxy=http://127.0.0.1:7890 http_proxy=http://127.0.0.1:7890 all_proxy=socks5://127.0.0.1:7890</span><br></code></pre></td></tr></table></figure><ul><li>图形化界面形式开启（装有GUI使用）</li></ul><p>打开系统设置，点击网络代理右边的 ⚙ 按钮，选择手动，填写 HTTP 和 HTTPS 代理为 127.0.0.1:7890，填写 Socks 主机为 127.0.0.1:7891，即可启用系统代理，如下图所示。</p><p><img src="/2024/06/23/ClashForDocker/clashSet.png" alt="图形化设置"></p><h2 id="4-测试验证"><a href="#4-测试验证" class="headerlink" title="4. 测试验证"></a>4. 测试验证</h2><ul><li>通过curl命令访问<a href="http://www.google.com,响应正常./">www.google.com，响应正常。</a></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml">[<span class="hljs-string">root@localhost</span> <span class="hljs-string">~</span>]<span class="hljs-comment"># curl www.google.com</span><br><span class="hljs-string">&lt;!doctype</span> <span class="hljs-string">html&gt;&lt;html</span> <span class="hljs-string">itemscope=&quot;&quot;</span> <span class="hljs-string">itemtype=&quot;http://schema.org/WebPage&quot;</span> <span class="hljs-string">lang=&quot;en&quot;&gt;&lt;head&gt;&lt;meta</span> <span class="hljs-string">content=&quot;Search</span> <span class="hljs-string">the</span> <span class="hljs-string">world&#x27;s</span> <span class="hljs-string">information,</span> <span class="hljs-string">including</span> <span class="hljs-string">webpages,</span> <span class="hljs-string">images,</span> <span class="hljs-string">videos</span> <span class="hljs-string">and</span> <span class="hljs-string">more.</span> <span class="hljs-string">Google</span> <span class="hljs-string">has</span> <span class="hljs-string">many</span> <span class="hljs-string">special</span> <span class="hljs-string">features</span> <span class="hljs-string">to</span> <span class="hljs-string">help</span> <span class="hljs-string">you</span> <span class="hljs-string">find</span> <span class="hljs-string">exactly</span> <span class="hljs-string">what</span> <span class="hljs-string">you&#x27;re</span> <span class="hljs-string">looking</span> <span class="hljs-string">for.&quot;</span> <span class="hljs-string">name=&quot;description&quot;&gt;&lt;meta</span> <span class="hljs-string">content=&quot;noodp,</span> <span class="hljs-string">&quot; name=&quot;</span><span class="hljs-string">robots&quot;&gt;&lt;meta</span> <span class="hljs-string">content=&quot;text/html;...</span><br></code></pre></td></tr></table></figure><ul><li>还可以通过访问<a href="https://clash.razord.top/#/connections%E6%9D%A5%E7%94%A8%E6%B5%8F%E8%A7%88%E5%99%A8%E5%AF%B9clash%E8%BF%9B%E8%A1%8C%E7%AE%A1%E7%90%86%EF%BC%8C%E8%BF%9B%E5%85%A5%E7%BD%91%E7%AB%99%E6%97%B6%E9%9C%80%E8%A6%81%E8%BE%93%E5%85%A5clash%E6%89%80%E5%9C%A8%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%9C%B0%E5%9D%80%E5%92%8C%E4%B8%8A%E8%BF%B0%E6%8F%90%E5%88%B0%E7%9A%84%60external-controller%60%E7%9A%84%E7%AB%AF%E5%8F%A3%E3%80%82%E6%B3%A8%E6%84%8F%E5%8F%AA%E6%9C%89%E8%AE%BE%E7%BD%AE%E6%88%90%E4%BA%860.0.0.0%E6%89%8D%E8%83%BD%E5%A4%9F%E9%80%9A%E8%BF%87%E5%85%B6%E4%BB%96%E7%94%B5%E8%84%91%E8%AE%BF%E9%97%AE%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%89%80%E5%9C%A8%E7%9A%84clash%E6%9C%8D%E5%8A%A1%E3%80%82">https://clash.razord.top/#/connections来用浏览器对clash进行管理，进入网站时需要输入clash所在的服务器的地址和上述提到的`external-controller`的端口。注意只有设置成了0.0.0.0才能够通过其他电脑访问服务器所在的clash服务。</a></li></ul><h2 id="5-配置开机自启动"><a href="#5-配置开机自启动" class="headerlink" title="5. 配置开机自启动"></a>5. 配置开机自启动</h2><h3 id="5-1-配置Clash服务"><a href="#5-1-配置Clash服务" class="headerlink" title="5.1 配置Clash服务"></a>5.1 配置Clash服务</h3><p><strong>1. 创建service文件</strong></p><p><code>touch /etc/systemd/system/clash.service</code></p><p><strong>2. 编辑service文件</strong></p><p>打开service文件</p><p><code>vi /etc/systemd/system/clash.service</code></p><p>填入以下内容**(注意修改clash文件夹路径)**</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">[Unit]<br>Description=clash daemon<br><br>[Service]<br>Type=simple<br>User=root<br>ExecStart=/opt/clash/clash -d /opt/clash/<br>Restart=on-failure<br><br>[Install]<br>WantedBy=multi-user.target<br></code></pre></td></tr></table></figure><p>保存并退出</p><p><strong>3. 启动Clash</strong></p><p><code>systemctl start clash.service</code></p><p><strong>4. 设置Clash开机自启动</strong></p><p><code>systemctl enable clash.service</code></p><p><strong>以下为Clash相关的管理命令</strong></p><ul><li><strong>启动Clash</strong></li></ul><p><code>systemctl start clash.service</code></p><ul><li><strong>重启Clash</strong></li></ul><p><code>systemctl restart clash.service</code></p><ul><li><strong>查看Clash运行状态</strong></li></ul><p><code>systemctl status clash.service</code></p><h3 id="5-2-配置自动启动代理接口"><a href="#5-2-配置自动启动代理接口" class="headerlink" title="5.2 配置自动启动代理接口"></a>5.2 配置自动启动代理接口</h3><ul><li><strong>编辑~&#x2F;.bashrc文件，添加如下的内容</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Clash proxy</span><br><span class="hljs-function"><span class="hljs-title">proxy</span></span>() &#123;<br>    <span class="hljs-keyword">case</span> <span class="hljs-string">&quot;<span class="hljs-variable">$1</span>&quot;</span> <span class="hljs-keyword">in</span><br>        on)<br>            <span class="hljs-built_in">export</span> https_proxy=http://127.0.0.1:7890<br>            <span class="hljs-built_in">export</span> http_proxy=http://127.0.0.1:7890<br>            <span class="hljs-built_in">export</span> all_proxy=socks5://127.0.0.1:7891<br>            <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Proxy is set to http://127.0.0.1:7890 and socks5://127.0.0.1:7891&quot;</span><br>            ;;<br>        off)<br>            <span class="hljs-built_in">unset</span> http_proxy<br>            <span class="hljs-built_in">unset</span> https_proxy<br>            <span class="hljs-built_in">unset</span> all_proxy<br>            <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Proxy settings are unset&quot;</span><br>            ;;<br>        *)<br>            <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Usage: proxy &#123;on|off&#125;&quot;</span><br>            <span class="hljs-built_in">return</span> 1<br>            ;;<br>    <span class="hljs-keyword">esac</span><br>&#125;<br><br><span class="hljs-comment"># 默认开启Clash代理，若不需要默认开启则注释下面这行</span><br>proxy on<br></code></pre></td></tr></table></figure><p>然后执行 <code>source ~/.bashrc</code>  重新加载脚本，这时候会提示已经设置了代理了，后续再新建bash的时候也会进行提醒，这里默认是新建bash的时候都会设置代理，如果不需要可以注释掉<code>proxy on</code> 那一行。后续使用的时候可以使用<code>proxy off</code>关闭代理，使用<code>proxy on</code>开启代理。</p><h2 id="6、配置定时更新订阅"><a href="#6、配置定时更新订阅" class="headerlink" title="6、配置定时更新订阅"></a>6、配置定时更新订阅</h2><ul><li>Clash For Linux 到目前为止没有自动订阅方式，我们做一个计划任务实现更新<code>config.yaml</code></li><li>用Cron执行计划任务</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@localhost ~]<span class="hljs-comment"># crontab -e</span><br></code></pre></td></tr></table></figure><ul><li>填入以下内容</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">29 6    * * *   root    pgrep clash | xargs <span class="hljs-built_in">kill</span> -s 9<br>30 6    * * *   root    <span class="hljs-built_in">mv</span> /opt/clash/config.yaml /opt/clash/configbackup.yaml<br>31 6    * * *   root    wget -P /opt/clash/ -O config.yaml [你的订阅链接]<br>32 6    * * *   root    <span class="hljs-built_in">nohup</span> /opt/clash/clash -d /opt/clash/<br><br></code></pre></td></tr></table></figure><ul><li>按Esc和:wq保存退出</li><li>重启crontab，使配置生效</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">[root@localhost ~]<span class="hljs-comment"># systemctl restart crond.service</span><br></code></pre></td></tr></table></figure><h1 id="配置docker使用Clash"><a href="#配置docker使用Clash" class="headerlink" title="配置docker使用Clash"></a>配置docker使用Clash</h1><p>上述设置的代理其实是给shell用的，Docker 守护进程没有使用 shell 中设置的代理环境变量，要让docker在pull镜像时也使用就需要在 Docker 的配置文件中显式指定代理设置。</p><h2 id="1-创建或编辑-Docker-的系统服务配置文件"><a href="#1-创建或编辑-Docker-的系统服务配置文件" class="headerlink" title="1. 创建或编辑 Docker 的系统服务配置文件"></a>1. 创建或编辑 Docker 的系统服务配置文件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo <span class="hljs-built_in">mkdir</span> -p /etc/systemd/system/docker.service.d<br>sudo nano /etc/systemd/system/docker.service.d/http-proxy.conf<br></code></pre></td></tr></table></figure><h2 id="2-在-http-proxy-conf-文件中添加以下内容："><a href="#2-在-http-proxy-conf-文件中添加以下内容：" class="headerlink" title="2. 在 http-proxy.conf 文件中添加以下内容："></a>2. 在 <code>http-proxy.conf</code> 文件中添加以下内容：</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">[Service]<br>Environment=<span class="hljs-string">&quot;HTTP_PROXY=http://127.0.0.1:7890&quot;</span><br>Environment=<span class="hljs-string">&quot;HTTPS_PROXY=http://127.0.0.1:7890&quot;</span><br>Environment=<span class="hljs-string">&quot;NO_PROXY=localhost,127.0.0.1&quot;</span><br></code></pre></td></tr></table></figure><p>如果你需要指定更多不需要代理的地址，可以将它们添加到 <code>NO_PROXY</code> 列表中，用逗号分隔。</p><h2 id="3-重新加载系统守护进程并重启-Docker"><a href="#3-重新加载系统守护进程并重启-Docker" class="headerlink" title="3. 重新加载系统守护进程并重启 Docker"></a>3. 重新加载系统守护进程并重启 Docker</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo systemctl daemon-reload<br>sudo systemctl restart docker<br></code></pre></td></tr></table></figure><h2 id="验证-Docker-代理设置"><a href="#验证-Docker-代理设置" class="headerlink" title="验证 Docker 代理设置"></a>验证 Docker 代理设置</h2><ol><li>查看 Docker 的环境变量是否已经生效：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo systemctl show --property=Environment docker<br></code></pre></td></tr></table></figure><ol start="2"><li>尝试拉取一些原本拉取不了的镜像：</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker pull registry.k8s.io/etcd:3.5.11-0<br></code></pre></td></tr></table></figure><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="https://github.com/ghostxu97/clash-for-linux">https://github.com/ghostxu97/clash-for-linux</a></li><li><a href="https://blog.iswiftai.com/posts/clash-linux/">https://blog.iswiftai.com/posts/clash-linux/</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CNCF项目全景图介绍</title>
    <link href="/2024/06/09/CNCFIntroduction/"/>
    <url>/2024/06/09/CNCFIntroduction/</url>
    
    <content type="html"><![CDATA[<h1 id="云原生计算基金会（CNCF）介绍"><a href="#云原生计算基金会（CNCF）介绍" class="headerlink" title="云原生计算基金会（CNCF）介绍"></a>云原生计算基金会（CNCF）介绍</h1><p>CNCF(Cloud Native Computing Foundation)官网链接：<a href="https://www.cncf.io/">https://www.cncf.io/</a></p><p>官方的介绍如下：</p><ul><li>云原生技术有利于各组织在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。</li><li>这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松地对系统作出频繁和可预测的重大变更。</li><li>云原生计算基金会（CNCF）致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。我们通过将最前沿的模式民主化，让这些创新为大众所用。</li></ul><p>简单来说，CNCF主要致力于推动开源的云原生技术的可持续发展。</p><h1 id="CNCF项目全景图"><a href="#CNCF项目全景图" class="headerlink" title="CNCF项目全景图"></a>CNCF项目全景图</h1><p>到2024年6月为止，CNCF已经有26个毕业项目，37个孵化项目，116个沙盒项目。毕业和孵化项目被认为是稳定的，并已在生产环境中成功使用。沙盒项目是新项目，还需要进行开发演进，不适合在生产环境中使用。</p><p>其目前的全景图可以从此链接中查看：<a href="https://landscape.cncf.io/">https://landscape.cncf.io/</a>，如下图所示：</p><p><img src="https://landscape.cncf.io/docs/landscape.png" alt="CNCF全景图"></p><p>下面简单介绍一下CNCF各个层次的项目，主要参考来源于CNCF的官方介绍：<a href="https://landscape.cncf.io/guide">https://landscape.cncf.io/guide</a></p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>配置是云原生环境中的第一层。它包含用于<em>创建和强化</em>构建云原生应用的基础的工具。它包括找到用于自动配置、创建和管理基础架构的工具，以及用于扫描、签名和存储容器映像的工具。该层还扩展到安全性，其中包含支持策略设置和实施、嵌入式身份验证和授权以及机密分发处理的工具。</p><h3 id="自动化和配置"><a href="#自动化和配置" class="headerlink" title="自动化和配置"></a>自动化和配置</h3><ul><li><p><strong>解决什么问题：</strong><br>传统的 IT 流程依赖于漫长且劳动密集的手动发布周期，通常为三到六个月，导致生产环境变更速度缓慢。这与需要快速开发周期的云原生开发不兼容。</p></li><li><p><strong>如何解决问题：</strong><br>自动化和配置工具通过编码环境设置，使工程师无需人工干预即可创建和管理计算资源，只需单击按钮即可重现所需状态。这些工具减少了手动配置的错误，提高了工作效率，并允许通过编程方式配置新服务器和应用程序，支持快速、动态的基础设施管理。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>Cloud Custodian （孵化中）</li><li>KubeEdge （孵化中）</li></ul></li></ul><h3 id="容器注册表"><a href="#容器注册表" class="headerlink" title="容器注册表"></a>容器注册表</h3><ul><li><p><strong>解决什么问题：</strong><br>容器注册表解决了云原生应用运行所需的容器镜像存储和分发问题，确保开发人员可以方便地访问和管理这些镜像。</p></li><li><p><strong>如何解决问题：</strong><br>容器注册表通过集中存储容器镜像，使开发人员能够轻松访问所需的镜像。它们提供 Web API 接口以存储和检索镜像，并集成功能如扫描、签名和分发，以增强安全性和效率。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>Harbor(已毕业)</li><li>Dragonfly （孵化中）</li></ul></li></ul><h3 id="安全与合规"><a href="#安全与合规" class="headerlink" title="安全与合规"></a>安全与合规</h3><ul><li><p><strong>解决什么问题：</strong><br>默认情况下，Kubernetes 具有极其宽松的访问控制设置，不适合生产使用。结果：对于任何想要攻击您的系统的人来说，Kubernetes 集群都是一个有吸引力的目标。<br>安全和合规性工具解决了云原生应用程序在快速迭代过程中需要确保代码和操作环境安全的需求，防止未经授权的访问和潜在的安全漏洞。</p></li><li><p><strong>如何解决问题：</strong><br>这些工具通过设置安全策略、扫描容器漏洞、签名镜像以防篡改，以及强化和监控 Kubernetes 集群来确保安全性。它们帮助发现错误配置并在系统出现异常行为时进行检测和响应。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>Falco (已毕业)</li><li>Open Policy Agent （已毕业）</li></ul></li></ul><h3 id="密钥管理"><a href="#密钥管理" class="headerlink" title="密钥管理"></a>密钥管理</h3><ul><li><p><strong>解决什么问题：</strong><br>密钥管理工具解决了在云原生环境中安全存储和分发密码、API 密钥等敏感数据的需求，同时确保身份验证和授权过程的安全性和自动化。</p></li><li><p><strong>如何解决问题：</strong><br>这些工具提供安全的密钥生成、存储、管理和轮换功能，支持自动化的密钥分发，并提供身份验证和授权服务。它们确保应用程序能安全地验证和授权请求，无需人工干预，提升整体安全性和效率。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>SPIFFE （已毕业）</li><li>SPIRE （已毕业）</li></ul></li></ul><h2 id="运行时"><a href="#运行时" class="headerlink" title="运行时"></a>运行时</h2><p>现在我们已经建立了云原生环境的基础，我们将上移一个基础设施层并放大到运行时层。它涵盖了容器在云原生环境中运行所需的一切。其中包括用于启动容器的代码（称为容器运行时）、使持久存储可用于容器的工具以及管理容器环境网络的工具。</p><p>但请注意，这些资源不应与上面讨论的配置层处理的网络和存储工作相混淆。这些资源专注于让容器平台运行。此类别中的工具用于启动和停止容器，帮助它们存储数据，并允许它们相互通信。</p><h3 id="云原生存储"><a href="#云原生存储" class="headerlink" title="云原生存储"></a>云原生存储</h3><ul><li><p><strong>解决什么问题：</strong><br>云原生架构的流动性和弹性使得持久保存数据非常具有挑战性，尤其是在容器不断创建、删除和改变位置的情况下。传统存储解决方案难以满足云原生环境中自动扩展和自我修复的需求。</p></li><li><p><strong>如何解决问题：</strong><br>云原生存储工具通过提供云原生兼容的存储选项、标准化存储接口、以及数据保护功能（如备份和恢复），使存储自动化配置，消除人为瓶颈，实现自动扩展和自我修复。例如，容器存储接口 (CSI) 提供标准 API 以向容器提供文件和块存储，工具如 Minio 和 Velero 提供对象存储和数据保护功能。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>Rook （已毕业）</li><li>CubeFS （孵化中）</li></ul></li></ul><h3 id="容器运行时"><a href="#容器运行时" class="headerlink" title="容器运行时"></a>容器运行时</h3><ul><li><p><strong>解决什么问题：</strong><br>容器运行时解决了启动容器镜像（包含应用程序规范的文件）的需求，确保应用程序以标准化、安全和隔离的方式运行，同时为其提供必要的资源如 CPU、存储和内存。</p></li><li><p><strong>如何解决问题：</strong><br>容器运行时软件在所有环境中标准化地启动应用程序，设置安全边界并隔离资源。工具如 CRI-O 和 gVisor 提供强化的安全边界，防止未经授权的访问和相互干扰，同时为应用程序设定资源限制，确保各应用程序稳定运行而不抢占资源。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>containerd （已毕业）</li><li>CRI-O （已毕业）</li></ul></li></ul><h3 id="云原生网络"><a href="#云原生网络" class="headerlink" title="云原生网络"></a>云原生网络</h3><ul><li><p><strong>解决什么问题：</strong><br>云原生网络解决了分布式应用程序中各个容器之间的通信问题，使这些容器能够私下、安全地相互通信。它还解决了数据流管理和网络策略扩展的挑战，尤其在需要连接虚拟机或外部服务时。</p></li><li><p><strong>如何解决问题：</strong><br>此类工具使用容器网络接口 (CNI) 创建覆盖网络，分配 IP 地址，提供基本连接或完整的软件定义网络层。工具如 Flannel 提供基本连接，而 NSX-T 等工具创建独立的虚拟网络，支持网络流量管理和策略执行，确保容器化应用程序的安全高效通信。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>Cilium （已毕业）</li><li>CNI （孵化中）</li></ul></li></ul><h2 id="编排与管理"><a href="#编排与管理" class="headerlink" title="编排与管理"></a>编排与管理</h2><h3 id="调度与编排"><a href="#调度与编排" class="headerlink" title="调度与编排"></a>调度与编排</h3><ul><li><p><strong>解决什么问题：</strong><br>在云原生架构中，应用程序被分解为许多微服务，每个服务运行在一个容器中。管理这些大量分散的容器需要自动化的编排和调度工具，以确保资源分配、监控和故障恢复。</p></li><li><p><strong>如何解决问题：</strong><br>容器编排工具如 Kubernetes自动管理容器，执行期望状态协调，确保集群实际状态与工程师设定的期望状态匹配。它通过创建、销毁和监控容器，自动处理应用程序的启动、扩展和恢复，使用户能够将整个集群视为单一资源池，简化了复杂的分布式应用程序管理。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>Kubernetes （已毕业）</li><li>Keda （已毕业）</li></ul></li></ul><h3 id="协调与服务发现"><a href="#协调与服务发现" class="headerlink" title="协调与服务发现"></a>协调与服务发现</h3><ul><li><p><strong>解决什么问题：</strong><br>在动态且流动的云原生架构中，服务的位置不断变化，服务之间需要协作但没有固定的位置。服务发现工具解决了识别和定位服务位置的问题，确保各服务能够找到并通信。</p></li><li><p><strong>如何解决问题：</strong><br>服务发现工具通过提供一个公共位置来存储和检索服务信息，分为服务发现引擎和名称解析工具。服务发现引擎（如 etcd）存储所有服务及其位置信息，而名称解析工具（如 CoreDNS）处理服务位置请求并返回网络地址信息。这些工具有效管理服务的注册和注销，确保服务间通信的可靠性和动态适应性。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>Etcd （已毕业）</li><li>CoreDNS （已毕业）</li></ul></li></ul><h3 id="远程过程调用"><a href="#远程过程调用" class="headerlink" title="远程过程调用"></a>远程过程调用</h3><ul><li><p><strong>解决什么问题：</strong><br>现代应用程序由众多单独的服务组成，这些服务必须进行通信才能协作。RPC 是处理应用程序之间通信的一种选择。</p></li><li><p><strong>如何解决问题：</strong><br>RPC 提供了一种紧耦合且高度规范的通信方式，允许高效的带宽利用和多种编程语言的支持，例如流行的 gRPC 实现。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>gRPC （孵化中）</li></ul></li></ul><h3 id="服务代理"><a href="#服务代理" class="headerlink" title="服务代理"></a>服务代理</h3><pre><code class="hljs">更关注于外部用户访问和 API 管理</code></pre><ul><li><p><strong>解决什么问题：</strong><br>服务代理解决了应用程序内部和应用程序之间的网络流量管理问题，使得流量可以受控地发送和接收，并允许对流量进行转换、重定向或拒绝请求等操作。</p></li><li><p><strong>如何解决问题：</strong><br>通过将流量管理功能从应用程序中外部化到服务代理，开发人员可以更专注于编写核心业务逻辑，而平台团队则可以管理通用任务，如路由、TLS 终止和流量均衡，从而提高通信的可靠性、安全性和效率。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>Envoy （已毕业）</li><li>CONTOUR（孵化中）</li></ul></li></ul><h3 id="API-网关"><a href="#API-网关" class="headerlink" title="API 网关"></a>API 网关</h3><ul><li><p><strong>解决什么问题：</strong><br>API 网关解决了组织管理多个 API、实施规则并提供通用接口的问题，同时简化了用户与应用程序之间的交互。</p></li><li><p><strong>如何解决问题：</strong><br>通过充当中间人，接收并评估用户请求，记录请求详细信息，并将请求转发到适当的服务，API 网关提供了统一的入口点和管理功能，使得开发人员可以更专注于核心业务逻辑。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>Emissary-Ingress （孵化中）</li></ul></li></ul><h3 id="服务网格"><a href="#服务网格" class="headerlink" title="服务网格"></a>服务网格</h3><pre><code class="hljs">关注于服务内部通信和交互</code></pre><ul><li><p><strong>解决什么问题：</strong><br>服务网格解决了在云原生环境中管理服务之间通信和添加功能的问题，同时降低了技术债务和复杂性。</p></li><li><p><strong>如何解决问题：</strong><br>通过统一管理集群中所有服务的可靠性、可观察性和安全性功能，服务网格使开发团队可以专注于编写业务逻辑，而无需触及底层代码。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>Istio （已毕业）</li><li>Linkerd （已毕业）</li></ul></li></ul><h2 id="应用程序定义和开发"><a href="#应用程序定义和开发" class="headerlink" title="应用程序定义和开发"></a>应用程序定义和开发</h2><p>到目前为止，我们讨论的所有内容都与构建可靠、安全的环境和提供所有必要的依赖项有关。现在，我们已到达 CNCF 云原生环境的顶层。顾名思义，应用程序定义和开发层专注于使工程师能够构建应用程序的工具。</p><h3 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h3><ul><li><p><strong>解决什么问题：</strong><br>数据库解决了应用程序需要有效存储和检索数据的需求，同时确保数据安全和合规性。</p></li><li><p><strong>如何解决问题：</strong><br>数据库通过提供通用接口和查询语言，使开发人员能够存储、查询和检索信息，并管理数据的备份、加密和访问控制。同时还加强数据库的容器化。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>TiKV （已毕业）</li><li>Vitess （已毕业）</li></ul></li></ul><h3 id="流媒体和消息传递"><a href="#流媒体和消息传递" class="headerlink" title="流媒体和消息传递"></a>流媒体和消息传递</h3><p><strong>解决什么问题：</strong><br>流媒体和消息传递解决了随着服务数量增加，应用程序通信管理变得更复杂的问题，提供了一个集中管理事件发布和读取的中心位置，使得应用程序能够协同工作。</p><p><strong>如何解决问题：</strong><br>通过发布-订阅的方法，服务可以将事件发布到消息传递工具或流式传输，其他需要了解这些事件的服务则可以订阅并观察这些工具，从而实现了解耦的架构。这种解耦使得工程师能够添加新功能，而无需更新下游应用程序或发送大量查询。</p><ul><li><strong>代表性项目：</strong><ul><li>CloudEvents （已毕业）</li><li>NATS （孵化中）</li></ul></li></ul><h3 id="应用程序定义和映像构建"><a href="#应用程序定义和映像构建" class="headerlink" title="应用程序定义和映像构建"></a>应用程序定义和映像构建</h3><p><strong>解决什么问题：</strong><br>应用程序定义和镜像构建是一个广泛的类别，可以分为两个主要子类。首先，以开发人员为中心的工具，可帮助将应用程序代码构建到容器和&#x2F;或 Kubernetes 中。其次，以运营为中心的工具，以标准化方式部署应用程序。无论您是打算加速或简化开发环境，提供标准化的方式来部署第三方应用程序，还是希望简化编写新 Kubernetes 扩展的过程，此类别都可以涵盖许多优化 Kubernetes 开发人员和操作员体验的项目和产品。<br>应用程序定义和映像构建工具致力于解决开发人员和运营商在 Kubernetes 和容器化环境中面临的挑战，包括构建可重现映像、标准化应用程序部署和简化环境配置等问题。</p><p><strong>如何解决问题：</strong><br>这些工具帮助开发人员简化扩展 Kubernetes、构建、部署和连接应用程序的过程，同时为运营商提供标准化的方式部署预打包的应用程序，快速部署服务网格等服务，以及简化映像创建和应用程序部署的过程。通过引入这些工具，可以加速应用程序开发、提高部署效率，并降低操作复杂性。</p><ul><li><strong>代表性项目：</strong><ul><li>Helm(已毕业)</li><li>Artifact Hub （孵化中）</li><li>KubeVirt （孵化中）</li></ul></li></ul><h3 id="持续集成和交付"><a href="#持续集成和交付" class="headerlink" title="持续集成和交付"></a>持续集成和交付</h3><p><strong>解决什么问题：</strong><br>持续集成 (CI) 和持续交付 (CD) 工具旨在解决开发人员和运营商在构建和部署应用程序时面临的挑战，包括确保代码质量、自动化构建和部署流程、提高交付速度和减少错误的问题。</p><p><strong>如何解决问题：</strong><br>这些工具通过自动化代码更改的构建、测试和部署过程，确保代码集成的连续性和质量。CI 系统可帮助开发人员快速发现并修复代码错误，而 CD 系统则将 CI 流程的结果推送到不同环境，从开发到生产，以确保交付的可靠性和一致性。这些工具与 Kubernetes 等技术结合使用，使得云原生环境下的持续集成和交付更加高效和可靠。</p><ul><li><strong>代表性项目：</strong><ul><li>Argo（已毕业）</li><li>Flux （已毕业）</li></ul></li></ul><h2 id="可观察性和分析"><a href="#可观察性和分析" class="headerlink" title="可观察性和分析"></a>可观察性和分析</h2><p>现在我们已经了解了 CNCF 格局的各个层面，我们将重点关注从可观察性和分析性开始的列。</p><p>在深入探讨这些类别之前，我们先来定义一下可观察性和分析性。可观察性是一种系统特性，它描述了系统从外部输出中可理解的程度。通过 CPU 时间、内存、磁盘空间、延迟、错误等来衡量，计算机系统的可观察性或多或少。分析是一种查看这些可观察数据并理解其意义的活动。</p><p>为了确保服务不会中断，您需要观察和分析应用程序的各个方面，以便立即检测并纠正每个异常。这就是此类别的全部内容。它运行并观察所有层，这就是为什么它位于侧面而不是嵌入特定层的原因。</p><p>此类别中的工具分为可观察性和混沌工程。请注意，类别名称有些误导 — 虽然此处列出了混沌工程，但请将其视为可靠性工具，而不是可观察性或分析工具。</p><h3 id="可观察性"><a href="#可观察性" class="headerlink" title="可观察性"></a>可观察性</h3><p><strong>解决什么问题：</strong><br>可观察性解决了复杂系统中出现故障或性能下降时难以定位问题的挑战。它通过提供有关系统运行状况、关键统计数据和问题调试信息的高质量遥测数据，帮助用户理解系统的内部运行情况。</p><p><strong>如何解决问题：</strong><br>可观察性框架收集并分析来自应用程序代码、底层基础设施以及编排系统等的日志、指标和跟踪数据。这些数据可以帮助运维人员更快地响应事件，开发人员在生产中调试应用程序，并帮助组织了解用户行为与系统性能之间的关系。在云原生环境下，可观察性对于快速定位和解决问题至关重要。</p><ul><li><strong>代表性项目：</strong><ul><li>Prometheus（已毕业）</li><li>Fluentd  （已毕业）</li></ul></li></ul><h3 id="混沌工程"><a href="#混沌工程" class="headerlink" title="混沌工程"></a>混沌工程</h3><p><strong>解决什么问题：</strong><br>混沌工程旨在解决复杂系统中故障带来的挑战，通过故意引入故障来测试系统的弹性，以确保系统和工程团队能够应对动荡和意外事件。</p><p><strong>如何解决问题：</strong><br>混沌工程工具提供了一种受控的方式来引入故障并针对应用程序的特定实例运行实验。通过这种方式，团队可以在生产环境中实验，以确保系统在面对实际故障时能够优雅地运行和恢复。这种做法有助于优化平均修复时间(MTTR)，使团队更具应变能力和可靠性。</p><ul><li><strong>代表性项目：</strong>  <ul><li>Chaos Mesh （孵化中）</li><li>Litmus（孵化中）</li></ul></li></ul><h2 id="平台"><a href="#平台" class="headerlink" title="平台"></a>平台</h2><p>正如我们目前所见，讨论的每个类别都解决了一个特定的问题。单靠存储并不能提供管理应用所需的一切。您需要一个编排工具、一个容器运行时、服务发现、网络、API 网关等。平台将不同层的不同工具捆绑在一起，解决更大的问题。</p><p>这些平台本身并没有什么新意。它们所做的一切都可以通过这些层或可观察性和分析列中的工具之一完成。您当然可以构建自己的平台，事实上，许多组织都这样做了。但是，可靠、安全地配置和微调不同的模块，同时确保所有技术始终保持最新状态并修补漏洞并非易事——您需要一个专门的团队来构建和维护它。如果您没有必要的资源或专业知识，您的团队可能最好使用平台。对于一些组织，尤其是那些工程团队较小的组织，平台是采用云原生方法的唯一途径。</p><p>你可能会注意到，所有平台都围绕 Kubernetes展开。这是因为它是云原生堆栈的核心。</p><h3 id="认证-Kubernetes-分发"><a href="#认证-Kubernetes-分发" class="headerlink" title="认证 Kubernetes - 分发"></a>认证 Kubernetes - 分发</h3><p><strong>解决什么问题：</strong><br>发行版（或发行版）解决了 Kubernetes 部署和管理的复杂性，为用户提供了可靠的 Kubernetes 安装方式，并提供了默认设置，以创建更好、更安全的操作环境。</p><p><strong>如何解决问题：</strong><br>通过将核心 Kubernetes 打包并提供处理集群安装和升级的机制，发行版简化了 Kubernetes 的采用和管理过程。它提供了可预测的配置和设置，为用户提供了支持和升级路径，并附带经过测试的软件扩展，使得在 Kubernetes 上部署其他应用程序更加容易。</p><ul><li><strong>代表性项目：</strong><ul><li>K3s （沙盒）</li></ul></li></ul><h3 id="经过认证的-Kubernetes-托管"><a href="#经过认证的-Kubernetes-托管" class="headerlink" title="经过认证的 Kubernetes - 托管"></a>经过认证的 Kubernetes - 托管</h3><p><strong>解决什么问题：</strong><br>托管 Kubernetes 是基础设施提供商（如AWS、Digital Ocean、Azure和Google）提供的服务，简化了 Kubernetes 的部署和管理，允许用户无需深入了解即可启动和管理 Kubernetes 集群。</p><p><strong>如何解决问题：</strong><br>用户可以通过在云提供商处设置帐户，并通过简单的界面或命令行工具启动 Kubernetes 集群，而无需自己进行配置或管理。托管 Kubernetes 将管理细节外包给供应商，使用户能够专注于应用程序开发和部署。</p><ul><li><strong>代表性项目：</strong><ul><li>无，主要是由云服务商提供</li></ul></li></ul><h3 id="认证-Kubernetes-安装程序"><a href="#认证-Kubernetes-安装程序" class="headerlink" title="认证 Kubernetes - 安装程序"></a>认证 Kubernetes - 安装程序</h3><ul><li><p><strong>解决什么问题：</strong><br>认证 Kubernetes - 安装程序解决了在机器上安装和配置 Kubernetes 的复杂性问题，特别是对于初学者和没有专业知识的用户而言。</p></li><li><p><strong>如何解决问题：</strong><br>它通过自动执行安装和配置过程，提供经过审查的源代码和环境配置，简化了 Kubernetes 的部署过程，让用户可以快速轻松地搭建起符合标准的 Kubernetes 环境。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>Kubean （沙盒）</li><li>Kind （沙盒）</li></ul></li></ul><h3 id="PaaS-容器服务"><a href="#PaaS-容器服务" class="headerlink" title="PaaS&#x2F;容器服务"></a>PaaS&#x2F;容器服务</h3><ul><li><p><strong>解决什么问题：</strong><br>PaaS&#x2F;容器服务解决了用户在运行应用程序时不必关心底层计算资源细节的问题，为开发人员提供了托管应用程序的环境。</p></li><li><p><strong>如何解决问题：</strong><br>通过将各种开源和闭源工具组合在一起，并提供处理安装、升级和运行时需求的机制，PaaS&#x2F;容器服务简化了应用程序部署和管理过程，加快了价值实现途径。</p></li><li><p><strong>代表性项目：</strong></p><ul><li>目前该领域没有 CNCF 项目，但大多数产品都是开源的，Cloud Foundry 由 Cloud Foundry 基金会管理。</li></ul></li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>现在我们已经分解了 CNCF 云原生全景图，并逐层、逐类进行了讨论，可能感觉没那么难了。它有一个逻辑结构，一旦你理解了它，浏览全景图就会变得容易得多。</p><p>CNCF Landscape 的各个层级相互依存。首先是配置层，其中包含奠定基础设施基础所需的工具。接下来是运行时层，其中的所有内容都围绕容器以及容器在云原生环境中运行所需的内容。编排和管理层包含用于编排和管理容器和应用程序的工具 - 换句话说，就是创建应用程序构建平台所需的工具。应用程序和定义与开发层涉及使应用程序能够存储和发送数据所需的工具，以及我们构建和部署应用程序的方式。</p><p>层旁边有两列。可观察性和分析列包括监控应用程序并在出现问题时发出警报的工具。由于所有层都必须受到监控，因此此类别涵盖所有层。最后是平台。平台不提供新功能，而是将不同层的多种工具捆绑在一起，对其进行配置和微调，以便随时可用。这简化了云原生技术的采用，甚至可能是组织能够利用这些技术的唯一方式。</p>]]></content>
    
    
    <categories>
      
      <category>其他</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>【K8s源码分析（六）】-K8s中Pod拓扑分布约束（Pod Topology Spread Constraints）插件介绍</title>
    <link href="/2024/05/12/k8sSource6/"/>
    <url>/2024/05/12/k8sSource6/</url>
    
    <content type="html"><![CDATA[<p>本次分析参考的K8s版本是<a href="https://github.com/kubernetes/kubernetes/tree/release-1.27">v1.27.0</a>。</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在 k8s 集群调度中，<strong>亲和性</strong>相关的概念本质上都是控制 Pod 如何被调度 – <strong>堆叠或打散</strong>。<code>podAffinity</code> 以及 <code>podAntiAffinity</code> 两个特性对 Pod 在不同拓扑域（拓扑键-拓扑值构成了一个拓扑域，例如region-east）的分布进行了一些控制，<code>podAffinity</code> 可以将无数个 Pod 调度到特定的某一个拓扑域，这是<strong>堆叠</strong>的体现；<code>podAntiAffinity</code> 则可以控制一个拓扑域只存在一个 Pod，这是<strong>打散</strong>的体现。前面已经进行了介绍，详见：<a href="https://slipegg.github.io/2024/05/11/k8sSource5/">K8s源码分析（五）-K8s中Pod亲和性调度插件介绍</a></p><p>但<code>podAffinity</code> 以及 <code>podAntiAffinity</code> 这两种情况都太极端了，在不少场景下都无法达到理想的效果，例如为了实现容灾和高可用，将业务 Pod 尽可能均匀的分布在不同可用区就很难实现。</p><p><code>PodTopologySpread</code>（Pod 拓扑分布约束） 特性的提出正是为了对 Pod 的调度分布提供更精细的控制，以提高服务可用性以及资源利用率，<code>PodTopologySpread</code> 由 <code>EvenPodsSpread</code> 特性门所控制，在 v1.16 版本第一次发布，并在 v1.18 版本进入 beta 阶段默认启用。</p><p>官方对其的介绍详见：<a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/">Pod Topology Spread Constraints</a></p><h2 id="使用规范"><a href="#使用规范" class="headerlink" title="使用规范"></a>使用规范</h2><p>这一特性的定义在<code>spec.topologySpreadConstraints</code>下，一些可以定义的字段如下</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">example-pod</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-comment"># Configure a topology spread constraint</span><br>  <span class="hljs-attr">topologySpreadConstraints:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">maxSkew:</span> <span class="hljs-string">&lt;integer&gt;</span><br>      <span class="hljs-attr">minDomains:</span> <span class="hljs-string">&lt;integer&gt;</span> <span class="hljs-comment"># optional</span><br>      <span class="hljs-attr">topologyKey:</span> <span class="hljs-string">&lt;string&gt;</span><br>      <span class="hljs-attr">whenUnsatisfiable:</span> <span class="hljs-string">&lt;string&gt;</span><br>      <span class="hljs-attr">labelSelector:</span> <span class="hljs-string">&lt;object&gt;</span><br>      <span class="hljs-attr">matchLabelKeys:</span> <span class="hljs-string">&lt;list&gt;</span> <span class="hljs-comment"># optional; beta since v1.27</span><br>      <span class="hljs-attr">nodeAffinityPolicy:</span> [<span class="hljs-string">Honor|Ignore</span>] <span class="hljs-comment"># optional; beta since v1.26</span><br>      <span class="hljs-attr">nodeTaintsPolicy:</span> [<span class="hljs-string">Honor|Ignore</span>] <span class="hljs-comment"># optional; beta since v1.26</span><br>  <span class="hljs-comment">### other Pod fields go here</span><br></code></pre></td></tr></table></figure><ul><li><p><strong>topologyKey：</strong>这个拓扑约束所生效的目标拓扑键，计算分布不均匀程度时以其作为单位。例如region、zone、hostName等。</p></li><li><p><strong>maxSkew：</strong>描述了允许的最大pod的分布不均匀程度。不均匀程度由同一个拓扑键下各个拓扑值中包含的所有的node中匹配的pod的数量之和的<strong>最大值减去最小值</strong>得到。</p><p>  <img src="/2024/05/12/k8sSource6/region.png" alt="示例"></p><p>  例如现在的拓扑键是region，总共有3个不同的拓扑值，分别为A、B、C，就是有3个拓扑域它们中分别有2、2、1个相匹配的pod，那么其不均匀程度就是2-1&#x3D;1。如过设置maxSkew为1，那么要维持这个maxSkew，pod就得调度到Region C中的任意一个node上去。</p></li><li><p><strong>whenUnsatisfiable：</strong>当不满足maxSkew约束的node时如何处理。值可以为：</p><ul><li><code>DoNotSchedule</code>（默认）告诉调度器不要调度。</li><li><code>ScheduleAnyway</code> 告诉调度器仍然继续调度，只是根据如何能将偏差最小化来对节点进行排序。</li></ul></li><li><p><strong>labelSelector：</strong>筛选需要匹配的pod的规则。</p></li><li><p><strong>minDomains（可选项）：</strong>符合条件的拓扑域的最小值。如果小于这个值，那么就设置全局中匹配到的pod的数量的最小值为0，这时候各个拓扑域下的node相匹配的pod数量需要小于等于maxSkew。如果拓扑域的数量大于这个值，那么就不会有影响。默认值为1，设置的值必须大于0。</p><p>  <strong>注意：</strong>在 Kubernetes v1.30 之前，<code>minDomains</code> 字段只有在启用了 <code>MinDomainsInPodTopologySpread</code> <a href="https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/feature-gates/">特性门控</a>时才可用（自 v1.28 起默认启用） 在早期的 Kubernetes 集群中，此特性门控可能被显式禁用或此字段可能不可用。</p></li><li><p><strong>matchLabelKeys（可选项，beta，v1.27启用）：</strong>在labelSelector之外进行额外的pod筛选，即labelSelector筛选后的pod<strong>必须还得包含这些key</strong>。注意其要求如果没有labelSelector就不能使用这个项，并且不能和labelSelector有相同的key。默认是空，即不可能该项。</p></li><li><p><strong>nodeAffinityPolicy（可选项，beta，v1.26启用）：</strong>定义如何对待nodeAffinity和nodeSelector。</p><ul><li><strong>Honor：</strong>只有匹配nodeAffinity&#x2F;nodeSelector的节点才会包含在计算中。</li><li><strong>Ignore：</strong>nodeAffinity&#x2F;nodeSelector 操作被忽略，所有节点都包含在计算中。</li></ul></li><li><p><strong>nodeTaintsPolicy （可选项，beta，v1.26启用）：定义如何处理污点节点。</strong></p><ul><li><strong>Honor：</strong>只查看没有污点的节点，以及要调度的pod 具有容忍度的污点的节点。</li><li><strong>Ignore：</strong>所有节点都包含在计算中。</li></ul></li></ul><p>当一个 Pod 定义多个<code>topologySpreadConstraint</code>时，这些约束将使用<strong>逻辑 AND 运算进行组合</strong>。因为可以有多个约束，所以有可能会出现调度到任意节点都无法满足的情况。</p><h1 id="源代码分析"><a href="#源代码分析" class="headerlink" title="源代码分析"></a>源代码分析</h1><p>该组件的相关代码都在<code>pkg/scheduler/framework/plugins/podtopologyspread</code>文件夹下，其包含的文件如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs go">podtopologyspread/<br>├── common.<span class="hljs-keyword">go</span><br>├── filtering.<span class="hljs-keyword">go</span><br>├── filtering_test.<span class="hljs-keyword">go</span><br>├── plugin.<span class="hljs-keyword">go</span><br>├── plugin_test.<span class="hljs-keyword">go</span><br>├── scoring.<span class="hljs-keyword">go</span><br>└── scoring_test.<span class="hljs-keyword">go</span><br></code></pre></td></tr></table></figure><h2 id="plugin-go"><a href="#plugin-go" class="headerlink" title="plugin.go"></a>plugin.go</h2><p>查看其初始的部分，如下。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// PodTopologySpread is a plugin that ensures pod&#x27;s topologySpreadConstraints is satisfied.</span><br><span class="hljs-keyword">type</span> PodTopologySpread <span class="hljs-keyword">struct</span> &#123;<br>systemDefaulted                              <span class="hljs-type">bool</span><br>parallelizer                                 parallelize.Parallelizer<br>defaultConstraints                           []v1.TopologySpreadConstraint<br>sharedLister                                 framework.SharedLister<br>services                                     corelisters.ServiceLister<br>replicationCtrls                             corelisters.ReplicationControllerLister<br>replicaSets                                  appslisters.ReplicaSetLister<br>statefulSets                                 appslisters.StatefulSetLister<br>enableMinDomainsInPodTopologySpread          <span class="hljs-type">bool</span><br>enableNodeInclusionPolicyInPodTopologySpread <span class="hljs-type">bool</span><br>enableMatchLabelKeysInPodTopologySpread      <span class="hljs-type">bool</span><br>&#125;<br><br><span class="hljs-keyword">var</span> _ framework.PreFilterPlugin = &amp;PodTopologySpread&#123;&#125;<br><span class="hljs-keyword">var</span> _ framework.FilterPlugin = &amp;PodTopologySpread&#123;&#125;<br><span class="hljs-keyword">var</span> _ framework.PreScorePlugin = &amp;PodTopologySpread&#123;&#125;<br><span class="hljs-keyword">var</span> _ framework.ScorePlugin = &amp;PodTopologySpread&#123;&#125;<br><span class="hljs-keyword">var</span> _ framework.EnqueueExtensions = &amp;PodTopologySpread&#123;&#125;<br><br><span class="hljs-comment">// Name is the name of the plugin used in the plugin registry and configurations.</span><br><span class="hljs-keyword">const</span> Name = names.PodTopologySpread<br></code></pre></td></tr></table></figure><p>可以看到与上一次分析的亲和性调度插件类似，也是定义并实现了以下接口。</p><ul><li><strong>PreFilter</strong></li><li><strong>Filter</strong></li><li><strong>PreScore</strong></li><li><strong>Score</strong></li><li><strong>EnqueueExtensions</strong></li></ul><p>结构体中还有一些特别的额外变量，其含义如下：</p><ul><li><strong>systemDefaulted：</strong>是否使用了系统默认的拓扑约束</li><li><strong>parallelizer：</strong>一个并行处理工具</li><li><strong>defaultConstraints：</strong>默认的拓扑约束</li><li><strong>sharedLister：</strong>一个共享的列表器，它提供了对 Kubernetes 资源（如节点、Pods 等）的访问</li><li><strong>services：</strong>一个服务列表器，用于列出 Kubernetes 中的服务资源。</li><li><strong>replicationCtrls：</strong>一个副本控制器列表器，用于列出和访问 Kubernetes 中的副本控制器对象。</li><li><strong>replicaSets：</strong>一个副本集列表器，用于列出和访问 Kubernetes 中的副本集对象。</li><li><strong>statefulSets：</strong>一个有状态集列表器，用于列出和访问 Kubernetes 中的有状态集对象。</li><li><strong>enableMinDomainsInPodTopologySpread：</strong>控制是否启用 Pod 拓扑分布的最小域 <code>minDomains</code> 特性。</li><li><strong>enableNodeInclusionPolicyInPodTopologySpread：</strong>控制是否启用了节点包含策略特性，如果<strong>没有启用</strong>，那么就只考虑通过节点亲和性及node selector的node。但是<strong>如果启用了</strong>，就会根据各个拓扑约束的nodeAffinityPolicy和nodeTaintsPolicy 来考虑是否检查节点能否通过亲和性约束及node selector和节点污点特性。</li><li><strong>enableMatchLabelKeysInPodTopologySpread：</strong>控制是否启用在 Pod 拓扑分布中的<code>matchLabelKeys</code>特性。</li></ul><p>再查看其注册的事件，可以看到当<strong>pod有任何修改</strong>或者<strong>node有添加、删除、更新label的操作</strong>时，就会将调度失败的pod进行重新调度。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// EventsToRegister returns the possible events that may make a Pod</span><br><span class="hljs-comment">// failed by this plugin schedulable.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *PodTopologySpread)</span></span> EventsToRegister() []framework.ClusterEvent &#123;<br><span class="hljs-keyword">return</span> []framework.ClusterEvent&#123;<br><span class="hljs-comment">// All ActionType includes the following events:</span><br><span class="hljs-comment">// - Add. An unschedulable Pod may fail due to violating topology spread constraints,</span><br><span class="hljs-comment">// adding an assigned Pod may make it schedulable.</span><br><span class="hljs-comment">// - Update. Updating on an existing Pod&#x27;s labels (e.g., removal) may make</span><br><span class="hljs-comment">// an unschedulable Pod schedulable.</span><br><span class="hljs-comment">// - Delete. An unschedulable Pod may fail due to violating an existing Pod&#x27;s topology spread constraints,</span><br><span class="hljs-comment">// deleting an existing Pod may make it schedulable.</span><br>&#123;Resource: framework.Pod, ActionType: framework.All&#125;,<br><span class="hljs-comment">// Node add|delete|updateLabel maybe lead an topology key changed,</span><br><span class="hljs-comment">// and make these pod in scheduling schedulable or unschedulable.</span><br>&#123;Resource: framework.Node, ActionType: framework.Add | framework.Delete | framework.UpdateNodeLabel&#125;,<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>再看其初始化的构建函数<code>New</code>，如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// New initializes a new plugin and returns it.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">New</span><span class="hljs-params">(plArgs runtime.Object, h framework.Handle, fts feature.Features)</span></span> (framework.Plugin, <span class="hljs-type">error</span>) &#123;<br><span class="hljs-keyword">if</span> h.SnapshotSharedLister() == <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, fmt.Errorf(<span class="hljs-string">&quot;SnapshotSharedlister is nil&quot;</span>)<br>&#125;<br>args, err := getArgs(plArgs)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err<br>&#125;<br><span class="hljs-keyword">if</span> err := validation.ValidatePodTopologySpreadArgs(<span class="hljs-literal">nil</span>, &amp;args); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err<br>&#125;<br>pl := &amp;PodTopologySpread&#123;<br>parallelizer:                        h.Parallelizer(),<br>sharedLister:                        h.SnapshotSharedLister(),<br>defaultConstraints:                  args.DefaultConstraints,<br>enableMinDomainsInPodTopologySpread: fts.EnableMinDomainsInPodTopologySpread,<br>enableNodeInclusionPolicyInPodTopologySpread: fts.EnableNodeInclusionPolicyInPodTopologySpread,<br>enableMatchLabelKeysInPodTopologySpread:      fts.EnableMatchLabelKeysInPodTopologySpread,<br>&#125;<br><span class="hljs-keyword">if</span> args.DefaultingType == config.SystemDefaulting &#123;<br>pl.defaultConstraints = systemDefaultConstraints<br>pl.systemDefaulted = <span class="hljs-literal">true</span><br>&#125;<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(pl.defaultConstraints) != <span class="hljs-number">0</span> &#123;<br><span class="hljs-keyword">if</span> h.SharedInformerFactory() == <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, fmt.Errorf(<span class="hljs-string">&quot;SharedInformerFactory is nil&quot;</span>)<br>&#125;<br>pl.setListers(h.SharedInformerFactory())<br>&#125;<br><span class="hljs-keyword">return</span> pl, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="filtering-go"><a href="#filtering-go" class="headerlink" title="filtering.go"></a>filtering.go</h2><h3 id="PreFilter"><a href="#PreFilter" class="headerlink" title="PreFilter"></a>PreFilter</h3><p>首先查看其<code>PreFilter</code>函数，如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// PreFilter invoked at the prefilter extension point.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *PodTopologySpread)</span></span> PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) &#123;<br>s, err := pl.calPreFilterState(ctx, pod)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, framework.AsStatus(err)<br>&#125;<br>cycleState.Write(preFilterStateKey, s)<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>再查看核心的<code>calPreFilterState</code>函数，如下，补充了部分注释。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// calPreFilterState computes preFilterState describing how pods are spread on topologies.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *PodTopologySpread)</span></span> calPreFilterState(ctx context.Context, pod *v1.Pod) (*preFilterState, <span class="hljs-type">error</span>) &#123;<br>    <span class="hljs-comment">// 获取所有节点信息的列表。</span><br>    allNodes, err := pl.sharedLister.NodeInfos().List()<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// 如果获取节点信息失败，返回错误。</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, fmt.Errorf(<span class="hljs-string">&quot;listing NodeInfos: %w&quot;</span>, err)<br>    &#125;<br><br>    <span class="hljs-keyword">var</span> constraints []topologySpreadConstraint<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(pod.Spec.TopologySpreadConstraints) &gt; <span class="hljs-number">0</span> &#123;<br>        <span class="hljs-comment">// 如果 Pod 规范中定义了拓扑分布约束，则使用这些约束。</span><br>        <span class="hljs-comment">// 由于 API 服务器中的特性门控会剥离规范，因此不需要再次检查特性门控，只需检查约束的长度。</span><br>        constraints, err = pl.filterTopologySpreadConstraints(<br>            pod.Spec.TopologySpreadConstraints, <span class="hljs-comment">// Pod 的拓扑分布约束。</span><br>            pod.Labels,                        <span class="hljs-comment">// Pod 的标签。</span><br>            v1.DoNotSchedule,                  <span class="hljs-comment">// Kubernetes 调度策略。</span><br>        )<br>        <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-comment">// 如果获取 Pod 的硬拓扑分布约束失败，返回错误。</span><br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, fmt.Errorf(<span class="hljs-string">&quot;obtaining pod&#x27;s hard topology spread constraints: %w&quot;</span>, err)<br>        &#125;<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        <span class="hljs-comment">// 如果 Pod 规范中没有定义拓扑分布约束，则构建默认的约束。</span><br>        constraints, err = pl.buildDefaultConstraints(pod, v1.DoNotSchedule)<br>        <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-comment">// 如果设置默认的硬拓扑分布约束失败，返回错误。</span><br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, fmt.Errorf(<span class="hljs-string">&quot;setting default hard topology spread constraints: %w&quot;</span>, err)<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 如果没有拓扑分布约束，则返回一个空的 preFilterState。</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(constraints) == <span class="hljs-number">0</span> &#123;<br>        <span class="hljs-keyword">return</span> &amp;preFilterState&#123;&#125;, <span class="hljs-literal">nil</span><br>    &#125;<br><br>    <span class="hljs-comment">// 初始化 preFilterState 结构体。</span><br>    s := preFilterState&#123;<br>        Constraints:          constraints,<br>        TpKeyToCriticalPaths: <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]*criticalPaths, <span class="hljs-built_in">len</span>(constraints)),<br>        TpPairToMatchNum:     <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[topologyPair]<span class="hljs-type">int</span>, sizeHeuristic(<span class="hljs-built_in">len</span>(allNodes), constraints)),<br>    &#125;<br><br>    <span class="hljs-comment">// 创建一个切片来存储每个节点的拓扑对计数。</span><br>    tpCountsByNode := <span class="hljs-built_in">make</span>([]<span class="hljs-keyword">map</span>[topologyPair]<span class="hljs-type">int</span>, <span class="hljs-built_in">len</span>(allNodes))<br>    <span class="hljs-comment">// 获取 Pod 的必需节点亲和性。</span><br>    requiredNodeAffinity := nodeaffinity.GetRequiredNodeAffinity(pod)<br>    processNode := <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(i <span class="hljs-type">int</span>)</span></span> &#123;<br>        <span class="hljs-comment">// 对每个节点应用处理函数。</span><br>        nodeInfo := allNodes[i]<br>        node := nodeInfo.Node()<br>        <span class="hljs-keyword">if</span> node == <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-comment">// 如果节点信息为空，则记录错误并返回。</span><br>            klog.ErrorS(<span class="hljs-literal">nil</span>, <span class="hljs-string">&quot;Node not found&quot;</span>)<br>            <span class="hljs-keyword">return</span><br>        &#125;<br><br>        <span class="hljs-keyword">if</span> !pl.enableNodeInclusionPolicyInPodTopologySpread &#123;<br>            <span class="hljs-comment">// 如果没有启用节点包含策略，则只对通过选择的节点应用分布。</span><br>            <span class="hljs-comment">// 忽略解析错误以保持向后兼容性。</span><br>            <span class="hljs-keyword">if</span> match, _ := requiredNodeAffinity.Match(node); !match &#123;<br>                <span class="hljs-keyword">return</span><br>            &#125;<br>        &#125;<br><br>        <span class="hljs-comment">// 确保当前节点的标签包含所有 &#x27;Constraints&#x27; 中的拓扑键。</span><br>        <span class="hljs-keyword">if</span> !nodeLabelsMatchSpreadConstraints(node.Labels, constraints) &#123;<br>            <span class="hljs-keyword">return</span><br>        &#125;<br><br>        tpCounts := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[topologyPair]<span class="hljs-type">int</span>, <span class="hljs-built_in">len</span>(constraints))<br>        <span class="hljs-keyword">for</span> _, c := <span class="hljs-keyword">range</span> constraints &#123;<br>            <span class="hljs-comment">// 检查节点是否与包含策略匹配。</span><br>            <span class="hljs-keyword">if</span> pl.enableNodeInclusionPolicyInPodTopologySpread &amp;&amp;<br>                !c.matchNodeInclusionPolicies(pod, node, requiredNodeAffinity) &#123;<br>                <span class="hljs-keyword">continue</span><br>            &#125;<br><br>            <span class="hljs-comment">// 创建拓扑对并计算与选择器匹配的 Pod 数量。</span><br>            pair := topologyPair&#123;key: c.TopologyKey, value: node.Labels[c.TopologyKey]&#125;<br>            count := countPodsMatchSelector(nodeInfo.Pods, c.Selector, pod.Namespace)<br>            tpCounts[pair] = count<br>        &#125;<br>        <span class="hljs-comment">// 将当前节点的拓扑对计数存储到切片中。</span><br>        tpCountsByNode[i] = tpCounts<br>    &#125;<br>    <span class="hljs-comment">// 并行处理所有节点。</span><br>    pl.parallelizer.Until(ctx, <span class="hljs-built_in">len</span>(allNodes), processNode, pl.Name())<br><br>    <span class="hljs-comment">// 累加每个节点的拓扑对计数到 preFilterState。</span><br>    <span class="hljs-keyword">for</span> _, tpCounts := <span class="hljs-keyword">range</span> tpCountsByNode &#123;<br>        <span class="hljs-keyword">for</span> tp, count := <span class="hljs-keyword">range</span> tpCounts &#123;<br>            s.TpPairToMatchNum[tp] += count<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">if</span> pl.enableMinDomainsInPodTopologySpread &#123;<br>        <span class="hljs-comment">// 如果启用了最小域特性，则计算每个拓扑键的域数量。</span><br>        s.TpKeyToDomainsNum = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-type">int</span>, <span class="hljs-built_in">len</span>(constraints))<br>        <span class="hljs-keyword">for</span> tp := <span class="hljs-keyword">range</span> s.TpPairToMatchNum &#123;<br>            s.TpKeyToDomainsNum[tp.key]++<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 为每个拓扑键计算最小匹配数。</span><br>    <span class="hljs-keyword">for</span> i := <span class="hljs-number">0</span>; i &lt; <span class="hljs-built_in">len</span>(constraints); i++ &#123;<br>        key := constraints[i].TopologyKey<br>        s.TpKeyToCriticalPaths[key] = newCriticalPaths()<br>    &#125;<br>    <span class="hljs-keyword">for</span> pair, num := <span class="hljs-keyword">range</span> s.TpPairToMatchNum &#123;<br>        s.TpKeyToCriticalPaths[pair.key].update(pair.value, num)<br>    &#125;<br><br>    <span class="hljs-comment">// 返回计算出的 preFilterState。</span><br>    <span class="hljs-keyword">return</span> &amp;s, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到该函数的主要作用还是统计各个拓扑域上符合条件的pod的数量。其流程如下：</p><ol><li><p>获取所有的节点信息</p></li><li><p>查看pod是否有自定义的拓扑分布约束，如果没有就构建一个默认的约束。</p><p> 构建默认约束的函数<code>buildDefaultConstraints</code>如下，注意其输入为要调度的pod和<code>noSchedule</code>级别。</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// buildDefaultConstraints builds the constraints for a pod using</span><br><span class="hljs-comment">// .DefaultConstraints and the selectors from the services, replication</span><br><span class="hljs-comment">// controllers, replica sets and stateful sets that match the pod.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *PodTopologySpread)</span></span> buildDefaultConstraints(p *v1.Pod, action v1.UnsatisfiableConstraintAction) ([]topologySpreadConstraint, <span class="hljs-type">error</span>) &#123;<br>constraints, err := pl.filterTopologySpreadConstraints(pl.defaultConstraints, p.Labels, action)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> || <span class="hljs-built_in">len</span>(constraints) == <span class="hljs-number">0</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err<br>&#125;<br>selector := helper.DefaultSelector(p, pl.services, pl.replicationCtrls, pl.replicaSets, pl.statefulSets)<br><span class="hljs-keyword">if</span> selector.Empty() &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, <span class="hljs-literal">nil</span><br>&#125;<br><span class="hljs-keyword">for</span> i := <span class="hljs-keyword">range</span> constraints &#123;<br>constraints[i].Selector = selector<br>&#125;<br><span class="hljs-keyword">return</span> constraints, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p> 其中主要的函数是<code>filterTopologySpreadConstraints</code>函数，其传入的是pl.defaultConstraints，如下，补充了部分注释。</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// filterTopologySpreadConstraints 从 Pod 的拓扑分布约束中筛选出满足 whenUnsatisfiable 为指定 action 的约束。</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *PodTopologySpread)</span></span> filterTopologySpreadConstraints(<br>    constraints []v1.TopologySpreadConstraint,  <span class="hljs-comment">// Kubernetes v1 版本的拓扑分布约束列表。</span><br>    podLabels <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-type">string</span>,               <span class="hljs-comment">// Pod 的标签。</span><br>    action v1.UnsatisfiableConstraintAction,  <span class="hljs-comment">// 当约束不可满足时的动作。</span><br>) ([]topologySpreadConstraint, <span class="hljs-type">error</span>) &#123;  <span class="hljs-comment">// 返回筛选后的拓扑分布约束列表和错误（如果有的话）。</span><br><br>    <span class="hljs-keyword">var</span> result []topologySpreadConstraint  <span class="hljs-comment">// 初始化一个用于存放筛选结果的切片。</span><br>    <span class="hljs-keyword">for</span> _, c := <span class="hljs-keyword">range</span> constraints &#123;       <span class="hljs-comment">// 遍历所有的拓扑分布约束。</span><br>        <span class="hljs-keyword">if</span> c.WhenUnsatisfiable == action &#123;  <span class="hljs-comment">// 检查约束的 whenUnsatisfiable 字段是否与指定的 action 匹配。</span><br>            selector, err := metav1.LabelSelectorAsSelector(c.LabelSelector)  <span class="hljs-comment">// 将 LabelSelector 转换为 Selector。</span><br>            <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;  <span class="hljs-comment">// 如果转换出错，返回错误。</span><br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err<br>            &#125;<br><br>            <span class="hljs-comment">// 如果启用了按标签键匹配，并且约束中定义了 MatchLabelKeys，则从 Pod 标签中提取相应的标签。</span><br>            <span class="hljs-keyword">if</span> pl.enableMatchLabelKeysInPodTopologySpread &amp;&amp; <span class="hljs-built_in">len</span>(c.MatchLabelKeys) &gt; <span class="hljs-number">0</span> &#123;<br>                matchLabels := <span class="hljs-built_in">make</span>(labels.Set)  <span class="hljs-comment">// 初始化一个标签集合。</span><br>                <span class="hljs-keyword">for</span> _, labelKey := <span class="hljs-keyword">range</span> c.MatchLabelKeys &#123;  <span class="hljs-comment">// 遍历 MatchLabelKeys。</span><br>                    <span class="hljs-keyword">if</span> value, ok := podLabels[labelKey]; ok &#123;  <span class="hljs-comment">// 如果 Pod 标签中包含该键，则添加到集合中。</span><br>                        matchLabels[labelKey] = value<br>                    &#125;<br>                &#125;<br>                <span class="hljs-comment">// 如果集合非空，则将标签集合与选择器合并。</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(matchLabels) &gt; <span class="hljs-number">0</span> &#123;<br>                    selector = mergeLabelSetWithSelector(matchLabels, selector)<br>                &#125;<br>            &#125;<br><br>            <span class="hljs-comment">// 创建 topologySpreadConstraint 结构体实例，填充筛选出的约束的相关信息。</span><br>            tsc := topologySpreadConstraint&#123;<br>                MaxSkew:            c.MaxSkew,           <span class="hljs-comment">// 最大偏差数。</span><br>                TopologyKey:        c.TopologyKey,        <span class="hljs-comment">// 拓扑键。</span><br>                Selector:           selector,             <span class="hljs-comment">// 与标签匹配的选择器。</span><br>                <span class="hljs-comment">// 如果 MinDomains 为 nil，我们将其视为 1。</span><br>                MinDomains:         <span class="hljs-number">1</span>,<br>                <span class="hljs-comment">// 如果 NodeAffinityPolicy 为 nil，我们将其视为 &quot;Honor&quot;。</span><br>                NodeAffinityPolicy: v1.NodeInclusionPolicyHonor,<br>                <span class="hljs-comment">// 如果 NodeTaintsPolicy 为 nil，我们将其视为 &quot;Ignore&quot;。</span><br>                NodeTaintsPolicy:   v1.NodeInclusionPolicyIgnore,<br>            &#125;<br>            <span class="hljs-comment">// 如果启用了最小域特性，并且约束中定义了 MinDomains，则使用该值。</span><br>            <span class="hljs-keyword">if</span> pl.enableMinDomainsInPodTopologySpread &amp;&amp; c.MinDomains != <span class="hljs-literal">nil</span> &#123;<br>                tsc.MinDomains = *c.MinDomains<br>            &#125;<br>            <span class="hljs-comment">// 如果启用了节点包含策略特性，则根据约束中的设置更新 NodeAffinityPolicy 和 NodeTaintsPolicy。</span><br>            <span class="hljs-keyword">if</span> pl.enableNodeInclusionPolicyInPodTopologySpread &#123;<br>                <span class="hljs-keyword">if</span> c.NodeAffinityPolicy != <span class="hljs-literal">nil</span> &#123;<br>                    tsc.NodeAffinityPolicy = *c.NodeAffinityPolicy<br>                &#125;<br>                <span class="hljs-keyword">if</span> c.NodeTaintsPolicy != <span class="hljs-literal">nil</span> &#123;<br>                    tsc.NodeTaintsPolicy = *c.NodeTaintsPolicy<br>                &#125;<br>            &#125;<br>            <span class="hljs-comment">// 将筛选出的拓扑分布约束添加到结果切片中。</span><br>            result = <span class="hljs-built_in">append</span>(result, tsc)<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">// 返回筛选后的拓扑分布约束列表，如果没有错误发生，则返回 nil。</span><br>    <span class="hljs-keyword">return</span> result, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p> 其会遍历所有的constrain，只有满足默认的action，即v1.DoNotSchedule才会继续往下执行：</p><ol><li><p>处理pod的selector，注意到这里如果LabelKeys是enable的，会将LabelKeys中定义的label转化成selector，其方法是查看pod中该key对应的value，组成一个selector，然后调用<code>mergeLabelSetWithSelector</code>函数合并进原来的selector。如下</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">mergeLabelSetWithSelector</span><span class="hljs-params">(matchLabels labels.Set, s labels.Selector)</span></span> labels.Selector &#123;<br>mergedSelector := labels.SelectorFromSet(matchLabels)<br><br>requirements, ok := s.Requirements()<br><span class="hljs-keyword">if</span> !ok &#123;<br><span class="hljs-keyword">return</span> s<br>&#125;<br><br><span class="hljs-keyword">for</span> _, r := <span class="hljs-keyword">range</span> requirements &#123;<br>mergedSelector = mergedSelector.Add(r)<br>&#125;<br><br><span class="hljs-keyword">return</span> mergedSelector<br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>查看是否默认配置启用了MinDomains特性，如果是就需改MinDomains为设定值。</p></li><li><p>查看是否默认配置了节点包含策略特性，如果没有就将Pod的节点亲和性约束和节点污点特性添加进来。</p></li></ol></li><li><p>如果拓扑约束为空就直接返回，否则初始化一个preFilterState结构体。该结构体的定义如下</p></li></ol><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// preFilterState computed at PreFilter and used at Filter.</span><br><span class="hljs-comment">// It combines TpKeyToCriticalPaths and TpPairToMatchNum to represent:</span><br><span class="hljs-comment">// (1) critical paths where the least pods are matched on each spread constraint.</span><br><span class="hljs-comment">// (2) number of pods matched on each spread constraint.</span><br><span class="hljs-comment">// A nil preFilterState denotes it&#x27;s not set at all (in PreFilter phase);</span><br><span class="hljs-comment">// An empty preFilterState object denotes it&#x27;s a legit state and is set in PreFilter phase.</span><br><span class="hljs-comment">// Fields are exported for comparison during testing.</span><br><span class="hljs-keyword">type</span> preFilterState <span class="hljs-keyword">struct</span> &#123;<br>Constraints []topologySpreadConstraint<br><span class="hljs-comment">// We record 2 critical paths instead of all critical paths here.</span><br><span class="hljs-comment">// criticalPaths[0].MatchNum always holds the minimum matching number.</span><br><span class="hljs-comment">// criticalPaths[1].MatchNum is always greater or equal to criticalPaths[0].MatchNum, but</span><br><span class="hljs-comment">// it&#x27;s not guaranteed to be the 2nd minimum match number.</span><br>TpKeyToCriticalPaths <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]*criticalPaths<br><span class="hljs-comment">// TpKeyToDomainsNum is keyed with topologyKey, and valued with the number of domains.</span><br>TpKeyToDomainsNum <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-type">int</span><br><span class="hljs-comment">// TpPairToMatchNum is keyed with topologyPair, and valued with the number of matching pods.</span><br>TpPairToMatchNum <span class="hljs-keyword">map</span>[topologyPair]<span class="hljs-type">int</span><br>&#125;<br></code></pre></td></tr></table></figure><p>各个变量的含义参考注释如下：</p><ul><li><p><strong>Constraints：</strong>拓扑约束</p></li><li><p><strong>TpKeyToCriticalPaths：</strong>是一个map，key是拓扑键，例如Region、hostName。注意再看一下<code>criticalPaths</code>的定义，如下</p>  <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// CAVEAT: the reason that `[2]criticalPath` can work is based on the implementation of current</span><br><span class="hljs-comment">// preemption algorithm, in particular the following 2 facts:</span><br><span class="hljs-comment">// Fact 1: we only preempt pods on the same node, instead of pods on multiple nodes.</span><br><span class="hljs-comment">// Fact 2: each node is evaluated on a separate copy of the preFilterState during its preemption cycle.</span><br><span class="hljs-comment">// If we plan to turn to a more complex algorithm like &quot;arbitrary pods on multiple nodes&quot;, this</span><br><span class="hljs-comment">// structure needs to be revisited.</span><br><span class="hljs-comment">// Fields are exported for comparison during testing.</span><br><span class="hljs-comment">// 翻译：</span><br><span class="hljs-comment">// [2]criticalPath 之所以能够工作，是基于当前的抢占算法实现，特别是以下两个事实：</span><br><span class="hljs-comment">// 事实 1：我们只在同一个节点上抢占 pods，而不是在多个节点上的 pods。</span><br><span class="hljs-comment">// 事实 2：在每个节点的抢占周期中，对其评估时使用的是 preFilterState 的一个独立副本。 </span><br><span class="hljs-comment">// 如果我们计划转向更复杂的算法，如“多个节点上的任意 pods”，则需要重新审视这种结构。 </span><br><span class="hljs-comment">// 字段在测试期间被导出以供比较。</span><br><span class="hljs-keyword">type</span> criticalPaths [<span class="hljs-number">2</span>]<span class="hljs-keyword">struct</span> &#123;<br><span class="hljs-comment">// TopologyValue denotes the topology value mapping to topology key.</span><br>TopologyValue <span class="hljs-type">string</span><br><span class="hljs-comment">// MatchNum denotes the number of matching pods.</span><br>MatchNum <span class="hljs-type">int</span><br>&#125;<br></code></pre></td></tr></table></figure><p>  可以看到<code>criticalPaths</code>定义了这个拓扑键下的拓扑域中匹配的pod数量。</p><p>  而看注释可以得知<code>TpKeyToCriticalPaths[0]</code> 定义了这个域中最少匹配的pod的数量及相应的value，而<code>TpKeyToCriticalPaths[1]</code> 记录了另一个value的情况，它匹配的数量肯定不会比<code>TpKeyToCriticalPaths[0]</code> 小。</p></li><li><p><strong>TpKeyToDomainsNum：</strong>各个拓扑键下的拓扑域的数量。</p></li><li><p><strong>TpPairToMatchNum：</strong>各个拓扑域匹配的pod的数量。可以注意一下topologyPair的定义，如下：</p>  <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> topologyPair <span class="hljs-keyword">struct</span> &#123;<br>key   <span class="hljs-type">string</span><br>value <span class="hljs-type">string</span><br>&#125;<br></code></pre></td></tr></table></figure></li></ul><ol><li><p>并行遍历各个node，统计各个node上各个拓扑域中匹配到的pod的数量。其流程如下</p><ol><li><p>得到node的信息。</p></li><li><p>如果没有启用节点包含策略特性，那么就检查节点是否符合pod的节点亲和性及node selector的要求，如果不符合就直接返回，如下</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Match checks whether the pod is schedulable onto nodes according to</span><br><span class="hljs-comment">// the requirements in both nodeSelector and nodeAffinity.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(s RequiredNodeAffinity)</span></span> Match(node *v1.Node) (<span class="hljs-type">bool</span>, <span class="hljs-type">error</span>) &#123;<br><span class="hljs-keyword">if</span> s.labelSelector != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">if</span> !s.labelSelector.Matches(labels.Set(node.Labels)) &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>, <span class="hljs-literal">nil</span><br>&#125;<br>&#125;<br><span class="hljs-keyword">if</span> s.nodeSelector != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> s.nodeSelector.Match(node)<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>, <span class="hljs-literal">nil</span><br>&#125;<br><br></code></pre></td></tr></table></figure></li><li><p>检查当前node是否包含了约束需要的拓扑键，即是否有类似于约束要求的region的定义，如果没有就直接返回。</p></li><li><p>遍历每个拓扑约束：</p><ol><li><p>如果启用了节点包含策略特性，那么就根据配置有选择性地检查当前节点是否符合pod的节点亲和性及node selector的要求，以及是否符合节点污点及容忍的要求。如果不符合，就跳过这个拓扑约束。<code>matchNodeInclusionPolicies</code>函数如下</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(tsc *topologySpreadConstraint)</span></span> matchNodeInclusionPolicies(pod *v1.Pod, node *v1.Node, require nodeaffinity.RequiredNodeAffinity) <span class="hljs-type">bool</span> &#123;<br><span class="hljs-keyword">if</span> tsc.NodeAffinityPolicy == v1.NodeInclusionPolicyHonor &#123;<br><span class="hljs-comment">// We ignore parsing errors here for backwards compatibility.</span><br><span class="hljs-keyword">if</span> match, _ := require.Match(node); !match &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">false</span><br>&#125;<br>&#125;<br><br><span class="hljs-keyword">if</span> tsc.NodeTaintsPolicy == v1.NodeInclusionPolicyHonor &#123;<br><span class="hljs-keyword">if</span> _, untolerated := v1helper.FindMatchingUntoleratedTaint(node.Spec.Taints, pod.Spec.Tolerations, helper.DoNotScheduleTaintsFilterFunc()); untolerated &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">false</span><br>&#125;<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">true</span><br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>计算这个node上满足selector要求的pod的数量，<code>countPodsMatchSelector</code>函数如下：</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">countPodsMatchSelector</span><span class="hljs-params">(podInfos []*framework.PodInfo, selector labels.Selector, ns <span class="hljs-type">string</span>)</span></span> <span class="hljs-type">int</span> &#123;<br><span class="hljs-keyword">if</span> selector.Empty() &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>&#125;<br>count := <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> _, p := <span class="hljs-keyword">range</span> podInfos &#123;<br><span class="hljs-comment">// Bypass terminating Pod (see #87621).</span><br><span class="hljs-keyword">if</span> p.Pod.DeletionTimestamp != <span class="hljs-literal">nil</span> || p.Pod.Namespace != ns &#123;<br><span class="hljs-keyword">continue</span><br>&#125;<br><span class="hljs-keyword">if</span> selector.Matches(labels.Set(p.Pod.Labels)) &#123;<br>count++<br>&#125;<br>&#125;<br><span class="hljs-keyword">return</span> count<br>&#125;<br></code></pre></td></tr></table></figure><p> 注意这里专门过来了已经删除的pod以及不和要调度的pod在同一namespace的pod。【或许未来可以考虑跨namespace的pod的拓扑约束】</p></li></ol></li><li><p>对各个拓扑域匹配的pod的数量进行记录。</p></li></ol></li><li><p>汇总统计结果，转化为记录各个拓扑域匹配的pod数量。</p></li><li><p>如果启用了<code>MinDomains</code>特性就统计各个拓扑键下拓扑值的数量。</p></li><li><p>统计各个拓扑域的最小匹配的pod数到TpKeyToCriticalPaths中，具体的更新方法如下，补充了部分注释：</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *criticalPaths)</span></span> update(tpVal <span class="hljs-type">string</span>, num <span class="hljs-type">int</span>) &#123;<br>    <span class="hljs-comment">// 首先验证 tpVal 是否已经存在于 criticalPaths 中</span><br>    i := <span class="hljs-number">-1</span> <span class="hljs-comment">// 初始化索引 i 为 -1，表示尚未找到匹配的拓扑值</span><br>    <span class="hljs-keyword">if</span> tpVal == p[<span class="hljs-number">0</span>].TopologyValue &#123;<br>        i = <span class="hljs-number">0</span> <span class="hljs-comment">// 如果 tpVal 与第一个元素的拓扑值匹配，则索引 i 设为 0</span><br>    &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> tpVal == p[<span class="hljs-number">1</span>].TopologyValue &#123;<br>        i = <span class="hljs-number">1</span> <span class="hljs-comment">// 如果 tpVal 与第二个元素的拓扑值匹配，则索引 i 设为 1</span><br>    &#125;<br><br>    <span class="hljs-keyword">if</span> i &gt;= <span class="hljs-number">0</span> &#123;<br>        <span class="hljs-comment">// 如果索引 i 是非负的，表示找到了 tpVal</span><br>        p[i].MatchNum = num <span class="hljs-comment">// 更新找到的拓扑值对应的匹配 pods 数量</span><br>        <span class="hljs-keyword">if</span> p[<span class="hljs-number">0</span>].MatchNum &gt; p[<span class="hljs-number">1</span>].MatchNum &#123;<br>            <span class="hljs-comment">// 如果第一个路径的匹配 pods 数量大于第二个路径，</span><br>            <span class="hljs-comment">// 则交换两个路径的拓扑值和匹配数量</span><br>            p[<span class="hljs-number">0</span>], p[<span class="hljs-number">1</span>] = p[<span class="hljs-number">1</span>], p[<span class="hljs-number">0</span>]<br>        &#125;<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        <span class="hljs-comment">// 如果索引 i 是 -1，表示 tpVal 在 criticalPaths 中不存在</span><br>        <span class="hljs-keyword">if</span> num &lt; p[<span class="hljs-number">0</span>].MatchNum &#123;<br>            <span class="hljs-comment">// 如果新的数量 num 小于第一个路径的匹配 pods 数量，</span><br>            <span class="hljs-comment">// 则用第一个路径的信息更新第二个路径</span><br>            p[<span class="hljs-number">1</span>] = p[<span class="hljs-number">0</span>]<br>            <span class="hljs-comment">// 然后更新第一个路径的拓扑值和匹配数量</span><br>            p[<span class="hljs-number">0</span>].TopologyValue, p[<span class="hljs-number">0</span>].MatchNum = tpVal, num<br>        &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> num &lt; p[<span class="hljs-number">1</span>].MatchNum &#123;<br>            <span class="hljs-comment">// 如果新的数量 num 小于第二个路径的匹配 pods 数量，</span><br>            <span class="hljs-comment">// 则只更新第二个路径的拓扑值和匹配数量</span><br>            p[<span class="hljs-number">1</span>].TopologyValue, p[<span class="hljs-number">1</span>].MatchNum = tpVal, num<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p> 这个方法的逻辑是，首先检查传入的拓扑键 <code>tpVal</code> 是否已经存在于 <code>criticalPaths</code> 中。如果存在，就更新对应的 <code>MatchNum</code>。如果不存在，并且新的数量 <code>num</code> 小于已有的最小数量<code>p[0].MatchNum</code>，则进行相应的更新或替换，同时把原本p[0]的内容转移到p[1]。如果num介于<code>p[0].MatchNum</code>与<code>p[1].MatchNum</code>之间就更新<code>p[1].MatchNum</code>。如此保证一般情况下p[0]是最小的，p[1]是第二小的，除非出现了传入的拓扑值 <code>tpVal</code> 是已经存在于 <code>criticalPaths</code> 中的特殊情况。</p></li></ol><h3 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h3><p><code>Filter</code>函数，如下，补充了部分注释。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Filter invoked at the filter extension point.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *PodTopologySpread)</span></span> Filter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status &#123;<br>    node := nodeInfo.Node()<br>    <span class="hljs-keyword">if</span> node == <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// 如果节点信息为空，则返回错误状态</span><br>        <span class="hljs-keyword">return</span> framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;node not found&quot;</span>))<br>    &#125;<br><br>    s, err := getPreFilterState(cycleState)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// 如果获取预过滤状态失败，则返回错误状态</span><br>        <span class="hljs-keyword">return</span> framework.AsStatus(err)<br>    &#125;<br><br>    <span class="hljs-comment">// 如果 preFilterState 为空，这是合法的，表示所有待调度的 Pods 都可以容忍</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(s.Constraints) == <span class="hljs-number">0</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>    &#125;<br><br>    podLabelSet := labels.Set(pod.Labels)<br>    <span class="hljs-keyword">for</span> _, c := <span class="hljs-keyword">range</span> s.Constraints &#123;<br>        tpKey := c.TopologyKey<br>        tpVal, ok := node.Labels[c.TopologyKey]<br>        <span class="hljs-keyword">if</span> !ok &#123;<br>            <span class="hljs-comment">// 如果节点缺少所需的标签，则记录日志并返回不可调度且不可解决的状态</span><br>            klog.V(<span class="hljs-number">5</span>).InfoS(<span class="hljs-string">&quot;Node doesn&#x27;t have required label&quot;</span>, <span class="hljs-string">&quot;node&quot;</span>, klog.KObj(node), <span class="hljs-string">&quot;label&quot;</span>, tpKey)<br>            <span class="hljs-keyword">return</span> framework.NewStatus(framework.UnschedulableAndUnresolvable, ErrReasonNodeLabelNotMatch)<br>        &#125;<br><br>        <span class="hljs-comment">// 得到全局最小匹配的pod数量</span><br>        minMatchNum, err := s.minMatchNum(tpKey, c.MinDomains, pl.enableMinDomainsInPodTopologySpread)<br>        <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-comment">// 如果在获取预过滤阶段预先计算的值时发生内部错误，则记录日志并继续</span><br>            klog.ErrorS(err, <span class="hljs-string">&quot;Internal error occurred while retrieving value precalculated in PreFilter&quot;</span>, <span class="hljs-string">&quot;topologyKey&quot;</span>, tpKey, <span class="hljs-string">&quot;paths&quot;</span>, s.TpKeyToCriticalPaths)<br>            <span class="hljs-keyword">continue</span><br>        &#125;<br><br>        selfMatchNum := <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> c.Selector.Matches(podLabelSet) &#123;<br>            <span class="hljs-comment">// 如果选择器与 Pod 的标签匹配，则自身匹配数量为 1</span><br>            selfMatchNum = <span class="hljs-number">1</span><br>        &#125;<br><br>        pair := topologyPair&#123;key: tpKey, value: tpVal&#125;<br>        matchNum := <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> tpCount, ok := s.TpPairToMatchNum[pair]; ok &#123;<br>            <span class="hljs-comment">// 如果找到拓扑对匹配的数量，则使用该数量</span><br>            matchNum = tpCount<br>        &#125;<br>        skew := matchNum + selfMatchNum - minMatchNum<br>        <span class="hljs-keyword">if</span> skew &gt; <span class="hljs-type">int</span>(c.MaxSkew) &#123;<br>            <span class="hljs-comment">// 如果偏差超过最大偏差，则记录日志并返回不可调度的状态</span><br>            klog.V(<span class="hljs-number">5</span>).InfoS(<span class="hljs-string">&quot;Node failed spreadConstraint: matchNum + selfMatchNum - minMatchNum &gt; maxSkew&quot;</span>, <span class="hljs-string">&quot;node&quot;</span>, klog.KObj(node), <span class="hljs-string">&quot;topologyKey&quot;</span>, tpKey, <span class="hljs-string">&quot;matchNum&quot;</span>, matchNum, <span class="hljs-string">&quot;selfMatchNum&quot;</span>, selfMatchNum, <span class="hljs-string">&quot;minMatchNum&quot;</span>, minMatchNum, <span class="hljs-string">&quot;maxSkew&quot;</span>, c.MaxSkew)<br>            <span class="hljs-keyword">return</span> framework.NewStatus(framework.Unschedulable, ErrReasonConstraintsNotMatch)<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 如果所有约束都满足，则返回 nil，表示节点通过过滤</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>这里的逻辑就比较简单了，整体流程如下。</p><ol><li>获取node信息及PreFilter的结果。</li><li>遍历处理每个拓扑约束constrain，注意这里的约束经过PreFilter筛选后都是没通过就不可以调度的硬约束。<ol><li><p>如果node没有这个约束需要查看的拓扑键就返回不通过筛选。</p></li><li><p>统计域中各个value全局最小的匹配到的node的数量<code>minMatchNum</code>，函数<code>minMatchNum</code>如下</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// minMatchNum returns the global minimum for the calculation of skew while taking MinDomains into account.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(s *preFilterState)</span></span> minMatchNum(tpKey <span class="hljs-type">string</span>, minDomains <span class="hljs-type">int32</span>, enableMinDomainsInPodTopologySpread <span class="hljs-type">bool</span>) (<span class="hljs-type">int</span>, <span class="hljs-type">error</span>) &#123;<br>paths, ok := s.TpKeyToCriticalPaths[tpKey]<br><span class="hljs-keyword">if</span> !ok &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>, fmt.Errorf(<span class="hljs-string">&quot;failed to retrieve path by topology key&quot;</span>)<br>&#125;<br><br>minMatchNum := paths[<span class="hljs-number">0</span>].MatchNum<br><span class="hljs-keyword">if</span> !enableMinDomainsInPodTopologySpread &#123;<br><span class="hljs-keyword">return</span> minMatchNum, <span class="hljs-literal">nil</span><br>&#125;<br><br>domainsNum, ok := s.TpKeyToDomainsNum[tpKey]<br><span class="hljs-keyword">if</span> !ok &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>, fmt.Errorf(<span class="hljs-string">&quot;failed to retrieve the number of domains by topology key&quot;</span>)<br>&#125;<br><br><span class="hljs-keyword">if</span> domainsNum &lt; <span class="hljs-type">int</span>(minDomains) &#123;<br><span class="hljs-comment">// When the number of eligible domains with matching topology keys is less than `minDomains`,</span><br><span class="hljs-comment">// it treats &quot;global minimum&quot; as 0.</span><br>minMatchNum = <span class="hljs-number">0</span><br>&#125;<br><br><span class="hljs-keyword">return</span> minMatchNum, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p> 可以看到如果没有启用MinDomains特性，就直接返回最小值，如果启用了就查看这个拓扑键下拓扑值的数量，如果小于minDomains，就返回0，否则返回最小值。</p></li><li><p>如果自身也符合这个约束的label筛选要求，那么就设置<code>selfMatchNum</code>为1，否则设置为0。</p></li><li><p>得到node所在的这个拓扑域的匹配的pod的数量<code>matchNum</code> </p></li><li><p>计算偏差度，即<code>skew=matchNum + selfMatchNum - minMatchNum</code></p></li><li><p>如果偏差度<code>skew</code>大于可以容忍的最大偏差MaxSkew，那么就返回不通过筛选</p></li></ol></li><li>通过了所有的约束检查，返回nil</li></ol><h2 id="Scoring-go"><a href="#Scoring-go" class="headerlink" title="Scoring.go"></a>Scoring.go</h2><h3 id="PreScore"><a href="#PreScore" class="headerlink" title="PreScore"></a>PreScore</h3><p><code>PreScore</code>函数如下，补充了部分注释</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// PreScore 在打分阶段之前构建并写入周期状态，这些状态将被 Score 和 NormalizeScore 使用。</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *PodTopologySpread)</span></span> PreScore(<br>    ctx context.Context,<br>    cycleState *framework.CycleState,<br>    pod *v1.Pod,<br>    filteredNodes []*v1.Node,<br>) *framework.Status &#123;<br>    allNodes, err := pl.sharedLister.NodeInfos().List()<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// 如果获取所有节点信息失败，则返回错误状态</span><br>        <span class="hljs-keyword">return</span> framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;getting all nodes: %w&quot;</span>, err))<br>    &#125;<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(filteredNodes) == <span class="hljs-number">0</span> || <span class="hljs-built_in">len</span>(allNodes) == <span class="hljs-number">0</span> &#123;<br>        <span class="hljs-comment">// 如果没有过滤后的节点或所有节点，则无需打分</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>    &#125;<br><br>    state := &amp;preScoreState&#123;<br>        IgnoredNodes:           sets.NewString(),<br>        TopologyPairToPodCounts: <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[topologyPair]*<span class="hljs-type">int64</span>),<br>    &#125;<br>    <span class="hljs-comment">// 仅在使用非系统默认的分布规则时，才要求节点拥有所有拓扑标签。</span><br>    <span class="hljs-comment">// 这允许没有区域标签的节点仍然可以进行主机名分布。</span><br>    requireAllTopologies := <span class="hljs-built_in">len</span>(pod.Spec.TopologySpreadConstraints) &gt; <span class="hljs-number">0</span> || !pl.systemDefaulted<br>    err = pl.initPreScoreState(state, pod, filteredNodes, requireAllTopologies)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// 如果计算 preScoreState 失败，则返回错误状态</span><br>        <span class="hljs-keyword">return</span> framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;calculating preScoreState: %w&quot;</span>, err))<br>    &#125;<br><br>    <span class="hljs-comment">// 如果传入的 Pod 没有软拓扑分布约束，则返回</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(state.Constraints) == <span class="hljs-number">0</span> &#123;<br>        cycleState.Write(preScoreStateKey, state)<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>    &#125;<br><br>    <span class="hljs-comment">// 为了向后兼容性，忽略解析错误。</span><br>    requiredNodeAffinity := nodeaffinity.GetRequiredNodeAffinity(pod)<br>    processAllNode := <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(i <span class="hljs-type">int</span>)</span></span> &#123;<br>        nodeInfo := allNodes[i]<br>        node := nodeInfo.Node()<br>        <span class="hljs-keyword">if</span> node == <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-keyword">return</span><br>        &#125;<br><br>        <span class="hljs-keyword">if</span> !pl.enableNodeInclusionPolicyInPodTopologySpread &#123;<br>            <span class="hljs-comment">// `node` 应该满足传入 Pod 的 NodeSelector/NodeAffinity</span><br>            <span class="hljs-keyword">if</span> match, _ := requiredNodeAffinity.Match(node); !match &#123;<br>                <span class="hljs-keyword">return</span><br>            &#125;<br>        &#125;<br><br>        <span class="hljs-comment">// 如果 requireAllTopologies 为 true，并且 `node` 缺少拓扑标签，则跳过该节点</span><br>        <span class="hljs-keyword">if</span> requireAllTopologies &amp;&amp; !nodeLabelsMatchSpreadConstraints(node.Labels, state.Constraints) &#123;<br>            <span class="hljs-keyword">return</span><br>        &#125;<br><br>        <span class="hljs-keyword">for</span> _, c := <span class="hljs-keyword">range</span> state.Constraints &#123;<br>            <span class="hljs-keyword">if</span> pl.enableNodeInclusionPolicyInPodTopologySpread &amp;&amp;<br>               !c.matchNodeInclusionPolicies(pod, node, requiredNodeAffinity) &#123;<br>                <span class="hljs-keyword">continue</span><br>            &#125;<br><br>            pair := topologyPair&#123;key: c.TopologyKey, value: node.Labels[c.TopologyKey]&#125;<br>            <span class="hljs-comment">// 如果当前拓扑对没有与任何候选节点关联，则继续以避免不必要的计算。</span><br>            <span class="hljs-comment">// 每个节点的计数也跳过，因为它们在 Score 阶段完成。</span><br>            tpCount := state.TopologyPairToPodCounts[pair]<br>            <span class="hljs-keyword">if</span> tpCount == <span class="hljs-literal">nil</span> &#123;<br>                <span class="hljs-keyword">continue</span><br>            &#125;<br>            count := countPodsMatchSelector(nodeInfo.Pods, c.Selector, pod.Namespace)<br>            <span class="hljs-comment">// 对于匹配选择器的 Pod 数量，增加拓扑对计数</span><br>            atomic.AddInt64(tpCount, <span class="hljs-type">int64</span>(count))<br>        &#125;<br>    &#125;<br>    pl.parallelizer.Until(ctx, <span class="hljs-built_in">len</span>(allNodes), processAllNode, pl.Name())<br><br>    <span class="hljs-comment">// 将 preScoreState 写入周期状态，以便后续的 Score 或 NormalizeScore 使用</span><br>    cycleState.Write(preScoreStateKey, state)<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>其流程如下：</p><ol><li><p>获取所有节点信息，检查是否有通过筛选的node，如果没有就直接返回</p></li><li><p>检查是否需要node有所有的拓扑键，如果采取的是自定义的约束或者不是系统默认的约束，就设置为必须得满足</p></li><li><p>初始化<code>preScoreState</code>变量，关键的<code>initPreScoreState</code>函数如下：</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// initPreScoreState 遍历 &quot;filteredNodes&quot; 以筛选出没有所需拓扑键的节点，并初始化：</span><br><span class="hljs-comment">// 1) s.TopologyPairToPodCounts: 以合格的拓扑对和节点名称为键。</span><br><span class="hljs-comment">// 2) s.IgnoredNodes: 不应被打分的节点集合。</span><br><span class="hljs-comment">// 3) s.TopologyNormalizingWeight: 基于拓扑中值的数量，给予每个约束的权重。</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *PodTopologySpread)</span></span> initPreScoreState(s *preScoreState, pod *v1.Pod, filteredNodes []*v1.Node, requireAllTopologies <span class="hljs-type">bool</span>) <span class="hljs-type">error</span> &#123;<br>    <span class="hljs-keyword">var</span> err <span class="hljs-type">error</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(pod.Spec.TopologySpreadConstraints) &gt; <span class="hljs-number">0</span> &#123;<br>        <span class="hljs-comment">// 如果 Pod 规范中定义了拓扑分布约束，则过滤这些约束</span><br>        s.Constraints, err = pl.filterTopologySpreadConstraints(<br>            pod.Spec.TopologySpreadConstraints, <span class="hljs-comment">// Pod 的拓扑分布约束</span><br>            pod.Labels,                       <span class="hljs-comment">// Pod 的标签</span><br>            v1.ScheduleAnyway,                <span class="hljs-comment">// 调度策略</span><br>        )<br>        <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-comment">// 如果获取 Pod 的软拓扑分布约束时出错，则返回错误</span><br>            <span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;obtaining pod&#x27;s soft topology spread constraints: %w&quot;</span>, err)<br>        &#125;<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        <span class="hljs-comment">// 如果 Pod 规范中没有定义拓扑分布约束，则构建默认约束</span><br>        s.Constraints, err = pl.buildDefaultConstraints(pod, v1.ScheduleAnyway)<br>        <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-comment">// 如果设置默认软拓扑分布约束时出错，则返回错误</span><br>            <span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;setting default soft topology spread constraints: %w&quot;</span>, err)<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(s.Constraints) == <span class="hljs-number">0</span> &#123;<br>        <span class="hljs-comment">// 如果没有拓扑分布约束，则不需要进一步处理，直接返回 nil</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>    &#125;<br>    topoSize := <span class="hljs-built_in">make</span>([]<span class="hljs-type">int</span>, <span class="hljs-built_in">len</span>(s.Constraints)) <span class="hljs-comment">// 为每个约束维护一个拓扑大小的切片</span><br>    <span class="hljs-keyword">for</span> _, node := <span class="hljs-keyword">range</span> filteredNodes &#123;<br>        <span class="hljs-comment">// 如果要求所有拓扑键并且节点标签不匹配分布约束，则将节点添加到忽略集合中</span><br>        <span class="hljs-keyword">if</span> requireAllTopologies &amp;&amp; !nodeLabelsMatchSpreadConstraints(node.Labels, s.Constraints) &#123;<br>            s.IgnoredNodes.Insert(node.Name)<br>            <span class="hljs-keyword">continue</span><br>        &#125;<br>        <span class="hljs-keyword">for</span> i, constraint := <span class="hljs-keyword">range</span> s.Constraints &#123;<br>            <span class="hljs-comment">// 对于每个约束，检查节点是否有相应的拓扑键</span><br>            <span class="hljs-comment">// 如果是主机名标签，则跳过，因为主机名分布独立处理</span><br>            <span class="hljs-keyword">if</span> constraint.TopologyKey == v1.LabelHostname &#123;<br>                <span class="hljs-keyword">continue</span><br>            &#125;<br>            pair := topologyPair&#123;key: constraint.TopologyKey, value: node.Labels[constraint.TopologyKey]&#125;<br>            <span class="hljs-comment">// 如果拓扑对在计数映射中不存在，则初始化它并增加相应约束的拓扑大小</span><br>            <span class="hljs-keyword">if</span> s.TopologyPairToPodCounts[pair] == <span class="hljs-literal">nil</span> &#123;<br>                s.TopologyPairToPodCounts[pair] = <span class="hljs-built_in">new</span>(<span class="hljs-type">int64</span>)<br>                topoSize[i]++<br>            &#125;<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 为每个约束初始化拓扑归一化权重，基于拓扑中值的数量</span><br>    s.TopologyNormalizingWeight = <span class="hljs-built_in">make</span>([]<span class="hljs-type">float64</span>, <span class="hljs-built_in">len</span>(s.Constraints))<br>    <span class="hljs-keyword">for</span> i, c := <span class="hljs-keyword">range</span> s.Constraints &#123;<br>        sz := topoSize[i]<br>        <span class="hljs-comment">// 如果拓扑键是主机名，则使用过滤后的节点数减去忽略的节点数作为大小</span><br>        <span class="hljs-keyword">if</span> c.TopologyKey == v1.LabelHostname &#123;<br>            sz = <span class="hljs-built_in">len</span>(filteredNodes) - <span class="hljs-built_in">len</span>(s.IgnoredNodes)<br>        &#125;<br>        <span class="hljs-comment">// 计算并设置拓扑归一化权重</span><br>        s.TopologyNormalizingWeight[i] = topologyNormalizingWeight(sz)<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p> 其运行流程如下</p><ol><li>查看pod是否有拓扑约束，如果有就读取，如果没有就创建一个系统默认的的拓扑约束，与PreFilter类似。</li><li>如果拓扑约束为空直接返回nil</li><li>开始遍历筛选过后的node<ol><li><p>如果需要检查是否包含所有的拓扑键，但是node没有全部的拓扑键，那么就将这个node加入到<code>IgnoredNodes</code>中，然后跳过这个node</p></li><li><p>遍历所有的拓扑约束：</p><ol><li>如果拓扑约束的拓扑键是<code>hostName</code>那么就跳过，因为其后面会特殊处理</li><li>查看是否在之前统计了这个node上对应的拓扑键-拓扑值，如果没有就加入，然后这个拓扑约束的<code>topoSize+1</code> 。故最后<code>topoSize</code>会记录各个拓扑约束中拓扑键对应的拓扑值的个数。</li></ol></li><li><p>计算各个拓扑约束的权重，也需要遍历所有的拓扑约束</p><ol><li>得到这个拓扑约束中<code>topoSize</code> 的值</li><li>如果拓扑约束的拓扑键是<code>hostName</code>，那么就将<code>topoSize</code> 设置为<code>len(filteredNodes) - len(s.IgnoredNodes)</code> ，即不能被忽略的node的个数</li><li>计算这个拓扑约束的权重，计算函数<code>topologyNormalizingWeight</code>如下</li></ol> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// topologyNormalizingWeight calculates the weight for the topology, based on</span><br><span class="hljs-comment">// the number of values that exist for a topology.</span><br><span class="hljs-comment">// Since &lt;size&gt; is at least 1 (all nodes that passed the Filters are in the</span><br><span class="hljs-comment">// same topology), and k8s supports 5k nodes, the result is in the interval</span><br><span class="hljs-comment">// &lt;1.09, 8.52&gt;.</span><br><span class="hljs-comment">//</span><br><span class="hljs-comment">// Note: &lt;size&gt; could also be zero when no nodes have the required topologies,</span><br><span class="hljs-comment">// however we don&#x27;t care about topology weight in this case as we return a 0</span><br><span class="hljs-comment">// score for all nodes.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">topologyNormalizingWeight</span><span class="hljs-params">(size <span class="hljs-type">int</span>)</span></span> <span class="hljs-type">float64</span> &#123;<br><span class="hljs-keyword">return</span> math.Log(<span class="hljs-type">float64</span>(size + <span class="hljs-number">2</span>))<br>&#125;<br></code></pre></td></tr></table></figure><p> 权重即为<code>ln(topoSize+2)</code> ，即拓扑键对应的拓扑值种类越多，权重就越大，一般而言肯定是<code>hostName</code>这种拓扑键的约束权重最大了。</p></li></ol></li></ol></li><li><p>如果权重不为空就将权重写入到<code>cycleState</code>中</p></li><li><p>得到要调度的pod的node亲和性</p></li><li><p>并行遍历各个node，流程如下</p><ol><li>得到node的信息</li><li>与PreFilter类似，如果没有启用节点包含策略，就检查是否通过了node亲和性及node selector的约束，如果没有直接返回</li><li>如果需要node有所有的拓扑键，但是node没有，那么也会直接返回</li><li>遍历每个拓扑约束，流程如下<ol><li><p>如果启用了节点包含策略特性，那么就根据配置有选择性地检查当前节点是否符合pod的节点亲和性及node selector的要求，以及是否符合节点污点及容忍的要求。如果不符合，就跳过这个拓扑约束。</p></li><li><p>如果拓扑键是hostName，也直接跳过（后面在<code>Score</code>时再计算）</p></li><li><p>计算这个拓扑约束的拓扑键对应的拓扑域上相匹配的pod的个数</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">countPodsMatchSelector</span><span class="hljs-params">(podInfos []*framework.PodInfo, selector labels.Selector, ns <span class="hljs-type">string</span>)</span></span> <span class="hljs-type">int</span> &#123;<br><span class="hljs-keyword">if</span> selector.Empty() &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>&#125;<br>count := <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> _, p := <span class="hljs-keyword">range</span> podInfos &#123;<br><span class="hljs-comment">// Bypass terminating Pod (see #87621).</span><br><span class="hljs-keyword">if</span> p.Pod.DeletionTimestamp != <span class="hljs-literal">nil</span> || p.Pod.Namespace != ns &#123;<br><span class="hljs-keyword">continue</span><br>&#125;<br><span class="hljs-keyword">if</span> selector.Matches(labels.Set(p.Pod.Labels)) &#123;<br>count++<br>&#125;<br>&#125;<br><span class="hljs-keyword">return</span> count<br>&#125;<br><br></code></pre></td></tr></table></figure></li></ol></li></ol></li><li><p>将结果写入到<code>cycleState</code>中，与PreFilter基本是一样的操作，最后得到是各个各个拓扑域对应的匹配的pod的数量</p></li></ol><h3 id="Score"><a href="#Score" class="headerlink" title="Score"></a>Score</h3><p>代码如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Score invoked at the Score extension point.</span><br><span class="hljs-comment">// The &quot;score&quot; returned in this function is the matching number of pods on the `nodeName`,</span><br><span class="hljs-comment">// it is normalized later.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *PodTopologySpread)</span></span> Score(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeName <span class="hljs-type">string</span>) (<span class="hljs-type">int64</span>, *framework.Status) &#123;<br>nodeInfo, err := pl.sharedLister.NodeInfos().Get(nodeName)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>, framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;getting node %q from Snapshot: %w&quot;</span>, nodeName, err))<br>&#125;<br><br>node := nodeInfo.Node()<br>s, err := getPreScoreState(cycleState)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>, framework.AsStatus(err)<br>&#125;<br><br><span class="hljs-comment">// Return if the node is not qualified.</span><br><span class="hljs-keyword">if</span> s.IgnoredNodes.Has(node.Name) &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>, <span class="hljs-literal">nil</span><br>&#125;<br><br><span class="hljs-comment">// For each present &lt;pair&gt;, current node gets a credit of &lt;matchSum&gt;.</span><br><span class="hljs-comment">// And we sum up &lt;matchSum&gt; and return it as this node&#x27;s score.</span><br><span class="hljs-keyword">var</span> score <span class="hljs-type">float64</span><br><span class="hljs-keyword">for</span> i, c := <span class="hljs-keyword">range</span> s.Constraints &#123;<br><span class="hljs-keyword">if</span> tpVal, ok := node.Labels[c.TopologyKey]; ok &#123;<br><span class="hljs-keyword">var</span> cnt <span class="hljs-type">int64</span><br><span class="hljs-keyword">if</span> c.TopologyKey == v1.LabelHostname &#123;<br>cnt = <span class="hljs-type">int64</span>(countPodsMatchSelector(nodeInfo.Pods, c.Selector, pod.Namespace))<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>pair := topologyPair&#123;key: c.TopologyKey, value: tpVal&#125;<br>cnt = *s.TopologyPairToPodCounts[pair]<br>&#125;<br>score += scoreForCount(cnt, c.MaxSkew, s.TopologyNormalizingWeight[i])<br>&#125;<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-type">int64</span>(math.Round(score)), <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>其流程比较简明，如下</p><ol><li><p>得到node的信息及PreScore的结果</p></li><li><p>如果node在没通过筛选需要被忽略的node，即<code>IgnoredNodes</code>中，那么返回分数为0</p></li><li><p>遍历各个拓扑约束，计算分数</p><ol><li><p>得到这个node对应在这个拓扑约束的拓扑键中的拓扑域</p></li><li><p>如果这个拓扑约束的拓扑键是hostName是hostName，那么就调用<code>countPodsMatchSelector</code>计算这个node上匹配的pod数量<code>cnt</code>，不然就从PreScore的结果中得到对应的拓扑域上匹配的pod数量<code>cnt</code></p></li><li><p>调用scoreForCount对当前的拓扑约束进行分数计算</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// scoreForCount calculates the score based on number of matching pods in a</span><br><span class="hljs-comment">// topology domain, the constraint&#x27;s maxSkew and the topology weight.</span><br><span class="hljs-comment">// `maxSkew-1` is added to the score so that differences between topology</span><br><span class="hljs-comment">// domains get watered down, controlling the tolerance of the score to skews.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">scoreForCount</span><span class="hljs-params">(cnt <span class="hljs-type">int64</span>, maxSkew <span class="hljs-type">int32</span>, tpWeight <span class="hljs-type">float64</span>)</span></span> <span class="hljs-type">float64</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-type">float64</span>(cnt)*tpWeight + <span class="hljs-type">float64</span>(maxSkew<span class="hljs-number">-1</span>)<br>&#125;<br></code></pre></td></tr></table></figure><p> 计算方式为<code>cnt*拓扑约束的权重+maxSkew-1</code> 。【至于为什么要这样打分暂时也没想清楚】</p></li></ol></li><li><p>最终得到了各个拓扑约束下的总分，并返回</p></li></ol><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="https://docs.youdianzhishi.com/k8s/scheduler/topology/">Pod 拓扑分布约束</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>k8s</category>
      
      <category>源码分析</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>源码分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【K8s源码分析（五）】-K8s中Pod亲和性调度插件介绍</title>
    <link href="/2024/05/11/k8sSource5/"/>
    <url>/2024/05/11/k8sSource5/</url>
    
    <content type="html"><![CDATA[<p>本次分析参考的K8s版本是<a href="https://github.com/kubernetes/kubernetes/tree/release-1.27">v1.27.0</a>。</p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>K8s调度器v1版的的默认的插件都在<code>pkg/scheduler/apis/config/v1/default_plugins.go:30</code> 中，如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// getDefaultPlugins returns the default set of plugins.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">getDefaultPlugins</span><span class="hljs-params">()</span></span> *v1.Plugins &#123;<br>plugins := &amp;v1.Plugins&#123;<br>MultiPoint: v1.PluginSet&#123;<br>Enabled: []v1.Plugin&#123;<br>&#123;Name: names.PrioritySort&#125;,<br>&#123;Name: names.NodeUnschedulable&#125;,<br>&#123;Name: names.NodeName&#125;,<br>&#123;Name: names.TaintToleration, Weight: pointer.Int32(<span class="hljs-number">3</span>)&#125;,<br>&#123;Name: names.NodeAffinity, Weight: pointer.Int32(<span class="hljs-number">2</span>)&#125;,<br>&#123;Name: names.NodePorts&#125;,<br>&#123;Name: names.NodeResourcesFit, Weight: pointer.Int32(<span class="hljs-number">1</span>)&#125;,<br>&#123;Name: names.VolumeRestrictions&#125;,<br>&#123;Name: names.EBSLimits&#125;,<br>&#123;Name: names.GCEPDLimits&#125;,<br>&#123;Name: names.NodeVolumeLimits&#125;,<br>&#123;Name: names.AzureDiskLimits&#125;,<br>&#123;Name: names.VolumeBinding&#125;,<br>&#123;Name: names.VolumeZone&#125;,<br>&#123;Name: names.PodTopologySpread, Weight: pointer.Int32(<span class="hljs-number">2</span>)&#125;,<br>&#123;Name: names.InterPodAffinity, Weight: pointer.Int32(<span class="hljs-number">2</span>)&#125;,<br>&#123;Name: names.DefaultPreemption&#125;,<br>&#123;Name: names.NodeResourcesBalancedAllocation, Weight: pointer.Int32(<span class="hljs-number">1</span>)&#125;,<br>&#123;Name: names.ImageLocality, Weight: pointer.Int32(<span class="hljs-number">1</span>)&#125;,<br>&#123;Name: names.DefaultBinder&#125;,<br>&#125;,<br>&#125;,<br>&#125;<br>applyFeatureGates(plugins)<br><br><span class="hljs-keyword">return</span> plugins<br>&#125;<br></code></pre></td></tr></table></figure><p>而本次我们主要关注的是<code>InterPodAffinity</code>插件，也就是pod的亲和性调度插件，此处是官方对其的介绍：<a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">Pod 间亲和性和反亲和性</a></p><p>在 Kubernetes 中，Pod 亲和性（Pod Affinity）是一种特性，它允许根据特定的规则来约束 Pod 可以调度的节点。使用 Pod 亲和性就可以控制 Pod 应该（或不应该）与某些其他 Pod 在同一个域内运行。这在需要一组 Pod 彼此靠近以优化网络通信或共享资源时非常有用。</p><p>Pod 亲和性分为两种类型：</p><ol><li><strong>Pod 亲和性（Pod Affinity）</strong>：定义了 Pod 应该与具有某些特定标签的 Pod 调度在同一域内。</li><li><strong>Pod 反亲和性（Pod Anti-Affinity）</strong>：定义了 Pod 不应该与具有某些特定标签的 Pod 调度在同一域内。</li></ol><p>注意如果当前正在调度的 Pod 是与其自身具有亲和性的系列中的第一个，则在通过所有其他亲和性检查后允许对其进行调度。这是通过验证集群中没有其他 pod 与该 pod 的命名空间和选择器匹配、该 pod 与其自己的术语匹配以及所选节点与所有请求的拓扑匹配来确定的。这可以确保即使所有 Pod 都指定了 Pod 间关联性，也不会出现死锁。</p><h2 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h2><p>我们现在首先定义一个基准Pod，名为<code>pod-podaffinity-target</code>，定义了它的label为<code>&#123;podenv: pro&#125;</code>并指定它必须放在node1上。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-podaffinity-target</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">dev</span><br>  <span class="hljs-attr">labels:</span><br>    <span class="hljs-attr">podenv:</span> <span class="hljs-string">pro</span> <span class="hljs-comment">#设置标签</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.17.1</span><br>  <span class="hljs-attr">nodeName:</span> <span class="hljs-string">node1</span> <span class="hljs-comment"># 将目标pod名确指定到node1上</span><br></code></pre></td></tr></table></figure><p>创建完这个Pod之后，我们再定义一个pod，名为<code>pod-podaffinity-target</code>，这个pod与<code>pod-podaffinity-target</code>具有亲和性要求，定义的域为<code>kubernetes.io/hostname</code>，即需要在同一个node上，配置文件如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-podaffinity-required</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">dev</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.17.1</span><br>  <span class="hljs-attr">affinity:</span>  <span class="hljs-comment">#亲和性设置</span><br>    <span class="hljs-attr">podAffinity:</span> <span class="hljs-comment">#设置pod亲和性</span><br>      <span class="hljs-attr">requiredDuringSchedulingIgnoredDuringExecution:</span> <span class="hljs-comment"># 硬限制</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">labelSelector:</span><br>          <span class="hljs-attr">matchExpressions:</span> <span class="hljs-comment"># 匹配env的值在[&quot;xxx&quot;]中的标签</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">podenv</span><br>            <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span><br>            <span class="hljs-attr">values:</span> [<span class="hljs-string">&quot;pro&quot;</span>]<br>        <span class="hljs-attr">namespaces:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">dev</span><br>        <span class="hljs-attr">namespaceSelector:</span><br>          <span class="hljs-attr">matchLabels:</span><br>            <span class="hljs-attr">environment:</span> <span class="hljs-string">production</span><br>        <span class="hljs-attr">topologyKey:</span> <span class="hljs-string">kubernetes.io/hostname</span><br></code></pre></td></tr></table></figure><p>然后我们再定义一个pod，名为<code>pod-podantiaffinity-required</code>，这个pod与<code>pod-podaffinity-target</code>具有反亲和性要求，定义的域为<code>kubernetes.io/hostname</code>，即不能在同一个node上运行，配置文件如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">pod-podantiaffinity-required</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">dev</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span><br>    <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.17.1</span><br>  <span class="hljs-attr">affinity:</span>  <span class="hljs-comment">#亲和性设置</span><br>    <span class="hljs-attr">podAntiAffinity:</span> <span class="hljs-comment">#设置pod亲和性</span><br>      <span class="hljs-attr">requiredDuringSchedulingIgnoredDuringExecution:</span> <span class="hljs-comment"># 硬限制</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">labelSelector:</span><br>          <span class="hljs-attr">matchExpressions:</span> <span class="hljs-comment"># 匹配podenv的值在[&quot;pro&quot;]中的标签</span><br>          <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">podenv</span><br>            <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span><br>            <span class="hljs-attr">values:</span> [<span class="hljs-string">&quot;pro&quot;</span>]<br>        <span class="hljs-attr">namespaces:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">dev</span><br>        <span class="hljs-attr">namespaceSelector:</span><br>          <span class="hljs-attr">matchLabels:</span><br>            <span class="hljs-attr">environment:</span> <span class="hljs-string">production</span><br>        <span class="hljs-attr">topologyKey:</span> <span class="hljs-string">kubernetes.io/hostname</span><br></code></pre></td></tr></table></figure><p>全部运行后，得到的所有pod如下，可以看到pod-podaffinity-required 与pod-podaffinity-target 都部署到了node1上，而pod-podantiaffinity-required没有与pod-podaffinity-target 部署在同一个节点上，而是部署在了master节点上。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># kubectl get po -n dev -o wide</span><br>NAME                           READY   STATUS    RESTARTS   AGE    IP                NODE     NOMINATED NODE   READINESS GATES<br>pod-podaffinity-required       1/1     Running   0          98s    192.168.166.169   node1    &lt;none&gt;           &lt;none&gt;<br>pod-podaffinity-target         1/1     Running   0          2m6s   192.168.166.166   node1    &lt;none&gt;           &lt;none&gt;<br>pod-podantiaffinity-required   1/1     Running   0          87s    192.168.219.106   master   &lt;none&gt;           &lt;none&gt;<br></code></pre></td></tr></table></figure><h2 id="亲和性细节介绍"><a href="#亲和性细节介绍" class="headerlink" title="亲和性细节介绍"></a>亲和性细节介绍</h2><p>下面整体介绍一下配置亲和性关系时的一些细节。</p><p>在<code>affinity</code>字段下面需要设置亲和性类型，类型有<code>nodeAffinity</code>、<code>nodeAntiAffinity</code>和<code>podAffinity</code> 、<code>podAntiAffinity</code>，而此次介绍的是<code>podAffinity</code>和<code>podAntiAffinity</code> 。</p><p>在<code>podAffinity</code> 和<code>podAntiAffinity</code>下面就需要指定各个级别限制的内容，限制的级别有：</p><ul><li><code>requiredDuringSchedulingIgnoredDuringExecution</code> :在调度时必须要满足亲和性要求，但是在运行过程中可以无视亲和性要求</li><li><code>preferredDuringSchedulingIgnoredDuringExecution</code> :其配置的亲和性要求是优选的，但不一定必须满足。在运行过程中也是可以无视。</li></ul><p><em>看名字K8s应该有准备在后面加入在运行过程中有变化时也得遵循的亲和性要求，但是目前还没有支持，可以后续期待一下。</em></p><p>每个限制级别下面就有各个选择器，用来选择检查和哪些pod的亲和性关系，亲和性粒度怎么样。主要有：</p><ul><li><strong>labelSelector</strong>：根据label来筛选的pod进行亲和性检查。</li><li><strong>namespaces：</strong>直接规定亲和性要求所适用的namespace名字，不标注或值为[] 则表示只在此pod所对应的namespace中生效。</li><li><strong>namespaceSelector：</strong>根据标签查询来规定亲和性要求所适用的namespace，和namespaces是或的关系，namespace只需要满足两者其一就可以了。值为{}表示适用于所有namespace ，值为null或不标注着表示只在此pod所对应的namespace中生效。</li><li><strong>topologyKey</strong>：确定亲和性处理的范围，<code>kubernetes.io/hostname</code>就代表不能在同一个主机上，还有其他的key，例如：<code>topology.kubernetes.io/zone</code>、<code>topology.kubernetes.io/region</code>、<code>topology.kubernetes.io/os</code>、<code>topology.kubernetes.io/architecture</code>等，也可以自行定义。</li><li><strong>matchLabelKeys、mismatchLabelKeys（在v1.29均为alpha阶段，高于本次讨论版本）：</strong>主要就是通过label的key值来确定要（或不要）使用的pod</li></ul><h1 id="亲和性调度插件代码分析"><a href="#亲和性调度插件代码分析" class="headerlink" title="亲和性调度插件代码分析"></a>亲和性调度插件代码分析</h1><p>亲和性调度插件的代码在<code>pkg/scheduler/framework/plugins/interpodaffinity</code> 文件夹下面，其文件包含如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">interpodaffinity/<br>├── filtering.go<br>├── filtering_test.go<br>├── plugin.go<br>├── plugin_test.go<br>├── scoring.go<br>└── scoring_test.go<br></code></pre></td></tr></table></figure><h2 id="plugin-go文件"><a href="#plugin-go文件" class="headerlink" title="plugin.go文件"></a>plugin.go文件</h2><p>首先查看其一开始部分的代码：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Name is the name of the plugin used in the plugin registry and configurations.</span><br><span class="hljs-keyword">const</span> Name = names.InterPodAffinity<br><br><span class="hljs-keyword">var</span> _ framework.PreFilterPlugin = &amp;InterPodAffinity&#123;&#125;<br><span class="hljs-keyword">var</span> _ framework.FilterPlugin = &amp;InterPodAffinity&#123;&#125;<br><span class="hljs-keyword">var</span> _ framework.PreScorePlugin = &amp;InterPodAffinity&#123;&#125;<br><span class="hljs-keyword">var</span> _ framework.ScorePlugin = &amp;InterPodAffinity&#123;&#125;<br><span class="hljs-keyword">var</span> _ framework.EnqueueExtensions = &amp;InterPodAffinity&#123;&#125;<br><br><span class="hljs-comment">// InterPodAffinity is a plugin that checks inter pod affinity</span><br><span class="hljs-keyword">type</span> InterPodAffinity <span class="hljs-keyword">struct</span> &#123;<br>parallelizer parallelize.Parallelizer<br>args         config.InterPodAffinityArgs<br>sharedLister framework.SharedLister<br>nsLister     listersv1.NamespaceLister<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到它定义了自己的名字，然后检查了自己是否实现了以下插件的接口：</p><ul><li><strong>PreFilter</strong></li><li><strong>Filter</strong></li><li><strong>PreScore</strong></li><li><strong>Score</strong></li><li><strong>EnqueueExtensions</strong></li></ul><p>然后看到其自身额外需要的变量也不多，各个需要的变量的含义如下：</p><ol><li><strong>parallelizer</strong>:<ul><li>类型：<code>parallelize.Parallelizer</code></li><li>用途：<code>Parallelizer</code> 是一个并行处理工具，它允许调度框架并行地执行某些操作，例如并行地对多个节点进行过滤或打分。这样可以提高调度的效率。</li></ul></li><li><strong>args</strong>:<ul><li>类型：<code>config.InterPodAffinityArgs</code></li><li>用途：<code>args</code> 存储了初始化插件时所需的参数。在 <code>InterPodAffinity</code> 的上下文中，<code>config.InterPodAffinityArgs</code> 可能包含与 Pod 亲和性相关的配置，如默认的命名空间选择行为或其他与亲和性规则相关的设置。</li></ul></li><li><strong>sharedLister</strong>:<ul><li>类型：<code>framework.SharedLister</code></li><li>用途：<code>sharedLister</code> 是一个共享的列表器（Lister），它提供了对 Kubernetes 资源的访问，如 Pods、Nodes 等。这个列表器允许插件在调度决策过程中获取当前集群状态的视图。</li></ul></li><li><strong>nsLister</strong>:<ul><li>类型：<code>listersv1.NamespaceLister</code></li><li>用途：<code>nsLister</code> 是一个专门用于命名空间（Namespace）的列表器。它允许插件获取关于 Kubernetes 命名空间的信息，这在处理跨命名空间的亲和性规则时非常有用。</li></ul></li></ol><p>注意到<code>EnqueueExtensions</code>的接口定义如下，即希望对于所有会让pod失败的插件都定义一个<code>EventsToRegister</code>函数，来让系统知道何时可以产生一个事件让失败的pod重新调度。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// EnqueueExtensions is an optional interface that plugins can implement to efficiently</span><br><span class="hljs-comment">// move unschedulable Pods in internal scheduling queues. Plugins</span><br><span class="hljs-comment">// that fail pod scheduling (e.g., Filter plugins) are expected to implement this interface.</span><br><span class="hljs-keyword">type</span> EnqueueExtensions <span class="hljs-keyword">interface</span> &#123;<br><span class="hljs-comment">// EventsToRegister returns a series of possible events that may cause a Pod</span><br><span class="hljs-comment">// failed by this plugin schedulable.</span><br><span class="hljs-comment">// The events will be registered when instantiating the internal scheduling queue,</span><br><span class="hljs-comment">// and leveraged to build event handlers dynamically.</span><br><span class="hljs-comment">// Note: the returned list needs to be static (not depend on configuration parameters);</span><br><span class="hljs-comment">// otherwise it would lead to undefined behavior.</span><br>EventsToRegister() []ClusterEvent<br>&#125;<br></code></pre></td></tr></table></figure><p>再查看该插件定义的<code>EventsToRegister</code>函数，可以看到如果使用这个插件，那么如果pod或者Node有添加、删除、label更新等操作都会触发事件，使得将失败的pod进行移入到activeQ或backOffQ中，详见之前的对调度队列的介绍：<a href="https://slipegg.github.io/2024/05/10/k8sSource2/">K8s源码分析（二）-K8s调度队列介绍</a></p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// EventsToRegister returns the possible events that may make a failed Pod</span><br><span class="hljs-comment">// schedulable</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *InterPodAffinity)</span></span> EventsToRegister() []framework.ClusterEvent &#123;<br><span class="hljs-keyword">return</span> []framework.ClusterEvent&#123;<br><span class="hljs-comment">// All ActionType includes the following events:</span><br><span class="hljs-comment">// - Delete. An unschedulable Pod may fail due to violating an existing Pod&#x27;s anti-affinity constraints,</span><br><span class="hljs-comment">// deleting an existing Pod may make it schedulable.</span><br><span class="hljs-comment">// - Update. Updating on an existing Pod&#x27;s labels (e.g., removal) may make</span><br><span class="hljs-comment">// an unschedulable Pod schedulable.</span><br><span class="hljs-comment">// - Add. An unschedulable Pod may fail due to violating pod-affinity constraints,</span><br><span class="hljs-comment">// adding an assigned Pod may make it schedulable.</span><br>&#123;Resource: framework.Pod, ActionType: framework.All&#125;,<br>&#123;Resource: framework.Node, ActionType: framework.Add | framework.UpdateNodeLabel&#125;,<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>再查看其定义的初始化代码New，如下。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// New initializes a new plugin and returns it.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">New</span><span class="hljs-params">(plArgs runtime.Object, h framework.Handle)</span></span> (framework.Plugin, <span class="hljs-type">error</span>) &#123;<br><span class="hljs-keyword">if</span> h.SnapshotSharedLister() == <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, fmt.Errorf(<span class="hljs-string">&quot;SnapshotSharedlister is nil&quot;</span>)<br>&#125;<br>args, err := getArgs(plArgs)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err<br>&#125;<br><span class="hljs-keyword">if</span> err := validation.ValidateInterPodAffinityArgs(<span class="hljs-literal">nil</span>, &amp;args); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err<br>&#125;<br>pl := &amp;InterPodAffinity&#123;<br>parallelizer: h.Parallelizer(),<br>args:         args,<br>sharedLister: h.SnapshotSharedLister(),<br>nsLister:     h.SharedInformerFactory().Core().V1().Namespaces().Lister(),<br>&#125;<br><br><span class="hljs-keyword">return</span> pl, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="filtering-go"><a href="#filtering-go" class="headerlink" title="filtering.go"></a>filtering.go</h2><p>filter筛选时都是根据硬约束，即<code>requiredDuringSchedulingIgnoredDuringExecution</code> 的要求来进行筛选。其代码在<code>pkg/scheduler/framework/plugins/interpodaffinity/filtering.go</code>中。</p><h3 id="PreFilter"><a href="#PreFilter" class="headerlink" title="PreFilter"></a>PreFilter</h3><p>其代码如下，补充了部分注释。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// PreFilter invoked at the prefilter extension point.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *InterPodAffinity)</span></span> PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) &#123;<br>    <span class="hljs-comment">// 定义几个变量，用于存储所有节点信息和具有必需的反亲和性 Pod 的节点信息</span><br>    <span class="hljs-keyword">var</span> allNodes []*framework.NodeInfo<br>    <span class="hljs-keyword">var</span> nodesWithRequiredAntiAffinityPods []*framework.NodeInfo<br>    <span class="hljs-keyword">var</span> err <span class="hljs-type">error</span><br>    <br>    <span class="hljs-comment">// 获取所有节点信息</span><br>    <span class="hljs-keyword">if</span> allNodes, err = pl.sharedLister.NodeInfos().List(); err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;failed to list NodeInfos: %w&quot;</span>, err))<br>    &#125;<br>    <span class="hljs-comment">// 获取具有必需的反亲和性 Pod 的节点列表</span><br>    <span class="hljs-keyword">if</span> nodesWithRequiredAntiAffinityPods, err = pl.sharedLister.NodeInfos().HavePodsWithRequiredAntiAffinityList(); err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;failed to list NodeInfos with pods with affinity: %w&quot;</span>, err))<br>    &#125;<br><br>    <span class="hljs-comment">// 初始化预过滤状态</span><br>    s := &amp;preFilterState&#123;&#125;<br><br>    <span class="hljs-comment">// 从 Pod 中解析 Pod 信息</span><br>    <span class="hljs-keyword">if</span> s.podInfo, err = framework.NewPodInfo(pod); err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, framework.NewStatus(framework.UnschedulableAndUnresolvable, fmt.Sprintf(<span class="hljs-string">&quot;parsing pod: %+v&quot;</span>, err))<br>    &#125;<br><br>    <span class="hljs-comment">// 处理 Pod 亲和性项的命名空间</span><br>    <span class="hljs-keyword">for</span> i := <span class="hljs-keyword">range</span> s.podInfo.RequiredAffinityTerms &#123;<br>        <span class="hljs-keyword">if</span> err := pl.mergeAffinityTermNamespacesIfNotEmpty(&amp;s.podInfo.RequiredAffinityTerms[i]); err != <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, framework.AsStatus(err)<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">// 处理 Pod 反亲和性项的命名空间</span><br>    <span class="hljs-keyword">for</span> i := <span class="hljs-keyword">range</span> s.podInfo.RequiredAntiAffinityTerms &#123;<br>        <span class="hljs-keyword">if</span> err := pl.mergeAffinityTermNamespacesIfNotEmpty(&amp;s.podInfo.RequiredAntiAffinityTerms[i]); err != <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, framework.AsStatus(err)<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">// 获取命名空间标签的快照</span><br>    s.namespaceLabels = GetNamespaceLabelsSnapshot(pod.Namespace, pl.nsLister)<br><br>    <span class="hljs-comment">// 获取现有反亲和性计数</span><br>    s.existingAntiAffinityCounts = pl.getExistingAntiAffinityCounts(ctx, pod, s.namespaceLabels, nodesWithRequiredAntiAffinityPods)<br>    <span class="hljs-comment">// 获取即将到来的亲和性和反亲和性计数</span><br>    s.affinityCounts, s.antiAffinityCounts = pl.getIncomingAffinityAntiAffinityCounts(ctx, s.podInfo, allNodes)<br><br>    <span class="hljs-comment">// 如果没有现有的反亲和性计数，并且 Pod 没有必需的亲和性或反亲和性项，则跳过</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(s.existingAntiAffinityCounts) == <span class="hljs-number">0</span> &amp;&amp; <span class="hljs-built_in">len</span>(s.podInfo.RequiredAffinityTerms) == <span class="hljs-number">0</span> &amp;&amp; <span class="hljs-built_in">len</span>(s.podInfo.RequiredAntiAffinityTerms) == <span class="hljs-number">0</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, framework.NewStatus(framework.Skip)<br>    &#125;<br><br>    <span class="hljs-comment">// 将预过滤状态写入周期状态</span><br>    cycleState.Write(preFilterStateKey, s)<br>    <span class="hljs-comment">// 函数返回 nil，表示不需要进一步处理</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>下面具体来分析一下这部分代码。</p><ol><li><p>获取了所有节点信息</p></li><li><p>获取了所有拥有反亲和性 Pod 的节点列表</p></li><li><p>合并亲和性生效的namespace，函数<code>mergeAffinityTermNamespacesIfNotEmpty</code>如下。其做法是将NamespaceSelector中筛选的所有namespace插入到直接规定的namespaces中，从这里也可以看出来上面提到的namespaceSelector和namespaces是或的关系。</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Updates Namespaces with the set of namespaces identified by NamespaceSelector.</span><br><span class="hljs-comment">// If successful, NamespaceSelector is set to nil.</span><br><span class="hljs-comment">// The assumption is that the term is for an incoming pod, in which case</span><br><span class="hljs-comment">// namespaceSelector is either unrolled into Namespaces (and so the selector</span><br><span class="hljs-comment">// is set to Nothing()) or is Empty(), which means match everything. Therefore,</span><br><span class="hljs-comment">// there when matching against this term, there is no need to lookup the existing</span><br><span class="hljs-comment">// pod&#x27;s namespace labels to match them against term&#x27;s namespaceSelector explicitly.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *InterPodAffinity)</span></span> mergeAffinityTermNamespacesIfNotEmpty(at *framework.AffinityTerm) <span class="hljs-type">error</span> &#123;<br><span class="hljs-keyword">if</span> at.NamespaceSelector.Empty() &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br>ns, err := pl.nsLister.List(at.NamespaceSelector)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br><span class="hljs-keyword">for</span> _, n := <span class="hljs-keyword">range</span> ns &#123;<br>at.Namespaces.Insert(n.Name)<br>&#125;<br>at.NamespaceSelector = labels.Nothing()<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>合并反亲和性生效的namespace，与合并亲和性类似。</p></li><li><p>得到namespace的label的快照，并保存起来。</p></li><li><p>查看已经部署运行具有反亲和性约束的pod，查看这些已有的反亲和性约束与要调度的pod之间的约束结果，函数<code>getExistingAntiAffinityCounts</code>如下。</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// calculates the following for each existing pod on each node:</span><br><span class="hljs-comment">//  1. Whether it has PodAntiAffinity</span><br><span class="hljs-comment">//  2. Whether any AntiAffinityTerm matches the incoming pod</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *InterPodAffinity)</span></span> getExistingAntiAffinityCounts(ctx context.Context, pod *v1.Pod, nsLabels labels.Set, nodes []*framework.NodeInfo) topologyToMatchedTermCount &#123;<br>topoMaps := <span class="hljs-built_in">make</span>([]topologyToMatchedTermCount, <span class="hljs-built_in">len</span>(nodes))<br>index := <span class="hljs-type">int32</span>(<span class="hljs-number">-1</span>)<br>processNode := <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(i <span class="hljs-type">int</span>)</span></span> &#123;<br>nodeInfo := nodes[i]<br>node := nodeInfo.Node()<br><span class="hljs-keyword">if</span> node == <span class="hljs-literal">nil</span> &#123;<br>klog.ErrorS(<span class="hljs-literal">nil</span>, <span class="hljs-string">&quot;Node not found&quot;</span>)<br><span class="hljs-keyword">return</span><br>&#125;<br>topoMap := <span class="hljs-built_in">make</span>(topologyToMatchedTermCount)<br><span class="hljs-keyword">for</span> _, existingPod := <span class="hljs-keyword">range</span> nodeInfo.PodsWithRequiredAntiAffinity &#123;<br>topoMap.updateWithAntiAffinityTerms(existingPod.RequiredAntiAffinityTerms, pod, nsLabels, node, <span class="hljs-number">1</span>)<br>&#125;<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(topoMap) != <span class="hljs-number">0</span> &#123;<br>topoMaps[atomic.AddInt32(&amp;index, <span class="hljs-number">1</span>)] = topoMap<br>&#125;<br>&#125;<br>pl.parallelizer.Until(ctx, <span class="hljs-built_in">len</span>(nodes), processNode, pl.Name())<br><br>result := <span class="hljs-built_in">make</span>(topologyToMatchedTermCount)<br><span class="hljs-keyword">for</span> i := <span class="hljs-number">0</span>; i &lt;= <span class="hljs-type">int</span>(index); i++ &#123;<br>result.<span class="hljs-built_in">append</span>(topoMaps[i])<br>&#125;<br><br><span class="hljs-keyword">return</span> result<br>&#125;<br></code></pre></td></tr></table></figure><p> 首先看一下这个函数的中的topologyToMatchedTermCount的定义，如下</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> topologyPair <span class="hljs-keyword">struct</span> &#123;<br>key   <span class="hljs-type">string</span><br>value <span class="hljs-type">string</span><br>&#125;<br><span class="hljs-keyword">type</span> topologyToMatchedTermCount <span class="hljs-keyword">map</span>[topologyPair]<span class="hljs-type">int64</span><br><br><span class="hljs-comment">// ...</span><br><br><span class="hljs-comment">// updates the topologyToMatchedTermCount map with the specified value</span><br><span class="hljs-comment">// for each anti-affinity term matched the target pod.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(m topologyToMatchedTermCount)</span></span> updateWithAntiAffinityTerms(terms []framework.AffinityTerm, pod *v1.Pod, nsLabels labels.Set, node *v1.Node, value <span class="hljs-type">int64</span>) &#123;<br><span class="hljs-comment">// Check anti-affinity terms.</span><br><span class="hljs-keyword">for</span> _, t := <span class="hljs-keyword">range</span> terms &#123;<br><span class="hljs-keyword">if</span> t.Matches(pod, nsLabels) &#123;<br>m.update(node, t.TopologyKey, value)<br>&#125;<br>&#125;<br>&#125;<br><br><span class="hljs-comment">// ...</span><br><br><span class="hljs-comment">// Matches returns true if the pod matches the label selector and namespaces or namespace selector.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(at *AffinityTerm)</span></span> Matches(pod *v1.Pod, nsLabels labels.Set) <span class="hljs-type">bool</span> &#123;<br><span class="hljs-keyword">if</span> at.Namespaces.Has(pod.Namespace) || at.NamespaceSelector.Matches(nsLabels) &#123;<br><span class="hljs-keyword">return</span> at.Selector.Matches(labels.Set(pod.Labels))<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">false</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(m topologyToMatchedTermCount)</span></span> update(node *v1.Node, tk <span class="hljs-type">string</span>, value <span class="hljs-type">int64</span>) &#123;<br><span class="hljs-keyword">if</span> tv, ok := node.Labels[tk]; ok &#123;<br>pair := topologyPair&#123;key: tk, value: tv&#125;<br>m[pair] += value<br><span class="hljs-comment">// value could be negative, hence we delete the entry if it is down to zero.</span><br><span class="hljs-keyword">if</span> m[pair] == <span class="hljs-number">0</span> &#123;<br><span class="hljs-built_in">delete</span>(m, pair)<br>&#125;<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p> 可以看到是一个topologyToMatchedTermCount是一个map，不过这里其value一直为1.函数<code>getExistingAntiAffinityCounts</code>这里会遍历有反亲和性约束的pod，然后看其该pod反亲和性约束所生效的namespace和所生效的label的范围是不是包含了要调度的pod，如果是就将作用域及作用域的具体值所对应的值加上value（也就是1），然后返回回去。最后得到的结果就例如：对于hostname域为node1的具有反亲和性约束的pod的数量为2，对于region域为east1的具有反亲和性约束的pod的数量为1。<strong>如此就将对于pod的反亲和性约束转变为了对于各个域的反亲和性约束。</strong></p></li><li><p>然后查看要调度的pod的亲和性与反亲和性约束，查看已有的pod与该pod的约束结果，函数<code>getIncomingAffinityAntiAffinityCounts</code>如下。</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// finds existing Pods that match affinity terms of the incoming pod&#x27;s (anti)affinity terms.</span><br><span class="hljs-comment">// It returns a topologyToMatchedTermCount that are checked later by the affinity</span><br><span class="hljs-comment">// predicate. With this topologyToMatchedTermCount available, the affinity predicate does not</span><br><span class="hljs-comment">// need to check all the pods in the cluster.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *InterPodAffinity)</span></span> getIncomingAffinityAntiAffinityCounts(ctx context.Context, podInfo *framework.PodInfo, allNodes []*framework.NodeInfo) (topologyToMatchedTermCount, topologyToMatchedTermCount) &#123;<br>affinityCounts := <span class="hljs-built_in">make</span>(topologyToMatchedTermCount)<br>antiAffinityCounts := <span class="hljs-built_in">make</span>(topologyToMatchedTermCount)<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(podInfo.RequiredAffinityTerms) == <span class="hljs-number">0</span> &amp;&amp; <span class="hljs-built_in">len</span>(podInfo.RequiredAntiAffinityTerms) == <span class="hljs-number">0</span> &#123;<br><span class="hljs-keyword">return</span> affinityCounts, antiAffinityCounts<br>&#125;<br><br>affinityCountsList := <span class="hljs-built_in">make</span>([]topologyToMatchedTermCount, <span class="hljs-built_in">len</span>(allNodes))<br>antiAffinityCountsList := <span class="hljs-built_in">make</span>([]topologyToMatchedTermCount, <span class="hljs-built_in">len</span>(allNodes))<br>index := <span class="hljs-type">int32</span>(<span class="hljs-number">-1</span>)<br>processNode := <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(i <span class="hljs-type">int</span>)</span></span> &#123;<br>nodeInfo := allNodes[i]<br>node := nodeInfo.Node()<br><span class="hljs-keyword">if</span> node == <span class="hljs-literal">nil</span> &#123;<br>klog.ErrorS(<span class="hljs-literal">nil</span>, <span class="hljs-string">&quot;Node not found&quot;</span>)<br><span class="hljs-keyword">return</span><br>&#125;<br>affinity := <span class="hljs-built_in">make</span>(topologyToMatchedTermCount)<br>antiAffinity := <span class="hljs-built_in">make</span>(topologyToMatchedTermCount)<br><span class="hljs-keyword">for</span> _, existingPod := <span class="hljs-keyword">range</span> nodeInfo.Pods &#123;<br>affinity.updateWithAffinityTerms(podInfo.RequiredAffinityTerms, existingPod.Pod, node, <span class="hljs-number">1</span>)<br><span class="hljs-comment">// The incoming pod&#x27;s terms have the namespaceSelector merged into the namespaces, and so</span><br><span class="hljs-comment">// here we don&#x27;t lookup the existing pod&#x27;s namespace labels, hence passing nil for nsLabels.</span><br>antiAffinity.updateWithAntiAffinityTerms(podInfo.RequiredAntiAffinityTerms, existingPod.Pod, <span class="hljs-literal">nil</span>, node, <span class="hljs-number">1</span>)<br>&#125;<br><br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(affinity) &gt; <span class="hljs-number">0</span> || <span class="hljs-built_in">len</span>(antiAffinity) &gt; <span class="hljs-number">0</span> &#123;<br>k := atomic.AddInt32(&amp;index, <span class="hljs-number">1</span>)<br>affinityCountsList[k] = affinity<br>antiAffinityCountsList[k] = antiAffinity<br>&#125;<br>&#125;<br>pl.parallelizer.Until(ctx, <span class="hljs-built_in">len</span>(allNodes), processNode, pl.Name())<br><br><span class="hljs-keyword">for</span> i := <span class="hljs-number">0</span>; i &lt;= <span class="hljs-type">int</span>(index); i++ &#123;<br>affinityCounts.<span class="hljs-built_in">append</span>(affinityCountsList[i])<br>antiAffinityCounts.<span class="hljs-built_in">append</span>(antiAffinityCountsList[i])<br>&#125;<br><br><span class="hljs-keyword">return</span> affinityCounts, antiAffinityCounts<br>&#125;<br></code></pre></td></tr></table></figure><p> 操作的一些细节与6类似，主要就是并行遍历所有的node，然后运行检查每个node时，逐个检查node上的每个pod，然后检查与要调度的pod存在的亲和性、反亲和性约束的结果，并转化为了对各个域的亲和性、反亲和性约束结果，最终汇总返回。</p></li><li><p>如果与现存的具有反亲和性约束的pod没有约束关系，或者本身也没有亲和性、反亲和性约束，那么就更新状态为可跳过，并返回。不然就写入这个预筛选结果到<code>cycleState</code>中</p></li></ol><h3 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h3><p>其代码如下，补充了部分注释。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Filter invoked at the filter extension point.</span><br><span class="hljs-comment">// It checks if a pod can be scheduled on the specified node with pod affinity/anti-affinity configuration.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *InterPodAffinity)</span></span> Filter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status &#123;<br>    <span class="hljs-comment">// 检查给定的节点信息是否有效，如果节点为空，则返回错误状态</span><br>    <span class="hljs-keyword">if</span> nodeInfo.Node() == <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> framework.NewStatus(framework.Error, <span class="hljs-string">&quot;node not found&quot;</span>)<br>    &#125;<br><br>    <span class="hljs-comment">// 获取调度前的过滤状态信息，如果获取失败，则返回错误状态</span><br>    state, err := getPreFilterState(cycleState)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> framework.AsStatus(err)<br>    &#125;<br><br>    <span class="hljs-comment">// 检查是否满足 Pod 的亲和性规则，如果不满足，则返回不可调度且不可解决的状态</span><br>    <span class="hljs-keyword">if</span> !satisfyPodAffinity(state, nodeInfo) &#123;<br>        <span class="hljs-keyword">return</span> framework.NewStatus(framework.UnschedulableAndUnresolvable, ErrReasonAffinityRulesNotMatch)<br>    &#125;<br><br>    <span class="hljs-comment">// 检查是否满足 Pod 的反亲和性规则，如果不满足，则返回不可调度的状态</span><br>    <span class="hljs-keyword">if</span> !satisfyPodAntiAffinity(state, nodeInfo) &#123;<br>        <span class="hljs-keyword">return</span> framework.NewStatus(framework.Unschedulable, ErrReasonAntiAffinityRulesNotMatch)<br>    &#125;<br><br>    <span class="hljs-comment">// 检查是否满足现有 Pod 的反亲和性规则，如果不满足，则返回不可调度的状态</span><br>    <span class="hljs-keyword">if</span> !satisfyExistingPodsAntiAffinity(state, nodeInfo) &#123;<br>        <span class="hljs-keyword">return</span> framework.NewStatus(framework.Unschedulable, ErrReasonExistingAntiAffinityRulesNotMatch)<br>    &#125;<br><br>    <span class="hljs-comment">// 如果所有检查都通过，则返回 nil，表示 Pod 可以被调度到该节点</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>整体的逻辑很简明，首先是得到预筛选的结果，然后检查该node是否满足要调度的pod的亲和性、反亲和性约束，已经是否满足已运行的pod的反亲和性约束。</p><p>查看<code>satisfyPodAffinity</code>函数，如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Checks if the node satisfies the incoming pod&#x27;s affinity rules.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">satisfyPodAffinity</span><span class="hljs-params">(state *preFilterState, nodeInfo *framework.NodeInfo)</span></span> <span class="hljs-type">bool</span> &#123;<br>podsExist := <span class="hljs-literal">true</span><br><span class="hljs-keyword">for</span> _, term := <span class="hljs-keyword">range</span> state.podInfo.RequiredAffinityTerms &#123;<br><span class="hljs-keyword">if</span> topologyValue, ok := nodeInfo.Node().Labels[term.TopologyKey]; ok &#123;<br>tp := topologyPair&#123;key: term.TopologyKey, value: topologyValue&#125;<br><span class="hljs-keyword">if</span> state.affinityCounts[tp] &lt;= <span class="hljs-number">0</span> &#123;<br>podsExist = <span class="hljs-literal">false</span><br>&#125;<br>&#125; <span class="hljs-keyword">else</span> &#123;<br><span class="hljs-comment">// All topology labels must exist on the node.</span><br><span class="hljs-keyword">return</span> <span class="hljs-literal">false</span><br>&#125;<br>&#125;<br><br><span class="hljs-keyword">if</span> !podsExist &#123;<br><span class="hljs-comment">// This pod may be the first pod in a series that have affinity to themselves. In order</span><br><span class="hljs-comment">// to not leave such pods in pending state forever, we check that if no other pod</span><br><span class="hljs-comment">// in the cluster matches the namespace and selector of this pod, the pod matches</span><br><span class="hljs-comment">// its own terms, and the node has all the requested topologies, then we allow the pod</span><br><span class="hljs-comment">// to pass the affinity check.</span><br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(state.affinityCounts) == <span class="hljs-number">0</span> &amp;&amp; podMatchesAllAffinityTerms(state.podInfo.RequiredAffinityTerms, state.podInfo.Pod) &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">true</span><br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">false</span><br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">true</span><br>&#125;<br></code></pre></td></tr></table></figure><p>在函数中首先遍历了要调度的pod的亲和性约束项，PreFilter转换的亲和性约束的结果查看node所在的域是否包含有亲和性pod，如果没有就不通过。例如要调度的pod的亲和性约束约束了region域，假设这个node的region的值为west（如果没有region域那么也不通过），然后根据PreFilter的结果来看，值为west的region中统计的与其具有亲和性的pod的数量是否大于0，如果是，那么这一项检查就通过了，如果不是就不通过。</p><p>然后这里也专门统计了是否存在与其具有亲和性约束的pod，如果完全不存在，那么这个pod可能就是所有亲和性约束的第一个，那么就检查一下当前的pod是否满足自己的亲和性要求，如下，如果通过了也能让其通过这部分的筛选。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// returns true IFF the given pod matches all the given terms.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">podMatchesAllAffinityTerms</span><span class="hljs-params">(terms []framework.AffinityTerm, pod *v1.Pod)</span></span> <span class="hljs-type">bool</span> &#123;<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(terms) == <span class="hljs-number">0</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">false</span><br>&#125;<br><span class="hljs-keyword">for</span> _, t := <span class="hljs-keyword">range</span> terms &#123;<br><span class="hljs-comment">// The incoming pod NamespaceSelector was merged into the Namespaces set, and so</span><br><span class="hljs-comment">// we are not explicitly passing in namespace labels.</span><br><span class="hljs-keyword">if</span> !t.Matches(pod, <span class="hljs-literal">nil</span>) &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">false</span><br>&#125;<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">true</span><br>&#125;<br></code></pre></td></tr></table></figure><p>再查看<code>satisfyPodAntiAffinity</code>函数，如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Checks if the node satisfies the incoming pod&#x27;s anti-affinity rules.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">satisfyPodAntiAffinity</span><span class="hljs-params">(state *preFilterState, nodeInfo *framework.NodeInfo)</span></span> <span class="hljs-type">bool</span> &#123;<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(state.antiAffinityCounts) &gt; <span class="hljs-number">0</span> &#123;<br><span class="hljs-keyword">for</span> _, term := <span class="hljs-keyword">range</span> state.podInfo.RequiredAntiAffinityTerms &#123;<br><span class="hljs-keyword">if</span> topologyValue, ok := nodeInfo.Node().Labels[term.TopologyKey]; ok &#123;<br>tp := topologyPair&#123;key: term.TopologyKey, value: topologyValue&#125;<br><span class="hljs-keyword">if</span> state.antiAffinityCounts[tp] &gt; <span class="hljs-number">0</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">false</span><br>&#125;<br>&#125;<br>&#125;<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">true</span><br>&#125;<br></code></pre></td></tr></table></figure><p>也是类似，检查node所在的域是否有与该要调度的pod具有反亲和性约束的pod，如果有，就不通过筛选。</p><p>在查看<code>satisfyExistingPodsAntiAffinity</code>函数，如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Checks if scheduling the pod onto this node would break any anti-affinity</span><br><span class="hljs-comment">// terms indicated by the existing pods.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">satisfyExistingPodsAntiAffinity</span><span class="hljs-params">(state *preFilterState, nodeInfo *framework.NodeInfo)</span></span> <span class="hljs-type">bool</span> &#123;<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(state.existingAntiAffinityCounts) &gt; <span class="hljs-number">0</span> &#123;<br><span class="hljs-comment">// Iterate over topology pairs to get any of the pods being affected by</span><br><span class="hljs-comment">// the scheduled pod anti-affinity terms</span><br><span class="hljs-keyword">for</span> topologyKey, topologyValue := <span class="hljs-keyword">range</span> nodeInfo.Node().Labels &#123;<br>tp := topologyPair&#123;key: topologyKey, value: topologyValue&#125;<br><span class="hljs-keyword">if</span> state.existingAntiAffinityCounts[tp] &gt; <span class="hljs-number">0</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">false</span><br>&#125;<br>&#125;<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">true</span><br>&#125;<br></code></pre></td></tr></table></figure><p>也是类似，检查node所在的域是否有与该要调度的pod具有反亲和性约束的pod，如果有，就不通过筛选。</p><h2 id="scoring-go"><a href="#scoring-go" class="headerlink" title="scoring.go"></a>scoring.go</h2><p>打分时则依靠<code>preferredDuringSchedulingIgnoredDuringExecution</code> 来进行倾向性的打分。其代码在<code>pkg/scheduler/framework/plugins/interpodaffinity/scoring.go</code>中。</p><h3 id="PreScore"><a href="#PreScore" class="headerlink" title="PreScore"></a>PreScore</h3><p><code>PreScore</code>函数的代码如下，添加了部分注释。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// PreScore builds and writes cycle state used by Score and NormalizeScore.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *InterPodAffinity)</span></span> PreScore(<br>pCtx context.Context,<br>cycleState *framework.CycleState,<br>pod *v1.Pod,<br>nodes []*v1.Node,<br>) *framework.Status &#123;<br>    <span class="hljs-comment">// 如果没有节点可供打分，则直接返回 nil。</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(nodes) == <span class="hljs-number">0</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>    &#125;<br><br>    <span class="hljs-comment">// 检查 sharedLister 是否为空，如果是，则返回错误状态。</span><br>    <span class="hljs-keyword">if</span> pl.sharedLister == <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> framework.NewStatus(framework.Error, <span class="hljs-string">&quot;empty shared lister in InterPodAffinity PreScore&quot;</span>)<br>    &#125;<br><br>    <span class="hljs-comment">// 获取 Pod 的亲和性配置。</span><br>    affinity := pod.Spec.Affinity<br>    <span class="hljs-comment">// 检查是否存在亲和性约束。</span><br>    hasPreferredAffinityConstraints := affinity != <span class="hljs-literal">nil</span> &amp;&amp; affinity.PodAffinity != <span class="hljs-literal">nil</span> &amp;&amp; <span class="hljs-built_in">len</span>(affinity.PodAffinity.PreferredDuringSchedulingIgnoredDuringExecution) &gt; <span class="hljs-number">0</span><br>    <span class="hljs-comment">// 检查是否存在反亲和性约束。</span><br>    hasPreferredAntiAffinityConstraints := affinity != <span class="hljs-literal">nil</span> &amp;&amp; affinity.PodAntiAffinity != <span class="hljs-literal">nil</span> &amp;&amp; <span class="hljs-built_in">len</span>(affinity.PodAntiAffinity.PreferredDuringSchedulingIgnoredDuringExecution) &gt; <span class="hljs-number">0</span><br><br>    <span class="hljs-comment">// 如果没有首选的亲和性或反亲和性约束，并且配置指示忽略现有 Pod 的首选术语，</span><br>    <span class="hljs-comment">// 则写入一个空的 topologyScore 映射并返回 nil。</span><br>    <span class="hljs-keyword">if</span> pl.args.IgnorePreferredTermsOfExistingPods &amp;&amp; !hasPreferredAffinityConstraints &amp;&amp; !hasPreferredAntiAffinityConstraints &#123;<br>        cycleState.Write(preScoreStateKey, &amp;preScoreState&#123;<br>            topologyScore: <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-type">int64</span>),<br>        &#125;)<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>    &#125;<br><br>    <span class="hljs-keyword">var</span> allNodes []*framework.NodeInfo<br>    <span class="hljs-keyword">var</span> err <span class="hljs-type">error</span><br>    <span class="hljs-comment">// 如果有亲和性或者反亲和性约束</span><br>    <span class="hljs-keyword">if</span> hasPreferredAffinityConstraints || hasPreferredAntiAffinityConstraints &#123;<br>        <span class="hljs-comment">// 获取所有节点的信息</span><br>        allNodes, err = pl.sharedLister.NodeInfos().List()<br>        <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-comment">// 如果从 sharedLister 获取所有节点失败，则返回错误状态。</span><br>            <span class="hljs-keyword">return</span> framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;failed to get all nodes from shared lister: %w&quot;</span>, err))<br>        &#125;<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        <span class="hljs-comment">// 如果没有亲和性也没用反亲和性约束，则只获取有亲和性pod的node</span><br>        allNodes, err = pl.sharedLister.NodeInfos().HavePodsWithAffinityList()<br>        <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-comment">// 如果从 sharedLister 获取有亲和性 Pod 的节点列表失败，则返回错误状态。</span><br>            <span class="hljs-keyword">return</span> framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;failed to get pods with affinity list: %w&quot;</span>, err))<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 初始化 preScoreState 结构体，用于存储拓扑打分信息。</span><br>    state := &amp;preScoreState&#123;<br>        topologyScore: <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-type">int64</span>),<br>    &#125;<br><br>    <span class="hljs-comment">// 尝试为 Pod 创建 PodInfo，如果失败，则返回错误状态。</span><br>    <span class="hljs-keyword">if</span> state.podInfo, err = framework.NewPodInfo(pod); err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// 理论上我们不会到这里，因为错误会在 PreFilter 中被捕获。</span><br>        <span class="hljs-keyword">return</span> framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;failed to parse pod: %w&quot;</span>, err))<br>    &#125;<br><br>    <span class="hljs-comment">// 处理 Pod 的首选亲和性术语，如果合并命名空间失败，则返回错误状态。</span><br>    <span class="hljs-keyword">for</span> i := <span class="hljs-keyword">range</span> state.podInfo.PreferredAffinityTerms &#123;<br>        <span class="hljs-keyword">if</span> err := pl.mergeAffinityTermNamespacesIfNotEmpty(&amp;state.podInfo.PreferredAffinityTerms[i].AffinityTerm); err != <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-keyword">return</span> framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;updating PreferredAffinityTerms: %w&quot;</span>, err))<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">// 处理 Pod 的首选反亲和性术语，如果合并命名空间失败，则返回错误状态。</span><br>    <span class="hljs-keyword">for</span> i := <span class="hljs-keyword">range</span> state.podInfo.PreferredAntiAffinityTerms &#123;<br>        <span class="hljs-keyword">if</span> err := pl.mergeAffinityTermNamespacesIfNotEmpty(&amp;state.podInfo.PreferredAntiAffinityTerms[i].AffinityTerm); err != <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-keyword">return</span> framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;updating PreferredAntiAffinityTerms: %w&quot;</span>, err))<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">// 获取命名空间标签的快照。</span><br>    state.namespaceLabels = GetNamespaceLabelsSnapshot(pod.Namespace, pl.nsLister)<br><br>    <span class="hljs-comment">// 初始化一个列表来存储每个节点的拓扑打分信息。</span><br>    topoScores := <span class="hljs-built_in">make</span>([]scoreMap, <span class="hljs-built_in">len</span>(allNodes))<br>    index := <span class="hljs-type">int32</span>(<span class="hljs-number">-1</span>)<br>    processNode := <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(i <span class="hljs-type">int</span>)</span></span> &#123;<br>        nodeInfo := allNodes[i]<br>        <span class="hljs-keyword">if</span> nodeInfo.Node() == <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-keyword">return</span><br>        &#125;<br>        <span class="hljs-comment">// 如果 Pod 没有首选的亲和性术语，我们只需要处理节点上的有亲和性 Pod。</span><br>        podsToProcess := nodeInfo.PodsWithAffinity<br>        <span class="hljs-keyword">if</span> hasPreferredAffinityConstraints || hasPreferredAntiAffinityConstraints &#123;<br>            <span class="hljs-comment">// 如果 Pod 有首选的亲和性或反亲和性术语，我们需要处理节点上的所有 Pod。</span><br>            podsToProcess = nodeInfo.Pods<br>        &#125;<br><br>        <span class="hljs-comment">// 初始化当前节点的拓扑打分映射。</span><br>        topoScore := <span class="hljs-built_in">make</span>(scoreMap)<br>        <span class="hljs-comment">// 遍历当前节点上的 Pod，处理现有 Pod 并计算拓扑打分。</span><br>        <span class="hljs-keyword">for</span> _, existingPod := <span class="hljs-keyword">range</span> podsToProcess &#123;<br>            pl.processExistingPod(state, existingPod, nodeInfo, pod, topoScore)<br>        &#125;<br>        <span class="hljs-comment">// 如果当前节点有拓扑打分，则将其添加到 topoScores 列表中。</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(topoScore) &gt; <span class="hljs-number">0</span> &#123;<br>            topoScores[atomic.AddInt32(&amp;index, <span class="hljs-number">1</span>)] = topoScore<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">// 并行处理所有节点。</span><br>    pl.parallelizer.Until(pCtx, <span class="hljs-built_in">len</span>(allNodes), processNode, pl.Name())<br><br>    <span class="hljs-comment">// 将所有节点的拓扑打分信息累加到 state 中。</span><br>    <span class="hljs-keyword">for</span> i := <span class="hljs-number">0</span>; i &lt;= <span class="hljs-type">int</span>(index); i++ &#123;<br>        state.topologyScore.<span class="hljs-built_in">append</span>(topoScores[i])<br>    &#125;<br><br>    <span class="hljs-comment">// 将构建好的周期状态写入 cycleState。</span><br>    cycleState.Write(preScoreStateKey, state)<br>    <span class="hljs-comment">// 返回 nil 表示成功。</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>整体的流程如下：</p><ol><li>获取pod的亲和性即反亲和性优选约束</li><li>如果pod没有相关的优选约束且系统也指明了要忽略优选约束，那么就直接返回。（如果没有忽略，我们就还需要考虑其他pod的优选倾向）</li><li>如果pod有亲和性优选或者反亲和性优选约束，那么考虑的node就是所有的node，如果都没有，那么就只考虑带有亲和性约束的pod的node。</li><li>与PreFilter类似，分别合并亲和性、反亲和性优选约束生效的namespace。</li><li>获取命名空间快照。</li><li>通过并行的方式给2中得到的每个node进行打分。<ol><li><p>在打分时首先查看要调度的pod是否有亲和性优选或者反亲和性优选约束，如果有要处理的pod就是node上的所有pod，否则就是带有亲和性约束的pod。</p></li><li><p>通过<code>processExistingPod</code>函数给每个要处理的pod进行打分，该函数如下</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *InterPodAffinity)</span></span> processExistingPod(<br>state *preScoreState,<br>existingPod *framework.PodInfo,<br>existingPodNodeInfo *framework.NodeInfo,<br>incomingPod *v1.Pod,<br>topoScore scoreMap,<br>) &#123;<br>existingPodNode := existingPodNodeInfo.Node()<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(existingPodNode.Labels) == <span class="hljs-number">0</span> &#123;<br><span class="hljs-keyword">return</span><br>&#125;<br><br><span class="hljs-comment">// For every soft pod affinity term of &lt;pod&gt;, if &lt;existingPod&gt; matches the term,</span><br><span class="hljs-comment">// increment &lt;p.counts&gt; for every node in the cluster with the same &lt;term.TopologyKey&gt;</span><br><span class="hljs-comment">// value as that of &lt;existingPods&gt;`s node by the term`s weight.</span><br><span class="hljs-comment">// Note that the incoming pod&#x27;s terms have the namespaceSelector merged into the namespaces, and so</span><br><span class="hljs-comment">// here we don&#x27;t lookup the existing pod&#x27;s namespace labels, hence passing nil for nsLabels.</span><br>topoScore.processTerms(state.podInfo.PreferredAffinityTerms, existingPod.Pod, <span class="hljs-literal">nil</span>, existingPodNode, <span class="hljs-number">1</span>)<br><br><span class="hljs-comment">// For every soft pod anti-affinity term of &lt;pod&gt;, if &lt;existingPod&gt; matches the term,</span><br><span class="hljs-comment">// decrement &lt;p.counts&gt; for every node in the cluster with the same &lt;term.TopologyKey&gt;</span><br><span class="hljs-comment">// value as that of &lt;existingPod&gt;`s node by the term`s weight.</span><br><span class="hljs-comment">// Note that the incoming pod&#x27;s terms have the namespaceSelector merged into the namespaces, and so</span><br><span class="hljs-comment">// here we don&#x27;t lookup the existing pod&#x27;s namespace labels, hence passing nil for nsLabels.</span><br>topoScore.processTerms(state.podInfo.PreferredAntiAffinityTerms, existingPod.Pod, <span class="hljs-literal">nil</span>, existingPodNode, <span class="hljs-number">-1</span>)<br><br><span class="hljs-comment">// For every hard pod affinity term of &lt;existingPod&gt;, if &lt;pod&gt; matches the term,</span><br><span class="hljs-comment">// increment &lt;p.counts&gt; for every node in the cluster with the same &lt;term.TopologyKey&gt;</span><br><span class="hljs-comment">// value as that of &lt;existingPod&gt;&#x27;s node by the constant &lt;args.hardPodAffinityWeight&gt;</span><br><span class="hljs-keyword">if</span> pl.args.HardPodAffinityWeight &gt; <span class="hljs-number">0</span> &amp;&amp; <span class="hljs-built_in">len</span>(existingPodNode.Labels) != <span class="hljs-number">0</span> &#123;<br><span class="hljs-keyword">for</span> _, t := <span class="hljs-keyword">range</span> existingPod.RequiredAffinityTerms &#123;<br>topoScore.processTerm(&amp;t, pl.args.HardPodAffinityWeight, incomingPod, state.namespaceLabels, existingPodNode, <span class="hljs-number">1</span>)<br>&#125;<br>&#125;<br><br><span class="hljs-comment">// For every soft pod affinity term of &lt;existingPod&gt;, if &lt;pod&gt; matches the term,</span><br><span class="hljs-comment">// increment &lt;p.counts&gt; for every node in the cluster with the same &lt;term.TopologyKey&gt;</span><br><span class="hljs-comment">// value as that of &lt;existingPod&gt;&#x27;s node by the term&#x27;s weight.</span><br>topoScore.processTerms(existingPod.PreferredAffinityTerms, incomingPod, state.namespaceLabels, existingPodNode, <span class="hljs-number">1</span>)<br><br><span class="hljs-comment">// For every soft pod anti-affinity term of &lt;existingPod&gt;, if &lt;pod&gt; matches the term,</span><br><span class="hljs-comment">// decrement &lt;pm.counts&gt; for every node in the cluster with the same &lt;term.TopologyKey&gt;</span><br><span class="hljs-comment">// value as that of &lt;existingPod&gt;&#x27;s node by the term&#x27;s weight.</span><br>topoScore.processTerms(existingPod.PreferredAntiAffinityTerms, incomingPod, state.namespaceLabels, existingPodNode, <span class="hljs-number">-1</span>)<br>&#125;<br></code></pre></td></tr></table></figure><p> 如注释所述，首先是查看这个已经存在的pod是否满足要调度的pod的亲和性优选约束，这会遍历各个亲和性优选约束，如果满足某个约束，那么就在这个node所属域的值上加上对应亲和性优选约束的weight。其主要的<code>processTerms</code>函数如下。例如亲和性优选约束是值为west的region，weight为10，那么如果当前pod也为west的region，那么值为west的region的分数就会加10.</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(m scoreMap)</span></span> processTerms(terms []framework.WeightedAffinityTerm, pod *v1.Pod, nsLabels labels.Set, node *v1.Node, multiplier <span class="hljs-type">int32</span>) &#123;<br><span class="hljs-keyword">for</span> _, term := <span class="hljs-keyword">range</span> terms &#123;<br>m.processTerm(&amp;term.AffinityTerm, term.Weight, pod, nsLabels, node, multiplier)<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(m scoreMap)</span></span> processTerm(term *framework.AffinityTerm, weight <span class="hljs-type">int32</span>, pod *v1.Pod, nsLabels labels.Set, node *v1.Node, multiplier <span class="hljs-type">int32</span>) &#123;<br><span class="hljs-keyword">if</span> term.Matches(pod, nsLabels) &#123;<br><span class="hljs-keyword">if</span> tpValue, tpValueExist := node.Labels[term.TopologyKey]; tpValueExist &#123;<br><span class="hljs-keyword">if</span> m[term.TopologyKey] == <span class="hljs-literal">nil</span> &#123;<br>m[term.TopologyKey] = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-type">int64</span>)<br>&#125;<br>m[term.TopologyKey][tpValue] += <span class="hljs-type">int64</span>(weight * multiplier)<br>&#125;<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p> 然后也会查看各个反亲和性优选约束，这里就是减去对应的比重，都是通过multiplier控制。</p><p> 然后如果<code>HardPodAffinityWeight&gt;0</code>还会去查看已经运行的pod的硬亲和性约束，如果匹配其具体的约束，那么该正在运行的pod对应的域的分数就会加上<code>HardPodAffinityWeight</code> 。</p><p> 然后还会统计以运行的pod的亲和性优选约束及反亲和性优选约束，如果匹配分数也会加上或者减去对应的权重值。</p></li><li><p>如果有统计到分数，那么就加入结果中。</p></li></ol></li><li>最后就能得到各类型的域及对应值的分数，并放入到cycleState中。</li></ol><h3 id="Score"><a href="#Score" class="headerlink" title="Score"></a>Score</h3><p><code>Score</code>函数的代码如下。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Score invoked at the Score extension point.</span><br><span class="hljs-comment">// The &quot;score&quot; returned in this function is the sum of weights got from cycleState which have its topologyKey matching with the node&#x27;s labels.</span><br><span class="hljs-comment">// it is normalized later.</span><br><span class="hljs-comment">// Note: the returned &quot;score&quot; is positive for pod-affinity, and negative for pod-antiaffinity.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *InterPodAffinity)</span></span> Score(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeName <span class="hljs-type">string</span>) (<span class="hljs-type">int64</span>, *framework.Status) &#123;<br>nodeInfo, err := pl.sharedLister.NodeInfos().Get(nodeName)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>, framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;failed to get node %q from Snapshot: %w&quot;</span>, nodeName, err))<br>&#125;<br>node := nodeInfo.Node()<br><br>s, err := getPreScoreState(cycleState)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>, framework.AsStatus(err)<br>&#125;<br><span class="hljs-keyword">var</span> score <span class="hljs-type">int64</span><br><span class="hljs-keyword">for</span> tpKey, tpValues := <span class="hljs-keyword">range</span> s.topologyScore &#123;<br><span class="hljs-keyword">if</span> v, exist := node.Labels[tpKey]; exist &#123;<br>score += tpValues[v]<br>&#125;<br>&#125;<br><br><span class="hljs-keyword">return</span> score, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>首先是获取PreScore的打分结果，然后匹配当前node的域与打分的域，如果匹配上了，就加上对应的分数。例如PreScore的结果是：</p><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs txt">&#123;&#123;&#123;region,east&#125;:-5&#125;, &#123;&#123;region,west&#125;:10&#125;, &#123;&#123;hostname,node1&#125;:-10&#125;, &#123;&#123;hostname,node2&#125;:8&#125;&#125;<br></code></pre></td></tr></table></figure><p>而node在值为east的region，所在的host为node2，那么它的分数就是-5+8&#x3D;3分。</p><h3 id="NormalizeScore"><a href="#NormalizeScore" class="headerlink" title="NormalizeScore"></a>NormalizeScore</h3><p>这里再关注一下<code>NormalizeScore</code>函数，如下，它的作用是规范化每个过滤后的节点的分数。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// NormalizeScore normalizes the score for each filteredNode.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *InterPodAffinity)</span></span> NormalizeScore(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status &#123;<br>    <span class="hljs-comment">// 从调度周期状态中获取预打分状态，如果获取失败，则返回错误状态。</span><br>    s, err := getPreScoreState(cycleState)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> framework.AsStatus(err)<br>    &#125;<br>    <span class="hljs-comment">// 如果拓扑打分映射为空，则直接返回 nil，表示没有分数需要规范化。</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(s.topologyScore) == <span class="hljs-number">0</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>    &#125;<br><br>    <span class="hljs-comment">// 初始化最小和最大计数变量为 int64 类型的最大和最小值。</span><br>    <span class="hljs-keyword">var</span> minCount <span class="hljs-type">int64</span> = math.MaxInt64<br>    <span class="hljs-keyword">var</span> maxCount <span class="hljs-type">int64</span> = math.MinInt64<br><br>    <span class="hljs-comment">// 遍历所有节点的分数，找出最大和最小分数。</span><br>    <span class="hljs-keyword">for</span> i := <span class="hljs-keyword">range</span> scores &#123;<br>        score := scores[i].Score<br>        <span class="hljs-comment">// 如果当前分数大于已知的最大分数，则更新最大分数。</span><br>        <span class="hljs-keyword">if</span> score &gt; maxCount &#123;<br>            maxCount = score<br>        &#125;<br>        <span class="hljs-comment">// 如果当前分数小于已知的最小分数，则更新最小分数。</span><br>        <span class="hljs-keyword">if</span> score &lt; minCount &#123;<br>            minCount = score<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 计算最大分数和最小分数之间的差异。</span><br>    maxMinDiff := maxCount - minCount<br><br>    <span class="hljs-comment">// 遍历所有节点的分数，进行规范化处理。</span><br>    <span class="hljs-keyword">for</span> i := <span class="hljs-keyword">range</span> scores &#123;<br>        fScore := <span class="hljs-type">float64</span>(<span class="hljs-number">0</span>) <span class="hljs-comment">// 初始化浮点分数为 0。</span><br><br>        <span class="hljs-comment">// 如果最大分数和最小分数之间的差异大于 0，则计算规范化后的分数。</span><br>        <span class="hljs-keyword">if</span> maxMinDiff &gt; <span class="hljs-number">0</span> &#123;<br>            <span class="hljs-comment">// 使用规范化公式计算浮点分数：系统规定最大分值 * (当前分数 - 最小分数) / (最大分数 - 最小分数)。</span><br>            fScore = <span class="hljs-type">float64</span>(framework.MaxNodeScore) * (<span class="hljs-type">float64</span>(scores[i].Score-minCount) / <span class="hljs-type">float64</span>(maxMinDiff))<br>        &#125;<br><br>        <span class="hljs-comment">// 将计算出的浮点分数转换为 int64 类型，并更新节点的分数。</span><br>        scores[i].Score = <span class="hljs-type">int64</span>(fScore)<br>    &#125;<br><br>    <span class="hljs-comment">// 返回 nil，表示分数规范化成功完成。</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>首先是检查了一下PreScore的结果是不是null，如果是就直接返回nil了。然后就是统计分数列别的最大值和最小值，然后使用规范化公式计算浮点分数：系统规定最大分值 * (当前分数 - 最小分数) &#x2F; (最大分数 - 最小分数) ，并将计算出的浮点分数转换为 int64 类型，并更新节点的分数。</p>]]></content>
    
    
    <categories>
      
      <category>k8s</category>
      
      <category>源码分析</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>源码分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【K8s源码分析（四）】-K8s调度器绑定周期介绍</title>
    <link href="/2024/05/10/k8sSource4/"/>
    <url>/2024/05/10/k8sSource4/</url>
    
    <content type="html"><![CDATA[<p>本次分析参考的K8s版本是<a href="https://github.com/kubernetes/kubernetes/tree/release-1.27">v1.27.0</a>。</p><p>K8s的整体调度框架如下图所示。</p><p><img src="/2024/05/10/k8sSource4/scheduling-framework-extensions.png" alt="Scheduling framework extension points"></p><h1 id="bindeCycle顶层函数"><a href="#bindeCycle顶层函数" class="headerlink" title="bindeCycle顶层函数"></a>bindeCycle顶层函数</h1><p>K8s调度器中绑定周期的函数<code>bindingCycle</code>在<code>pkg/scheduler/schedule_one.go:225</code>中，如下，补充了一些注释。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// bindingCycle tries to bind an assumed Pod.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(sched *Scheduler)</span></span> bindingCycle(<br>    ctx context.Context, <span class="hljs-comment">// 调度上下文</span><br>    state *framework.CycleState, <span class="hljs-comment">// 调度周期状态</span><br>    fwk framework.Framework, <span class="hljs-comment">// 调度框架</span><br>    scheduleResult ScheduleResult, <span class="hljs-comment">// 调度结果</span><br>    assumedPodInfo *framework.QueuedPodInfo, <span class="hljs-comment">// 假定的 Pod 信息</span><br>    start time.Time, <span class="hljs-comment">// 绑定周期开始时间</span><br>    podsToActivate *framework.PodsToActivate) *framework.Status &#123; <span class="hljs-comment">// 待激活的 Pods</span><br><br>    assumedPod := assumedPodInfo.Pod <span class="hljs-comment">// 获取假定的 Pod</span><br><br>    <span class="hljs-comment">// 运行 &quot;permit&quot; 插件，检查是否允许绑定操作</span><br>    <span class="hljs-keyword">if</span> status := fwk.WaitOnPermit(ctx, assumedPod); !status.IsSuccess() &#123;<br>        <span class="hljs-keyword">return</span> status<br>    &#125;<br><br>    <span class="hljs-comment">// 运行 &quot;prebind&quot; 插件，执行绑定前的检查和操作</span><br>    <span class="hljs-keyword">if</span> status := fwk.RunPreBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost); !status.IsSuccess() &#123;<br>        <span class="hljs-keyword">return</span> status<br>    &#125;<br><br>    <span class="hljs-comment">// 运行 &quot;bind&quot; 插件，实际执行 Pod 到节点的绑定操作</span><br>    <span class="hljs-keyword">if</span> status := sched.bind(ctx, fwk, assumedPod, scheduleResult.SuggestedHost, state); !status.IsSuccess() &#123;<br>        <span class="hljs-keyword">return</span> status<br>    &#125;<br><br>    <span class="hljs-comment">// 日志记录 Pod 绑定成功的信息</span><br>    klog.V(<span class="hljs-number">2</span>).InfoS(<span class="hljs-string">&quot;Successfully bound pod to node&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(assumedPod), <span class="hljs-string">&quot;node&quot;</span>, scheduleResult.SuggestedHost, <span class="hljs-string">&quot;evaluatedNodes&quot;</span>, scheduleResult.EvaluatedNodes, <span class="hljs-string">&quot;feasibleNodes&quot;</span>, scheduleResult.FeasibleNodes)<br><br>    <span class="hljs-comment">// 更新 Pod 调度的指标</span><br>    metrics.PodScheduled(fwk.ProfileName(), metrics.SinceInSeconds(start))<br>    metrics.PodSchedulingAttempts.Observe(<span class="hljs-type">float64</span>(assumedPodInfo.Attempts))<br>    metrics.PodSchedulingDuration.WithLabelValues(getAttemptsLabel(assumedPodInfo)).Observe(metrics.SinceInSeconds(assumedPodInfo.InitialAttemptTimestamp))<br><br>    <span class="hljs-comment">// 运行 &quot;postbind&quot; 插件，执行绑定后的检查和操作</span><br>    fwk.RunPostBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost)<br><br>    <span class="hljs-comment">// 成功绑定周期结束后，如果有必要，将一些 Pods 移动到activeQ队列中</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(podsToActivate.Map) != <span class="hljs-number">0</span> &#123;<br>        sched.SchedulingQueue.Activate(podsToActivate.Map)<br>        <span class="hljs-comment">// 与 schedulingCycle() 中的逻辑不同，我们不删除条目，</span><br>        <span class="hljs-comment">// 因为 podsToActivate.Map 不再被消费</span><br>    &#125;<br><br>    <span class="hljs-comment">// 返回 nil 表示没有错误</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到我们这里运行了permit、prebind、bind、postbind插件。下面具体来看这几个插件是如何运行的。</p><h1 id="Permit插件"><a href="#Permit插件" class="headerlink" title="Permit插件"></a>Permit插件</h1><p>其运行函数<code>WaitOnPermit</code>在<code>pkg/scheduler/framework/runtime/framework.go:1250</code>中，如下，补充了部分注释</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// WaitOnPermit will block, if the pod is a waiting pod, until the waiting pod is rejected or allowed.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> WaitOnPermit(ctx context.Context, pod *v1.Pod) *framework.Status &#123;<br>    waitingPod := f.waitingPods.get(pod.UID) <span class="hljs-comment">// 根据pod的id来查看其是否在等待队列中</span><br>    <span class="hljs-keyword">if</span> waitingPod == <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span> <span class="hljs-comment">// 如果不在等待队列中，直接返回</span><br>    &#125;<br>    <span class="hljs-keyword">defer</span> f.waitingPods.remove(pod.UID) <span class="hljs-comment">// 无论函数如何结束，都从等待列表中移除该 Pod</span><br>    <br>    klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Pod waiting on permit&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod)) <span class="hljs-comment">// 日志记录 Pod 正在等待许可</span><br><br>    startTime := time.Now() <span class="hljs-comment">// 记录开始等待的时间</span><br>    s := &lt;-waitingPod.s <span class="hljs-comment">// 从pod的通道中接收状态</span><br>    metrics.PermitWaitDuration.WithLabelValues(s.Code().String()).Observe(metrics.SinceInSeconds(startTime)) <span class="hljs-comment">// 记录许可等待的持续时间</span><br><br>    <span class="hljs-keyword">if</span> !s.IsSuccess() &#123;<br>        <span class="hljs-keyword">if</span> s.IsUnschedulable() &#123;<br>            klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Pod rejected while waiting on permit&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;status&quot;</span>, s.Message()) <span class="hljs-comment">// 日志记录 Pod 在等待许可时被拒绝</span><br>            <span class="hljs-keyword">return</span> s <span class="hljs-comment">// 返回拒绝状态</span><br>        &#125;<br>        err := s.AsError() <span class="hljs-comment">// 将状态转换为错误</span><br>        klog.ErrorS(err, <span class="hljs-string">&quot;Failed waiting on permit for pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod)) <span class="hljs-comment">// 日志记录等待许可失败</span><br>        <span class="hljs-keyword">return</span> framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;waiting on permit for pod: %w&quot;</span>, err)).WithFailedPlugin(s.FailedPlugin()) <span class="hljs-comment">// 返回错误状态</span><br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span> <span class="hljs-comment">// 如果状态成功，则返回 nil 表示没有错误</span><br>&#125;<br></code></pre></td></tr></table></figure><p>主要流程包括：</p><ol><li>查看等待队列是否包含该pod，如果不包含直接返回</li><li>如果包含，就记录开始等待的时间，并通过通道来进行阻塞等待</li><li>记录许可等待的持续时间。</li><li>检查接收到的状态。如果状态不成功：<ul><li>如果状态是不可调度的（Unschedulable），记录日志并返回拒绝状态。</li><li>如果状态不是成功的，记录错误日志，并返回错误状态。</li></ul></li><li>如果成功就直接返回。</li></ol><h1 id="PreBind插件"><a href="#PreBind插件" class="headerlink" title="PreBind插件"></a>PreBind插件</h1><p>该插件对应的执行函数<code>RunPreBindPlugins</code>在<code>pkg/scheduler/framework/runtime/framework.go:1048</code>中，如下，补充了部分注释。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// RunPreBindPlugins runs the set of configured prebind plugins. It returns a</span><br><span class="hljs-comment">// failure (bool) if any of the plugins returns an error. It also returns an</span><br><span class="hljs-comment">// error containing the rejection message or the error occurred in the plugin.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> RunPreBindPlugins(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName <span class="hljs-type">string</span>) (status *framework.Status) &#123;<br>    startTime := time.Now() <span class="hljs-comment">// 记录 PreBind 插件开始运行的时间</span><br>    <span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>        <span class="hljs-comment">// 记录 PreBind 插件的运行时间和最终状态</span><br>        metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.PreBind, status.Code().String(), f.profileName).Observe(metrics.SinceInSeconds(startTime))<br>    &#125;()<br>    <span class="hljs-keyword">for</span> _, pl := <span class="hljs-keyword">range</span> f.preBindPlugins &#123;<br>        <span class="hljs-comment">// 遍历所有的 PreBind 插件</span><br>        status = f.runPreBindPlugin(ctx, pl, state, pod, nodeName)<br>        <span class="hljs-keyword">if</span> !status.IsSuccess() &#123;<br>            <span class="hljs-keyword">if</span> status.IsUnschedulable() &#123;<br>                <span class="hljs-comment">// 如果插件返回的状态是不可调度的，则记录日志并返回该状态</span><br>                klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Pod rejected by PreBind plugin&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;node&quot;</span>, nodeName, <span class="hljs-string">&quot;plugin&quot;</span>, pl.Name(), <span class="hljs-string">&quot;status&quot;</span>, status.Message())<br>                status.SetFailedPlugin(pl.Name()) <span class="hljs-comment">// 设置失败的插件名称</span><br>                <span class="hljs-keyword">return</span> status<br>            &#125;<br>            err := status.AsError() <span class="hljs-comment">// 将状态转换为错误</span><br>            klog.ErrorS(err, <span class="hljs-string">&quot;Failed running PreBind plugin&quot;</span>, <span class="hljs-string">&quot;plugin&quot;</span>, pl.Name(), <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;node&quot;</span>, nodeName)<br>            <span class="hljs-comment">// 如果插件运行失败，记录错误日志并返回错误状态</span><br>            <span class="hljs-keyword">return</span> framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;running PreBind plugin %q: %w&quot;</span>, pl.Name(), err))<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span> <span class="hljs-comment">// 如果所有插件都成功运行，则返回 nil 表示没有错误</span><br>&#125;<br></code></pre></td></tr></table></figure><p>主要还是遍历所有的PreBind插件，并通过<code>runPreBindPlugin</code>函数进行运行，如果不成功就需要进行相关记录。<code>runPreBindPlugin</code> 函数在<code>pkg/scheduler/framework/runtime/framework.go:1069</code>中</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> runPreBindPlugin(ctx context.Context, pl framework.PreBindPlugin, state *framework.CycleState, pod *v1.Pod, nodeName <span class="hljs-type">string</span>) *framework.Status &#123;<br><span class="hljs-keyword">if</span> !state.ShouldRecordPluginMetrics() &#123;<br><span class="hljs-keyword">return</span> pl.PreBind(ctx, state, pod, nodeName)<br>&#125;<br>startTime := time.Now()<br>status := pl.PreBind(ctx, state, pod, nodeName)<br>f.metricsRecorder.ObservePluginDurationAsync(metrics.PreBind, pl.Name(), status.Code().String(), metrics.SinceInSeconds(startTime))<br><span class="hljs-keyword">return</span> status<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到主要是调用插件的PreBind函数进行检查。</p><h1 id="Bind插件"><a href="#Bind插件" class="headerlink" title="Bind插件"></a>Bind插件</h1><p>该插件对应的执行函数<code>Bind</code>在<code>pkg/scheduler/schedule_one.go:796</code>中，如下。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// bind binds a pod to a given node defined in a binding object.</span><br><span class="hljs-comment">// The precedence for binding is: (1) extenders and (2) framework plugins.</span><br><span class="hljs-comment">// We expect this to run asynchronously, so we handle binding metrics internally.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(sched *Scheduler)</span></span> bind(ctx context.Context, fwk framework.Framework, assumed *v1.Pod, targetNode <span class="hljs-type">string</span>, state *framework.CycleState) (status *framework.Status) &#123;<br><span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>sched.finishBinding(fwk, assumed, targetNode, status)<br>&#125;()<br><br>bound, err := sched.extendersBinding(assumed, targetNode)<br><span class="hljs-keyword">if</span> bound &#123;<br><span class="hljs-keyword">return</span> framework.AsStatus(err)<br>&#125;<br><span class="hljs-keyword">return</span> fwk.RunBindPlugins(ctx, state, assumed, targetNode)<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到首先会调用<code>extendersBinding</code>这个拓展插件进行运行，其代码在<code>pkg/scheduler/schedule_one.go:809</code>中，如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// TODO(#87159): Move this to a Plugin.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(sched *Scheduler)</span></span> extendersBinding(pod *v1.Pod, node <span class="hljs-type">string</span>) (<span class="hljs-type">bool</span>, <span class="hljs-type">error</span>) &#123;<br><span class="hljs-keyword">for</span> _, extender := <span class="hljs-keyword">range</span> sched.Extenders &#123;<br><span class="hljs-keyword">if</span> !extender.IsBinder() || !extender.IsInterested(pod) &#123;<br><span class="hljs-keyword">continue</span><br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>, extender.Bind(&amp;v1.Binding&#123;<br>ObjectMeta: metav1.ObjectMeta&#123;Namespace: pod.Namespace, Name: pod.Name, UID: pod.UID&#125;,<br>Target:     v1.ObjectReference&#123;Kind: <span class="hljs-string">&quot;Node&quot;</span>, Name: node&#125;,<br>&#125;)<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>从TODO中可以看到这部分代码在后面可能也变成插件的形式。这里遍历每个拓展，检查看是不是拓展是不是对该pod感兴趣，需要去主动的bind。</p><p>如果这些拓展都不处理pod，就会调用<code>RunBindPlugins</code>来进行bind，代码在中。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// RunBindPlugins runs the set of configured bind plugins until one returns a non `Skip` status.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> RunBindPlugins(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName <span class="hljs-type">string</span>) (status *framework.Status) &#123;<br>startTime := time.Now()<br><span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.Bind, status.Code().String(), f.profileName).Observe(metrics.SinceInSeconds(startTime))<br>&#125;()<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(f.bindPlugins) == <span class="hljs-number">0</span> &#123;<br><span class="hljs-keyword">return</span> framework.NewStatus(framework.Skip, <span class="hljs-string">&quot;&quot;</span>)<br>&#125;<br><span class="hljs-keyword">for</span> _, pl := <span class="hljs-keyword">range</span> f.bindPlugins &#123;<br>status = f.runBindPlugin(ctx, pl, state, pod, nodeName)<br><span class="hljs-keyword">if</span> status.IsSkip() &#123;<br><span class="hljs-keyword">continue</span><br>&#125;<br><span class="hljs-keyword">if</span> !status.IsSuccess() &#123;<br><span class="hljs-keyword">if</span> status.IsUnschedulable() &#123;<br>klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Pod rejected by Bind plugin&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;node&quot;</span>, nodeName, <span class="hljs-string">&quot;plugin&quot;</span>, pl.Name(), <span class="hljs-string">&quot;status&quot;</span>, status.Message())<br>status.SetFailedPlugin(pl.Name())<br><span class="hljs-keyword">return</span> status<br>&#125;<br>err := status.AsError()<br>klog.ErrorS(err, <span class="hljs-string">&quot;Failed running Bind plugin&quot;</span>, <span class="hljs-string">&quot;plugin&quot;</span>, pl.Name(), <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;node&quot;</span>, nodeName)<br><span class="hljs-keyword">return</span> framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;running Bind plugin %q: %w&quot;</span>, pl.Name(), err))<br>&#125;<br><span class="hljs-keyword">return</span> status<br>&#125;<br><span class="hljs-keyword">return</span> status<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到他会遍历每一个bindPlug，看这些插件然后去对他进行相关的绑定处理，如果有一个不成功，就会被视为失败，然后进行相关的记录。</p><p>每一个binPlug的绑定操作<code>runBindPlugin</code>在<code>pkg/scheduler/framework/runtime/framework.go:1108</code>中。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> runBindPlugin(ctx context.Context, bp framework.BindPlugin, state *framework.CycleState, pod *v1.Pod, nodeName <span class="hljs-type">string</span>) *framework.Status &#123;<br><span class="hljs-keyword">if</span> !state.ShouldRecordPluginMetrics() &#123;<br><span class="hljs-keyword">return</span> bp.Bind(ctx, state, pod, nodeName)<br>&#125;<br>startTime := time.Now()<br>status := bp.Bind(ctx, state, pod, nodeName)<br>f.metricsRecorder.ObservePluginDurationAsync(metrics.Bind, bp.Name(), status.Code().String(), metrics.SinceInSeconds(startTime))<br><span class="hljs-keyword">return</span> status<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到是调用每个插件的Bind函数。</p><h1 id="PostBind插件"><a href="#PostBind插件" class="headerlink" title="PostBind插件"></a>PostBind插件</h1><p>该插件对应的执行函数<code>RunPostBindPlugins</code>在<code>pkg/scheduler/framework/runtime/framework.go:1119</code>中，如下。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// RunPostBindPlugins runs the set of configured postbind plugins.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> RunPostBindPlugins(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName <span class="hljs-type">string</span>) &#123;<br>startTime := time.Now()<br><span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.PostBind, framework.Success.String(), f.profileName).Observe(metrics.SinceInSeconds(startTime))<br>&#125;()<br><span class="hljs-keyword">for</span> _, pl := <span class="hljs-keyword">range</span> f.postBindPlugins &#123;<br>f.runPostBindPlugin(ctx, pl, state, pod, nodeName)<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到其会运行每一个<code>postBindPlugins</code>插件。</p><p>具体运行的函数<code>runPostBindPlugin</code>在<code>pkg/scheduler/framework/runtime/framework.go:1129</code>中，如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> runPostBindPlugin(ctx context.Context, pl framework.PostBindPlugin, state *framework.CycleState, pod *v1.Pod, nodeName <span class="hljs-type">string</span>) &#123;<br><span class="hljs-keyword">if</span> !state.ShouldRecordPluginMetrics() &#123;<br>pl.PostBind(ctx, state, pod, nodeName)<br><span class="hljs-keyword">return</span><br>&#125;<br>startTime := time.Now()<br>pl.PostBind(ctx, state, pod, nodeName)<br>f.metricsRecorder.ObservePluginDurationAsync(metrics.PostBind, pl.Name(), framework.Success.String(), metrics.SinceInSeconds(startTime))<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到是调用了每个插件的PostBind函数。</p>]]></content>
    
    
    <categories>
      
      <category>k8s</category>
      
      <category>源码分析</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>源码分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【K8s源码分析（三）】-K8s调度器调度周期介绍</title>
    <link href="/2024/05/10/k8sSource3/"/>
    <url>/2024/05/10/k8sSource3/</url>
    
    <content type="html"><![CDATA[<p>本次分析参考的K8s版本是<a href="https://github.com/kubernetes/kubernetes/tree/release-1.27">v1.27.0</a>。</p><p>K8s的整体调度框架如下图所示。</p><p><img src="/2024/05/10/k8sSource3/scheduling-framework-extensions.png" alt="Scheduling framework extension points"></p><h1 id="调度框架顶层函数"><a href="#调度框架顶层函数" class="headerlink" title="调度框架顶层函数"></a>调度框架顶层函数</h1><p>K8s调度器调度的核心函数<code>schedulerone</code>在<code>pkg/scheduler/schedule_one.go:62</code>，如下，这里将一些解释写在了注释里</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// scheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm&#x27;s host fitting.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(sched *Scheduler)</span></span> scheduleOne(ctx context.Context) &#123;<br>    <span class="hljs-comment">// 获取调度队列中的下一个 Pod 信息</span><br>    podInfo := sched.NextPod()<br>    <span class="hljs-comment">// 如果 podInfo 或者其包含的 Pod 为 nil，说明调度队列关闭或者没有 Pod 需要调度，直接返回</span><br>    <span class="hljs-keyword">if</span> podInfo == <span class="hljs-literal">nil</span> || podInfo.Pod == <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span><br>    &#125;<br>    <span class="hljs-comment">// 获取 Pod 对象</span><br>    pod := podInfo.Pod<br>    <span class="hljs-comment">// 为当前 Pod 选择一个调度框架（scheduler framework）</span><br>    fwk, err := sched.frameworkForPod(pod)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// 这种情况不应该发生，因为我们只接受那些指定了匹配调度器名称的 Pod 进行调度</span><br>        klog.ErrorS(err, <span class="hljs-string">&quot;Error occurred&quot;</span>)<br>        <span class="hljs-keyword">return</span><br>    &#125;<br>    <span class="hljs-comment">// 如果跳过调度，则直接返回</span><br>    <span class="hljs-keyword">if</span> sched.skipPodSchedule(fwk, pod) &#123;<br>        <span class="hljs-keyword">return</span><br>    &#125;<br><br>    <span class="hljs-comment">// 记录尝试调度 Pod 的日志</span><br>    klog.V(<span class="hljs-number">3</span>).InfoS(<span class="hljs-string">&quot;Attempting to schedule pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br><br>    <span class="hljs-comment">// 开始计时，尝试为 Pod 找到合适的宿主机</span><br>    start := time.Now()<br>    <span class="hljs-comment">// 初始化调度周期状态</span><br>    state := framework.NewCycleState()<br>    <span class="hljs-comment">// 设置是否记录插件指标的随机概率</span><br>    state.SetRecordPluginMetrics(rand.Intn(<span class="hljs-number">100</span>) &lt; pluginMetricsSamplePercent)<br><br>    <span class="hljs-comment">// 初始化一个空的 podsToActivate 结构，这个结构将由插件填充或者保持为空</span><br>    podsToActivate := framework.NewPodsToActivate()<br>    <span class="hljs-comment">// 将 podsToActivate 写入状态中</span><br>    state.Write(framework.PodsToActivateKey, podsToActivate)<br><br>    <span class="hljs-comment">// 创建一个新的带有取消功能的上下文，用于调度周期</span><br>    schedulingCycleCtx, cancel := context.WithCancel(ctx)<br>    <span class="hljs-keyword">defer</span> cancel()<br><br>    <span class="hljs-comment">// 执行调度周期，尝试为 Pod 找到合适的宿主机</span><br>    scheduleResult, assumedPodInfo, status := sched.schedulingCycle(schedulingCycleCtx, state, fwk, podInfo, start, podsToActivate)<br>    <span class="hljs-comment">// 如果调度失败，则调用失败处理器</span><br>    <span class="hljs-keyword">if</span> !status.IsSuccess() &#123;<br>        sched.FailureHandler(schedulingCycleCtx, fwk, assumedPodInfo, status, scheduleResult.nominatingInfo, start)<br>        <span class="hljs-keyword">return</span><br>    &#125;<br><br>    <span class="hljs-comment">// 异步绑定 Pod 到其宿主机（可以这样做是因为上面的假设步骤）</span><br>    <span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>        <span class="hljs-comment">// 创建一个新的带有取消功能的上下文，用于绑定周期</span><br>        bindingCycleCtx, cancel := context.WithCancel(ctx)<br>        <span class="hljs-keyword">defer</span> cancel()<br><br>        <span class="hljs-comment">// 增加绑定阶段的 goroutine 指标</span><br>        metrics.SchedulerGoroutines.WithLabelValues(metrics.Binding).Inc()<br>        <span class="hljs-keyword">defer</span> metrics.SchedulerGoroutines.WithLabelValues(metrics.Binding).Dec()<br>        metrics.Goroutines.WithLabelValues(metrics.Binding).Inc()<br>        <span class="hljs-keyword">defer</span> metrics.Goroutines.WithLabelValues(metrics.Binding).Dec()<br><br>        <span class="hljs-comment">// 执行绑定周期，尝试将 Pod 绑定到宿主机</span><br>        status := sched.bindingCycle(bindingCycleCtx, state, fwk, scheduleResult, assumedPodInfo, start, podsToActivate)<br>        <span class="hljs-comment">// 如果绑定失败，则处理绑定周期错误</span><br>        <span class="hljs-keyword">if</span> !status.IsSuccess() &#123;<br>            sched.handleBindingCycleError(bindingCycleCtx, state, fwk, assumedPodInfo, start, scheduleResult, status)<br>        &#125;<br>    &#125;()<br>&#125;<br></code></pre></td></tr></table></figure><p>这段代码的主要功能是：</p><ol><li>从调度队列中获取下一个要调度的 Pod。</li><li>为 Pod 选择一个调度框架。</li><li>如果配置允许，跳过调度。</li><li>记录日志并开始调度周期。</li><li>如果调度成功，异步地尝试将 Pod 绑定到选定的宿主机。</li><li>如果调度或绑定失败，执行相应的错误处理逻辑。</li></ol><p>此处也指明了两个周期，分别为调度周期<code>schedulingCycle</code>和绑定周期<code>bindingCycle</code>，绑定周期会在后面一节进行介绍，这里主要关注<code>schedulingCycle</code> 。</p><p>查看关键的<code>schedulingCycle</code>函数，在<code>pkg/scheduler/schedule_one.go:120</code>中,补充了部分注释。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// schedulingCycle tries to schedule a single Pod.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(sched *Scheduler)</span></span> schedulingCycle(<br>    ctx context.Context, <span class="hljs-comment">// 调度上下文</span><br>    state *framework.CycleState, <span class="hljs-comment">// 调度周期状态</span><br>    fwk framework.Framework, <span class="hljs-comment">// 调度框架</span><br>    podInfo *framework.QueuedPodInfo, <span class="hljs-comment">// 待调度的 Pod 信息</span><br>    start time.Time, <span class="hljs-comment">// 调度开始时间</span><br>    podsToActivate *framework.PodsToActivate, <span class="hljs-comment">// 待激活的 Pods</span><br>) (ScheduleResult, *framework.QueuedPodInfo, *framework.Status) &#123;<br>    <span class="hljs-comment">// 获取待调度的 Pod</span><br>    pod := podInfo.Pod<br>    <span class="hljs-comment">// 调用调度器的 SchedulePod 方法尝试调度 Pod</span><br>    scheduleResult, err := sched.SchedulePod(ctx, fwk, state, pod)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// 如果没有可用节点，则返回错误状态</span><br>        <span class="hljs-keyword">if</span> err == ErrNoNodesAvailable &#123;<br>            status := framework.NewStatus(framework.UnschedulableAndUnresolvable).WithError(err)<br>            <span class="hljs-keyword">return</span> ScheduleResult&#123;nominatingInfo: clearNominatedNode&#125;, podInfo, status<br>        &#125;<br><br>        <span class="hljs-comment">// 如果错误是 FitError 类型，则说明 Pod 无法适应任何节点</span><br>        fitError, ok := err.(*framework.FitError)<br>        <span class="hljs-keyword">if</span> !ok &#123;<br>            klog.ErrorS(err, <span class="hljs-string">&quot;Error selecting node for pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>            <span class="hljs-keyword">return</span> ScheduleResult&#123;nominatingInfo: clearNominatedNode&#125;, podInfo, framework.AsStatus(err)<br>        &#125;<br><br>        <span class="hljs-comment">// 如果没有 PostFilter 插件，则不执行抢占</span><br>        <span class="hljs-keyword">if</span> !fwk.HasPostFilterPlugins() &#123;<br>            klog.V(<span class="hljs-number">3</span>).InfoS(<span class="hljs-string">&quot;No PostFilter plugins are registered, so no preemption will be performed&quot;</span>)<br>            <span class="hljs-keyword">return</span> ScheduleResult&#123;&#125;, podInfo, framework.NewStatus(framework.Unschedulable).WithError(err)<br>        &#125;<br><br>        <span class="hljs-comment">// 运行 PostFilter 插件，尝试使 Pod 在未来的调度周期中可调度</span><br>        result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatusMap)<br>        msg := status.Message()<br>        fitError.Diagnosis.PostFilterMsg = msg<br>        <span class="hljs-keyword">if</span> status.Code() == framework.Error &#123;<br>            klog.ErrorS(<span class="hljs-literal">nil</span>, <span class="hljs-string">&quot;Status after running PostFilter plugins for pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;status&quot;</span>, msg)<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            klog.V(<span class="hljs-number">5</span>).InfoS(<span class="hljs-string">&quot;Status after running PostFilter plugins for pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;status&quot;</span>, msg)<br>        &#125;<br><br>        <span class="hljs-comment">// 获取 PostFilter 插件返回的 NominatingInfo</span><br>        <span class="hljs-keyword">var</span> nominatingInfo *framework.NominatingInfo<br>        <span class="hljs-keyword">if</span> result != <span class="hljs-literal">nil</span> &#123;<br>            nominatingInfo = result.NominatingInfo<br>        &#125;<br>        <span class="hljs-keyword">return</span> ScheduleResult&#123;nominatingInfo: nominatingInfo&#125;, podInfo, framework.NewStatus(framework.Unschedulable).WithError(err)<br>    &#125;<br><br>    <span class="hljs-comment">// 计算并记录调度算法的延迟</span><br>    metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInSeconds(start))<br>    <span class="hljs-comment">// 假设 Pod 已经在给定节点上运行，这样子就不用等它实际绑定就可以执行后续的操作了</span><br>    assumedPodInfo := podInfo.DeepCopy()<br>    assumedPod := assumedPodInfo.Pod<br>    <span class="hljs-comment">// 假设操作，设置 Pod 的 NodeName 为调度结果推荐的宿主机</span><br>    err = sched.assume(assumedPod, scheduleResult.SuggestedHost)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// 如果假设操作失败，这可能是重试逻辑中的一个 BUG</span><br>        <span class="hljs-comment">// 报告错误以便重新调度 Pod</span><br>        <span class="hljs-keyword">return</span> ScheduleResult&#123;nominatingInfo: clearNominatedNode&#125;,<br>            assumedPodInfo,<br>            framework.AsStatus(err)<br>    &#125;<br><br>    <span class="hljs-comment">// 运行预留插件的 Reserve 方法</span><br>    <span class="hljs-keyword">if</span> sts := fwk.RunReservePluginsReserve(ctx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() &#123;<br>        <span class="hljs-comment">// 如果预留失败，触发取消预留以清理与预留 Pod 相关的资源</span><br>        fwk.RunReservePluginsUnreserve(ctx, state, assumedPod, scheduleResult.SuggestedHost)<br>        <span class="hljs-keyword">if</span> forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != <span class="hljs-literal">nil</span> &#123;<br>            klog.ErrorS(forgetErr, <span class="hljs-string">&quot;Scheduler cache ForgetPod failed&quot;</span>)<br>        &#125;<br><br>        <span class="hljs-keyword">return</span> ScheduleResult&#123;nominatingInfo: clearNominatedNode&#125;,<br>            assumedPodInfo,<br>            sts<br>    &#125;<br><br>    <span class="hljs-comment">// 运行 &quot;permit&quot; 插件</span><br>    runPermitStatus := fwk.RunPermitPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost)<br>    <span class="hljs-keyword">if</span> !runPermitStatus.IsWait() &amp;&amp; !runPermitStatus.IsSuccess() &#123;<br>        <span class="hljs-comment">// 如果许可检查失败，触发取消预留以清理与预留 Pod 相关的资源</span><br>        fwk.RunReservePluginsUnreserve(ctx, state, assumedPod, scheduleResult.SuggestedHost)<br>        <span class="hljs-keyword">if</span> forgetErr := sched.Cache.ForgetPod(assumedPod); forgetErr != <span class="hljs-literal">nil</span> &#123;<br>            klog.ErrorS(forgetErr, <span class="hljs-string">&quot;Scheduler cache ForgetPod failed&quot;</span>)<br>        &#125;<br><br>        <span class="hljs-keyword">return</span> ScheduleResult&#123;nominatingInfo: clearNominatedNode&#125;,<br>            assumedPodInfo,<br>            runPermitStatus<br>    &#125;<br><br>    <span class="hljs-comment">// 成功调度周期结束后，查看是否有必要设置一些pod为可调度的状态</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(podsToActivate.Map) != <span class="hljs-number">0</span> &#123;<br>        sched.SchedulingQueue.Activate(podsToActivate.Map)<br>        <span class="hljs-comment">// 激活后清空条目</span><br>        podsToActivate.Map = <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]*v1.Pod)<br>    &#125;<br><br>    <span class="hljs-comment">// 返回调度结果</span><br>    <span class="hljs-keyword">return</span> scheduleResult, assumedPodInfo, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>主要流程包括：</p><ol><li>尝试调度 Pod，并处理可能出现的错误。</li><li>如果调度失败，根据错误类型执行不同的逻辑，如处理节点不可用或 Pod 不适应任何节点的情况。</li><li>如果调度成功，记录调度算法的延迟，并提前假设 Pod 已经在推荐的节点上运行。</li><li>运行预留插件的 Reserve 方法，并处理预留成功或失败的情况。</li><li>运行抢占插件，并根据结果进行相应的处理。</li><li>如果有待转为active的 Pods，执行激活操作。</li><li>返回调度结果。</li></ol><h1 id="一般调度"><a href="#一般调度" class="headerlink" title="一般调度"></a>一般调度</h1><p>这里最关键的是<code>SchedulePod</code>函数，在<code>pkg/scheduler/schedule_one.go:334</code>中</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// schedulePod tries to schedule the given pod to one of the nodes in the node list.</span><br><span class="hljs-comment">// If it succeeds, it will return the name of the node.</span><br><span class="hljs-comment">// If it fails, it will return a FitError with reasons.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(sched *Scheduler)</span></span> schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err <span class="hljs-type">error</span>) &#123;<br>trace := utiltrace.New(<span class="hljs-string">&quot;Scheduling&quot;</span>, utiltrace.Field&#123;Key: <span class="hljs-string">&quot;namespace&quot;</span>, Value: pod.Namespace&#125;, utiltrace.Field&#123;Key: <span class="hljs-string">&quot;name&quot;</span>, Value: pod.Name&#125;)<br><span class="hljs-keyword">defer</span> trace.LogIfLong(<span class="hljs-number">100</span> * time.Millisecond)<br><br><span class="hljs-keyword">if</span> err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> result, err<br>&#125;<br>trace.Step(<span class="hljs-string">&quot;Snapshotting scheduler cache and node infos done&quot;</span>)<br><br><span class="hljs-keyword">if</span> sched.nodeInfoSnapshot.NumNodes() == <span class="hljs-number">0</span> &#123;<br><span class="hljs-keyword">return</span> result, ErrNoNodesAvailable<br>&#125;<br><br>feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> result, err<br>&#125;<br>trace.Step(<span class="hljs-string">&quot;Computing predicates done&quot;</span>)<br><br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(feasibleNodes) == <span class="hljs-number">0</span> &#123;<br><span class="hljs-keyword">return</span> result, &amp;framework.FitError&#123;<br>Pod:         pod,<br>NumAllNodes: sched.nodeInfoSnapshot.NumNodes(),<br>Diagnosis:   diagnosis,<br>&#125;<br>&#125;<br><br><span class="hljs-comment">// When only one node after predicate, just use it.</span><br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(feasibleNodes) == <span class="hljs-number">1</span> &#123;<br><span class="hljs-keyword">return</span> ScheduleResult&#123;<br>SuggestedHost:  feasibleNodes[<span class="hljs-number">0</span>].Name,<br>EvaluatedNodes: <span class="hljs-number">1</span> + <span class="hljs-built_in">len</span>(diagnosis.NodeToStatusMap),<br>FeasibleNodes:  <span class="hljs-number">1</span>,<br>&#125;, <span class="hljs-literal">nil</span><br>&#125;<br><br>priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> result, err<br>&#125;<br><br>host, err := selectHost(priorityList)<br>trace.Step(<span class="hljs-string">&quot;Prioritizing done&quot;</span>)<br><br><span class="hljs-keyword">return</span> ScheduleResult&#123;<br>SuggestedHost:  host,<br>EvaluatedNodes: <span class="hljs-built_in">len</span>(feasibleNodes) + <span class="hljs-built_in">len</span>(diagnosis.NodeToStatusMap),<br>FeasibleNodes:  <span class="hljs-built_in">len</span>(feasibleNodes),<br>&#125;, err<br>&#125;<br></code></pre></td></tr></table></figure><p>在这里我们就能具体的看到predicates筛选过程和Prioritizing打分过程，整体的逻辑也比较简单，首先是筛选出合适的node，如果只有一个node了，那么就直接返回这个node，如果有多个就进行打分，然后选择评分最高的node返回回去。</p><h2 id="筛选过程"><a href="#筛选过程" class="headerlink" title="筛选过程"></a>筛选过程</h2><p>然后我们查看predicates筛选过程，其代码在<code>pkg/scheduler/schedule_one.go:387</code>中，如下，补充了一些注释</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Filters the nodes to find the ones that fit the pod based on the framework</span><br><span class="hljs-comment">// filter plugins and filter extenders.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(sched *Scheduler)</span></span> findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, <span class="hljs-type">error</span>) &#123;<br>    <span class="hljs-comment">// 初始化诊断信息，用于记录调度过程中的详细信息</span><br>    diagnosis := framework.Diagnosis&#123;<br>        NodeToStatusMap:      <span class="hljs-built_in">make</span>(framework.NodeToStatusMap),<br>        UnschedulablePlugins: sets.NewString(),<br>    &#125;<br><br>    <span class="hljs-comment">// 获取所有节点的信息</span><br>    allNodes, err := sched.nodeInfoSnapshot.NodeInfos().List()<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, diagnosis, err<br>    &#125;<br>    <span class="hljs-comment">// 运行 &quot;prefilter&quot; 插件</span><br>    preRes, s := fwk.RunPreFilterPlugins(ctx, state, pod)<br>    <span class="hljs-keyword">if</span> !s.IsSuccess() &#123;<br>        <span class="hljs-keyword">if</span> !s.IsUnschedulable() &#123;<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, diagnosis, s.AsError()<br>        &#125;<br>        <span class="hljs-comment">// 如果 PreFilter 插件返回的状态是不可调度的，记录相关信息</span><br>        msg := s.Message()<br>        diagnosis.PreFilterMsg = msg<br>        klog.V(<span class="hljs-number">5</span>).InfoS(<span class="hljs-string">&quot;Status after running PreFilter plugins for pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;status&quot;</span>, msg)<br>        <span class="hljs-comment">// 如果有插件失败，记录失败的插件名称</span><br>        <span class="hljs-keyword">if</span> s.FailedPlugin() != <span class="hljs-string">&quot;&quot;</span> &#123;<br>            diagnosis.UnschedulablePlugins.Insert(s.FailedPlugin())<br>        &#125;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, diagnosis, <span class="hljs-literal">nil</span><br>    &#125;<br><br>    <span class="hljs-comment">// 如果 Pod 已经被提名到一个节点上（可能由于之前的抢占操作），</span><br>    <span class="hljs-comment">// 这个节点很可能是唯一一个合适的节点，所以首先评估这个节点</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(pod.Status.NominatedNodeName) &gt; <span class="hljs-number">0</span> &#123;<br>        feasibleNodes, err := sched.evaluateNominatedNode(ctx, pod, fwk, state, diagnosis)<br>        <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>            klog.ErrorS(err, <span class="hljs-string">&quot;Evaluation failed on nominated node&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;node&quot;</span>, pod.Status.NominatedNodeName)<br>        &#125;<br>        <span class="hljs-comment">// 如果提名的节点通过了所有的过滤，调度器可以决定将这个节点分配给 Pod</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(feasibleNodes) != <span class="hljs-number">0</span> &#123;<br>            <span class="hljs-keyword">return</span> feasibleNodes, diagnosis, <span class="hljs-literal">nil</span><br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 根据 PreFilter 插件的结果，可能需要过滤掉一些节点</span><br>    nodes := allNodes<br>    <span class="hljs-keyword">if</span> !preRes.AllNodes() &#123;<br>        nodes = <span class="hljs-built_in">make</span>([]*framework.NodeInfo, <span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(preRes.NodeNames))<br>        <span class="hljs-keyword">for</span> n := <span class="hljs-keyword">range</span> preRes.NodeNames &#123;<br>            nInfo, err := sched.nodeInfoSnapshot.NodeInfos().Get(n)<br>            <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, diagnosis, err<br>            &#125;<br>            nodes = <span class="hljs-built_in">append</span>(nodes, nInfo)<br>        &#125;<br>    &#125;<br>    <span class="hljs-comment">// 寻找通过过滤的节点</span><br>    feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, nodes)<br>    <span class="hljs-comment">// 无论是否发生错误，都尝试更新下一次开始搜索节点的索引</span><br>    processedNodes := <span class="hljs-built_in">len</span>(feasibleNodes) + <span class="hljs-built_in">len</span>(diagnosis.NodeToStatusMap)<br>    sched.nextStartNodeIndex = (sched.nextStartNodeIndex + processedNodes) % <span class="hljs-built_in">len</span>(nodes)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, diagnosis, err<br>    &#125;<br><br>    <span class="hljs-comment">// 检查过滤扩展器以找到更多通过过滤的节点</span><br>    feasibleNodes, err = findNodesThatPassExtenders(sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, diagnosis, err<br>    &#125;<br>    <span class="hljs-comment">// 返回所有通过过滤的节点</span><br>    <span class="hljs-keyword">return</span> feasibleNodes, diagnosis, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>这部分首先运行preFilter插件首先进行一些轻量级的检查，然后再运行filter插件进行正式筛选，然后在运行filter拓展插件。</p><p>这里我们主要关注filter插件的运行，查看其对应的findNodesThatPassFilters函数，在<code>pkg/scheduler/schedule_one.go:475</code>中，如下，补充了部分注释</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// findNodesThatPassFilters finds the nodes that fit the filter plugins.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(sched *Scheduler)</span></span> findNodesThatPassFilters(<br>    ctx context.Context, <span class="hljs-comment">// 调度上下文</span><br>    fwk framework.Framework, <span class="hljs-comment">// 调度框架</span><br>    state *framework.CycleState, <span class="hljs-comment">// 调度周期状态</span><br>    pod *v1.Pod, <span class="hljs-comment">// 待调度的 Pod</span><br>    diagnosis framework.Diagnosis, <span class="hljs-comment">// 调度诊断信息</span><br>    nodes []*framework.NodeInfo) ([]*v1.Node, <span class="hljs-type">error</span>) &#123; <span class="hljs-comment">// 所有节点信息</span><br>    numAllNodes := <span class="hljs-built_in">len</span>(nodes) <span class="hljs-comment">// 所有节点的数量</span><br>    <span class="hljs-comment">// 计算应该找到的可行节点数量</span><br>    numNodesToFind := sched.numFeasibleNodesToFind(fwk.PercentageOfNodesToScore(), <span class="hljs-type">int32</span>(numAllNodes))<br><br>    <span class="hljs-comment">// 创建一个足够大的列表来存储通过过滤的节点，以避免在运行时增长该列表</span><br>    feasibleNodes := <span class="hljs-built_in">make</span>([]*v1.Node, numNodesToFind)<br><br>    <span class="hljs-comment">// 如果框架没有过滤插件，直接使用所有节点</span><br>    <span class="hljs-keyword">if</span> !fwk.HasFilterPlugins() &#123;<br>        <span class="hljs-keyword">for</span> i := <span class="hljs-keyword">range</span> feasibleNodes &#123;<br>            <span class="hljs-comment">// 从上一个调度周期停止的地方开始检查节点</span><br>            feasibleNodes[i] = nodes[(sched.nextStartNodeIndex+i)%numAllNodes].Node()<br>        &#125;<br>        <span class="hljs-keyword">return</span> feasibleNodes, <span class="hljs-literal">nil</span><br>    &#125;<br><br>    <span class="hljs-comment">// 用于并行处理时的错误通道</span><br>    errCh := parallelize.NewErrorChannel()<br>    <span class="hljs-keyword">var</span> statusesLock sync.Mutex <span class="hljs-comment">// 用于保护对诊断信息的并发访问</span><br>    <span class="hljs-keyword">var</span> feasibleNodesLen <span class="hljs-type">int32</span> <span class="hljs-comment">// 通过过滤的节点数量</span><br>    ctx, cancel := context.WithCancel(ctx) <span class="hljs-comment">// 创建一个可取消的上下文</span><br>    <span class="hljs-keyword">defer</span> cancel()<br><br>    <span class="hljs-comment">// 检查每个节点是否通过过滤</span><br>    checkNode := <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(i <span class="hljs-type">int</span>)</span></span> &#123;<br>        nodeInfo := nodes[(sched.nextStartNodeIndex+i)%numAllNodes] <span class="hljs-comment">// 获取节点信息</span><br>        status := fwk.RunFilterPluginsWithNominatedPods(ctx, state, pod, nodeInfo) <span class="hljs-comment">// 运行过滤插件</span><br>        <span class="hljs-keyword">if</span> status.Code() == framework.Error &#123;<br>            errCh.SendErrorWithCancel(status.AsError(), cancel) <span class="hljs-comment">// 发送错误并可能取消整个操作</span><br>            <span class="hljs-keyword">return</span><br>        &#125;<br>        <span class="hljs-keyword">if</span> status.IsSuccess() &#123;<br>            <span class="hljs-comment">// 如果节点通过过滤，将其添加到可行节点列表中</span><br>            length := atomic.AddInt32(&amp;feasibleNodesLen, <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> length &gt; numNodesToFind &#123;<br>                cancel() <span class="hljs-comment">// 如果找到的节点超过了预定数量，取消剩余的检查</span><br>                atomic.AddInt32(&amp;feasibleNodesLen, <span class="hljs-number">-1</span>)<br>            &#125; <span class="hljs-keyword">else</span> &#123;<br>                feasibleNodes[length<span class="hljs-number">-1</span>] = nodeInfo.Node()<br>            &#125;<br>        &#125; <span class="hljs-keyword">else</span> &#123;<br>            <span class="hljs-comment">// 如果节点没有通过过滤，记录其状态</span><br>            statusesLock.Lock()<br>            diagnosis.NodeToStatusMap[nodeInfo.Node().Name] = status<br>            diagnosis.UnschedulablePlugins.Insert(status.FailedPlugin())<br>            statusesLock.Unlock()<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 记录开始检查节点的时间</span><br>    beginCheckNode := time.Now()<br>    statusCode := framework.Success<br>    <span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>        <span class="hljs-comment">// 记录 Filter 扩展点的延迟</span><br>        metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.Filter, statusCode.String(), fwk.ProfileName()).Observe(metrics.SinceInSeconds(beginCheckNode))<br>    &#125;()<br><br>    <span class="hljs-comment">// 并行检查所有节点，直到找到预定数量的可行节点或检查完所有节点</span><br>    fwk.Parallelizer().Until(ctx, numAllNodes, checkNode, metrics.Filter)<br><br>    <span class="hljs-comment">// 截断可行节点列表到实际找到的节点数量</span><br>    feasibleNodes = feasibleNodes[:feasibleNodesLen]<br>    <span class="hljs-keyword">if</span> err := errCh.ReceiveError(); err != <span class="hljs-literal">nil</span> &#123;<br>        statusCode = framework.Error<br>        <span class="hljs-keyword">return</span> feasibleNodes, err<br>    &#125;<br>    <span class="hljs-keyword">return</span> feasibleNodes, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>注意到这里首先计算了需要筛选的node的数量，这主要是为了在大规模场景下降低筛选的数量，查看其对应的函数，在<code>pkg/scheduler/schedule_one.go:548</code>中，如下，补充了部分注释。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// numFeasibleNodesToFind returns the number of feasible nodes that once found, the scheduler stops</span><br><span class="hljs-comment">// its search for more feasible nodes.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(sched *Scheduler)</span></span> numFeasibleNodesToFind(percentageOfNodesToScore *<span class="hljs-type">int32</span>, numAllNodes <span class="hljs-type">int32</span>) (numNodes <span class="hljs-type">int32</span>) &#123;<br>    <span class="hljs-keyword">if</span> numAllNodes &lt; minFeasibleNodesToFind &#123;<br>        <span class="hljs-comment">// 如果所有节点的数量小于预设的最小可行节点数，则返回所有节点的数量</span><br>        <span class="hljs-keyword">return</span> numAllNodes<br>    &#125;<br><br>    <span class="hljs-comment">// 使用框架（profile）中设置的百分比，如果没有设置，则使用全局的百分比</span><br>    <span class="hljs-keyword">var</span> percentage <span class="hljs-type">int32</span><br>    <span class="hljs-keyword">if</span> percentageOfNodesToScore != <span class="hljs-literal">nil</span> &#123;<br>        percentage = *percentageOfNodesToScore<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        percentage = sched.percentageOfNodesToScore<br>    &#125;<br><br>    <span class="hljs-keyword">if</span> percentage == <span class="hljs-number">0</span> &#123;<br>        <span class="hljs-comment">// 如果没有提供百分比，则使用默认的计算方式</span><br>        percentage = <span class="hljs-type">int32</span>(<span class="hljs-number">50</span>) - numAllNodes/<span class="hljs-number">125</span><br>        <span class="hljs-keyword">if</span> percentage &lt; minFeasibleNodesPercentageToFind &#123;<br>            <span class="hljs-comment">// 确保百分比不低于预设的最小值</span><br>            percentage = minFeasibleNodesPercentageToFind<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 计算基于总节点数和百分比的节点数</span><br>    numNodes = numAllNodes * percentage / <span class="hljs-number">100</span><br>    <span class="hljs-keyword">if</span> numNodes &lt; minFeasibleNodesToFind &#123;<br>        <span class="hljs-comment">// 如果计算出的节点数小于最小可行节点数，则返回最小值</span><br>        <span class="hljs-keyword">return</span> minFeasibleNodesToFind<br>    &#125;<br><br>    <span class="hljs-comment">// 返回计算出的可行节点数</span><br>    <span class="hljs-keyword">return</span> numNodes<br>&#125;<br></code></pre></td></tr></table></figure><p>然后定义了内部的checkNode函数，其输入是要检查的node 的id相对于<code>sched.nextStartNodeIndex</code>的偏移。注意这里使用了k8s内部定义的并行函数fwk.Parallelizer().Until，其定义如下，在<code>pkg/scheduler/framework/parallelize/parallelism.go:56</code>和<code>staging/src/k8s.io/client-go/util/workqueue/parallelizer.go:46</code>中：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Until is a wrapper around workqueue.ParallelizeUntil to use in scheduling algorithms.</span><br><span class="hljs-comment">// A given operation will be a label that is recorded in the goroutine metric.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p Parallelizer)</span></span> Until(ctx context.Context, pieces <span class="hljs-type">int</span>, doWorkPiece workqueue.DoWorkPieceFunc, operation <span class="hljs-type">string</span>) &#123;<br>goroutinesMetric := metrics.Goroutines.WithLabelValues(operation)<br>withMetrics := <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(piece <span class="hljs-type">int</span>)</span></span> &#123;<br>goroutinesMetric.Inc()<br>doWorkPiece(piece)<br>goroutinesMetric.Dec()<br>&#125;<br><br>workqueue.ParallelizeUntil(ctx, p.parallelism, pieces, withMetrics, workqueue.WithChunkSize(chunkSizeFor(pieces, p.parallelism)))<br>&#125;<br><span class="hljs-comment">// ParallelizeUntil is a framework that allows for parallelizing N</span><br><span class="hljs-comment">// independent pieces of work until done or the context is canceled.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">ParallelizeUntil</span><span class="hljs-params">(ctx context.Context, workers, pieces <span class="hljs-type">int</span>, doWorkPiece DoWorkPieceFunc, opts ...Options)</span></span> &#123;<br><span class="hljs-keyword">if</span> pieces == <span class="hljs-number">0</span> &#123;<br><span class="hljs-keyword">return</span><br>&#125;<br>o := options&#123;&#125;<br><span class="hljs-keyword">for</span> _, opt := <span class="hljs-keyword">range</span> opts &#123;<br>opt(&amp;o)<br>&#125;<br>chunkSize := o.chunkSize<br><span class="hljs-keyword">if</span> chunkSize &lt; <span class="hljs-number">1</span> &#123;<br>chunkSize = <span class="hljs-number">1</span><br>&#125;<br><br>chunks := ceilDiv(pieces, chunkSize)<br>toProcess := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> <span class="hljs-type">int</span>, chunks)<br><span class="hljs-keyword">for</span> i := <span class="hljs-number">0</span>; i &lt; chunks; i++ &#123;<br>toProcess &lt;- i<br>&#125;<br><span class="hljs-built_in">close</span>(toProcess)<br><br><span class="hljs-keyword">var</span> stop &lt;-<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;<br><span class="hljs-keyword">if</span> ctx != <span class="hljs-literal">nil</span> &#123;<br>stop = ctx.Done()<br>&#125;<br><span class="hljs-keyword">if</span> chunks &lt; workers &#123;<br>workers = chunks<br>&#125;<br>wg := sync.WaitGroup&#123;&#125;<br>wg.Add(workers)<br><span class="hljs-keyword">for</span> i := <span class="hljs-number">0</span>; i &lt; workers; i++ &#123;<br><span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br><span class="hljs-keyword">defer</span> utilruntime.HandleCrash()<br><span class="hljs-keyword">defer</span> wg.Done()<br><span class="hljs-keyword">for</span> chunk := <span class="hljs-keyword">range</span> toProcess &#123;<br>start := chunk * chunkSize<br>end := start + chunkSize<br><span class="hljs-keyword">if</span> end &gt; pieces &#123;<br>end = pieces<br>&#125;<br><span class="hljs-keyword">for</span> p := start; p &lt; end; p++ &#123;<br><span class="hljs-keyword">select</span> &#123;<br><span class="hljs-keyword">case</span> &lt;-stop:<br><span class="hljs-keyword">return</span><br><span class="hljs-keyword">default</span>:<br>doWorkPiece(p)<br>&#125;<br>&#125;<br>&#125;<br>&#125;()<br>&#125;<br>wg.Wait()<br>&#125;<br></code></pre></td></tr></table></figure><p>checkNode函数内部检查对应的node是否能通过所有filter插件的过滤(<code>RunFilterPluginsWithNominatedPods</code>)如果通过就将筛选过的node数量+1，并记录相关的值，同时还会检查是否已经筛选到了足够的node，如果足够了，那么就发送取消信号，停止并行进程，不再继续筛选。</p><p>对于每个node进行筛选的函数<code>RunFilterPluginsWithNominatedPods</code>在<code>pkg/scheduler/framework/runtime/framework.go:816</code>中，如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> RunFilterPluginsWithNominatedPods(<br>    ctx context.Context, <span class="hljs-comment">// 调度上下文</span><br>    state *framework.CycleState, <span class="hljs-comment">// 当前周期状态</span><br>    pod *v1.Pod, <span class="hljs-comment">// 待调度的 Pod</span><br>    info *framework.NodeInfo, <span class="hljs-comment">// 节点信息</span><br>) *framework.Status &#123;<br>    <span class="hljs-keyword">var</span> status *framework.Status<br><br>    podsAdded := <span class="hljs-literal">false</span><br>  <span class="hljs-comment">// We run filters twice in some cases. If the node has greater or equal priority</span><br><span class="hljs-comment">// nominated pods, we run them when those pods are added to PreFilter state and nodeInfo.</span><br><span class="hljs-comment">// If all filters succeed in this pass, we run them again when these</span><br><span class="hljs-comment">// nominated pods are not added. This second pass is necessary because some</span><br><span class="hljs-comment">// filters such as inter-pod affinity may not pass without the nominated pods.</span><br><span class="hljs-comment">// If there are no nominated pods for the node or if the first run of the</span><br><span class="hljs-comment">// filters fail, we don&#x27;t run the second pass.</span><br><span class="hljs-comment">// We consider only equal or higher priority pods in the first pass, because</span><br><span class="hljs-comment">// those are the current &quot;pod&quot; must yield to them and not take a space opened</span><br><span class="hljs-comment">// for running them. It is ok if the current &quot;pod&quot; take resources freed for</span><br><span class="hljs-comment">// lower priority pods.</span><br><span class="hljs-comment">// Requiring that the new pod is schedulable in both circumstances ensures that</span><br><span class="hljs-comment">// we are making a conservative decision: filters like resources and inter-pod</span><br><span class="hljs-comment">// anti-affinity are more likely to fail when the nominated pods are treated</span><br><span class="hljs-comment">// as running, while filters like pod affinity are more likely to fail when</span><br><span class="hljs-comment">// the nominated pods are treated as not running. We can&#x27;t just assume the</span><br><span class="hljs-comment">// nominated pods are running because they are not running right now and in fact,</span><br><span class="hljs-comment">// they may end up getting scheduled to a different node.</span><br>    <span class="hljs-comment">// 我们可能需要两次运行过滤插件。如果节点上有优先级更高或相等的被提名的 Pods，</span><br>    <span class="hljs-comment">// 我们会在这些 Pods 被添加到 PreFilter 状态和 nodeInfo 时运行它们。</span><br>    <span class="hljs-comment">// 如果所有过滤插件在这一轮通过，我们会在这些被提名的 Pods 没有被添加的情况下再次运行它们。</span><br>    <span class="hljs-comment">// 第二轮运行是必要的，因为一些过滤插件（如 Pod 亲和性）可能在没有被提名的 Pods 的情况下无法通过。</span><br>    <span class="hljs-comment">// 如果节点没有被提名的 Pods 或者第一轮过滤插件失败，我们不会进行第二轮。</span><br>    <span class="hljs-comment">// 我们只考虑第一轮中优先级相等或更高的 Pods，因为当前的 &quot;pod&quot; 必须为它们让路，而不是占用为它们运行而开放的空间。</span><br>    <span class="hljs-comment">// 如果当前的 &quot;pod&quot; 占用了为低优先级 Pods 释放的资源，这是可以的。</span><br>    <span class="hljs-comment">// 要求新的 Pod 在这两种情况下都是可调度的，确保我们做出的是保守的决定：</span><br>    <span class="hljs-comment">// 像资源和 Pod 反亲和性这样的过滤器在将被提名的 Pods 视为运行时更有可能失败，</span><br>    <span class="hljs-comment">// 而像 Pod 亲和性这样的过滤器在将被提名的 Pods 视为未运行时更有可能失败。</span><br>    <span class="hljs-comment">// 我们不能仅仅假设被提名的 Pods 正在运行，因为它们现在并没有运行，事实上，</span><br>    <span class="hljs-comment">// 它们最终可能会被调度到一个不同的节点上。</span><br>    <span class="hljs-keyword">for</span> i := <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">2</span>; i++ &#123;<br>        stateToUse := state<br>        nodeInfoToUse := info<br>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> &#123;<br>            <span class="hljs-comment">// 第一轮：添加被提名的 Pods 到周期状态和节点信息</span><br>            <span class="hljs-keyword">var</span> err <span class="hljs-type">error</span><br>            podsAdded, stateToUse, nodeInfoToUse, err = addNominatedPods(ctx, f, pod, state, info)<br>            <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>                <span class="hljs-keyword">return</span> framework.AsStatus(err)<br>            &#125;<br>        &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> !podsAdded || !status.IsSuccess() &#123;<br>            <span class="hljs-keyword">break</span><br>        &#125;<br><br>        <span class="hljs-comment">// 运行过滤插件</span><br>        status = f.RunFilterPlugins(ctx, stateToUse, pod, nodeInfoToUse)<br>        <span class="hljs-keyword">if</span> !status.IsSuccess() &amp;&amp; !status.IsUnschedulable() &#123;<br>            <span class="hljs-keyword">return</span> status<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-keyword">return</span> status<br>&#125;<br></code></pre></td></tr></table></figure><p>注意到这里执行了两遍筛选，主要是考虑到这个node上面可能存在一些预计要被调度过来的pod，在第一轮中会假设这些pod真的会被调度过来，然后查看是否满足pod筛选需求，在第二列会假设这些pod最后没有被调度过来，然后检查是否满足pod的筛选需求。因为在第一轮中可能会存在反亲和性要求，导致无法通过筛选，在第二轮中可能会存在亲和性要求，导致无法通过筛选，这是一种很保守的筛选方式。</p><p>利用各个插件进行筛选的函数（<code>RunFilterPlugins</code>）在<code>pkg/scheduler/framework/runtime/framework.go:725</code>中，如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// RunFilterPlugins runs the set of configured Filter plugins for pod on</span><br><span class="hljs-comment">// the given node. If any of these plugins doesn&#x27;t return &quot;Success&quot;, the</span><br><span class="hljs-comment">// given node is not suitable for running pod.</span><br><span class="hljs-comment">// Meanwhile, the failure message and status are set for the given node.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> RunFilterPlugins(<br>ctx context.Context,<br>state *framework.CycleState,<br>pod *v1.Pod,<br>nodeInfo *framework.NodeInfo,<br>) *framework.Status &#123;<br><span class="hljs-keyword">for</span> _, pl := <span class="hljs-keyword">range</span> f.filterPlugins &#123;<br><span class="hljs-keyword">if</span> state.SkipFilterPlugins.Has(pl.Name()) &#123;<br><span class="hljs-keyword">continue</span><br>&#125;<br>metrics.PluginEvaluationTotal.WithLabelValues(pl.Name(), metrics.Filter, f.profileName).Inc()<br><span class="hljs-keyword">if</span> status := f.runFilterPlugin(ctx, pl, state, pod, nodeInfo); !status.IsSuccess() &#123;<br><span class="hljs-keyword">if</span> !status.IsUnschedulable() &#123;<br><span class="hljs-comment">// Filter plugins are not supposed to return any status other than</span><br><span class="hljs-comment">// Success or Unschedulable.</span><br>status = framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;running %q filter plugin: %w&quot;</span>, pl.Name(), status.AsError()))<br>&#125;<br>status.SetFailedPlugin(pl.Name())<br><span class="hljs-keyword">return</span> status<br>&#125;<br>&#125;<br><br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>这里的逻辑很简单，就是遍历各个筛选的插件，依次检查是否符合要求。</p><p>可以继续看<code>runFilterPlugin</code>这运行一个筛选插件进行检查的函数，在pkg&#x2F;scheduler&#x2F;framework&#x2F;runtime&#x2F;framework.go:750中。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> runFilterPlugin(ctx context.Context, pl framework.FilterPlugin, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status &#123;<br><span class="hljs-keyword">if</span> !state.ShouldRecordPluginMetrics() &#123;<br><span class="hljs-keyword">return</span> pl.Filter(ctx, state, pod, nodeInfo)<br>&#125;<br>startTime := time.Now()<br>status := pl.Filter(ctx, state, pod, nodeInfo)<br>f.metricsRecorder.ObservePluginDurationAsync(metrics.Filter, pl.Name(), status.Code().String(), metrics.SinceInSeconds(startTime))<br><span class="hljs-keyword">return</span> status<br>&#125;<br></code></pre></td></tr></table></figure><p>主要也就是调用插件的Filter函数，具体插件的介绍后面再补充。</p><h2 id="打分过程"><a href="#打分过程" class="headerlink" title="打分过程"></a>打分过程</h2><p>打分的函数<code>prioritizeNodes</code> 在<code>pkg/scheduler/schedule_one.go</code> 中，如下，补充了部分注释</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">prioritizeNodes</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">    ctx context.Context,</span></span><br><span class="hljs-params"><span class="hljs-function">    extenders []framework.Extender,</span></span><br><span class="hljs-params"><span class="hljs-function">    fwk framework.Framework,</span></span><br><span class="hljs-params"><span class="hljs-function">    state *framework.CycleState,</span></span><br><span class="hljs-params"><span class="hljs-function">    pod *v1.Pod,</span></span><br><span class="hljs-params"><span class="hljs-function">    nodes []*v1.Node,</span></span><br><span class="hljs-params"><span class="hljs-function">)</span></span> ([]framework.NodePluginScores, <span class="hljs-type">error</span>) &#123;<br>    <span class="hljs-comment">// 如果没有提供优先级配置，则所有节点的分数都设为 1。</span><br>    <span class="hljs-comment">// 这是为了在所需的格式中生成优先级列表</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(extenders) == <span class="hljs-number">0</span> &amp;&amp; !fwk.HasScorePlugins() &#123;<br>        result := <span class="hljs-built_in">make</span>([]framework.NodePluginScores, <span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(nodes))<br>        <span class="hljs-keyword">for</span> i := <span class="hljs-keyword">range</span> nodes &#123;<br>            result = <span class="hljs-built_in">append</span>(result, framework.NodePluginScores&#123;<br>                Name:       nodes[i].Name,<br>                TotalScore: <span class="hljs-number">1</span>,<br>            &#125;)<br>        &#125;<br>        <span class="hljs-keyword">return</span> result, <span class="hljs-literal">nil</span><br>    &#125;<br><br>    <span class="hljs-comment">// 运行 PreScore 插件。</span><br>    preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes)<br>    <span class="hljs-keyword">if</span> !preScoreStatus.IsSuccess() &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, preScoreStatus.AsError()<br>    &#125;<br><br>    <span class="hljs-comment">// 运行 Score 插件。</span><br>    nodesScores, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes)<br>    <span class="hljs-keyword">if</span> !scoreStatus.IsSuccess() &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, scoreStatus.AsError()<br>    &#125;<br><br>    <span class="hljs-comment">// 如果启用了详细日志记录，记录每个插件对每个节点的打分</span><br>    klogV := klog.V(<span class="hljs-number">10</span>)<br>    <span class="hljs-keyword">if</span> klogV.Enabled() &#123;<br>        <span class="hljs-keyword">for</span> _, nodeScore := <span class="hljs-keyword">range</span> nodesScores &#123;<br>            <span class="hljs-keyword">for</span> _, pluginScore := <span class="hljs-keyword">range</span> nodeScore.Scores &#123;<br>                klogV.InfoS(<span class="hljs-string">&quot;Plugin scored node for pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;plugin&quot;</span>, pluginScore.Name, <span class="hljs-string">&quot;node&quot;</span>, nodeScore.Name, <span class="hljs-string">&quot;score&quot;</span>, pluginScore.Score)<br>            &#125;<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 如果有扩展器并且有节点，运行扩展器</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(extenders) != <span class="hljs-number">0</span> &amp;&amp; nodes != <span class="hljs-literal">nil</span> &#123;<br>        allNodeExtendersScores := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]*framework.NodePluginScores, <span class="hljs-built_in">len</span>(nodes))<br>        <span class="hljs-keyword">var</span> mu sync.Mutex<br>        <span class="hljs-keyword">var</span> wg sync.WaitGroup<br>        <span class="hljs-comment">// 并发运行每个扩展器的优先级函数</span><br>        <span class="hljs-keyword">for</span> i := <span class="hljs-keyword">range</span> extenders &#123;<br>            <span class="hljs-keyword">if</span> !extenders[i].IsInterested(pod) &#123;<br>                <span class="hljs-keyword">continue</span><br>            &#125;<br>            wg.Add(<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(extIndex <span class="hljs-type">int</span>)</span></span> &#123;<br>                <span class="hljs-keyword">defer</span> wg.Done()<br>                metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Inc()<br>                metrics.Goroutines.WithLabelValues(metrics.PrioritizingExtender).Inc()<br>                <span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>                    metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Dec()<br>                    metrics.Goroutines.WithLabelValues(metrics.PrioritizingExtender).Dec()<br>                &#125;()<br>                prioritizedList, weight, err := extenders[extIndex].Prioritize(pod, nodes)<br>                <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>                    klog.V(<span class="hljs-number">5</span>).InfoS(<span class="hljs-string">&quot;Failed to run extender&#x27;s priority function. No score given by this extender.&quot;</span>, <span class="hljs-string">&quot;error&quot;</span>, err, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;extender&quot;</span>, extenders[extIndex].Name())<br>                    <span class="hljs-keyword">return</span><br>                &#125;<br>                mu.Lock()<br>                <span class="hljs-keyword">defer</span> mu.Unlock()<br>                <span class="hljs-keyword">for</span> i := <span class="hljs-keyword">range</span> *prioritizedList &#123;<br>                    nodename := (*prioritizedList)[i].Host<br>                    score := (*prioritizedList)[i].Score<br>                    klogV.InfoS(<span class="hljs-string">&quot;Extender scored node for pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;extender&quot;</span>, extenders[extIndex].Name(), <span class="hljs-string">&quot;node&quot;</span>, nodename, <span class="hljs-string">&quot;score&quot;</span>, score)<br>                    <span class="hljs-comment">// 将扩展器的分数转换为调度器使用的分数范围</span><br>                    finalscore := score * weight * (framework.MaxNodeScore / extenderv1.MaxExtenderPriority)<br>                    <span class="hljs-keyword">if</span> allNodeExtendersScores[nodename] == <span class="hljs-literal">nil</span> &#123;<br>                        allNodeExtendersScores[nodename] = &amp;framework.NodePluginScores&#123;<br>                            Name:   nodename,<br>                            Scores: <span class="hljs-built_in">make</span>([]framework.PluginScore, <span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(extenders)),<br>                        &#125;<br>                    &#125;<br>                    allNodeExtendersScores[nodename].Scores = <span class="hljs-built_in">append</span>(allNodeExtendersScores[nodename].Scores, framework.PluginScore&#123;<br>                        Name:  extenders[extIndex].Name(),<br>                        Score: finalscore,<br>                    &#125;)<br>                    allNodeExtendersScores[nodename].TotalScore += finalscore<br>                &#125;<br>            &#125;(i)<br>        &#125;<br>        wg.Wait() <span class="hljs-comment">// 等待所有扩展器完成</span><br>        <span class="hljs-comment">// 将扩展器的分数添加到节点分数中</span><br>        <span class="hljs-keyword">for</span> i := <span class="hljs-keyword">range</span> nodesScores &#123;<br>            <span class="hljs-keyword">if</span> score, ok := allNodeExtendersScores[nodes[i].Name]; ok &#123;<br>                nodesScores[i].Scores = <span class="hljs-built_in">append</span>(nodesScores[i].Scores, score.Scores...)<br>                nodesScores[i].TotalScore += score.TotalScore<br>            &#125;<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 记录每个节点的最终分数</span><br>    <span class="hljs-keyword">if</span> klogV.Enabled() &#123;<br>        <span class="hljs-keyword">for</span> i := <span class="hljs-keyword">range</span> nodesScores &#123;<br>            klogV.InfoS(<span class="hljs-string">&quot;Calculated node&#x27;s final score for pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;node&quot;</span>, nodesScores[i].Name, <span class="hljs-string">&quot;score&quot;</span>, nodesScores[i].TotalScore)<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">return</span> nodesScores, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>主要流程包括：</p><ol><li>如果没有提供任何扩展器或打分插件，则为所有节点设置默认分数，并返回。</li><li>运行 PreScore 插件，为打分阶段做准备。</li><li>运行 Score 插件，获取每个节点的分数。</li><li>如果有扩展器并且有节点，则并发运行每个扩展器的优先级函数，获取扩展器为节点分配的分数。</li><li>将扩展器的分数转换为调度器使用的分数范围，并添加到节点分数中。</li><li>记录每个节点的最终分数。</li></ol><p>这里补充一下其记录节点分数的结构体<code>NodePluginScores</code>，在文件<code>pkg/scheduler/framework/interface.go:55</code>中，其定义如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// NodePluginScores is a struct with node name and scores for that node.</span><br><span class="hljs-keyword">type</span> NodePluginScores <span class="hljs-keyword">struct</span> &#123;<br><span class="hljs-comment">// Name is node name.</span><br>Name <span class="hljs-type">string</span><br><span class="hljs-comment">// Scores is scores from plugins and extenders.</span><br>Scores []PluginScore<br><span class="hljs-comment">// TotalScore is the total score in Scores.</span><br>TotalScore <span class="hljs-type">int64</span><br>&#125;<br><br><span class="hljs-comment">// PluginScore is a struct with plugin/extender name and score.</span><br><span class="hljs-keyword">type</span> PluginScore <span class="hljs-keyword">struct</span> &#123;<br><span class="hljs-comment">// Name is the name of plugin or extender.</span><br>Name  <span class="hljs-type">string</span><br>Score <span class="hljs-type">int64</span><br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到每个插件给node打分都是一个int64的类型，一个节点可能会被多个插件进行打分，最后再汇总。</p><p>再回到插件打分，这里我们主要关注关键的打分插件<code>RunScorePlugins</code> ，在<code>pkg/scheduler/framework/runtime/framework.go:931</code>中，如下，补充了部分注释</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> RunScorePlugins(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodes []*v1.Node) (ns []framework.NodePluginScores, status *framework.Status) &#123;<br>    startTime := time.Now()<br>    <span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>        <span class="hljs-comment">// 记录打分扩展点的持续时间</span><br>        metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.Score, status.Code().String(), f.profileName).Observe(metrics.SinceInSeconds(startTime))<br>    &#125;()<br>    allNodePluginScores := <span class="hljs-built_in">make</span>([]framework.NodePluginScores, <span class="hljs-built_in">len</span>(nodes))<br>    numPlugins := <span class="hljs-built_in">len</span>(f.scorePlugins) - state.SkipScorePlugins.Len()<br>    plugins := <span class="hljs-built_in">make</span>([]framework.ScorePlugin, <span class="hljs-number">0</span>, numPlugins)<br>    pluginToNodeScores := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]framework.NodeScoreList, numPlugins)<br>    <span class="hljs-comment">// 为每个插件创建一个节点分数列表</span><br>    <span class="hljs-keyword">for</span> _, pl := <span class="hljs-keyword">range</span> f.scorePlugins &#123;<br>        <span class="hljs-keyword">if</span> state.SkipScorePlugins.Has(pl.Name()) &#123;<br>            <span class="hljs-keyword">continue</span><br>        &#125;<br>        plugins = <span class="hljs-built_in">append</span>(plugins, pl)<br>        pluginToNodeScores[pl.Name()] = <span class="hljs-built_in">make</span>(framework.NodeScoreList, <span class="hljs-built_in">len</span>(nodes))<br>    &#125;<br>    ctx, cancel := context.WithCancel(ctx)<br>    <span class="hljs-keyword">defer</span> cancel()<br>    errCh := parallelize.NewErrorChannel()<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(plugins) &gt; <span class="hljs-number">0</span> &#123;<br>        <span class="hljs-comment">// 并行地为每个节点运行每个插件的 Score 方法</span><br>        f.Parallelizer().Until(ctx, <span class="hljs-built_in">len</span>(nodes), <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(index <span class="hljs-type">int</span>)</span></span> &#123;<br>            nodeName := nodes[index].Name<br>            <span class="hljs-keyword">for</span> _, pl := <span class="hljs-keyword">range</span> plugins &#123;<br>                s, status := f.runScorePlugin(ctx, pl, state, pod, nodeName)<br>                <span class="hljs-keyword">if</span> !status.IsSuccess() &#123;<br>                    err := fmt.Errorf(<span class="hljs-string">&quot;plugin %q failed with: %w&quot;</span>, pl.Name(), status.AsError())<br>                    errCh.SendErrorWithCancel(err, cancel)<br>                    <span class="hljs-keyword">return</span><br>                &#125;<br>                pluginToNodeScores[pl.Name()][index] = framework.NodeScore&#123;<br>                    Name:  nodeName,<br>                    Score: s,<br>                &#125;<br>            &#125;<br>        &#125;, metrics.Score)<br>        <span class="hljs-keyword">if</span> err := errCh.ReceiveError(); err != <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;running Score plugins: %w&quot;</span>, err))<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 并行地为每个打分插件运行 NormalizeScore 方法</span><br>    f.Parallelizer().Until(ctx, <span class="hljs-built_in">len</span>(plugins), <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(index <span class="hljs-type">int</span>)</span></span> &#123;<br>        pl := plugins[index]<br>        <span class="hljs-keyword">if</span> pl.ScoreExtensions() == <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-keyword">return</span><br>        &#125;<br>        nodeScoreList := pluginToNodeScores[pl.Name()]<br>        status := f.runScoreExtension(ctx, pl, state, pod, nodeScoreList)<br>        <span class="hljs-keyword">if</span> !status.IsSuccess() &#123;<br>            err := fmt.Errorf(<span class="hljs-string">&quot;plugin %q failed with: %w&quot;</span>, pl.Name(), status.AsError())<br>            errCh.SendErrorWithCancel(err, cancel)<br>            <span class="hljs-keyword">return</span><br>        &#125;<br>    &#125;, metrics.Score)<br>    <span class="hljs-keyword">if</span> err := errCh.ReceiveError(); err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;running Normalize on Score plugins: %w&quot;</span>, err))<br>    &#125;<br><br>    <span class="hljs-comment">// 并行地为每个打分插件应用分数权重，并构建 allNodePluginScores</span><br>    f.Parallelizer().Until(ctx, <span class="hljs-built_in">len</span>(nodes), <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(index <span class="hljs-type">int</span>)</span></span> &#123;<br>        nodePluginScores := framework.NodePluginScores&#123;<br>            Name:   nodes[index].Name,<br>            Scores: <span class="hljs-built_in">make</span>([]framework.PluginScore, <span class="hljs-built_in">len</span>(plugins)),<br>        &#125;<br><br>        <span class="hljs-keyword">for</span> i, pl := <span class="hljs-keyword">range</span> plugins &#123;<br>            weight := f.scorePluginWeight[pl.Name()]<br>            nodeScoreList := pluginToNodeScores[pl.Name()]<br>            score := nodeScoreList[index].Score<br><br>            <span class="hljs-keyword">if</span> score &gt; framework.MaxNodeScore || score &lt; framework.MinNodeScore &#123;<br>                err := fmt.Errorf(<span class="hljs-string">&quot;plugin %q returns an invalid score %v, it should in the range of [%v, %v] after normalizing&quot;</span>, pl.Name(), score, framework.MinNodeScore, framework.MaxNodeScore)<br>                errCh.SendErrorWithCancel(err, cancel)<br>                <span class="hljs-keyword">return</span><br>            &#125;<br>            weightedScore := score * <span class="hljs-type">int64</span>(weight)<br>            nodePluginScores.Scores[i] = framework.PluginScore&#123;<br>                Name:  pl.Name(),<br>                Score: weightedScore,<br>            &#125;<br>            nodePluginScores.TotalScore += weightedScore<br>        &#125;<br>        allNodePluginScores[index] = nodePluginScores<br>    &#125;, metrics.Score)<br>    <span class="hljs-keyword">if</span> err := errCh.ReceiveError(); err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;applying score defaultWeights on Score plugins: %w&quot;</span>, err))<br>    &#125;<br><br>    <span class="hljs-comment">// 返回所有节点的插件分数</span><br>    <span class="hljs-keyword">return</span> allNodePluginScores, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>主要流程包括：</p><ol><li>为每个插件创建一个节点分数列表。</li><li>使用并行处理为每个节点运行每个插件的 <code>Score</code> 方法。</li><li>为每个插件运行 <code>NormalizeScore</code> 方法，以标准化分数。</li><li>应用每个插件的分数权重，构建最终的节点分数。</li><li>返回各个节点的分数</li></ol><p>查看插件打分的函数runScorePlugin，在<code>pkg/scheduler/framework/runtime/framework.go:1025</code> 中，如下。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> runScorePlugin(ctx context.Context, pl framework.ScorePlugin, state *framework.CycleState, pod *v1.Pod, nodeName <span class="hljs-type">string</span>) (<span class="hljs-type">int64</span>, *framework.Status) &#123;<br><span class="hljs-keyword">if</span> !state.ShouldRecordPluginMetrics() &#123;<br><span class="hljs-keyword">return</span> pl.Score(ctx, state, pod, nodeName)<br>&#125;<br>startTime := time.Now()<br>s, status := pl.Score(ctx, state, pod, nodeName)<br>f.metricsRecorder.ObservePluginDurationAsync(metrics.Score, pl.Name(), status.Code().String(), metrics.SinceInSeconds(startTime))<br><span class="hljs-keyword">return</span> s, status<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到主要是调用插件的Score方法。</p><h1 id="一般调度的后期处理"><a href="#一般调度的后期处理" class="headerlink" title="一般调度的后期处理"></a>一般调度的后期处理</h1><h2 id="PostFilter插件"><a href="#PostFilter插件" class="headerlink" title="PostFilter插件"></a>PostFilter插件</h2><p>在<code>schedulingCycle</code>中可以看到如果上述的一般调度没有为Pod找到合适的node，并且错误不是没有合适的node，即<code>ErrNoNodesAvailable</code> 的话，就会检查是否存在有PostFilterPlugins，如果有就运行，即运行<code>RunPostFilterPlugins</code>函数，来进行相关的处理，例如释放一些资源，从而希望使得该pod在下一次调度时有机会成功调度，当然这被释放的资源也可能被其他不同的pod给占用了，但是这对系统是无害的，所以也不管。</p><p>该<code>RunPostFilterPlugins</code>函数在<code>pkg/scheduler/framework/runtime/framework.go:762</code>中，如下所示</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// RunPostFilterPlugins runs the set of configured PostFilter plugins until the first</span><br><span class="hljs-comment">// Success, Error or UnschedulableAndUnresolvable is met; otherwise continues to execute all plugins.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> RunPostFilterPlugins(ctx context.Context, state *framework.CycleState, pod *v1.Pod, filteredNodeStatusMap framework.NodeToStatusMap) (_ *framework.PostFilterResult, status *framework.Status) &#123;<br>startTime := time.Now()<br><span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.PostFilter, status.Code().String(), f.profileName).Observe(metrics.SinceInSeconds(startTime))<br>&#125;()<br><br><span class="hljs-comment">// `result` records the last meaningful(non-noop) PostFilterResult.</span><br><span class="hljs-keyword">var</span> result *framework.PostFilterResult<br><span class="hljs-keyword">var</span> reasons []<span class="hljs-type">string</span><br><span class="hljs-keyword">var</span> failedPlugin <span class="hljs-type">string</span><br><span class="hljs-keyword">for</span> _, pl := <span class="hljs-keyword">range</span> f.postFilterPlugins &#123;<br>r, s := f.runPostFilterPlugin(ctx, pl, state, pod, filteredNodeStatusMap)<br><span class="hljs-keyword">if</span> s.IsSuccess() &#123;<br><span class="hljs-keyword">return</span> r, s<br>&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> s.Code() == framework.UnschedulableAndUnresolvable &#123;<br><span class="hljs-keyword">return</span> r, s.WithFailedPlugin(pl.Name())<br>&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> !s.IsUnschedulable() &#123;<br><span class="hljs-comment">// Any status other than Success, Unschedulable or UnschedulableAndUnresolvable is Error.</span><br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, framework.AsStatus(s.AsError()).WithFailedPlugin(pl.Name())<br>&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> r != <span class="hljs-literal">nil</span> &amp;&amp; r.Mode() != framework.ModeNoop &#123;<br>result = r<br>&#125;<br><br>reasons = <span class="hljs-built_in">append</span>(reasons, s.Reasons()...)<br><span class="hljs-comment">// Record the first failed plugin unless we proved that</span><br><span class="hljs-comment">// the latter is more relevant.</span><br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(failedPlugin) == <span class="hljs-number">0</span> &#123;<br>failedPlugin = pl.Name()<br>&#125;<br>&#125;<br><br><span class="hljs-keyword">return</span> result, framework.NewStatus(framework.Unschedulable, reasons...).WithFailedPlugin(failedPlugin)<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到他就是遍历了所有的postFilter插件，然后使用函数<code>runPostFilterPlugin</code>运行这些插件，其在<code>pkg/scheduler/framework/runtime/framework.go:796</code>中</p><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs pf">func (f *frameworkImpl) runPostFilterPlugin(ctx context.Context, pl framework.PostFilterPlugin, <span class="hljs-keyword">state</span> *framework.CycleState, pod *v1.Pod, filteredNodeStatusMap framework.NodeToStatusMap) (*framework.PostFilterResult, *framework.Status) &#123;<br>if !<span class="hljs-keyword">state</span>.ShouldRecordPluginMetrics() &#123;<br>return pl.PostFilter(ctx, <span class="hljs-keyword">state</span>, pod, filteredNodeStatusMap)<br>&#125;<br>startTime := time.Now()<br>r, s := pl.PostFilter(ctx, <span class="hljs-keyword">state</span>, pod, filteredNodeStatusMap)<br>f.metricsRecorder.ObservePluginDurationAsync(metrics.PostFilter, pl.Name(), s.Code().String(), metrics.SinceInSeconds(startTime))<br>return r, s<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="Reserve插件"><a href="#Reserve插件" class="headerlink" title="Reserve插件"></a>Reserve插件</h2><p>得到想要调度到的pod后，可能需要执行一些资源预留的操作，就需要定义在reserve插件中，该插件对应的调用函数为RunReservePluginsReserve，在<code>pkg/scheduler/framework/runtime/framework.go:1144</code> 中</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// RunReservePluginsReserve runs the Reserve method in the set of configured</span><br><span class="hljs-comment">// reserve plugins. If any of these plugins returns an error, it does not</span><br><span class="hljs-comment">// continue running the remaining ones and returns the error. In such a case,</span><br><span class="hljs-comment">// the pod will not be scheduled and the caller will be expected to call</span><br><span class="hljs-comment">// RunReservePluginsUnreserve.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> RunReservePluginsReserve(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName <span class="hljs-type">string</span>) (status *framework.Status) &#123;<br>startTime := time.Now()<br><span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.Reserve, status.Code().String(), f.profileName).Observe(metrics.SinceInSeconds(startTime))<br>&#125;()<br><span class="hljs-keyword">for</span> _, pl := <span class="hljs-keyword">range</span> f.reservePlugins &#123;<br>status = f.runReservePluginReserve(ctx, pl, state, pod, nodeName)<br><span class="hljs-keyword">if</span> !status.IsSuccess() &#123;<br>err := status.AsError()<br>klog.ErrorS(err, <span class="hljs-string">&quot;Failed running Reserve plugin&quot;</span>, <span class="hljs-string">&quot;plugin&quot;</span>, pl.Name(), <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br><span class="hljs-keyword">return</span> framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;running Reserve plugin %q: %w&quot;</span>, pl.Name(), err))<br>&#125;<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>这里也是遍历所有的reserve插件，如果有任意一个插件失败了那么就失败了。单个插件的调用函数在<code>pkg/scheduler/framework/runtime/framework.go:1160</code>中，如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> runReservePluginReserve(ctx context.Context, pl framework.ReservePlugin, state *framework.CycleState, pod *v1.Pod, nodeName <span class="hljs-type">string</span>) *framework.Status &#123;<br><span class="hljs-keyword">if</span> !state.ShouldRecordPluginMetrics() &#123;<br><span class="hljs-keyword">return</span> pl.Reserve(ctx, state, pod, nodeName)<br>&#125;<br>startTime := time.Now()<br>status := pl.Reserve(ctx, state, pod, nodeName)<br>f.metricsRecorder.ObservePluginDurationAsync(metrics.Reserve, pl.Name(), status.Code().String(), metrics.SinceInSeconds(startTime))<br><span class="hljs-keyword">return</span> status<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="Permit插件"><a href="#Permit插件" class="headerlink" title="Permit插件"></a>Permit插件</h2><p>找到了要调度的pod后还需要运行permit插件，该插件主要用来查看记录是否还需要等待一下其他操作，例如抢占某个pod的资源，那么就需要等待被抢占pod的资源释放掉。</p><p>该插件对应的函数<code>RunPermitPlugins</code> 在<code>pkg/scheduler/framework/runtime/framework.go:1200</code>中，如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// RunPermitPlugins runs the set of configured permit plugins. If any of these</span><br><span class="hljs-comment">// plugins returns a status other than &quot;Success&quot; or &quot;Wait&quot;, it does not continue</span><br><span class="hljs-comment">// running the remaining plugins and returns an error. Otherwise, if any of the</span><br><span class="hljs-comment">// plugins returns &quot;Wait&quot;, then this function will create and add waiting pod</span><br><span class="hljs-comment">// to a map of currently waiting pods and return status with &quot;Wait&quot; code.</span><br><span class="hljs-comment">// Pod will remain waiting pod for the minimum duration returned by the permit plugins.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(f *frameworkImpl)</span></span> RunPermitPlugins(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName <span class="hljs-type">string</span>) (status *framework.Status) &#123;<br>    startTime := time.Now() <span class="hljs-comment">// 记录permit插件开始运行的时间</span><br>    <span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>        <span class="hljs-comment">// 记录permit插件的运行时间和最终状态</span><br>        metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.Permit, status.Code().String(), f.profileName).Observe(metrics.SinceInSeconds(startTime))<br>    &#125;()<br>    pluginsWaitTime := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]time.Duration) <span class="hljs-comment">// 存储每个插件的等待时间</span><br>    statusCode := framework.Success <span class="hljs-comment">// 初始化状态码为成功</span><br>    <span class="hljs-keyword">for</span> _, pl := <span class="hljs-keyword">range</span> f.permitPlugins &#123;<br>        <span class="hljs-comment">// 运行当前permit插件</span><br>        status, timeout := f.runPermitPlugin(ctx, pl, state, pod, nodeName)<br>        <span class="hljs-keyword">if</span> !status.IsSuccess() &#123;<br>            <span class="hljs-keyword">if</span> status.IsUnschedulable() &#123;<br>                <span class="hljs-comment">// 如果插件返回不可调度的状态，则记录日志并返回该状态</span><br>                klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;Pod rejected by permit plugin&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;plugin&quot;</span>, pl.Name(), <span class="hljs-string">&quot;status&quot;</span>, status.Message())<br>                status.SetFailedPlugin(pl.Name()) <span class="hljs-comment">// 设置失败的插件名称</span><br>                <span class="hljs-keyword">return</span> status<br>            &#125;<br>            <span class="hljs-keyword">if</span> status.IsWait() &#123;<br>                <span class="hljs-comment">// 如果插件返回等待的状态，则记录等待时间，但不立即返回</span><br>                <span class="hljs-comment">// 允许的最长等待时间由 maxTimeout 限制</span><br>                <span class="hljs-keyword">if</span> timeout &gt; maxTimeout &#123;<br>                    timeout = maxTimeout<br>                &#125;<br>                pluginsWaitTime[pl.Name()] = timeout<br>                statusCode = framework.Wait <span class="hljs-comment">// 更新状态码为等待</span><br>            &#125; <span class="hljs-keyword">else</span> &#123;<br>                <span class="hljs-comment">// 如果插件返回错误状态，则记录错误日志并返回错误状态</span><br>                err := status.AsError()<br>                klog.ErrorS(err, <span class="hljs-string">&quot;Failed running Permit plugin&quot;</span>, <span class="hljs-string">&quot;plugin&quot;</span>, pl.Name(), <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>                <span class="hljs-keyword">return</span> framework.AsStatus(fmt.Errorf(<span class="hljs-string">&quot;running Permit plugin %q: %w&quot;</span>, pl.Name(), err)).WithFailedPlugin(pl.Name())<br>            &#125;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">if</span> statusCode == framework.Wait &#123;<br>        <span class="hljs-comment">// 如果任何插件返回等待状态，则创建并添加等待中的 Pod 到映射中，并返回等待状态</span><br>        waitingPod := newWaitingPod(pod, pluginsWaitTime)<br>        f.waitingPods.add(waitingPod)<br>        msg := fmt.Sprintf(<span class="hljs-string">&quot;one or more plugins asked to wait and no plugin rejected pod %q&quot;</span>, pod.Name)<br>        klog.V(<span class="hljs-number">4</span>).InfoS(<span class="hljs-string">&quot;One or more plugins asked to wait and no plugin rejected pod&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>        <span class="hljs-keyword">return</span> framework.NewStatus(framework.Wait, msg)<br>    &#125;<br>    <span class="hljs-comment">// 如果所有插件都成功或返回等待，且没有插件拒绝 Pod，则返回 nil 表示没有错误</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>主要流程包括：</p><ol><li>记录开始运行许可插件的时间。</li><li>使用 <code>defer</code> 语句确保无论函数如何结束，都记录许可插件的运行时间和状态。</li><li>遍历所有的permit插件。</li><li>运行当前插件，并将结果状态保存到 <code>status</code>。</li><li>检查状态：<ul><li>如果状态是成功的，则继续运行下一个插件。</li><li>如果状态是不可调度的，则记录日志并返回该状态。</li><li>如果状态是等待的，则记录等待时间，并更新状态码为等待，然后继续运行下一个插件。</li><li>如果状态是错误，则记录错误日志，并返回错误状态。</li></ul></li><li>如果任何插件返回等待状态，则创建等待中的 Pod 并添加到映射中，然后返回等待状态。</li><li>如果所有插件都成功或返回等待，且没有插件拒绝 Pod，则返回 <code>nil</code>。</li></ol>]]></content>
    
    
    <categories>
      
      <category>k8s</category>
      
      <category>源码分析</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>源码分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【K8s源码分析（二）】-K8s调度队列介绍</title>
    <link href="/2024/05/10/k8sSource2/"/>
    <url>/2024/05/10/k8sSource2/</url>
    
    <content type="html"><![CDATA[<p>本次分析参考的K8s版本是<a href="https://github.com/kubernetes/kubernetes/tree/release-1.27">v1.27.0</a>。</p><h1 id="调度队列简介"><a href="#调度队列简介" class="headerlink" title="调度队列简介"></a>调度队列简介</h1><p>这里是官方对于K8s中调度队列的介绍，很值得一看：<a href="https://github.com/kubernetes/community/blob/f03b6d5692bd979f07dd472e7b6836b2dad0fd9b/contributors/devel/sig-scheduling/scheduler_queues.md">Scheduling queue in kube-scheduler</a>。整体的架构如下图所示。</p><p><img src="/2024/05/10/k8sSource2/scheduling_queues.png" alt="调度队列"></p><p>简单来说K8s中的调度队列主要有3种：</p><ul><li><strong>ActiveQ</strong>（heap结构）：在每个调度周期开始时都会从这里取出一个Pod尝试调度。一开始提交的所有没有指定<code>.spec.nodeName</code>的Pod都会发送到这里，也会接收来自unschedulableQ和BackoffQ刷新来的pod。默认的排序规则是按照优先级进行排列，高优先级的Pod在前面。</li><li><strong>UnschedulableQ</strong>（Map结构）：存储调度失败的Pod，以等待资源更新、其他相关Pod调度成功等事件，从而将其的Pod其进行重调度。</li><li><strong>BackoffQ</strong>（heap结构）：用来暂时退避的队列，默认的排列规则是按退避时间的长度进行排序，需要退避的时间短的Pod在前面。为了防止Pod频繁的重调度，每个Pod都会记录自己的重调度次数，退避时间随着每次失败的调度尝试呈指数增长，直到达到最大值，例如尝试失败 3 次的 Pod 的目标退避超时设置为 curTime + 2s^3 (8s)。注意有两种情况下Pod会进入到BackoffQ队列中：<ul><li>unscheduleableQ会定时对其中的所有pod进行重调度，那么就需要计算各个pod是否退避了足够的时间，如果没有就放入到BackoffQ中再退避一段时间。</li><li>如果一个Pod调度失败时，正好这时又异步地发生了资源变更事件（<code>p.moveRequestCycle **&gt;=** podSchedulingCycle</code> ）(<code>schedulingCycle</code> 是当前调度的周期，ActiveQ队列每pop一个pod，就加1，<code>moveRequestCycle</code>是事件发生时<code>schedulingCycle</code> 的值），那么就不会放入UnschedulableQ中，而是会直接放入到BackoffQ中。</li></ul></li></ul><p>调度队列机制有两个在后台运行的定期刷新 go协程，负责将 pod 移动到活动队列，后续也将详细介绍相关代码：</p><ul><li><strong>flushUnschedulablePodsLeftover：</strong>每 30 秒运行一次，将 Pod 从UnschedulableQ中移动，以允许未由任何事件移动的不可调度的 Pod 再次重试。</li><li><strong>flushBackoffQCompleted：</strong>每1秒运行一次，将BackoffQ中已经回避了足够久的Pod移动到ActiveQ队列中</li></ul><p>移动请求（move request）会触发一个事件，该事件负责将 Pod 从UnschedulableQ移动到ActiveQ或BackoffQ。集群中许多事件可以触发移动请求的发生，包括了 Pod、节点、服务、PV、PVC、存储类和 CSI 节点的更改。例如当某些pod被调度时，UnschedulableQ中与其具有亲和性要求而导致之前无法调度的pod就会被移动出去，或者当某个新node加入时，原本因为资源不够导致无法调度的Pod也会被移动出去。</p><h1 id="调度队列源代码分析"><a href="#调度队列源代码分析" class="headerlink" title="调度队列源代码分析"></a>调度队列源代码分析</h1><h2 id="队列初始化"><a href="#队列初始化" class="headerlink" title="队列初始化"></a>队列初始化</h2><p>Scheduler中的调度队列<code>SchedulingQueue</code>为<code>internalqueue.SchedulingQueue</code>类型，该类型的实现在pkg&#x2F;scheduler&#x2F;internal&#x2F;queue&#x2F;scheduling_queue.go:92，如下。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// SchedulingQueue is an interface for a queue to store pods waiting to be scheduled.</span><br><span class="hljs-comment">// The interface follows a pattern similar to cache.FIFO and cache.Heap and</span><br><span class="hljs-comment">// makes it easy to use those data structures as a SchedulingQueue.</span><br><span class="hljs-keyword">type</span> SchedulingQueue <span class="hljs-keyword">interface</span> &#123;<br>framework.PodNominator<br>Add(pod *v1.Pod) <span class="hljs-type">error</span><br><span class="hljs-comment">// Activate moves the given pods to activeQ iff they&#x27;re in unschedulablePods or backoffQ.</span><br><span class="hljs-comment">// The passed-in pods are originally compiled from plugins that want to activate Pods,</span><br><span class="hljs-comment">// by injecting the pods through a reserved CycleState struct (PodsToActivate).</span><br>Activate(pods <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]*v1.Pod)<br><span class="hljs-comment">// AddUnschedulableIfNotPresent adds an unschedulable pod back to scheduling queue.</span><br><span class="hljs-comment">// The podSchedulingCycle represents the current scheduling cycle number which can be</span><br><span class="hljs-comment">// returned by calling SchedulingCycle().</span><br>AddUnschedulableIfNotPresent(pod *framework.QueuedPodInfo, podSchedulingCycle <span class="hljs-type">int64</span>) <span class="hljs-type">error</span><br><span class="hljs-comment">// SchedulingCycle returns the current number of scheduling cycle which is</span><br><span class="hljs-comment">// cached by scheduling queue. Normally, incrementing this number whenever</span><br><span class="hljs-comment">// a pod is popped (e.g. called Pop()) is enough.</span><br>SchedulingCycle() <span class="hljs-type">int64</span><br><span class="hljs-comment">// Pop removes the head of the queue and returns it. It blocks if the</span><br><span class="hljs-comment">// queue is empty and waits until a new item is added to the queue.</span><br>Pop() (*framework.QueuedPodInfo, <span class="hljs-type">error</span>)<br>Update(oldPod, newPod *v1.Pod) <span class="hljs-type">error</span><br>Delete(pod *v1.Pod) <span class="hljs-type">error</span><br>MoveAllToActiveOrBackoffQueue(event framework.ClusterEvent, preCheck PreEnqueueCheck)<br>AssignedPodAdded(pod *v1.Pod)<br>AssignedPodUpdated(pod *v1.Pod)<br>PendingPods() ([]*v1.Pod, <span class="hljs-type">string</span>)<br><span class="hljs-comment">// Close closes the SchedulingQueue so that the goroutine which is</span><br><span class="hljs-comment">// waiting to pop items can exit gracefully.</span><br>Close()<br><span class="hljs-comment">// Run starts the goroutines managing the queue.</span><br>Run()<br>&#125;<br></code></pre></td></tr></table></figure><p>上述代码定义了其需要的对队列中的元素添加、删除、更新、获取、运行等方法。而其标准实现<code>PriorityQueue</code> 在</p><p> <code>pkg/scheduler/internal/queue/scheduling_queue.go:145</code> 中，首先查看其需要的变量：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// PriorityQueue implements a scheduling queue.</span><br><span class="hljs-comment">// The head of PriorityQueue is the highest priority pending pod. This structure</span><br><span class="hljs-comment">// has two sub queues and a additional data structure, namely: activeQ,</span><br><span class="hljs-comment">// backoffQ and unschedulablePods.</span><br><span class="hljs-comment">//   - activeQ holds pods that are being considered for scheduling.</span><br><span class="hljs-comment">//   - backoffQ holds pods that moved from unschedulablePods and will move to</span><br><span class="hljs-comment">//     activeQ when their backoff periods complete.</span><br><span class="hljs-comment">//   - unschedulablePods holds pods that were already attempted for scheduling and</span><br><span class="hljs-comment">//     are currently determined to be unschedulable.</span><br><span class="hljs-keyword">type</span> PriorityQueue <span class="hljs-keyword">struct</span> &#123;<br>*nominator<br><br>stop  <span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;<br>clock clock.Clock<br><br><span class="hljs-comment">// pod initial backoff duration.</span><br>podInitialBackoffDuration time.Duration<br><span class="hljs-comment">// pod maximum backoff duration.</span><br>podMaxBackoffDuration time.Duration<br><span class="hljs-comment">// the maximum time a pod can stay in the unschedulablePods.</span><br>podMaxInUnschedulablePodsDuration time.Duration<br><br>cond sync.Cond<br><br><span class="hljs-comment">// activeQ is heap structure that scheduler actively looks at to find pods to</span><br><span class="hljs-comment">// schedule. Head of heap is the highest priority pod.</span><br>activeQ *heap.Heap<br><span class="hljs-comment">// podBackoffQ is a heap ordered by backoff expiry. Pods which have completed backoff</span><br><span class="hljs-comment">// are popped from this heap before the scheduler looks at activeQ</span><br>podBackoffQ *heap.Heap<br><span class="hljs-comment">// unschedulablePods holds pods that have been tried and determined unschedulable.</span><br>unschedulablePods *UnschedulablePods<br><span class="hljs-comment">// schedulingCycle represents sequence number of scheduling cycle and is incremented</span><br><span class="hljs-comment">// when a pod is popped.</span><br>schedulingCycle <span class="hljs-type">int64</span><br><span class="hljs-comment">// moveRequestCycle caches the sequence number of scheduling cycle when we</span><br><span class="hljs-comment">// received a move request. Unschedulable pods in and before this scheduling</span><br><span class="hljs-comment">// cycle will be put back to activeQueue if we were trying to schedule them</span><br><span class="hljs-comment">// when we received move request.</span><br>moveRequestCycle <span class="hljs-type">int64</span><br><br>clusterEventMap <span class="hljs-keyword">map</span>[framework.ClusterEvent]sets.String<br><span class="hljs-comment">// preEnqueuePluginMap is keyed with profile name, valued with registered preEnqueue plugins.</span><br>preEnqueuePluginMap <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>][]framework.PreEnqueuePlugin<br><br><span class="hljs-comment">// closed indicates that the queue is closed.</span><br><span class="hljs-comment">// It is mainly used to let Pop() exit its control loop while waiting for an item.</span><br>closed <span class="hljs-type">bool</span><br><br>nsLister listersv1.NamespaceLister<br><br>metricsRecorder metrics.MetricAsyncRecorder<br><span class="hljs-comment">// pluginMetricsSamplePercent is the percentage of plugin metrics to be sampled.</span><br>pluginMetricsSamplePercent <span class="hljs-type">int</span><br>&#125;<br></code></pre></td></tr></table></figure><p>在<code>pkg/scheduler/internal/queue/scheduling_queue.go:291</code>  中给出了生成了该队列的初始化方法</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// NewPriorityQueue creates a PriorityQueue object.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">NewPriorityQueue</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">lessFn framework.LessFunc,</span></span><br><span class="hljs-params"><span class="hljs-function">informerFactory informers.SharedInformerFactory,</span></span><br><span class="hljs-params"><span class="hljs-function">opts ...Option,</span></span><br><span class="hljs-params"><span class="hljs-function">)</span></span> *PriorityQueue &#123;<br>options := defaultPriorityQueueOptions<br><span class="hljs-keyword">if</span> options.podLister == <span class="hljs-literal">nil</span> &#123;<br>options.podLister = informerFactory.Core().V1().Pods().Lister()<br>&#125;<br><span class="hljs-keyword">for</span> _, opt := <span class="hljs-keyword">range</span> opts &#123;<br>opt(&amp;options)<br>&#125;<br><br>comp := <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(podInfo1, podInfo2 <span class="hljs-keyword">interface</span>&#123;&#125;)</span></span> <span class="hljs-type">bool</span> &#123;<br>pInfo1 := podInfo1.(*framework.QueuedPodInfo)<br>pInfo2 := podInfo2.(*framework.QueuedPodInfo)<br><span class="hljs-keyword">return</span> lessFn(pInfo1, pInfo2)<br>&#125;<br><br>pq := &amp;PriorityQueue&#123;<br>nominator:                         newPodNominator(options.podLister),<br>clock:                             options.clock,<br>stop:                              <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;),<br>podInitialBackoffDuration:         options.podInitialBackoffDuration,<br>podMaxBackoffDuration:             options.podMaxBackoffDuration,<br>podMaxInUnschedulablePodsDuration: options.podMaxInUnschedulablePodsDuration,<br>activeQ:                           heap.NewWithRecorder(podInfoKeyFunc, comp, metrics.NewActivePodsRecorder()),<br>unschedulablePods:                 newUnschedulablePods(metrics.NewUnschedulablePodsRecorder(), metrics.NewGatedPodsRecorder()),<br>moveRequestCycle:                  <span class="hljs-number">-1</span>,<br>clusterEventMap:                   options.clusterEventMap,<br>preEnqueuePluginMap:               options.preEnqueuePluginMap,<br>metricsRecorder:                   options.metricsRecorder,<br>pluginMetricsSamplePercent:        options.pluginMetricsSamplePercent,<br>&#125;<br>pq.cond.L = &amp;pq.lock<br>pq.podBackoffQ = heap.NewWithRecorder(podInfoKeyFunc, pq.podsCompareBackoffCompleted, metrics.NewBackoffPodsRecorder())<br>pq.nsLister = informerFactory.Core().V1().Namespaces().Lister()<br><br><span class="hljs-keyword">return</span> pq<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到其包含了许多我们上面介绍的概念，包括<code>activeQ</code> 、<code>unschedulablePods</code> 、<code>podBackoffQ</code> 、<code>schedulingCycle</code> 、<code>moveRequestCycle</code>  。</p><h2 id="QueuedPodInfo元素介绍"><a href="#QueuedPodInfo元素介绍" class="headerlink" title="QueuedPodInfo元素介绍"></a>QueuedPodInfo元素介绍</h2><p>这里也多次出现了<code>QueuedPodInfo</code>这个关键的数据结构，它是Pod中的基础元素，在此进行介绍，其定义在<code>pkg/scheduler/framework/types.go:98</code> 中，包括了PodInfo、添加时间、尝试次数等</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// QueuedPodInfo is a Pod wrapper with additional information related to</span><br><span class="hljs-comment">// the pod&#x27;s status in the scheduling queue, such as the timestamp when</span><br><span class="hljs-comment">// it&#x27;s added to the queue.</span><br><span class="hljs-keyword">type</span> QueuedPodInfo <span class="hljs-keyword">struct</span> &#123;<br>*PodInfo<br><span class="hljs-comment">// The time pod added to the scheduling queue.</span><br>Timestamp time.Time<br><span class="hljs-comment">// Number of schedule attempts before successfully scheduled.</span><br><span class="hljs-comment">// It&#x27;s used to record the # attempts metric.</span><br>Attempts <span class="hljs-type">int</span><br><span class="hljs-comment">// The time when the pod is added to the queue for the first time. The pod may be added</span><br><span class="hljs-comment">// back to the queue multiple times before it&#x27;s successfully scheduled.</span><br><span class="hljs-comment">// It shouldn&#x27;t be updated once initialized. It&#x27;s used to record the e2e scheduling</span><br><span class="hljs-comment">// latency for a pod.</span><br>InitialAttemptTimestamp time.Time<br><span class="hljs-comment">// If a Pod failed in a scheduling cycle, record the plugin names it failed by.</span><br>UnschedulablePlugins sets.String<br><span class="hljs-comment">// Whether the Pod is scheduling gated (by PreEnqueuePlugins) or not.</span><br>Gated <span class="hljs-type">bool</span><br>&#125;<br></code></pre></td></tr></table></figure><p>PodInfo的定义在<code>pkg/scheduler/framework/types.go:131</code></p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// PodInfo is a wrapper to a Pod with additional pre-computed information to</span><br><span class="hljs-comment">// accelerate processing. This information is typically immutable (e.g., pre-processed</span><br><span class="hljs-comment">// inter-pod affinity selectors).</span><br><span class="hljs-keyword">type</span> PodInfo <span class="hljs-keyword">struct</span> &#123;<br>Pod                        *v1.Pod<br>RequiredAffinityTerms      []AffinityTerm<br>RequiredAntiAffinityTerms  []AffinityTerm<br>PreferredAffinityTerms     []WeightedAffinityTerm<br>PreferredAntiAffinityTerms []WeightedAffinityTerm<br>&#125;<br></code></pre></td></tr></table></figure><p>Pod的定义在<code>staging/src/k8s.io/api/core/v1/types.go:4202</code>中</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Pod is a collection of containers that can run on a host. This resource is created</span><br><span class="hljs-comment">// by clients and scheduled onto hosts.</span><br><span class="hljs-keyword">type</span> Pod <span class="hljs-keyword">struct</span> &#123;<br>metav1.TypeMeta <span class="hljs-string">`json:&quot;,inline&quot;`</span><br><span class="hljs-comment">// Standard object&#x27;s metadata.</span><br><span class="hljs-comment">// More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata</span><br><span class="hljs-comment">// +optional</span><br>metav1.ObjectMeta <span class="hljs-string">`json:&quot;metadata,omitempty&quot; protobuf:&quot;bytes,1,opt,name=metadata&quot;`</span><br><br><span class="hljs-comment">// Specification of the desired behavior of the pod.</span><br><span class="hljs-comment">// More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status</span><br><span class="hljs-comment">// +optional</span><br>Spec PodSpec <span class="hljs-string">`json:&quot;spec,omitempty&quot; protobuf:&quot;bytes,2,opt,name=spec&quot;`</span><br><br><span class="hljs-comment">// Most recently observed status of the pod.</span><br><span class="hljs-comment">// This data may not be up to date.</span><br><span class="hljs-comment">// Populated by the system.</span><br><span class="hljs-comment">// Read-only.</span><br><span class="hljs-comment">// More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status</span><br><span class="hljs-comment">// +optional</span><br>Status PodStatus <span class="hljs-string">`json:&quot;status,omitempty&quot; protobuf:&quot;bytes,3,opt,name=status&quot;`</span><br>&#125;<br></code></pre></td></tr></table></figure><h2 id="ActiveQ源代码介绍"><a href="#ActiveQ源代码介绍" class="headerlink" title="ActiveQ源代码介绍"></a>ActiveQ源代码介绍</h2><p>从初始化代码中可以看到ActiveQ是一个heap，其相关定义在<code>pkg/scheduler/internal/heap/heap.go</code> 中</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Heap is a producer/consumer queue that implements a heap data structure.</span><br><span class="hljs-comment">// It can be used to implement priority queues and similar data structures.</span><br><span class="hljs-keyword">type</span> Heap <span class="hljs-keyword">struct</span> &#123;<br><span class="hljs-comment">// data stores objects and has a queue that keeps their ordering according</span><br><span class="hljs-comment">// to the heap invariant.</span><br>data *data<br><span class="hljs-comment">// metricRecorder updates the counter when elements of a heap get added or</span><br><span class="hljs-comment">// removed, and it does nothing if it&#x27;s nil</span><br>metricRecorder metrics.MetricRecorder<br>&#125;<br><span class="hljs-comment">// data is an internal struct that implements the standard heap interface</span><br><span class="hljs-comment">// and keeps the data stored in the heap.</span><br><span class="hljs-keyword">type</span> data <span class="hljs-keyword">struct</span> &#123;<br><span class="hljs-comment">// items is a map from key of the objects to the objects and their index.</span><br><span class="hljs-comment">// We depend on the property that items in the map are in the queue and vice versa.</span><br>items <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]*heapItem<br><span class="hljs-comment">// queue implements a heap data structure and keeps the order of elements</span><br><span class="hljs-comment">// according to the heap invariant. The queue keeps the keys of objects stored</span><br><span class="hljs-comment">// in &quot;items&quot;.</span><br>queue []<span class="hljs-type">string</span><br><br><span class="hljs-comment">// keyFunc is used to make the key used for queued item insertion and retrieval, and</span><br><span class="hljs-comment">// should be deterministic.</span><br>keyFunc KeyFunc<br><span class="hljs-comment">// lessFunc is used to compare two objects in the heap.</span><br>lessFunc lessFunc<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到他是用queue实现了一个heap。</p><p>ActiveQ的默认排序代码在<code>pkg/scheduler/framework/plugins/queuesort/priority_sort.go:42</code>中,即按优先级进行排序，如果优先级相同就提交时间的早晚进行排序。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Less is the function used by the activeQ heap algorithm to sort pods.</span><br><span class="hljs-comment">// It sorts pods based on their priority. When priorities are equal, it uses</span><br><span class="hljs-comment">// PodQueueInfo.timestamp.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(pl *PrioritySort)</span></span> Less(pInfo1, pInfo2 *framework.QueuedPodInfo) <span class="hljs-type">bool</span> &#123;<br>p1 := corev1helpers.PodPriority(pInfo1.Pod)<br>p2 := corev1helpers.PodPriority(pInfo2.Pod)<br><span class="hljs-keyword">return</span> (p1 &gt; p2) || (p1 == p2 &amp;&amp; pInfo1.Timestamp.Before(pInfo2.Timestamp))<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="UnschedulableQ源代码介绍"><a href="#UnschedulableQ源代码介绍" class="headerlink" title="UnschedulableQ源代码介绍"></a>UnschedulableQ源代码介绍</h2><p>UnschedulableQ进行初始化的具体代码在<code>pkg/scheduler/internal/queue/scheduling_queue.go:998</code></p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// newUnschedulablePods initializes a new object of UnschedulablePods.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">newUnschedulablePods</span><span class="hljs-params">(unschedulableRecorder, gatedRecorder metrics.MetricRecorder)</span></span> *UnschedulablePods &#123;<br><span class="hljs-keyword">return</span> &amp;UnschedulablePods&#123;<br>podInfoMap:            <span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]*framework.QueuedPodInfo),<br>keyFunc:               util.GetPodFullName,<br>unschedulableRecorder: unschedulableRecorder,<br>gatedRecorder:         gatedRecorder,<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>其具体的定义代码在<code>pkg/scheduler/internal/queue/scheduling_queue.go:939</code> ,</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// UnschedulablePods holds pods that cannot be scheduled. This data structure</span><br><span class="hljs-comment">// is used to implement unschedulablePods.</span><br><span class="hljs-keyword">type</span> UnschedulablePods <span class="hljs-keyword">struct</span> &#123;<br><span class="hljs-comment">// podInfoMap is a map key by a pod&#x27;s full-name and the value is a pointer to the QueuedPodInfo.</span><br>podInfoMap <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]*framework.QueuedPodInfo<br>keyFunc    <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(*v1.Pod)</span></span> <span class="hljs-type">string</span><br><span class="hljs-comment">// unschedulableRecorder/gatedRecorder updates the counter when elements of an unschedulablePodsMap</span><br><span class="hljs-comment">// get added or removed, and it does nothing if it&#x27;s nil.</span><br>unschedulableRecorder, gatedRecorder metrics.MetricRecorder<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到他没有进行heap的包装，而是直接采用Map结构进行保存。</p><h2 id="BackoffQ源代码介绍"><a href="#BackoffQ源代码介绍" class="headerlink" title="BackoffQ源代码介绍"></a><strong>BackoffQ</strong>源代码介绍</h2><p>BackoffQ也是一个heap，与ActiveQ不同的一点在于排序函数不同，其排序函数的定义在<code>pkg/scheduler/internal/queue/scheduling_queue.go:888</code> </p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *PriorityQueue)</span></span> podsCompareBackoffCompleted(podInfo1, podInfo2 <span class="hljs-keyword">interface</span>&#123;&#125;) <span class="hljs-type">bool</span> &#123;<br>pInfo1 := podInfo1.(*framework.QueuedPodInfo)<br>pInfo2 := podInfo2.(*framework.QueuedPodInfo)<br>bo1 := p.getBackoffTime(pInfo1)<br>bo2 := p.getBackoffTime(pInfo2)<br><span class="hljs-keyword">return</span> bo1.Before(bo2)<br>&#125;<br></code></pre></td></tr></table></figure><p><code>getBackoffTime</code>的定义在<code>pkg/scheduler/internal/queue/scheduling_queue.go:911</code>中，即计算完成避让的时间</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// getBackoffTime returns the time that podInfo completes backoff</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *PriorityQueue)</span></span> getBackoffTime(podInfo *framework.QueuedPodInfo) time.Time &#123;<br>duration := p.calculateBackoffDuration(podInfo)<br>backoffTime := podInfo.Timestamp.Add(duration)<br><span class="hljs-keyword">return</span> backoffTime<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到队列排序时会将完成避让最早的pod放在前面。</p><p>然后再看其是如何计算避让时间的，在<code>pkg/scheduler/internal/queue/scheduling_queue.go</code> 中</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// calculateBackoffDuration is a helper function for calculating the backoffDuration</span><br><span class="hljs-comment">// based on the number of attempts the pod has made.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *PriorityQueue)</span></span> calculateBackoffDuration(podInfo *framework.QueuedPodInfo) time.Duration &#123;<br>duration := p.podInitialBackoffDuration<br><span class="hljs-keyword">for</span> i := <span class="hljs-number">1</span>; i &lt; podInfo.Attempts; i++ &#123;<br><span class="hljs-comment">// Use subtraction instead of addition or multiplication to avoid overflow.</span><br><span class="hljs-keyword">if</span> duration &gt; p.podMaxBackoffDuration-duration &#123;<br><span class="hljs-keyword">return</span> p.podMaxBackoffDuration<br>&#125;<br>duration += duration<br>&#125;<br><span class="hljs-keyword">return</span> duration<br>&#125;<br></code></pre></td></tr></table></figure><p>其计算可以理解为初次为<code>p.podInitialBackoffDuration</code>，每次需要的避让时间都是前一次的两倍，如果计算得到的避让时间大于<code>p.podMaxBackoffDuration/2</code> ，就将避让时间设置为<code>p.podMaxBackoffDuration</code> 。</p><h2 id="队列弹出待调度的Pod"><a href="#队列弹出待调度的Pod" class="headerlink" title="队列弹出待调度的Pod"></a>队列弹出待调度的Pod</h2><p>其代码在<code>pkg/scheduler/internal/queue/scheduling_queue.go:593</code> 中</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Pop removes the head of the active queue and returns it. It blocks if the</span><br><span class="hljs-comment">// activeQ is empty and waits until a new item is added to the queue. It</span><br><span class="hljs-comment">// increments scheduling cycle when a pod is popped.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *PriorityQueue)</span></span> Pop() (*framework.QueuedPodInfo, <span class="hljs-type">error</span>) &#123;<br>p.lock.Lock()<br><span class="hljs-keyword">defer</span> p.lock.Unlock()<br><span class="hljs-keyword">for</span> p.activeQ.Len() == <span class="hljs-number">0</span> &#123;<br><span class="hljs-comment">// When the queue is empty, invocation of Pop() is blocked until new item is enqueued.</span><br><span class="hljs-comment">// When Close() is called, the p.closed is set and the condition is broadcast,</span><br><span class="hljs-comment">// which causes this loop to continue and return from the Pop().</span><br><span class="hljs-keyword">if</span> p.closed &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, fmt.Errorf(queueClosed)<br>&#125;<br>p.cond.Wait()<br>&#125;<br>obj, err := p.activeQ.Pop()<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, err<br>&#125;<br>pInfo := obj.(*framework.QueuedPodInfo)<br>pInfo.Attempts++<br>p.schedulingCycle++<br><span class="hljs-keyword">return</span> pInfo, <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到如果activeQ中没有需要调度的Pod了那么就会使用<code>p.cond.Wait</code>来进行等待，否则就冲<code>activeQ</code>中Pop一个元素<code>QueuedPodInfo</code>，同时这个<code>QueuedPodInfo</code> 的Attempts会+1，整个队列中的schedulingCycle也会增加。</p><h2 id="队列增加新的待调度的Pod"><a href="#队列增加新的待调度的Pod" class="headerlink" title="队列增加新的待调度的Pod"></a>队列增加新的待调度的Pod</h2><p>其代码在<code>pkg/scheduler/internal/queue/scheduling_queue.go:398</code>中</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Add adds a pod to the active queue. It should be called only when a new pod</span><br><span class="hljs-comment">// is added so there is no chance the pod is already in active/unschedulable/backoff queues</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *PriorityQueue)</span></span> Add(pod *v1.Pod) <span class="hljs-type">error</span> &#123;<br>p.lock.Lock()<br><span class="hljs-keyword">defer</span> p.lock.Unlock()<br><br>pInfo := p.newQueuedPodInfo(pod)<br>gated := pInfo.Gated<br><span class="hljs-keyword">if</span> added, err := p.addToActiveQ(pInfo); !added &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br><span class="hljs-keyword">if</span> p.unschedulablePods.get(pod) != <span class="hljs-literal">nil</span> &#123;<br>klog.ErrorS(<span class="hljs-literal">nil</span>, <span class="hljs-string">&quot;Error: pod is already in the unschedulable queue&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>p.unschedulablePods.<span class="hljs-built_in">delete</span>(pod, gated)<br>&#125;<br><span class="hljs-comment">// Delete pod from backoffQ if it is backing off</span><br><span class="hljs-keyword">if</span> err := p.podBackoffQ.Delete(pInfo); err == <span class="hljs-literal">nil</span> &#123;<br>klog.ErrorS(<span class="hljs-literal">nil</span>, <span class="hljs-string">&quot;Error: pod is already in the podBackoff queue&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>&#125;<br>klog.V(<span class="hljs-number">5</span>).InfoS(<span class="hljs-string">&quot;Pod moved to an internal scheduling queue&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;event&quot;</span>, PodAdd, <span class="hljs-string">&quot;queue&quot;</span>, activeQName)<br>metrics.SchedulerQueueIncomingPods.WithLabelValues(<span class="hljs-string">&quot;active&quot;</span>, PodAdd).Inc()<br>p.addNominatedPodUnlocked(pInfo.PodInfo, <span class="hljs-literal">nil</span>)<br>p.cond.Broadcast()<br><br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>主要是将要加入的<code>pod</code>转化为<code>QueuedPodInfo</code>类型，然后添加到<code>activeQ</code>队列中，还需要检查其他队列中是否有这个pod，如果有就删除，同时做一些日志相关记录，然后还会调用<code>p.cond.Broadcast()</code>来解除上述提到的<code>p.cond.Wait</code> 的等待。</p><h2 id="pod调度失败返回队列的处理"><a href="#pod调度失败返回队列的处理" class="headerlink" title="pod调度失败返回队列的处理"></a>pod调度失败返回队列的处理</h2><p>当Pod调度失败后，会调用来<code>AddUnschedulableIfNotPresent</code>函数来进行处理，其代码位置在<code>pkg/scheduler/internal/queue/scheduling_queue.go</code> 中。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// AddUnschedulableIfNotPresent inserts a pod that cannot be scheduled into</span><br><span class="hljs-comment">// the queue, unless it is already in the queue. Normally, PriorityQueue puts</span><br><span class="hljs-comment">// unschedulable pods in `unschedulablePods`. But if there has been a recent move</span><br><span class="hljs-comment">// request, then the pod is put in `podBackoffQ`.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *PriorityQueue)</span></span> AddUnschedulableIfNotPresent(pInfo *framework.QueuedPodInfo, podSchedulingCycle <span class="hljs-type">int64</span>) <span class="hljs-type">error</span> &#123;<br>p.lock.Lock()<br><span class="hljs-keyword">defer</span> p.lock.Unlock()<br>pod := pInfo.Pod<br><span class="hljs-keyword">if</span> p.unschedulablePods.get(pod) != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;Pod %v is already present in unschedulable queue&quot;</span>, klog.KObj(pod))<br>&#125;<br><br><span class="hljs-keyword">if</span> _, exists, _ := p.activeQ.Get(pInfo); exists &#123;<br><span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;Pod %v is already present in the active queue&quot;</span>, klog.KObj(pod))<br>&#125;<br><span class="hljs-keyword">if</span> _, exists, _ := p.podBackoffQ.Get(pInfo); exists &#123;<br><span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;Pod %v is already present in the backoff queue&quot;</span>, klog.KObj(pod))<br>&#125;<br><br><span class="hljs-comment">// Refresh the timestamp since the pod is re-added.</span><br>pInfo.Timestamp = p.clock.Now()<br><br><span class="hljs-comment">// If a move request has been received, move it to the BackoffQ, otherwise move</span><br><span class="hljs-comment">// it to unschedulablePods.</span><br><span class="hljs-keyword">for</span> plugin := <span class="hljs-keyword">range</span> pInfo.UnschedulablePlugins &#123;<br>metrics.UnschedulableReason(plugin, pInfo.Pod.Spec.SchedulerName).Inc()<br>&#125;<br><span class="hljs-keyword">if</span> p.moveRequestCycle &gt;= podSchedulingCycle &#123;<br><span class="hljs-keyword">if</span> err := p.podBackoffQ.Add(pInfo); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;error adding pod %v to the backoff queue: %v&quot;</span>, klog.KObj(pod), err)<br>&#125;<br>klog.V(<span class="hljs-number">5</span>).InfoS(<span class="hljs-string">&quot;Pod moved to an internal scheduling queue&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;event&quot;</span>, ScheduleAttemptFailure, <span class="hljs-string">&quot;queue&quot;</span>, backoffQName)<br>metrics.SchedulerQueueIncomingPods.WithLabelValues(<span class="hljs-string">&quot;backoff&quot;</span>, ScheduleAttemptFailure).Inc()<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>p.unschedulablePods.addOrUpdate(pInfo)<br>klog.V(<span class="hljs-number">5</span>).InfoS(<span class="hljs-string">&quot;Pod moved to an internal scheduling queue&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;event&quot;</span>, ScheduleAttemptFailure, <span class="hljs-string">&quot;queue&quot;</span>, unschedulablePods)<br>metrics.SchedulerQueueIncomingPods.WithLabelValues(<span class="hljs-string">&quot;unschedulable&quot;</span>, ScheduleAttemptFailure).Inc()<br><br>&#125;<br><br>p.addNominatedPodUnlocked(pInfo.PodInfo, <span class="hljs-literal">nil</span>)<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>这里首先检查了其他队列中是否含有该pod，如果有就返回错误，然后比较<code>moveRequestCycle</code> 和<code>podSchedulingCycle</code> ，如果<code>p.moveRequestCycle &gt;= podSchedulingCycle</code>  那就说明在刚刚调度这个pod的时候集群发生了变化，可能现在可以成功调度这个pod了，将其转入backoffQ中，不然就正常加入unschedulableQ中。</p><h2 id="flushBackoffQCompleted"><a href="#flushBackoffQCompleted" class="headerlink" title="flushBackoffQCompleted"></a>flushBackoffQCompleted</h2><p>在队列运行时会初始化两个go协程，来分别不停检查<code>backoffQ</code>和<code>unschedulableQ</code>，以及时将相关的Pod移出。代码在<code>pkg/scheduler/internal/queue/scheduling_queue.go:333</code> 中</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Run starts the goroutine to pump from podBackoffQ to activeQ</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *PriorityQueue)</span></span> Run() &#123;<br><span class="hljs-keyword">go</span> wait.Until(p.flushBackoffQCompleted, <span class="hljs-number">1.0</span>*time.Second, p.stop)<br><span class="hljs-keyword">go</span> wait.Until(p.flushUnschedulablePodsLeftover, <span class="hljs-number">30</span>*time.Second, p.stop)<br>&#125;<br></code></pre></td></tr></table></figure><p>对于<code>flushBackoffQCompleted</code>即是每1s运行一次，直到接收到<code>p.stop</code>信息。对<code>flushBackoffQCompleted</code> 函数的具体定义在<code>pkg/scheduler/internal/queue/scheduling_queue.go:537</code>中，如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// flushBackoffQCompleted Moves all pods from backoffQ which have completed backoff in to activeQ</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *PriorityQueue)</span></span> flushBackoffQCompleted() &#123;<br>p.lock.Lock()<br><span class="hljs-keyword">defer</span> p.lock.Unlock()<br>activated := <span class="hljs-literal">false</span><br><span class="hljs-keyword">for</span> &#123;<br>rawPodInfo := p.podBackoffQ.Peek()<br><span class="hljs-keyword">if</span> rawPodInfo == <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">break</span><br>&#125;<br>pInfo := rawPodInfo.(*framework.QueuedPodInfo)<br>pod := pInfo.Pod<br><span class="hljs-keyword">if</span> p.isPodBackingoff(pInfo) &#123;<br><span class="hljs-keyword">break</span><br>&#125;<br>_, err := p.podBackoffQ.Pop()<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>klog.ErrorS(err, <span class="hljs-string">&quot;Unable to pop pod from backoff queue despite backoff completion&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br><span class="hljs-keyword">break</span><br>&#125;<br><span class="hljs-keyword">if</span> err := p.activeQ.Add(pInfo); err != <span class="hljs-literal">nil</span> &#123;<br>klog.ErrorS(err, <span class="hljs-string">&quot;Error adding pod to the active queue&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pInfo.Pod))<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>klog.V(<span class="hljs-number">5</span>).InfoS(<span class="hljs-string">&quot;Pod moved to an internal scheduling queue&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod), <span class="hljs-string">&quot;event&quot;</span>, BackoffComplete, <span class="hljs-string">&quot;queue&quot;</span>, activeQName)<br>metrics.SchedulerQueueIncomingPods.WithLabelValues(<span class="hljs-string">&quot;active&quot;</span>, BackoffComplete).Inc()<br>activated = <span class="hljs-literal">true</span><br>&#125;<br>&#125;<br><br><span class="hljs-keyword">if</span> activated &#123;<br>p.cond.Broadcast()<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>其主要内容就是从<code>backOffQ</code>的首个元素开始查看，检查器是否已经过了避让时间，如果过了就将其放入到<code>activeQ</code>队列中，直到首个元素没有达到避让时间或者队列为空。</p><h2 id="flushUnschedulablePodsLeftover"><a href="#flushUnschedulablePodsLeftover" class="headerlink" title="flushUnschedulablePodsLeftover"></a>flushUnschedulablePodsLeftover</h2><p><code>flushUnschedulablePodsLeftover</code>每30s运行一次，这部分的代码在<code>pkg/scheduler/internal/queue/scheduling_queue.go:572</code>中，如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// flushUnschedulablePodsLeftover moves pods which stay in unschedulablePods</span><br><span class="hljs-comment">// longer than podMaxInUnschedulablePodsDuration to backoffQ or activeQ.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *PriorityQueue)</span></span> flushUnschedulablePodsLeftover() &#123;<br>p.lock.Lock()<br><span class="hljs-keyword">defer</span> p.lock.Unlock()<br><br><span class="hljs-keyword">var</span> podsToMove []*framework.QueuedPodInfo<br>currentTime := p.clock.Now()<br><span class="hljs-keyword">for</span> _, pInfo := <span class="hljs-keyword">range</span> p.unschedulablePods.podInfoMap &#123;<br>lastScheduleTime := pInfo.Timestamp<br><span class="hljs-keyword">if</span> currentTime.Sub(lastScheduleTime) &gt; p.podMaxInUnschedulablePodsDuration &#123;<br>podsToMove = <span class="hljs-built_in">append</span>(podsToMove, pInfo)<br>&#125;<br>&#125;<br><br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(podsToMove) &gt; <span class="hljs-number">0</span> &#123;<br>p.movePodsToActiveOrBackoffQueue(podsToMove, UnschedulableTimeout)<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到其主要作用是遍历所有的pod，如果其在unschedulableQ中呆的时间如果超过了最大的p.podMaxInUnschedulablePodsDuration时间，就会将其移出去，至于是移动到activeQ中还是移动到backoffQ中，取决于movePodsToActiveOrBackoffQueue函数，在<code>pkg/scheduler/internal/queue/scheduling_queue.go:771</code>中，如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// <span class="hljs-doctag">NOTE:</span> this function assumes lock has been acquired in caller</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(p *PriorityQueue)</span></span> movePodsToActiveOrBackoffQueue(podInfoList []*framework.QueuedPodInfo, event framework.ClusterEvent) &#123;<br>activated := <span class="hljs-literal">false</span><br><span class="hljs-keyword">for</span> _, pInfo := <span class="hljs-keyword">range</span> podInfoList &#123;<br><span class="hljs-comment">// If the event doesn&#x27;t help making the Pod schedulable, continue.</span><br><span class="hljs-comment">// Note: we don&#x27;t run the check if pInfo.UnschedulablePlugins is nil, which denotes</span><br><span class="hljs-comment">// either there is some abnormal error, or scheduling the pod failed by plugins other than PreFilter, Filter and Permit.</span><br><span class="hljs-comment">// In that case, it&#x27;s desired to move it anyways.</span><br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(pInfo.UnschedulablePlugins) != <span class="hljs-number">0</span> &amp;&amp; !p.podMatchesEvent(pInfo, event) &#123;<br><span class="hljs-keyword">continue</span><br>&#125;<br>pod := pInfo.Pod<br><span class="hljs-keyword">if</span> p.isPodBackingoff(pInfo) &#123;<br><span class="hljs-keyword">if</span> err := p.podBackoffQ.Add(pInfo); err != <span class="hljs-literal">nil</span> &#123;<br>klog.ErrorS(err, <span class="hljs-string">&quot;Error adding pod to the backoff queue&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pod))<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>klog.V(<span class="hljs-number">5</span>).InfoS(<span class="hljs-string">&quot;Pod moved to an internal scheduling queue&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pInfo.Pod), <span class="hljs-string">&quot;event&quot;</span>, event, <span class="hljs-string">&quot;queue&quot;</span>, backoffQName)<br>metrics.SchedulerQueueIncomingPods.WithLabelValues(<span class="hljs-string">&quot;backoff&quot;</span>, event.Label).Inc()<br>p.unschedulablePods.<span class="hljs-built_in">delete</span>(pod, pInfo.Gated)<br>&#125;<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>gated := pInfo.Gated<br><span class="hljs-keyword">if</span> added, _ := p.addToActiveQ(pInfo); added &#123;<br>klog.V(<span class="hljs-number">5</span>).InfoS(<span class="hljs-string">&quot;Pod moved to an internal scheduling queue&quot;</span>, <span class="hljs-string">&quot;pod&quot;</span>, klog.KObj(pInfo.Pod), <span class="hljs-string">&quot;event&quot;</span>, event, <span class="hljs-string">&quot;queue&quot;</span>, activeQName)<br>activated = <span class="hljs-literal">true</span><br>metrics.SchedulerQueueIncomingPods.WithLabelValues(<span class="hljs-string">&quot;active&quot;</span>, event.Label).Inc()<br>p.unschedulablePods.<span class="hljs-built_in">delete</span>(pod, gated)<br>&#125;<br>&#125;<br>&#125;<br>p.moveRequestCycle = p.schedulingCycle<br><span class="hljs-keyword">if</span> activated &#123;<br>p.cond.Broadcast()<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>注意这个函数不仅仅是在<code>flushUnschedulablePodsLeftover</code>中被调用，还会在处理其他移动请求时触发，只不过这里的移动请求是<code>UnschedulableTimeout</code> ，判断到底是如何移动也很容易从代码中看出，如果已经到达了避让时间，就加入到<code>activeQ</code>中，如果没有就加入到<code>backoffQ</code>中，注意到如果有移动进<code>activeQ</code>中，也是需要执行<code>p.cond.Broadcast()</code>，同时注意到这里更新了<code>moveRequestCycle</code>为<code>schedulingCycle</code>，这也是其统一更新<code>moveRequestCycle</code> 的地方。</p><h1 id="调度队列总结"><a href="#调度队列总结" class="headerlink" title="调度队列总结"></a>调度队列总结</h1><p>考虑到调度队列的细节，我们可以用下图来对其进行归纳回顾。</p><p><img src="/2024/05/10/k8sSource2/summary.png" alt="调度队列总结"></p>]]></content>
    
    
    <categories>
      
      <category>k8s</category>
      
      <category>源码分析</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>源码分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【K8s源码分析（一）】-K8s调度框架及调度器初始化介绍</title>
    <link href="/2024/05/10/k8sSource1/"/>
    <url>/2024/05/10/k8sSource1/</url>
    
    <content type="html"><![CDATA[<p>本次分析参考的K8s版本是<a href="https://github.com/kubernetes/kubernetes/tree/release-1.27">v1.27.0</a>。</p><h1 id="调度框架介绍"><a href="#调度框架介绍" class="headerlink" title="调度框架介绍"></a>调度框架介绍</h1><p>这是官方对于<strong>v1.27</strong>调度框架的介绍文档：<a href="https://v1-27.docs.kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/">https://v1-27.docs.kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/</a></p><p>将调度器的实现转化为插件的形式有助于加强调度器的拓展性、灵活性，同时也使得调度核心的实现更加的轻量、可维护。</p><p>下图展示了Pod的调度上下文以及调度框架暴露的扩展点。图中“Filter”相当于“Predicate”，“Scoring”相当于“Priority function”。</p><p><img src="/2024/05/10/k8sSource1/scheduling-framework-extensions.png" alt="Scheduling framework extension points"></p><p>总体而言，首先新创建的Pod或还没有调度的Pod会存在队列中，然后经过调度周期的筛选得到符合条件的Node，然后在调度周期内再对各个符合条件的Node进行打分，最高分的Node就是需要调度到的Node，然后经过绑定周期将Pod放置到Node上。</p><p>各个拓展点的具体介绍建议参考上面提到的官方介绍文档，这里不再赘述。</p><h1 id="K8s-scheduler-介绍"><a href="#K8s-scheduler-介绍" class="headerlink" title="K8s scheduler 介绍"></a>K8s scheduler 介绍</h1><p>首先需要明确的一个点，K8s中的scheduler是以pod的形式运行在系统中的，通过如下的命令能找到其对应的pod。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># kubectl get pod -n kube-system</span><br>NAME                                       READY   STATUS             RESTARTS         AGE<br>...<br>kube-scheduler-master                      1/1     Running            0                2d4h<br>...<br></code></pre></td></tr></table></figure><p>Pod中的容器会存在一个scheduler程序并一直在前台运行，接收要调度的pod并给出调度结果。本文主要分析的也就是这个scheduler程序所对应的源代码。</p><p>这是官方对K8s scheduler代码层次结构的介绍文档：<a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md">Scheduler code hierarchy overview</a>。也很推荐观看！</p><p>整体的关键代码的结构如下所示：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">.<br>├── cmd<br>│   └── kube-<span class="hljs-keyword">scheduler</span><br><span class="hljs-keyword"></span>│       └── app - 控制器代码位置以及命令行接口参数定义（遵循所有Kubernetes控制器的标准设置）<br>├── pkg<br>│   └── <span class="hljs-keyword">scheduler </span>- 默认调度器代码库的根目录<br>│       ├── core - 默认调度算法的位置<br>│       ├── framework - 调度框架及其插件<br>│       └── internal - 缓存、队列和其他内部元素的实现<br>├── staging<br>│   └── src<br>│       └── k8s.io<br>│           └── kube-<span class="hljs-keyword">scheduler </span>- ComponentConfig API类型的所在位置<br>└── test<br>    ├── e2e<br>    │   └── <span class="hljs-keyword">scheduling </span>- 端到端调度测试<br>    │<br>    ├── integration<br>        ├── <span class="hljs-keyword">scheduler </span>- 调度器集成测试<br>        └── <span class="hljs-keyword">scheduler_perf </span>- 调度性能基准测试<br></code></pre></td></tr></table></figure><h1 id="K8s-scheduler的初始化"><a href="#K8s-scheduler的初始化" class="headerlink" title="K8s scheduler的初始化"></a>K8s scheduler的初始化</h1><h2 id="Cobra介绍"><a href="#Cobra介绍" class="headerlink" title="Cobra介绍"></a>Cobra介绍</h2><p>K8s中大部分组件其实都采用的是<a href="https://github.com/spf13/cobra">Cobra</a>结构。Cobra是一个用于创建现代命令行应用程序的库，云原生中很多项目都采用了它，包括<a href="https://kubernetes.io/">Kubernetes</a>、<a href="https://gohugo.io/">Hugo、</a><a href="https://github.com/cli/cli">GitHub CLI</a>等，目前都有36.2k个start了。而K8s中的scheduler实际上也是通过Cobra构建的。</p><p>Cobra的具体介绍可以参见<a href="https://xie.infoq.cn/article/915006cf3760c99ad0028d895">万字长文——Go 语言现代命令行框架 Cobra 详解</a>。</p><p>这边以一个小demo为例进行简单介绍。一个demo项目定义了一个名为hugo的命令行工具，代码如下所示：</p><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs maxima">.<br>├── cmd<br>│   ├── root.<span class="hljs-built_in">go</span><br>│   └── version.<span class="hljs-built_in">go</span><br>├── <span class="hljs-built_in">go</span>.<span class="hljs-built_in">mod</span><br>├── <span class="hljs-built_in">go</span>.<span class="hljs-built_in">sum</span><br>└── main.<span class="hljs-built_in">go</span><br></code></pre></td></tr></table></figure><p><code>main.go</code>的内容如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<br><br><span class="hljs-keyword">import</span> (<br><span class="hljs-string">&quot;hugo/cmd&quot;</span><br>)<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<br>cmd.Execute()<br>&#125;<br></code></pre></td></tr></table></figure><p><code>root.go</code>的内容如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">package</span> cmd<br><br><span class="hljs-keyword">import</span> (<br><span class="hljs-string">&quot;fmt&quot;</span><br><span class="hljs-string">&quot;os&quot;</span><br><br><span class="hljs-string">&quot;github.com/spf13/cobra&quot;</span><br>)<br><br><span class="hljs-keyword">var</span> rootCmd = &amp;cobra.Command&#123;<br>Use:   <span class="hljs-string">&quot;hugo&quot;</span>,<br>Short: <span class="hljs-string">&quot;Hugo is a very fast static site generator&quot;</span>,<br>Long: <span class="hljs-string">`A Fast and Flexible Static Site Generator built with</span><br><span class="hljs-string">  love by spf13 and friends in Go.</span><br><span class="hljs-string">  Complete documentation is available at https://gohugo.io`</span>,<br>RunE: <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(cmd *cobra.Command, args []<span class="hljs-type">string</span>)</span></span> <span class="hljs-type">error</span> &#123;<br>fmt.Println(<span class="hljs-string">&quot;run hugo...&quot;</span>)<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;,<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">Execute</span><span class="hljs-params">()</span></span> &#123;<br><span class="hljs-keyword">if</span> err := rootCmd.Execute(); err != <span class="hljs-literal">nil</span> &#123;<br>fmt.Println(err)<br>os.Exit(<span class="hljs-number">1</span>)<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p><code>version.go</code>的内容如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">package</span> cmd<br><br><span class="hljs-keyword">import</span> (<br><span class="hljs-string">&quot;fmt&quot;</span><br><br><span class="hljs-string">&quot;github.com/spf13/cobra&quot;</span><br>)<br><br><span class="hljs-keyword">var</span> versionCmd = &amp;cobra.Command&#123;<br>Use:   <span class="hljs-string">&quot;version&quot;</span>,<br>Short: <span class="hljs-string">&quot;Print the version number of Hugo&quot;</span>,<br>Long:  <span class="hljs-string">`All software has versions. This is Hugo&#x27;s`</span>,<br>RunE: <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(cmd *cobra.Command, args []<span class="hljs-type">string</span>)</span></span> <span class="hljs-type">error</span> &#123;<br>fmt.Println(<span class="hljs-string">&quot;Hugo Static Site Generator v0.9 -- HEAD&quot;</span>)<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;,<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">init</span><span class="hljs-params">()</span></span> &#123;<br>rootCmd.AddCommand(versionCmd)<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到<code>main.go</code>的主要内容就是调用<code>root.go</code>中的<code>Execute()</code>函数，然后这个函数又是调用cobra定义的<code>rootCmd</code>对其进行执行。<code>rootCmd</code>是一个<code>cobra.Command</code>类，它定义时写了自己的说明文本，然后Run函数是最关键的，定义了自己的运行内容，也就是打印一句字符，这就是单独在命令行中输入hugo后需要执行的程序。如果想要进行命令嵌套，那么就得像<code>version.go</code>文件中的处理方法一样再定义另一个cobra的cmd变量<code>versionCmd</code> ，然后通过<code>AddCommand</code>函数就可以加入进去，如此之后就可以通过<code>hugo version</code>来运行<code>versionCmd</code> 中的<code>Run</code>对应的函数。</p><p>项目build之后得到执行文件<code>hugo</code>，运行结果如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># ./hugo </span><br>run hugo..<br><span class="hljs-comment"># ./hugo -h</span><br>A Fast and Flexible Static Site Generator built with<br>                                  love by spf13 and friends <span class="hljs-keyword">in</span> Go.<br>                                  Complete documentation is available at https://gohugo.io<br><br>Usage:<br>  hugo [flags]<br>  hugo [<span class="hljs-built_in">command</span>]<br><br>Available Commands:<br>  completion  Generate the autocompletion script <span class="hljs-keyword">for</span> the specified shell<br>  <span class="hljs-built_in">help</span>        Help about any <span class="hljs-built_in">command</span><br>  version     Print the version number of Hugo<br><br>Flags:<br>  -h, --<span class="hljs-built_in">help</span>   <span class="hljs-built_in">help</span> <span class="hljs-keyword">for</span> hugo<br><br>Use <span class="hljs-string">&quot;hugo [command] --help&quot;</span> <span class="hljs-keyword">for</span> more information about a <span class="hljs-built_in">command</span>.<br><span class="hljs-comment"># ./hugo version</span><br>Hugo Static Site Generator v0.9 -- HEAD<br></code></pre></td></tr></table></figure><h2 id="K8s-scheduler中初始化的源代码解析"><a href="#K8s-scheduler中初始化的源代码解析" class="headerlink" title="K8s scheduler中初始化的源代码解析"></a>K8s scheduler中初始化的源代码解析</h2><p>K8s的scheduler也是类似于上面的hugo程序，只不过更加复杂。</p><p>首先在<code>cmd/kube-scheduler/scheduler.go:29</code>中我们能看见scheduler的入口函数：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<br>command := app.NewSchedulerCommand()<br>code := cli.Run(command)<br>os.Exit(code)<br>&#125;<br></code></pre></td></tr></table></figure><p>这里也是通过<code>app.NewSchedulerCommand</code>得到了一个<code>cobra.Command</code> 类，然后让这个类运行起来。</p><p>具体看<code>cmd/kube-scheduler/app/server.go:76</code> </p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// NewSchedulerCommand creates a *cobra.Command object with default parameters and registryOptions</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">NewSchedulerCommand</span><span class="hljs-params">(registryOptions ...Option)</span></span> *cobra.Command &#123;<br>opts := options.NewOptions()<br><br>cmd := &amp;cobra.Command&#123;<br>Use: <span class="hljs-string">&quot;kube-scheduler&quot;</span>,<br>Long: <span class="hljs-string">`The Kubernetes scheduler is a control plane process which assigns</span><br><span class="hljs-string">Pods to Nodes. The scheduler determines which Nodes are valid placements for</span><br><span class="hljs-string">each Pod in the scheduling queue according to constraints and available</span><br><span class="hljs-string">resources. The scheduler then ranks each valid Node and binds the Pod to a</span><br><span class="hljs-string">suitable Node. Multiple different schedulers may be used within a cluster;</span><br><span class="hljs-string">kube-scheduler is the reference implementation.</span><br><span class="hljs-string">See [scheduling](https://kubernetes.io/docs/concepts/scheduling-eviction/)</span><br><span class="hljs-string">for more information about scheduling and the kube-scheduler component.`</span>,<br>RunE: <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(cmd *cobra.Command, args []<span class="hljs-type">string</span>)</span></span> <span class="hljs-type">error</span> &#123;<br><span class="hljs-keyword">return</span> runCommand(cmd, opts, registryOptions...)<br>&#125;,<br>Args: <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(cmd *cobra.Command, args []<span class="hljs-type">string</span>)</span></span> <span class="hljs-type">error</span> &#123;<br><span class="hljs-keyword">for</span> _, arg := <span class="hljs-keyword">range</span> args &#123;<br><span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(arg) &gt; <span class="hljs-number">0</span> &#123;<br><span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;%q does not take any arguments, got %q&quot;</span>, cmd.CommandPath(), args)<br>&#125;<br>&#125;<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;,<br>&#125;<br><br><span class="hljs-comment">//...</span><br><br><span class="hljs-keyword">return</span> cmd<br>&#125;<br></code></pre></td></tr></table></figure><p>这里定义了一个<code>cobra.Command</code>，与之前的示例类似，主要的内容还是在<code>runCommand</code>中。</p><p>查看其对应的内容，<code>cmd/kube-scheduler/app/server.go:121</code> </p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// runCommand runs the scheduler.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">runCommand</span><span class="hljs-params">(cmd *cobra.Command, opts *options.Options, registryOptions ...Option)</span></span> <span class="hljs-type">error</span> &#123;<br>verflag.PrintAndExitIfRequested()<br><br><span class="hljs-comment">// Activate logging as soon as possible, after that</span><br><span class="hljs-comment">// show flags with the final logging configuration.</span><br><span class="hljs-keyword">if</span> err := logsapi.ValidateAndApply(opts.Logs, utilfeature.DefaultFeatureGate); err != <span class="hljs-literal">nil</span> &#123;<br>fmt.Fprintf(os.Stderr, <span class="hljs-string">&quot;%v\n&quot;</span>, err)<br>os.Exit(<span class="hljs-number">1</span>)<br>&#125;<br>cliflag.PrintFlags(cmd.Flags())<br><br>ctx, cancel := context.WithCancel(context.Background())<br><span class="hljs-keyword">defer</span> cancel()<br><span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>stopCh := server.SetupSignalHandler()<br>&lt;-stopCh<br>cancel()<br>&#125;()<br><br>cc, sched, err := Setup(ctx, opts, registryOptions...)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> err<br>&#125;<br><span class="hljs-comment">// add feature enablement metrics</span><br>utilfeature.DefaultMutableFeatureGate.AddMetrics()<br><span class="hljs-keyword">return</span> Run(ctx, cc, sched)<br>&#125;<br></code></pre></td></tr></table></figure><p>前面的内容主要是一些配置文件，其中最主要的初始化配置函数是<code>Setup(ctx, opts, registryOptions...)</code> ，初始化完毕后就会返回一个scheduler。</p><p>具体的内容在<code>cmd/kube-scheduler/app/server.go:309</code> ，对这部分代码的一些解释放在了注释里。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Setup creates a completed config and a scheduler based on the command args and options</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">Setup</span><span class="hljs-params">(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option)</span></span> (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, <span class="hljs-type">error</span>) &#123;<br>    <span class="hljs-comment">// 尝试获取默认的调度器配置</span><br>    <span class="hljs-keyword">if</span> cfg, err := latest.Default(); err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, <span class="hljs-literal">nil</span>, err<br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        opts.ComponentConfig = cfg <span class="hljs-comment">// 如果没有错误，将配置赋值给opts</span><br>    &#125;<br><br>    <span class="hljs-comment">// 验证opts中的选项是否有效</span><br>    <span class="hljs-keyword">if</span> errs := opts.Validate(); <span class="hljs-built_in">len</span>(errs) &gt; <span class="hljs-number">0</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, <span class="hljs-literal">nil</span>, utilerrors.NewAggregate(errs) <span class="hljs-comment">// 如果有验证错误，返回它们</span><br>    &#125;<br><br>    <span class="hljs-comment">// 从opts创建一个调度器的配置对象</span><br>    c, err := opts.Config(ctx)<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, <span class="hljs-literal">nil</span>, err<br>    &#125;<br><br>    <span class="hljs-comment">// 从调度器配置对象中获取完整的配置</span><br>    cc := c.Complete()<br><br>    <span class="hljs-comment">// 创建一个用于存放外部插件的注册表</span><br>    outOfTreeRegistry := <span class="hljs-built_in">make</span>(runtime.Registry)<br>    <span class="hljs-keyword">for</span> _, option := <span class="hljs-keyword">range</span> outOfTreeRegistryOptions &#123;<br>        <span class="hljs-keyword">if</span> err := option(outOfTreeRegistry); err != <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, <span class="hljs-literal">nil</span>, err<br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 获取事件记录器工厂</span><br>    recorderFactory := getRecorderFactory(&amp;cc)<br><br>    <span class="hljs-comment">// 创建一个空的调度器配置概要切片</span><br>    completedProfiles := <span class="hljs-built_in">make</span>([]kubeschedulerconfig.KubeSchedulerProfile, <span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment">// 使用一系列参数和配置选项创建一个新的调度器实例</span><br>    sched, err := scheduler.New(<br>        cc.Client,                                 <span class="hljs-comment">// 客户端对象</span><br>        cc.InformerFactory,                       <span class="hljs-comment">// Informer工厂</span><br>        cc.DynInformerFactory,                    <span class="hljs-comment">// 动态Informer工厂</span><br>        recorderFactory,                          <span class="hljs-comment">// 事件记录器工厂</span><br>        ctx.Done(),                               <span class="hljs-comment">// 上下文取消通道</span><br>        scheduler.WithComponentConfigVersion(cc.ComponentConfig.TypeMeta.APIVersion),  <span class="hljs-comment">// 组件配置版本</span><br>        scheduler.WithKubeConfig(cc.KubeConfig),                                      <span class="hljs-comment">// Kube配置</span><br>        scheduler.WithProfiles(cc.ComponentConfig.Profiles...),                       <span class="hljs-comment">// 调度器配置概要</span><br>        scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore), <span class="hljs-comment">// 节点评分百分比</span><br>        scheduler.WithFrameworkOutOfTreeRegistry(outOfTreeRegistry),                <span class="hljs-comment">// 外部插件注册表</span><br>        scheduler.WithPodMaxBackoffSeconds(cc.ComponentConfig.PodMaxBackoffSeconds),  <span class="hljs-comment">// Pod最大退避秒数</span><br>        scheduler.WithPodInitialBackoffSeconds(cc.ComponentConfig.PodInitialBackoffSeconds),  <span class="hljs-comment">// Pod初始退避秒数</span><br>        scheduler.WithPodMaxInUnschedulablePodsDuration(cc.PodMaxInUnschedulablePodsDuration), <span class="hljs-comment">// Pod在不可调度Pod列表中的最大持续时间</span><br>        scheduler.WithExtenders(cc.ComponentConfig.Extenders...),  <span class="hljs-comment">// 扩展器</span><br>        scheduler.WithParallelism(cc.ComponentConfig.Parallelism),  <span class="hljs-comment">// 并行度</span><br>        scheduler.WithBuildFrameworkCapturer(<span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(profile kubeschedulerconfig.KubeSchedulerProfile)</span></span> &#123;<br>            <span class="hljs-comment">// 在框架实例化期间处理概要以设置默认插件和配置，并捕获它们以记录日志</span><br>            completedProfiles = <span class="hljs-built_in">append</span>(completedProfiles, profile)<br>        &#125;),<br>    )<br>    <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, <span class="hljs-literal">nil</span>, err<br>    &#125;<br><br>    <span class="hljs-comment">// 记录或写入配置和概要信息</span><br>    <span class="hljs-keyword">if</span> err := options.LogOrWriteConfig(klog.FromContext(ctx), opts.WriteConfigTo, &amp;cc.ComponentConfig, completedProfiles); err != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span>, <span class="hljs-literal">nil</span>, err<br>    &#125;<br><br>    <span class="hljs-comment">// 返回完整的配置和调度器实例</span><br>    <span class="hljs-keyword">return</span> &amp;cc, sched, <span class="hljs-literal">nil</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>得到scheduler后运行的函数还是在后面的<code>Run(ctx, cc, sched)</code>里。</p><p>查看其对应的内容，<code>cmd/kube-scheduler/app/server.go:150</code> ，补充了一部分解释放在代码的注释里。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Run executes the scheduler based on the given configuration. It only returns on error or when context is done.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">Run</span><span class="hljs-params">(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *scheduler.Scheduler)</span></span> <span class="hljs-type">error</span> &#123;<br>    logger := klog.FromContext(ctx) <span class="hljs-comment">// 从上下文中获取日志记录器</span><br><br>    <span class="hljs-comment">// 为了帮助调试，立即记录版本信息</span><br>    logger.Info(<span class="hljs-string">&quot;Starting Kubernetes Scheduler&quot;</span>, <span class="hljs-string">&quot;version&quot;</span>, version.Get())<br><br>    <span class="hljs-comment">// 记录 Golang 的设置，这些环境变量会影响 Go 运行时的行为</span><br>    logger.Info(<span class="hljs-string">&quot;Golang settings&quot;</span>, <span class="hljs-string">&quot;GOGC&quot;</span>, os.Getenv(<span class="hljs-string">&quot;GOGC&quot;</span>), <span class="hljs-string">&quot;GOMAXPROCS&quot;</span>, os.Getenv(<span class="hljs-string">&quot;GOMAXPROCS&quot;</span>), <span class="hljs-string">&quot;GOTRACEBACK&quot;</span>, os.Getenv(<span class="hljs-string">&quot;GOTRACEBACK&quot;</span>))<br><br>    <span class="hljs-comment">// Configz 注册，Configz 允许通过 HTTP 端点公开当前的配置</span><br>    <span class="hljs-keyword">if</span> cz, err := configz.New(<span class="hljs-string">&quot;componentconfig&quot;</span>); err == <span class="hljs-literal">nil</span> &#123;<br>        cz.Set(cc.ComponentConfig) <span class="hljs-comment">// 设置调度器的组件配置</span><br>    &#125; <span class="hljs-keyword">else</span> &#123;<br>        <span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;unable to register configz: %s&quot;</span>, err) <span class="hljs-comment">// 如果注册失败，返回错误</span><br>    &#125;<br><br>    <span class="hljs-comment">// 启动事件处理流水线</span><br>    cc.EventBroadcaster.StartRecordingToSink(ctx.Done()) <span class="hljs-comment">// 开始录制事件</span><br>    <span class="hljs-keyword">defer</span> cc.EventBroadcaster.Shutdown()                   <span class="hljs-comment">// 延后关闭事件广播</span><br><br>    <span class="hljs-comment">// 设置健康检查</span><br>    <span class="hljs-keyword">var</span> checks []healthz.HealthChecker<br>    <span class="hljs-keyword">if</span> cc.ComponentConfig.LeaderElection.LeaderElect &#123;<br>        checks = <span class="hljs-built_in">append</span>(checks, cc.LeaderElection.WatchDog) <span class="hljs-comment">// 如果启用了领导者选举，添加 WatchDog 健康检查</span><br>    &#125;<br><br>    <span class="hljs-comment">// 等待领导者选举的通道</span><br>    waitingForLeader := <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;)<br>    isLeader := <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> <span class="hljs-type">bool</span> &#123;<br>        <span class="hljs-keyword">select</span> &#123;<br>        <span class="hljs-keyword">case</span> _, ok := &lt;-waitingForLeader:<br>            <span class="hljs-comment">// 如果通道关闭，我们是领导者</span><br>            <span class="hljs-keyword">return</span> !ok<br>        <span class="hljs-keyword">default</span>:<br>            <span class="hljs-comment">// 通道是打开的，我们正在等待领导者</span><br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span><br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 启动健康检查服务器</span><br>    <span class="hljs-keyword">if</span> cc.SecureServing != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// 构建处理函数链</span><br>        handler := buildHandlerChain(newHealthzAndMetricsHandler(&amp;cc.ComponentConfig, cc.InformerFactory, isLeader, checks...), cc.Authentication.Authenticator, cc.Authorization.Authorizer)<br>        <span class="hljs-comment">// 启动安全服务器，注意处理返回的 stoppedCh 和 listenerStoppedCh</span><br>        <span class="hljs-keyword">if</span> _, _, err := cc.SecureServing.Serve(handler, <span class="hljs-number">0</span>, ctx.Done()); err != <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;failed to start secure server: %v&quot;</span>, err) <span class="hljs-comment">// 如果启动失败，返回错误</span><br>        &#125;<br>    &#125;<br><br>    <span class="hljs-comment">// 启动所有的 informer</span><br>    cc.InformerFactory.Start(ctx.Done()) <span class="hljs-comment">// 启动 informer 工厂</span><br>    <span class="hljs-comment">// DynInformerFactory 可以在测试中为 nil</span><br>    <span class="hljs-keyword">if</span> cc.DynInformerFactory != <span class="hljs-literal">nil</span> &#123;<br>        cc.DynInformerFactory.Start(ctx.Done()) <span class="hljs-comment">// 启动动态 informer 工厂</span><br>    &#125;<br><br>    <span class="hljs-comment">// 等待所有缓存同步后再进行调度</span><br>    cc.InformerFactory.WaitForCacheSync(ctx.Done()) <span class="hljs-comment">// 等待 informer 工厂的缓存同步</span><br>    <span class="hljs-keyword">if</span> cc.DynInformerFactory != <span class="hljs-literal">nil</span> &#123;<br>        cc.DynInformerFactory.WaitForCacheSync(ctx.Done()) <span class="hljs-comment">// 等待动态 informer 工厂的缓存同步</span><br>    &#125;<br><br>    <span class="hljs-comment">// 如果启用了领导者选举，通过 LeaderElector 运行直到完成并退出</span><br>    <span class="hljs-keyword">if</span> cc.LeaderElection != <span class="hljs-literal">nil</span> &#123;<br>        <span class="hljs-comment">// 设置领导者选举的回调</span><br>        cc.LeaderElection.Callbacks = leaderelection.LeaderCallbacks&#123;<br>            OnStartedLeading: <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(ctx context.Context)</span></span> &#123;<br>                <span class="hljs-built_in">close</span>(waitingForLeader) <span class="hljs-comment">// 关闭等待领导者的通道，表示我们现在是领导者</span><br>                sched.Run(ctx)           <span class="hljs-comment">// 运行调度器</span><br>            &#125;,<br>            OnStoppedLeading: <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>                <span class="hljs-keyword">select</span> &#123;<br>                <span class="hljs-keyword">case</span> &lt;-ctx.Done():<br>                    <span class="hljs-comment">// 我们被请求终止。退出 0。</span><br>                    logger.Info(<span class="hljs-string">&quot;Requested to terminate, exiting&quot;</span>)<br>                    os.Exit(<span class="hljs-number">0</span>)<br>                <span class="hljs-keyword">default</span>:<br>                    <span class="hljs-comment">// 我们失去了锁。</span><br>                    logger.Error(<span class="hljs-literal">nil</span>, <span class="hljs-string">&quot;Leaderelection lost&quot;</span>)<br>                    klog.FlushAndExit(klog.ExitFlushTimeout, <span class="hljs-number">1</span>)<br>                &#125;<br>            &#125;,<br>        &#125;<br>        <span class="hljs-comment">// 创建新的领导者选举</span><br>        leaderElector, err := leaderelection.NewLeaderElector(*cc.LeaderElection)<br>        <span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>            <span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;couldn&#x27;t create leader elector: %v&quot;</span>, err) <span class="hljs-comment">// 如果创建失败，返回错误</span><br>        &#125;<br><br>        leaderElector.Run(ctx) <span class="hljs-comment">// 运行领导者选举</span><br><br>        <span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;lost lease&quot;</span>) <span class="hljs-comment">// 如果失去租约，返回错误</span><br>    &#125;<br><br>    <span class="hljs-comment">// 领导者选举被禁用，因此内联运行直到完成</span><br>    <span class="hljs-built_in">close</span>(waitingForLeader) <span class="hljs-comment">// 关闭等待领导者的通道</span><br>    sched.Run(ctx)           <span class="hljs-comment">// 运行调度器</span><br>    <span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;finished without leader&quot;</span>) <span class="hljs-comment">// 如果没有领导者，返回错误</span><br>&#125;<br></code></pre></td></tr></table></figure><p>这个函数首先设置日志记录器，记录版本和 Golang 环境设置，然后注册配置以供调试使用。接着，它启动事件处理流水线，并设置健康检查和健康检查服务器。之后，函数启动 informer 并等待缓存同步。如果配置了领导者选举，它会通过领导者选举器运行调度器，否则直接运行调度器。如果在任何步骤中出现错误，函数会返回该错误。</p><p>查看<code>sched.Run(ctx)</code> 这部分调度器实际运行的内容，<code>pkg/scheduler/scheduler.go:355</code> ，补充了一部分解释放在代码的注释里。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// Run begins watching and scheduling. It starts scheduling and blocked until the context is done.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(sched *Scheduler)</span></span> Run(ctx context.Context) &#123;<br>    <span class="hljs-comment">// 启动调度队列，这将允许调度器观察新的、需要调度的 Pods</span><br>    sched.SchedulingQueue.Run()<br><br>  <span class="hljs-comment">// We need to start scheduleOne loop in a dedicated goroutine,</span><br>  <span class="hljs-comment">// because scheduleOne function hangs on getting the next item</span><br>  <span class="hljs-comment">// from the SchedulingQueue.</span><br>  <span class="hljs-comment">// If there are no new pods to schedule, it will be hanging there</span><br>  <span class="hljs-comment">// and if done in this goroutine it will be blocking closing</span><br>  <span class="hljs-comment">// SchedulingQueue, in effect causing a deadlock on shutdown.</span><br>    <span class="hljs-comment">// 翻译：</span><br>    <span class="hljs-comment">// 我们需要在一个独立的 goroutine 中启动 scheduleOne 循环，</span><br>    <span class="hljs-comment">// 因为 scheduleOne 函数在从 SchedulingQueue 获取下一个项目时会挂起。</span><br>    <span class="hljs-comment">// 如果没有新的 Pods 需要调度，它会在那里挂起，</span><br>    <span class="hljs-comment">// 如果在这个 goroutine 中执行，它将阻止关闭 SchedulingQueue，</span><br>    <span class="hljs-comment">// 从而在关闭时造成死锁。</span><br>    <span class="hljs-keyword">go</span> wait.UntilWithContext(ctx, sched.scheduleOne, <span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment">// 当上下文完成（即 ctx.Done() 通道关闭）时，阻塞直到收到信号</span><br>    &lt;-ctx.Done()<br><br>    <span class="hljs-comment">// 关闭调度队列，这将停止调度器的事件循环</span><br>    sched.SchedulingQueue.Close()<br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到到了这里就剩下了两个主要的实体：调度队列和调度算法。</p><ul><li><p>调度队列收集需要调度的Pod，然后提交给scheduler调度，具体将在后面进行介绍。</p></li><li><p><code>go wait.UntilWithContext(ctx, sched.scheduleOne, 0)</code>启用了一个go协程，然后负责一个一个调度pod。注意一下<code>go wait.UntilWithContext</code> ，它 是 Kubernetes 项目中用于周期性运行函数的工具。它是一个包装了 <code>time.Ticker</code> 和 <code>context.Context</code> 的机制，允许在给定的时间间隔内重复执行某个函数，直到提供的上下文被取消。函数的基本签名如下：</p>  <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">UntilWithContext</span><span class="hljs-params">(ctx context.Context, f <span class="hljs-keyword">func</span>(context.Context)</span></span>, period time.Duration)<br></code></pre></td></tr></table></figure><p>  当 <code>ctx.Done()</code> 通道关闭时，<code>wait.UntilWithContext</code> 将停止执行其周期性的任务<code>sched.scheduleOne</code>，<code>0</code> 表示两次迭代之间没有间隔，<code>sched.scheduleOne</code> 将尽可能快地被调用。具体<code>sched.scheduleOne</code>的介绍将在后面进行。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>k8s</category>
      
      <category>源码分析</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
      <tag>源码分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读】Gödel:Unified Large-Scale Resource Management and Scheduling at ByteDance</title>
    <link href="/2024/05/01/godelPaperRead/"/>
    <url>/2024/05/01/godelPaperRead/</url>
    
    <content type="html"><![CDATA[<h1 id="论文基础信息"><a href="#论文基础信息" class="headerlink" title="论文基础信息"></a>论文基础信息</h1><p><strong>论文地址：</strong> <a href="https://dl.acm.org/doi/10.1145/3620678.3624663">Gödel: Unified Large-Scale Resource Management and Scheduling at ByteDance</a></p><p><strong>收录会议：</strong> 云计算顶会-ACM Symposium on Cloud Computing(SoCC2023)</p><p><strong>作者机构：</strong> 字节跳动基础架构团队</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h2><p><strong>大规模：</strong> 字节跳动在全球范围内运营着数十个大规模集群，每个集群包含有数十万台机器。<br><strong>高异构：</strong> 数据中心内的机器也是异构的。包括不同型号的GPU、不同架构的CPU等。<br><strong>资源利用率低，弹性差：</strong> 之前的做法是每个业务组都有一个独立的集群，这会导致集群的资源利用率低，出现资源碎片。同时也导致在集群之间进行资源移动时需要的运营开销高，即弹性差。</p><h2 id="请求"><a href="#请求" class="headerlink" title="请求"></a>请求</h2><ul><li><p><strong>高并发：</strong> 请求的并发是数千个容器每秒，每天运行着数千万到数亿个容器。</p></li><li><p><strong>高异构：</strong> 在生成环境中包含着不同资源需求、不同服务级别协议（SLA）的异构任务，如下图所示。例如，1) 微服务​​、在线推理和数据库等关键业务服务，它们也可以抢占非关键业务的资源，2) 广泛的数据分析或机器学习 (ML) 模型训练等低优先级作业。</p><p>  注意这里提到了NUMA节点。在多CPU处理器架构中，NUMA节点中CPU有自己的内存，可以非常快速地访问自己的内存。例如对于推荐系统，推荐系统中的模型就需要存在内存中以降低获取延迟，这类任务就通常必须独占NUMA节点或者与其他非内存密集型任务共享 NUMA 节点。</p></li></ul><p><img src="/2024/05/01/godelPaperRead/workloads.jpg" alt="字节跳动集群中不同类型的任务"></p><h2 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h2><p>生产调度系统的主要要求是在异构机器上调度不同的任务，提高资源利用率，跟上每个计算集群不断增长的机器规模并实现高吞吐量。</p><p>现有的开源调度程序不能满足所有的需求。</p><ul><li><strong>Kubernetes：</strong> <ul><li>可以为微服务提供灵活的资源分配，但是面临可拓展性问题。一方面他的调度吞吐量差，另一方面他最多也只能支持5000个节点。</li><li>它也缺乏工作负载感知能力，无法很好地调度批处理作业。例如将机器学习任务放在一组共享相同网络前缀的节点上或在具有同质设备（例如相同的 GPU 型号）的特定节点上运行。虽然存在一些社区解决方案，但它们也存在一些问题，无法用于生产环境。</li><li>它也缺乏拓扑感知能力，即在调度Pod时遵守Pod的资源拓扑约束。传统的k8s做法是在调度到节点后再进行资源约束的检查，更合适的做法是在调度时就考虑资源拓扑约束。</li></ul></li><li><strong>YARN：</strong> <ul><li>适合复杂的批处理作业调度，但是不适用于微服务。</li><li>在将作业彼此隔离以及处理依赖性控制方面存在不足。</li><li>还缺乏资源碎片优化，导致资源利用率低。</li></ul></li></ul><p>学术界也有很多调度架构的研究，但是在该场景下也都存在一些问题。</p><ul><li><strong>单体式：</strong> 拓展性受限，无法解决高吞吐问题，且不够灵活，难以添加定制的调度策略。</li><li><strong>两级式：</strong> 悲观的资源分配策略会损坏资源的弹性，同时存在资源碎片问题，资源利用率低。</li><li><strong>状态共享式：</strong> 使用乐观并发控制来解决全局调度带来的冲突问题，其典型实现Kubernetes在调度到节点时进行冲突解决，但是这种解决方式在整个调度流程中都太晚了，降低了吞吐量和集群规模。</li><li><strong>分布式&#x2F;混合式：</strong> 分布式缺乏集中式调度器会限制调度的灵活性。</li></ul><p>字节跳动在早期参考过往经验将Kubernetes和YARN组合使用，经历了如下3个阶段：</p><ol><li>分别使用 Kubernetes 和 YARN 在<strong>单独的资源池</strong>中运行在线（即微服务）和离线（即批处理）工作负载。</li><li>通过一个协调器持续监控 Kubernetes 和 YARN 的资源供应和需求，然后根据流量模式<strong>在两个系统之间移动资源</strong>。它还利用历史数据来做出资源分配决策。移动资源依赖的是Kubernetes的<strong>污点机制</strong>。</li><li>进一步完善协调器等机制，使Kubernetes和YARN代理能够不断地进行通信。<strong>共享实时资源信息</strong>，以便离线工作负载可以利用同一节点上在线工作负载未使用的<strong>机会资源</strong>。</li></ol><p>但是这种组合做法仍然存在一些问题：</p><ul><li>组合<strong>无助于放置具有特殊要求的工作负载</strong>，例如更高的网络带宽、GPU 或 NUMA 关联性。</li><li>交换节点与节点上运行的任务无关。通过这种粗粒度的方法，选择驱逐的节点可<strong>能会造成级联故障</strong>（例如，驱逐运行参数服务器的节点会使训练工作人员失去所有进度）。</li><li>这种方法会产生<strong>大量的运营开销</strong>。例如，为了准备春节等特殊活动的高峰使用，运营团队必须提前几周开始与多个团队协调以预测扩大资源池的估计需求，这非常耗时且无法适应我们不断增长的需求基础设施。</li></ul><p>考虑到Kubernetes良好的社区生态，字节跳动决定在Kubernetes上构建一个新的调度系统Gödel，以解决上述问题。</p><h1 id="创新与贡献"><a href="#创新与贡献" class="headerlink" title="创新与贡献"></a>创新与贡献</h1><ol><li><p>引入了一种<strong>面向在线和离线任务调度的新范式</strong>，从而提供更好的拓扑亲和性（topology affinity），更高的资源弹性度，以及在非常大规模情况下更低的运营开销。</p></li><li><p>基于Kubernetes<strong>设计并实现了新的资源管理和调度系统Gödel</strong>，并提出了对普通 Kubernetes 的一些优化和增强的方法，以提高调度性能。 </p></li><li><p>在字节跳动的多个数据中心实际部署了Gödel，拥有数万台机器，并在真实工作负载下进行了<strong>评估</strong>，并在模拟环境中进行了<strong>测试</strong>。结果表明了它的优越的实用性和性能。</p><p>实践表明Gödel可以实现高达 5000 个 Pod&#x2F;秒的吞吐量，同时在单个 Gödel 集群上保持约 60% 的总体资源利用率。对于内存敏感的工作负载，借助 Gödel 支持的拓扑感知调度，数据获取延迟可减少 20% 以上。此外，与字节跳动的传统部署模式相比，Gödel 可以在几分钟内（而不是几小时或几天）在关键业务服务和低优先级作业之间转移计算资源，以响应紧急流量变化，而无需人工干预。</p></li></ol><p>调度系统已开源：<a href="https://github.com/kubewharf/godel-scheduler">https://github.com/kubewharf/godel-scheduler</a></p><h1 id="Godel系统架构"><a href="#Godel系统架构" class="headerlink" title="Gödel系统架构"></a>Gödel系统架构</h1><p><img src="/2024/05/01/godelPaperRead/overview.jpg" alt="Gödel系统架构"></p><p>Gödel系统整体的设计思想与Kubernetes类似，整体架构如上图所示。由于发现Etcd可能是调度的瓶颈，故Gödel默认使用字节跳动自研的开源KubeBrain作为高性能后背存储。为了提高调度的吞吐量，Gödel调度器被设计为分布式的状态共享式调度器。Gödel的关键组件包括：</p><ol><li><strong>Dispatcher</strong> </li><li><strong>Scheduler</strong></li><li><strong>Binder</strong></li><li><strong>CustomNodeResource(CNR)</strong></li></ol><h2 id="Dispatcher"><a href="#Dispatcher" class="headerlink" title="Dispatcher"></a>Dispatcher</h2><p>Dispatcher是Gödel的调度<strong>逻辑入口</strong>，它验证所有收到的作业请求，并根据资源请求和 QoS 优先级将它们存储在基于<strong>优先级的队列</strong>中，并最终将每个有效请求转发到所需的调度程序实例。</p><p>在Dispatcher中，我们使用逻辑队列来表示分配给不同业务组的<strong>资源配额</strong>，每当一个业务组提交一个部署Pod的任务请求时，就将该Pod放入到逻辑队列中，然后扣除逻辑队列的对应的可用资源。队列的排序策略支持多种。</p><p>Dispatcher还有一大功能是<strong>分区</strong>以控制调度器的冲突。初始只有一个分区，但是，在集群分配超过 90% 或吞吐量超过 2000 个 Pod&#x2F;秒且多个调度器之间的调度冲突高于 1%（所有阈值均可配置）的场景下，Dispatcher 会动态对集群进行分区并分配每个调度器实例相应的一组分区。</p><h2 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h2><p>Scheduler接收Dispatcher转发的作业请求，并做出调度决策（支持抢占）。Scheduler的调度单元不是单个Pod而是一个<strong>Scheduling Unit</strong>。Scheduler可以存在多个，采用状态共享式，如前所述，会受到分区调控，既可以调度到本分区也可以调度到整个集群。调度到本分区可以避免冲突，降低节点扫描开销，但是代价是调度质量的降低，还会资源碎片的存在，导致无法在本地找到所有需要的资源。系统会自动调节调度器的调度模式，例如当资源不足冲突上升时调节到只能在分区中调度。</p><p>系统还支持添加、删除调度器，但是经过实验验证，更多调度器会导致更高的调度冲突，从而导致调度器并不是越多越好。</p><h2 id="Binder"><a href="#Binder" class="headerlink" title="Binder"></a>Binder</h2><p>Binder使用乐观并发控制来解决调度冲突，支持抢占、协同调度，最终将pod绑定到Scheduler选择的特定节点上。它的工作原理与 Kubernetes 默认调度程序的绑定周期类似，但处理地更快。</p><p>Binder 使用优先级队列来执行多个调度程序发送的调度决策，并按顺序处理 Pod 绑定。被拒绝的 Pod 将返回给 Dispatcher 以进行调度重试。其冲突处理的细节如下：</p><ol><li>检查节点是否有足够的资源，是否满足 Pod 的拓扑约束。</li><li>对于抢占调度，如果多个Scheduler试图抢占同一个Pod，那么只满足第一个</li><li>对于Gang调度，会尝试解决所有Pod的冲突，通过了就绑定所有的Pod，否则不绑定任何Pod。</li></ol><h2 id="CustomNodeResource-CNR"><a href="#CustomNodeResource-CNR" class="headerlink" title="CustomNodeResource(CNR)"></a>CustomNodeResource(CNR)</h2><p>CNR代表自定义节点资源，用以支持拓扑感知调度，集群中的每台服务器都会有一个CNR对象。每个 CNR 对象代表特定节点的拓扑和资源使用情况以及该节点上每个 pod 的拓扑。依赖这些信息，Gödel 调度器就可以在调度阶段就考虑资源拓扑约束。</p><h2 id="Scheduling-Unit"><a href="#Scheduling-Unit" class="headerlink" title="Scheduling Unit"></a>Scheduling Unit</h2><p>Scheduling Unit是Gödel的基本调度单元，每个Scheduling Unit包含一个或多个Running Unit。整体采取的是一个两级结构，Job是一个Scheduling Unit，Job中的pod或者subtasks是Running Unit，只有当调度器为Scheduling Unit至少Min Member个Running Units找到可用资源时，Scheduling Unit才会被标记为可调度的，否则不会有pod运行。</p><p>Min Member是一个为了应对不同类型作业所提出的一个巧妙的设计，对于需要gang调度的作业，可以将Min Member设置为所有的Running Unit数量，如果运行的是微服务作业，可以将Min Member设置为1。</p><p>目前，我们在字节跳动生产集群中一般部署三种或更多类型的应用：微服务（线上）、批处理作业（线下）和机器学习（线下）。Scheduling Unit帮助其在调度程序级别上填补在线和离线作业之间的语义差距。</p><p>此外为了平滑地将原本的YARN任务迁移到Gödel，其还创建了多种自定义资源定义（CRD）来模仿YARN的资源请求，从而将所有原本的YARN任务迁移过来。</p><h2 id="Performance-Optimization"><a href="#Performance-Optimization" class="headerlink" title="Performance Optimization"></a>Performance Optimization</h2><p>Gödel 采用与 Kubernetes 相似的顺序来进行调度决策：<br>1）筛选；<br>2）打分，确定优先顺序；<br>3）选择节点。</p><p>但是Gödel在这个基础上做了一些优化：</p><ol><li><strong>缓存可行节点：</strong> 我们观察到，来自同一用户的一项作业的大约 90% 的部署 Pod 通常具有相同的资源请求。例如，社交媒体团队可能要求运行 20,000 个 HTTP Web 服务器，每个服务器有 4 个 CPU 和 8GB 内存。因此在筛选和打分阶段确定的Node也可以用于相同的后续Pod，直到节点状态改变。</li><li><strong>降低打分百分比：</strong> 扫描集群中的所有节点并进行打分十分耗时，基于调度质量和调度耗时之间的衡量，调度器在筛选步骤中选择（Scheduling Unit+50）个可行节点。</li><li><strong>丰富评分插件：</strong> Gödel 调度器允许通过实现不同的评分插件来定义特定于工作负载的评分策略。</li></ol><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>实验通过Kubemark构建了测试平台，测试平台由 40 台 Debian x86_64 服务器组成，每台服务器包含 256 个逻辑 CPU、2TB 内存和 7TB SSD 存储。我们使用多达 21 台服务器作为 Kubemark 主节点来托管 Gödel 调度程序和其他相关组件，包括后备存储集群。其余 19 台服务器充当空心节点，虚拟托管所有创建的 Pod。</p><h2 id="可拓展性测试"><a href="#可拓展性测试" class="headerlink" title="可拓展性测试"></a>可拓展性测试</h2><p><img src="/2024/05/01/godelPaperRead/scalability.jpg" alt="Gödel的可拓展性实验结果"></p><h3 id="在线工作负载测试"><a href="#在线工作负载测试" class="headerlink" title="在线工作负载测试"></a>在线工作负载测试</h3><p>只使用单调度器，固定Pod的提交速率为2800Pod&#x2F;s（使得单调度器基本饱和），将集群节点数量从100提高到20000，对比Gödel、Kubernetes、更改Etcd为Kubebrain的调度性能，结果如上图3中的图5所示，可以看到Gödel的性能遥遥领先，且在2w的节点下仍然能够保持高吞吐量。</p><h3 id="多调度器测试"><a href="#多调度器测试" class="headerlink" title="多调度器测试"></a>多调度器测试</h3><p>在1w个节点上，将Pod提交速率提高到10000Pod&#x2F;s（使得多调度器基本饱和），禁用了分区模式，调度器在整个集群中调度，结果如上图中的图6所示，可以观察到加速不是线性的，因为运行的调度器越多也就会导致冲突越多，目前也正在研究节点改组等解决方案来解决这个问题。</p><h3 id="离线工作负载测试"><a href="#离线工作负载测试" class="headerlink" title="离线工作负载测试"></a>离线工作负载测试</h3><p><img src="/2024/05/01/godelPaperRead/offline.jpg" alt="Gödel的离线工作负载测试结果"></p><p>在1w个节点中，固定Pod的提交速率为2800Pod&#x2F;s，将Gödel与Kubernetes-volcano（k8s社区中用于离线作业调度的调度器）、YARN进行对比，结果如上图所示，已经可以看的Gödel的性能优势。</p><h3 id="在离线混合工作负载测试"><a href="#在离线混合工作负载测试" class="headerlink" title="在离线混合工作负载测试"></a>在离线混合工作负载测试</h3><p>在1w个节点中，将提交中在线服务的比例分别更改为0%、25%、50%、75%和100%，其余工作负载为离线作业。测试结果如上图中的图7所示，Gödel的调度性能非常稳定。</p><h2 id="拓扑感知调度测试"><a href="#拓扑感知调度测试" class="headerlink" title="拓扑感知调度测试"></a>拓扑感知调度测试</h2><p><img src="/2024/05/01/godelPaperRead/topology-aware.jpg" alt="Gödel的拓扑感知调度测试结果"></p><p>为了验证我们是否可以从上述拓扑感知调度中受益，我们评估了具有和不具有拓扑亲和性的调度器的性能。我们采用不同的调度器调度需要调度到NUMA节点的推荐任务，然后测试调度后各个任务的数据获取延迟，结果如上图所示，可以看到Gödel能够将平均延迟和 P99 延迟分别减少 21% 和 22.8%。</p><h2 id="性能优化分析测试"><a href="#性能优化分析测试" class="headerlink" title="性能优化分析测试"></a>性能优化分析测试</h2><p><img src="/2024/05/01/godelPaperRead/optimization.jpg" alt="Gödel的性能优化分析测试结果"></p><p>为了得到性能优化中缓存可行节点和降低打分百分比这两个操作的优化收益，我们重复运行了在线工作负载测试，然后结果如上图所示，可以看到，可行的节点缓存和降低评分百分比合计贡献了90%以上的性能提升。</p><h1 id="生产经验"><a href="#生产经验" class="headerlink" title="生产经验"></a>生产经验</h1><p><img src="/2024/05/01/godelPaperRead/production.jpg" alt="Gödel在字节跳动的实践经验"></p><p>Gödel已经实际部署在了字节跳动的生产集群中。上图图9展示了在2022年8月2号时，随着上午7点在线工作负载的增加，Gödel调度器自动撤回尽力而为的低优先级离线作业的资源，然后在凌晨3点左右又自动将多余的资源分配给了尽力而为的低优先级离线作业。两个转换在几分钟内无缝完成，无需人工干预。</p><p>在上图10展示了Gödel的集群资源利用率，可以看到其高达60%，而行业平均利用率不到30%。</p><p>在 Gödel 中实施更好的装箱算法有助于减少机器学习工作负载的 GPU Pod 碎片。之前使用 YARN 时，由于碎片问题，我们损失了 30% 的可分配容量，现在已减少到 10%，如上图 11 所示。 </p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>集群调度</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Docker 面试题汇总(附答案)</title>
    <link href="/2024/04/18/dockerInterview/"/>
    <url>/2024/04/18/dockerInterview/</url>
    
    <content type="html"><![CDATA[<p>建议和这篇一起观看，更加全面一些：<a href="https://slipegg.github.io/2024/04/18/DockerStudy/">万字总结！Docker简介及底层关键技术剖析</a></p><h1 id="Docker-底层原理、概念类问题"><a href="#Docker-底层原理、概念类问题" class="headerlink" title="Docker 底层原理、概念类问题"></a>Docker 底层原理、概念类问题</h1><h2 id="1-Docker-和-LXC-有什么区别？"><a href="#1-Docker-和-LXC-有什么区别？" class="headerlink" title="1. Docker 和 LXC 有什么区别？"></a>1. Docker 和 LXC 有什么区别？</h2><p>LXC是在Linux上相关技术实现的容器，docker则在如下的几个方面进行了改进：</p><p>1、移植性：通过抽象容器配置，容器可以实现一个平台移植到另一个平台。</p><p>2、镜像系统：基于AUFS的镜像系统为容器的分发带来了很多的便利，通是共同的镜像层只需要存储一份，实现高效率的存储。</p><p>3、版本管理：类似于GIT的版本管理理念，用户可以更方便的创建、管理镜像文件。</p><p>4、仓库系统:仓库系统大大降低了镜像的分发和管理的成本。</p><p>5、周边工具：各种现有的工具（配置管理、云平台）对docker的支持，以及基于docker的pass、Cl等系统，让docker的应用更加方便和多样。</p><h2 id="2-Docker-容器有几种状态？"><a href="#2-Docker-容器有几种状态？" class="headerlink" title="2. Docker 容器有几种状态？"></a>2. Docker 容器有几种状态？</h2><p>四种状态：运行、已停止、重新启动、已退出。</p><h2 id="3-Docker-有哪些优缺点？"><a href="#3-Docker-有哪些优缺点？" class="headerlink" title="3. Docker 有哪些优缺点？"></a>3. Docker 有哪些优缺点？</h2><p><strong>docker优点</strong></p><p>1、部署方便</p><p>你一定还有印象，在我们最开始学习编程的时候，搭建环境这一步往往会耗费我们好几个小时的时间，而且其中一个小问题可能需要找很久才能够解决。你还会得到关于环境搭建方面的团队其他成员的求助。而有了容器之后，这些都变得非常容易，你的开发环境就只是一个或者几个容器镜像的地址，最多再需要一个控制部署流程的执行脚本。或者进一步将你的环境镜像以及镜像脚本放入一个git项目，发布到云端，需要的时候将它拉到本地就可以了。</p><p>2、部署安全</p><p>当我们收到一个bug反馈的时候，很多时候心里面的第一反应一定是“我本地是好的啊”！这种情况的发生就在于环境的不一致，我们在开发过程中的调试往往不能保证其他环境的问题，但是我们却要为此买单，这真是一件令人苦恼的事情。有了容器之后，这将很少发生。我们可以通过容器技术将开发环境和测试环境以及生产环境保持版本和依赖上的统一，保证代码在一个高度统一的环境上执行。而测试环境的统一，也同样能解决CI流程对环境的要求。</p><p>3、隔离性好</p><p>不管是开发还是生产，往往我们一台机器上可能需要跑多个服务，而服务各自需要的依赖配置不尽相同，假如说两个应用需要使用同一个依赖，或者两个应用需要的依赖之间会有一些冲突，这个时候就很容易出现问题了。所以同一台服务器上不同应用提供的不同服务，最好还是将其隔离起来。而容器在这方面有天生的优势，每一个容器就是一个隔离的环境，你对容器内部提供服务的要求，容器可以自依赖的全部提供。</p><p>4、快速回滚</p><p>容器之前的回滚机制，一般需要基于上个版本的应用重新部署，且替换掉目前的问题版本。在最初的时代，可能是一套完整的开发到部署的流程，而执行这一套流程往往需要很长的时间。在基于git的环境中，可能是回退某个历史提交，然后重新部署。这些跟容器技术相比都不够快，而且可能会引起新的问题（因为是基于新版本的修改）。而容器技术天生带有回滚属性，因为每个历史容器或者镜像都会有保存，而替换一个容器或者某个历史镜像是非常快速和简单的。</p><p>5、成本低</p><p>这可能是一个最明显和有用的优点了，在容器出现之前，我们往往构筑一个应用就需要一台新的服务器或者一台虚机。服务器的购置成本和运维成本都很高，而虚机需要占用很多不必要的资源。相比之下，容器技术就小巧轻便的多，只需要给一个容器内部构建应用需要的依赖就可以了，这也是容器技术发展迅速的最主要原因。</p><p>6、管理成本更低</p><p>随着大环境的发展，docker等容器的使用和学习的成本也是愈发降低，成为更多开发者和企业的选择。</p><p><strong>docker缺点</strong></p><p>1、隔离性</p><p>基于hypervisor的虚机技术，在隔离性上比容器技术要更好，它们的系统硬件资源完全是虚拟化的，当一台虚机出现系统级别的问题，往往不会蔓延到同一宿主机上的其他虚机。但是容器就不一样了，容器之间共享同一个操作系统内核以及其他组件，所以在收到攻击之类的情况发生时，更容易通过底层操作系统影响到其他容器。当然，这个问题可以通过在虚机中部署容器来解决，可是这样又会引出新的问题，比如成本的增加以及下面要提到的问题：性能。</p><p>2、性能</p><p>不管是虚机还是容器，都是运用不同的技术，对应用本身进行了一定程度的封装和隔离，在降低应用和应用之间以及应用和环境之间的耦合性上做了很多努力，但是随机而来的，就会产生更多的网络连接转发以及数据交互，这在低并发系统上表现不会太明显，而且往往不会成为一个应用的瓶颈（可能会分散于不同的虚机或者服务器上），但是当同一虚机或者服务器下面的容器需要更高并发量支撑的时候，也就是并发问题成为应用瓶颈的时候，容器会将这个问题放大，所以，并不是所有的应用场景都是适用于容器技术的。</p><h2 id="4-什么是Docker容器？"><a href="#4-什么是Docker容器？" class="headerlink" title="4. 什么是Docker容器？"></a>4. 什么是Docker容器？</h2><p>Docker容器在应用层创建了一个抽象，并将应用与所有的依赖关系打包在一起。这使我们能够快速、可靠地部署应用程序。容器不要求我们安装一个不同的操作系统。相反，它们使用底层系统的CPU和内存来执行任务。这意味着，任何容器化的应用程序都可以在任何平台上运行，而不受底层操作系统的影响。我们也可以把容器看作是Docker镜像的运行时实例。</p><h2 id="5-解释一下Docker组件。"><a href="#5-解释一下Docker组件。" class="headerlink" title="5. 解释一下Docker组件。"></a>5. 解释一下Docker组件。</h2><p>三个架构组件包括Docker客户端、主机和注册表。</p><p>Docker客户端。这个组件执行构建和运行操作，与Docker主机通信。<br>Docker主机。这个组件持有Docker守护程序、Docker镜像和Docker容器。该守护程序设置了与Docker注册中心的连接。<br>Docker Registry。这个组件存储Docker镜像。它可以是一个公共注册表，如Docker Hub或Docker Cloud，或一个私人注册表。</p><h2 id="6-虚拟化和容器化之间有什么区别？"><a href="#6-虚拟化和容器化之间有什么区别？" class="headerlink" title="6. 虚拟化和容器化之间有什么区别？"></a>6. 虚拟化和容器化之间有什么区别？</h2><p>虚拟化<br>虚拟化帮助我们在一台物理服务器上运行和托管多个操作系统。在虚拟化中，管理程序给客人操作系统一个虚拟机。虚拟机形成了硬件层的抽象，所以主机上的每个虚拟机都可以作为一个物理机。<br>容器化<br>容器化为我们提供了一个隔离的环境来运行我们的应用程序。我们可以在一台服务器或虚拟机上使用同一操作系统部署多个应用程序。容器形成了应用层的抽象，所以每个容器代表了不同的应用。</p><h2 id="7-描述一下Docker容器的生命周期。"><a href="#7-描述一下Docker容器的生命周期。" class="headerlink" title="7. 描述一下Docker容器的生命周期。"></a>7. 描述一下Docker容器的生命周期。</h2><p>Docker容器会经历以下几个阶段。</p><p>创建一个容器<br>运行该容器<br>暂停容器(可选)<br>解除容器的暂停（可选<br>启动容器<br>停止容器<br>重新启动容器<br>杀死容器<br>销毁容器</p><h2 id="8-Docker-容器有几种在状态？"><a href="#8-Docker-容器有几种在状态？" class="headerlink" title="8. Docker 容器有几种在状态？"></a>8. Docker 容器有几种在状态？</h2><p>starting 运行状态<br>Exited 退出状态<br>Paused 暂停状态<br>healthy 健康状态<br>unhealthy 非健康状态</p><h2 id="9-平时安装进虚拟机的-CentOS-镜像都是-4-4-GB-，为什么-Docker-的镜像才只有-200-MB-？"><a href="#9-平时安装进虚拟机的-CentOS-镜像都是-4-4-GB-，为什么-Docker-的镜像才只有-200-MB-？" class="headerlink" title="9. 平时安装进虚拟机的 CentOS 镜像都是 4.4 GB ，为什么 Docker 的镜像才只有 200 MB ？"></a>9. 平时安装进虚拟机的 CentOS 镜像都是 4.4 GB ，为什么 Docker 的镜像才只有 200 MB ？</h2><p>Docker 镜像仅包含运行所需的最小 runtime 环境，即仅需要 rootfs 即可。对于一个精简的 OS ，rootfs 可以很小，只需要包括最基本的命令、工具和程序库即可，因为底层直接共用 Host 主机的 kernel ，自己只需要提供 rootfs 即可。由此可见对于不同的 Linux 发行版，bootfs 基本是一致的，只有 rootfs 会有差别，因此不同的发行版可以共用 bootfs 。</p><h1 id="Docker-基本操作类问题"><a href="#Docker-基本操作类问题" class="headerlink" title="Docker 基本操作类问题"></a>Docker 基本操作类问题</h1><h2 id="2-容器退出后，-通过-docker-ps-命令查看不到，数据会丢失么？"><a href="#2-容器退出后，-通过-docker-ps-命令查看不到，数据会丢失么？" class="headerlink" title="2. 容器退出后， 通过 docker ps 命令查看不到，数据会丢失么？"></a>2. 容器退出后， 通过 docker ps 命令查看不到，数据会丢失么？</h2><p>容器退出后会处于终止（exited）状态， 此时可以通过docker ps -a查看。</p><p>其中的数据也不会丢失，还可以通过docker [container] start命令来启动它。只有删除掉容器才会清除所有数据。</p><h2 id="2-什么是Docker-Hub？"><a href="#2-什么是Docker-Hub？" class="headerlink" title="2. 什么是Docker Hub？"></a>2. 什么是Docker Hub？</h2><p>Docker hub是一个基于云的注册表服务，允许您链接到代码存储库，构建镜像并测试它们，存储手动推送的镜像以及指向Docker云的链接，以便可以将镜像部署到主机。它为整个开发流程中的容器镜像发现，分发和变更管理，用户和团队协作以及工作流自动化提供了集中资源。</p><h2 id="3-如何临时退出一个正在交互的容器的终端，而不终止它？"><a href="#3-如何临时退出一个正在交互的容器的终端，而不终止它？" class="headerlink" title="3. 如何临时退出一个正在交互的容器的终端，而不终止它？"></a>3. 如何临时退出一个正在交互的容器的终端，而不终止它？</h2><p>按Ctrl+p，后按Ctrl+q，如果按Ctrl+c会使容器内的应用进程终止，进而会使容器终止。</p><h2 id="4-很多应用容器都是默认后台运行的，怎么查看它们的输出和日志信息？"><a href="#4-很多应用容器都是默认后台运行的，怎么查看它们的输出和日志信息？" class="headerlink" title="4. 很多应用容器都是默认后台运行的，怎么查看它们的输出和日志信息？"></a>4. 很多应用容器都是默认后台运行的，怎么查看它们的输出和日志信息？</h2><p>使用docker logs，后面跟容器的名称或者ID信息</p><h2 id="5-可以在一个容器中同时运行多个应用进程吗？"><a href="#5-可以在一个容器中同时运行多个应用进程吗？" class="headerlink" title="5. 可以在一个容器中同时运行多个应用进程吗？"></a>5. 可以在一个容器中同时运行多个应用进程吗？</h2><p>一般不推荐在同一个容器内运行多个应用进程，如果有类似需求，可以通过额外的进程管理机制，比如supervisord来管理所运行的进程</p><h2 id="6-如何从Docker镜像中创建一个Docker容器？"><a href="#6-如何从Docker镜像中创建一个Docker容器？" class="headerlink" title="6. 如何从Docker镜像中创建一个Docker容器？"></a>6. 如何从Docker镜像中创建一个Docker容器？</h2><p>为了从镜像中创建一个容器，我们从Docker资源库中拉出我们想要的镜像并创建一个容器。我们可以使用以下命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker run -it -d &lt;image_name&gt;<br></code></pre></td></tr></table></figure><p>-it: 这两个选项结合起来表示在交互模式下运行容器*，并且分配一个伪终端（pseudo-TTY）。<br>-d: 这个选项表示在后台（detached）模式下运行容器，即使你退出终端或会话，容器也会继续运行。<br><image_name>: 这里应该替换成你要运行的容器的镜像名称。</p><h2 id="7-有什么常用的-Docker-命令？"><a href="#7-有什么常用的-Docker-命令？" class="headerlink" title="7. 有什么常用的 Docker 命令？"></a>7. 有什么常用的 Docker 命令？</h2><ul><li>docker pull 拉取镜像</li><li>docker create 创建容器</li><li>docker rm 删除容器</li><li>docker ps 列出正在运行的容器列表</li><li>docker run 创建容器并运行指定命令</li><li>docker start 启动容器</li><li>docker stop 停止运行容器</li><li>docker restart 重启容器</li><li>docker rm 删除容器</li><li>docker exec 容器执行指定命令</li><li>docker rmi 删除镜像</li></ul><h2 id="8-如何把主机的东西拷贝到容器内部？"><a href="#8-如何把主机的东西拷贝到容器内部？" class="headerlink" title="8. 如何把主机的东西拷贝到容器内部？"></a>8. 如何把主机的东西拷贝到容器内部？</h2><ul><li>通过 docker cp 命令即可，还能把容器内部内容拷贝到主机</li></ul><h2 id="9-如何让容器随着-Docker-服务启动而自动启动？"><a href="#9-如何让容器随着-Docker-服务启动而自动启动？" class="headerlink" title="9. 如何让容器随着 Docker 服务启动而自动启动？"></a>9. 如何让容器随着 Docker 服务启动而自动启动？</h2><ul><li>创建容器时，加上 –restart&#x3D;always 参数</li><li>创建容器后，通过修改容器配置文件的 RestartPolicy 参数值</li><li>创建容器后，使用 docker update 命令更新容器的 –restart 参数值</li></ul><h2 id="10-如何查看官方镜像服务的默认端口是什么？"><a href="#10-如何查看官方镜像服务的默认端口是什么？" class="headerlink" title="10. 如何查看官方镜像服务的默认端口是什么？"></a>10. 如何查看官方镜像服务的默认端口是什么？</h2><ul><li>可以通过 docker inspect 查看镜像信息，然后找到端口映射一栏</li><li>也可以先用该镜像创建一个容器并运行，通过 docker ps 查看运行端口是什么</li></ul><h2 id="11-如何修改容器的端口映射？"><a href="#11-如何修改容器的端口映射？" class="headerlink" title="11. 如何修改容器的端口映射？"></a>11. 如何修改容器的端口映射？</h2><ul><li>删除容器，重新创建容器，并指定端口映射</li><li>通过容器配置文件修改端口映射</li><li>通过 docker commit 将容器构建为一个全新的镜像，然后再通过该镜像创建新的容器，并指定端口映射</li></ul><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="https://www.cnblogs.com/stry/p/17041111.html">2022年最全Docker面试题附答案解析大汇总</a></li><li><a href="https://juejin.cn/post/7088894767047639054">排名前20的Docker面试问题（附答案）</a></li><li><a href="https://www.nowcoder.com/discuss/352905402784264192">Docker面试题总结（配答案）</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
      <tag>面试题</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>万字总结！Docker简介及底层关键技术剖析</title>
    <link href="/2024/04/18/DockerStudy/"/>
    <url>/2024/04/18/DockerStudy/</url>
    
    <content type="html"><![CDATA[<h1 id="Docker-简介"><a href="#Docker-简介" class="headerlink" title="Docker 简介"></a>Docker 简介</h1><p>Docker 是一个开源的应用容器引擎，基于 Go 语言 并遵从 Apache2.0 协议开源。Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）。几乎没有性能开销，可以很容易地在机器和数据中心中运行。最重要的是，他们不依赖于任何语言、框架或包装系统。</p><p>Docker 基于 Linux 内核的 cgroup，namespace，以及 UnionFS 等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。最初实现是基于 LXC，从 0.7 版本以后开始去除 LXC，转而使用自行开发的 libcontainer，从 1.11 版本开始，则进一步演进为使用 runC 和 containerd。</p><pre><code class="hljs">runc 是一个 Linux 命令行工具，用于根据 OCI容器运行时规范 创建和运行容器。containerd 是一个守护程序，它管理容器生命周期，提供了在一个节点上执行容器和管理镜像的最小功能集。</code></pre><p><img src="/2024/04/18/DockerStudy/dockerArc1.jpg" alt="Docker 架构"></p><h2 id="Docker-与LXC-的区别"><a href="#Docker-与LXC-的区别" class="headerlink" title="Docker 与LXC 的区别"></a>Docker 与LXC 的区别</h2><p>LXC的全名为Linux Container，它是一种轻量级的Linux内核容器虚拟化技术，允许在同一主机上运行多个相互隔离的Linux Container，每个容器都有自己的完整的文件系统、网络、进程和资源隔离环境。</p><p>LXC与传统的虚拟机技术不同，LXC不需要运行完整的操作系统镜像。LXC使用Linux内核提供的cgroups和命名空间（Namespaces）功能来实现容器隔离。它有效地将由单个操作系统管理的资源划分到孤立的组中，以更好地在孤立的组之间平衡有冲突的资源使用需求。</p><p>docker 出现之初，便是采用了 lxc 技术作为 docker 底层，对容器虚拟化的控制。后来随着 docker 的发展，它自己封装了 libcontainer （golang 的库）来实现 Cgroup 和 Namespace 控制，从而消除了对 lxc 的依赖。</p><p>现在Docker相较于LXC已经有了十足的发展，其生态也更加完善。主要有以下几个方面的区别。</p><ul><li><strong>抽象级别：</strong> Docker 提供了更高级别的抽象，它强调应用程序和服务的容器化，提供了易于使用的工具和接口，如 Docker CLI、Docker Compose 等, Docker 的目的是为了尽可能减少容器中运行的程序，减少到只运行单个程序，并且通过 Docker 来管理这个程序，有了 Docker，可以从底层应用程序通过 Docker 来配置，网络，存储和编排。而 LXC 更接近于操作系统级的虚拟化，需要更多的系统管理和配置工作，LXC 用正常操作系统环境回避网络、存储等问题，并且因此可以快速兼容所有应用程序和工具，以及任意管理和编制层次，来替代虚拟机。</li><li><strong>镜像构建方式：</strong> Docker 使用 Dockerfile 来定义镜像的构建过程，这使得镜像的构建和管理变得非常简单和可重复。而在 LXC 中，镜像的创建和管理相对复杂，它没有类似Dockerfile的镜像构建方式，它使用基于文件系统的容器模板来创建容器。</li><li><strong>资源隔离：</strong> Docker 相比于 LXC 增加了更多的资源隔离和管理功能，如 CPU、内存、网络等资源的控制。这使得 Docker 更适合于生产环境中的容器化部署。</li><li><strong>占用资源不同：</strong> 相对于LXC，Docker的容器启动速度更快，占用资源更少。这是因为Docker容器使用了更多的技术手段来优化容器启动和运行的效率，例如使用联合文件系统（UnionFS）来共享文件系统，使用镜像层缓存来加速镜像构建，使用Docker镜像仓库等。</li><li><strong>生态系统：</strong> Docker 有着庞大的生态系统，包括 Docker Hub（用于分享和拉取镜像）、Docker Swarm（用于容器编排和集群管理）、Docker Compose（用于多容器应用编排）等工具和服务。相比之下，LXC 的生态系统相对较小。</li></ul><h2 id="Docker-与传统虚拟化的区别"><a href="#Docker-与传统虚拟化的区别" class="headerlink" title="Docker 与传统虚拟化的区别"></a>Docker 与传统虚拟化的区别</h2><p>下面的图片比较了 Docker 和传统虚拟化方式的不同之处。<strong>传统虚拟机技术</strong>是<strong>虚拟出一套硬件</strong>后，在其上<strong>运行一个完整操作系统</strong>，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内<strong>没有自己的内核</strong>，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。</p><p><img src="/2024/04/18/DockerStudy/dockerVsVm.jpeg" alt="Docker与传统虚拟化的区别"></p><p>其区别可以从以下几点展开。</p><table><thead><tr><th>对比项</th><th>Docker 容器</th><th>虚拟机</th></tr></thead><tbody><tr><td>隔离性</td><td>较弱的隔离，属于进程之间的隔离，各个容器共享宿主机的内核</td><td>强隔离，属于系统级别的隔离，会模拟出一整个操作系统和硬件，各个虚拟机之间完全隔离</td></tr><tr><td>启动速度</td><td>秒级</td><td>分钟级</td></tr><tr><td>镜像大小</td><td>一般为 MB</td><td>一般为 GB</td></tr><tr><td>托管主体</td><td>Docker Engine 在操作系统和 Docker 容器之间进行协调。</td><td>虚拟机监控器在计算机的物理硬件和虚拟机之间进行协调。</td></tr><tr><td>运行性能</td><td>接近原生（损耗小于 2%）</td><td>损耗小于 15%</td></tr><tr><td>镜像可移植性</td><td>平台无关</td><td>平台相关</td></tr><tr><td>占用资源量</td><td>Docker 只是一个进程，只需要将应用以及相关的组件打包，在运行时占用很少的资源，单机上可支持上千个容器</td><td>虚拟机是一个完整的操作系统，需要占用大量的磁盘、内存和 CPU 资源，一般一个主机上只能支持几十个虚拟机</td></tr><tr><td>安全性</td><td>1. 容器内的用户从普通用户权限提升为 root 权限，就直接具备了宿主机的 root 权限。<br>2. 容器中没有硬件隔离，使得容器容易受到攻击。</td><td>1. 虚拟机租户 root 权限和主机的 root 虚拟机权限是分离的。<br>2. 硬件隔离技术：防止虚拟机突破和宿主机交互。</td></tr><tr><td>高可用性</td><td>docker对业务的⾼可⽤⽀持是通过快速重新部署实现的。</td><td>虚拟化具备负载均衡，⾼可⽤，容错，迁移和数据保护等经过⽣产实践检验的成熟保障机制，VMware可承诺虚拟机99.999%⾼可⽤，保证业务连续性。</td></tr><tr><td>资源共享</td><td>按需共享，依据cgroups进行控制。</td><td>按固定数量共享，在虚拟机映像的配置要求中设置。</td></tr></tbody></table><h3 id="什么情况下只能用虚拟机"><a href="#什么情况下只能用虚拟机" class="headerlink" title="什么情况下只能用虚拟机"></a>什么情况下只能用虚拟机</h3><ol><li><strong>需要操作系统级别的隔离：</strong>虚拟机提供了更高的隔离性，每个虚拟机都有自己的操作系统，可以实现不同操作系统之间的隔离，虚拟机可以做到这一个主机上同时运行多个不同操作系统的虚拟机，且互不影响。而Docker容器调用的实际上是宿主机的内核，因此容器之间共享宿主机的内核，无法实现操作系统级的隔离，例如如果 Docker 容器中运行的应用程序存在内核漏洞或者容器内部进行了特权操作（例如修改主机文件系统），这可能会导致容器越过 Docker 的隔离性，影响到宿主机或其他容器。也是因此在一些隔离性要求较高的场景下，例如云上的多租户场景，就还是需要使用虚拟机。</li><li><strong>需要更强的资源隔离和控制：</strong>虚拟机可以对 CPU、内存、磁盘空间等以固定的单位进行分配，而Docker 容器的资源隔离和控制较为简单，通常是通过限制容器的 CPU 使用率、内存限制等来实现。</li></ol><p>总的来说，Docker或者说容器技术和虚拟机并非简单的取舍关系，如果你希望一个完全隔离的和资源有保障的环境，那么虚拟机是你的不二选择;如果你只希望进程之间相互隔离，同时拥有轻量化的属性，那么linux容器技术或者Docker，才是更好的选择。</p><h2 id="Docker的重要概念"><a href="#Docker的重要概念" class="headerlink" title="Docker的重要概念"></a>Docker的重要概念</h2><h3 id="镜像（Image）"><a href="#镜像（Image）" class="headerlink" title="镜像（Image）"></a>镜像（Image）</h3><p>当我们使用docker容器时，首先需要首先先下载一个对应的镜像，镜像相比于虚拟机的进行更加轻量，原因在于它实际上主要只是一个rootfs（当然还包括一些额外的配置文件）。对于一个精简的 OS ，rootfs 可以很小，只需要包括最基本的命令、工具和程序库即可，因为底层直接共用 Host 主机的 kernel。</p><p><img src="/2024/04/18/DockerStudy/dockerLayer1.png" alt="镜像分层"></p><p>镜像中的rootfs采取的是分层存储的方式，即每个镜像都是由多个只读层叠加而成。这种分层存储的特性使得镜像的复用、定制、共享变的更为容易。同时，每一层都可以被容器读取，容器的文件系统就是这些层的叠加。镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。</p><p><img src="/2024/04/18/DockerStudy/dockerImage.jpg" alt="Docker镜像结构"></p><p>从上图中我们可以看到，当由 ubuntu:14.04 镜像启动容器时，ubuntu:14.04 镜像的镜像层内容将作为容器的 rootfs；而 ubuntu:14.04 镜像的 json 文件，会由 docker daemon 解析，并提取出其中的容器执行入口 CMD 信息，以及容器进程的环境变量 ENV 信息，最终初始化容器进程。当然，容器进程的执行入口来源于镜像提供的 rootfs。</p><h4 id="rootfs"><a href="#rootfs" class="headerlink" title="rootfs"></a>rootfs</h4><p>rootfs代表了一个系统中的根文件系统，在传统的 Linux 操作系统内核启动时，首先挂载一个只读的 rootfs，当系统检测其完整性之后，再将其切换为读写模式。</p><p>在docker容器中，rootfs是docker容器在启动时内部进程可见的文件系统，即docker容器的根目录。rootfs通常包含一个操作系统运行所需的文件系统，例如可能包含典型的类 Unix 操作系统中的目录系统，如 &#x2F;dev、&#x2F;proc、&#x2F;bin、&#x2F;etc、&#x2F;lib、&#x2F;usr、&#x2F;tmp 及运行docker容器所需的配置文件、工具等。</p><p>docker实现rootfs依靠的实联合挂载技术。</p><h4 id="联合挂载（UnionFS）"><a href="#联合挂载（UnionFS）" class="headerlink" title="联合挂载（UnionFS）"></a>联合挂载（UnionFS）</h4><p>最新的docker采用的联合挂载技术为overlay2。对于overlay2，它的主要作用就是将一堆目录下的内容联合挂载到一个目录下，它包含以下几个目录：</p><ul><li>LowerDir：只读层，包含镜像的各个层。</li><li>UpperDir：可读写层，容器运行时的写入操作都会在这个目录下进行。</li><li>WorkDir：工作基础目录，挂载后内容会被清空，且在使用过程中其内容用户不可见。</li><li>MergeDir：合并目录，是 LowerDir 和 UpperDir 的合并结果,本身没有任何文件，只是一个挂载点。</li></ul><p>在合并的目录中进行操作时，各个目录之间有上下顺序，上层目录的同名文件会遮盖住下层的文件。如果对LowerDir中的文件进行了修改，那么实际的文件是不会改变的，而是会在UpperDir中对其进行拷贝，然后在UpperDir中进行修改，这也称为<strong>写时复制技术</strong>。如果对LowerDir中的文件进行删除，那么实际的文件也不会被删除，而是在UpperDir中创建一个同名文件，并将其标记为删除状态。</p><h4 id="docker镜像的完整分层"><a href="#docker镜像的完整分层" class="headerlink" title="docker镜像的完整分层"></a>docker镜像的完整分层</h4><p>一个容器完整的层应由三个部分组成：</p><ol><li>镜像层：也称为rootfs，提供容器启动的文件系统。rootfs也就是image中提供的文件。</li><li>init层： 用于修改容器中一些文件如&#x2F;etc&#x2F;hostname，&#x2F;etc&#x2F;hosts，&#x2F;etc&#x2F;resolv.conf等。需要这一层的原因是当容器启动时候，这些本该属于image层的文件或目录，比如hostname，用户需要修改，但是image层又不允许修改，所以启动时候通过单独挂载一层init层，通过修改init层中的文件达到修改这些文件目的。</li><li>容器层：使用联合挂载统一给用户提供的可读写目录。当不对容器进行任何操作时，容器层是空的，当容器中有文件写入时，这些文件会被写入到容器层。</li></ol><h3 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h3><p>容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间、文件系统、网络配置等。</p><p>如上所述，容器在运行时会添加一层可读写的容器层，其生命周期与docker一致。按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用 数据卷（Volume）、或者 绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高。</p><h3 id="厂库"><a href="#厂库" class="headerlink" title="厂库"></a>厂库</h3><p>为了便于镜像的上传和下载，我们就需要一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本。我们可以通过 &lt;仓库名&gt;:&lt;标签&gt; 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签。</p><p>仓库名经常以 两段式路径 形式出现，比如 jwilder&#x2F;nginx-proxy，前者往往意味着 Docker Registry 多用户环境下的用户名，后者则往往是对应的软件名。</p><p>最常使用的 Registry 公开服务是官方的 <a href="https://hub.docker.com/">Docker Hub</a>，这也是默认的 Registry，并拥有大量的高质量的官方镜像。除此以外，还有 Red Hat 的 <a href="https://quay.io/repository/">Quay.io</a>；Google 的 <a href="https://cloud.google.com/container-registry/">Google Container Registry</a>，Kubernetes 的镜像使用的就是这个服务；代码托管平台 GitHub 推出的 <a href="https://docs.github.com/cn/packages/working-with-a-github-packages-registry/working-with-the-container-registry">ghcr.io</a>。</p><h1 id="Docker-程序架构"><a href="#Docker-程序架构" class="headerlink" title="Docker 程序架构"></a>Docker 程序架构</h1><p><img src="/2024/04/18/DockerStudy/dockerSoftArc.jpg" alt="Docker 程序架构"></p><p>Docker 是一个客户端-服务器（C&#x2F;S）架构程序。Docker 客户端只需要向 Docker 服务器或者守护进程发出请求，服务器或者守护进程将完成所有工作并返回结果。Docker 提供了一个命令行工具以及一整套 RESTful API。你可以在同一台宿主机上运行 Docker 守护进程和客户端，也可以从本地的 Docker 客户端连接到运行在另一台宿主机上的远程 Docker 守护进程。</p><p>Docker 服务端是 Docker 所有后台服务的统称。其中 dockerd 是一个非常重要的后台管理进程，它负责响应和处理来自 Docker 客户端的请求，然后将客户端的请求转化为 Docker 的具体操作。例如镜像、容器、网络和挂载卷等具体对象的操作和管理。</p><p>Docker 从诞生到现在，服务端经历了多次架构重构。起初，服务端的组件是全部集成在 docker 二进制里。但是从 1.11 版本开始， dockerd 已经成了独立的二进制，此时的容器也不是直接由 dockerd 来启动了，而是集成了 containerd、runC 等多个组件。</p><h1 id="Docker-关键底层技术"><a href="#Docker-关键底层技术" class="headerlink" title="Docker 关键底层技术"></a>Docker 关键底层技术</h1><p>Linux 命名空间、控制组和 UnionFS 三大技术支撑了目前 Docker 的实现，也是 Docker 能够出现的最重要原因，此外Docker网络、Docker存储也尤为重要。UnionFS前面已经介绍过了，不再赘述。</p><h2 id="Linux-命名空间-namespace"><a href="#Linux-命名空间-namespace" class="headerlink" title="Linux 命名空间(namespace)"></a>Linux 命名空间(namespace)</h2><p>命名空间是 是 Linux 提供的一种内核级别环境隔离的方法，本质就是对全局系统资源的一种封装隔离。每个容器都有自己单独的命名空间，运行在其中的应用都像是在独立的操作系统中运行一样。命名空间保证了容器之间彼此隔离，互不影响。</p><p>目前linux内核支持以下6种命名空间：</p><ul><li><strong>PID命名空间：</strong> 使得容器内的进程拥有独立的进程空间。在容器中运行ps -ef，只能看到容器内的进程，而看不到宿主机上的进程。</li><li><strong>net命名空间：</strong> pid 命名空间将进程pid情况进行了隔离，但是网络端口还是共享 host 的端口。网络隔离是通过 net 命名空间实现的， 每个 net 命名空间有独立的网络设备，IP 地址，路由表，&#x2F;proc&#x2F;net 目录。这样每个容器的网络就能隔离开来。Docker 默认采用 veth 的方式，将容器中的虚拟网卡同 host 上的一 个Docker 网桥 docker0 连接在一起。</li><li><strong>ipc 命名空间：</strong> ipc全称interprocess communication，指的是一种进程间的交互方法，包括信号量、消息队列和共享内存等。Docker 默认会为每个容器创建一个独立的 IPC 命名空间，这样容器内的进程就可以在一个隔离的 IPC 环境中运行，不会影响到其他容器或者宿主机的进程。</li><li><strong>mnt 命名空间：</strong> mnt 命名空间用于隔离文件系统挂载点，每个容器拥有自己独立的挂载点视图。挂载主要指的是将宿主机的某个目录挂载到自己容器内的某个目录下，通过命名空间的隔离，就可以本本命名空间内记录自己的挂载点，而不会影响到其他容器。</li><li><strong>uts 命名空间：</strong> UTS(“UNIX Time-sharing System”) 命名空间允许每个容器拥有独立的 hostname 和 domain name， 使其在网络上可以被视作一个独立的节点，从而避免在网络传输过程中在依靠主机名进行信息传递时出现冲突。</li><li><strong>user 命名空间：</strong> 每个容器可以有不同的用户和组 id， 也就是说可以在容器内用容器内部的用户执行程序而非主机上的用户。</li></ul><h2 id="控制组（Cgroup）"><a href="#控制组（Cgroup）" class="headerlink" title="控制组（Cgroup）"></a>控制组（Cgroup）</h2><p>控制组是linux内核的一个特性，主要用来对共享资源进行隔离、限制，审计等。避免多个容器同时运行时的系统资源竞争，它最早是由 Google 的程序员 2006 年起提出，Linux 内核自 2.6.24 开始支持。</p><p>Cgroup 可以限制一个进程组的资源使用，包括 CPU、内存、磁盘IO、网络带宽等。在 Docker 中，Cgroup 主要用来限制容器的相关资源使用。</p><p>Cgroups 分 v1 和 v2 两个版本：</p><ul><li>v1 实现较早，功能比较多，但是由于它里面的功能都是零零散散的实现的，所以规划的不是很好，导致了一些使用和维护上的不便。</li><li>v2 的出现就是为了解决 v1 的问题，在最新的 4.5 内核中，Cgroups v2 声称已经可以用于生产环境了，但它所支持的功能还很有限。</li></ul><p>Cgroups 主要包括下面几部分：<br>*** cgroups 本身：** cgroup 是对进程分组管理的一种机制，一个 cgroup 包含一组进程。</p><ul><li><strong>subsystem：</strong> 一个 subsystem 就是一个内核模块，可以调度、限制和监控特定资源的使用情况，他被关联到一颗 cgroup 树之后，就会在树的每个节点（进程组）上做具体的操作。到目前为止，Linux 支持 12 种 subsystem，比如限制 CPU 的使用时间，限制使用的内存，统计 CPU 的使用情况，冻结和恢复一组进程等。</li><li><strong>hierarchy：</strong> 一个 hierarchy 可以理解为一棵 cgroup 树，树的每个节点就是一个进程组，每棵树都会与零到多个 subsystem 关联，也就是可以对树上的每个节点（进程组）做一些对应的subsystem提供的操作，一个 subsystem 只能附加到 一 个 hierarchy 上面。系统中可以有很多颗 cgroup 树，每棵树都和不同的 subsystem 关联，一个进程可以属于多颗树，即一个进程可以属于多个进程组。当一颗 cgroup 树不和任何 subsystem 关联的时候，意味着这棵树只是将进程进行分组，至于要在分组的基础上做些什么，将由应用程序自己决定，systemd 就是一个这样的例子。</li></ul><p>个人理解：</p><ul><li>cgroup 用于对进程进行分组。</li><li>hierarchy 将多个 cgroup 组成一棵树，并提供控制的相关功能，提供了一个控制模板。</li><li>subsystem 则负责资源限制的工作，将 subsystem 和 hierarchy 绑定后，该 hierarchy 上的所有 cgroup 下的进程都会被 subsystem 给限制，但是限制的值可以自定义。</li><li>子 cgroup 会继承父 cgroup 的 subsystem，但是子 cgroup 之后可以自定义自己的配置。</li></ul><p><img src="/2024/04/18/DockerStudy/Cgroups.png" alt="Cgroups 示例"></p><p>比如上图表示两个 hierarchiy，每一个 hierarchiy 中是一颗树形结构，树的每一个节点是一个 cgroup （比如 cpu_cgrp, memory_cgrp）。</p><ul><li>第一个 hierarchiy attach 了 cpu 子系统和 cpuacct 子系统， 因此当前 hierarchiy 中的 cgroup 就可以对 cpu 的资源进行限制，并且对进程的 cpu 使用情况进行统计。</li><li>第二个 hierarchiy attach 了 memory 子系统，因此当前 hierarchiy 中的 cgroup 就可以对 memory 的资源进行限制。</li></ul><p>在每一个 hierarchiy 中，每一个节点（cgroup）可以设置对资源不同的限制权重（即自定义配置）。比如上图中 cgrp1 组中的进程可以使用 60%的 cpu 时间片，而 cgrp2 组中的进程可以使用 20%的 cpu 时间片。</p><h2 id="Docker-网络"><a href="#Docker-网络" class="headerlink" title="Docker 网络"></a>Docker 网络</h2><p>Docker的网络模式主要有以下几种：</p><table><thead><tr><th>模式</th><th>描述</th></tr></thead><tbody><tr><td>bridge</td><td>为每一个容器分配、设置 IP 等，并将容器连接到一个 docker0 虚拟网桥，默认为该模式。</td></tr><tr><td>host</td><td>容器将不会虚拟出自己的网卡，配置自己的 IP 等，而是使用宿主机的 IP 和端口。</td></tr><tr><td>none</td><td>容器有独立的 Network namespace，但并没有对其进行任何网络设置，如分配 veth pair 和网桥连接，IP 等。</td></tr><tr><td>container</td><td>新创建的容器不会创建自己的网卡和配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。</td></tr></tbody></table><h3 id="bridge-模式"><a href="#bridge-模式" class="headerlink" title="bridge 模式"></a>bridge 模式</h3><p><img src="/2024/04/18/DockerStudy/dockerNetworkBridge.jpeg" alt="Docker bridge网络模式"></p><p>bridge 模式是 Docker 默认的网络模式。在Docker服务启动后，会在主机上创建一个名为 docker0 的虚拟网桥，当我们创建一个bridge网络模式的容器时，首先容器会新建一个网络命名空间，然后它会连接到这个虚拟网桥上。连接的方法是创建一对虚拟网卡veth pair设备，其中一个端口连接到容器内部，命名为eth0，另一个端口连接到docker0上，命名为vethxxx模式的名字。这种虚拟网卡利用内存从来进行数据包的收发，当一端收到数据包后会自动转发给另一端，并对外表现为一个独立的网络设备。同时也会从docker0子网中分配一个 IP 给容器使用，并设置 docker0 的 IP 地址为容器的默认网关。</p><p>当容器需要对外通信时，数据包会先被容器内的 veth 网卡接收，然后通过 veth pair 传递给 docker0，再由 docker0 转发给宿主机的物理网卡，然后会对数据包进行Net包装，表现的就像是主机自己发出的数据包一样，最终到达目的地。</p><p>如果需要同一个主机下的容器之间通信，那么需要设置–link参数来允许容器之间的通信，注意默认情况下，容器之间是无法通信的。而随着Docker 1.9版本的发布，Docker官方推荐使用用户自定义的网络来代替–link参数。</p><p>这种模式下的一个大缺点就在于容器都没有一个公有IP，即和宿主机不处于同一个网段。导致的结果是宿主机以外的世界不能直接和容器进行通信。</p><h3 id="host-模式"><a href="#host-模式" class="headerlink" title="host 模式"></a>host 模式</h3><p>host模式相当于Vmware中的NAT模式，与宿主机在同一个网络中，但没有独立IP地址。启动容器使用host模式，容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。</p><p>使用host模式的容器可以直接使用宿主机的IP地址与外界通信，容器内部的服务端口也可以使用宿主机的端口，不需要进行NAT，host最大的优势就是网络性能比较好，docker host上已经使用的端口就不能再用了，网络的隔离性不好。</p><p><img src="/2024/04/18/DockerStudy/dockerNetworkHost.jpeg" alt="Docker host网络模式"></p><h3 id="none-模式"><a href="#none-模式" class="headerlink" title="none 模式"></a>none 模式</h3><p>使用none模式，Docker 容器拥有自己的 Network Namespace，但是，并不为Docker 容器进行任何网络配置。也就是说，这个 Docker 容器没有网卡、IP、路由等信息。需要我们自己为 Docker 容器添加网卡、配置 IP 等。 None模式示意图:</p><p><img src="/2024/04/18/DockerStudy/dockerNetworkNone.jpeg" alt="Docker none网络模式"></p><h3 id="container-模式"><a href="#container-模式" class="headerlink" title="container 模式"></a>container 模式</h3><p>这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备进行高效通信。 Container模式示意图：</p><p><img src="/2024/04/18/DockerStudy/dockerNetworkContainer.jpeg" alt="Docker container网络模式"></p><h2 id="Docker-存储"><a href="#Docker-存储" class="headerlink" title="Docker 存储"></a>Docker 存储</h2><p>根据前面的UnionFS的介绍我们可知，在默认情况下在Docker容器内创建和修改的所有文件都在可写层，只能在容器内部使用，难以被其他服务共享。且容器删除后，数据也跟着删除。同时写入容器的可写层需要Docker存储驱动管理文件系统。存储驱动使用Linux内核提供的联合文件系统，其性能不如直接写入主机文件系统的Docker卷。所以docker也提供了一些存储持久化的功能，主要可以分为：volume、bind mount和tmpfs。</p><h3 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h3><p>Volumes 是Docker推荐的挂载方式，它在主机中创建一个数据卷，并对应到容器中的一个目录。与把数据存储在容器的可写层相比，使用Volume可以避免增加容器的容量大小，还可以使存储的数据与容器的生命周期独立。Volumes存储在主机文件系统中由Docker管理的位置，在Linux主机上该位置默认就是&#x2F;var&#x2F;lib&#x2F;docker&#x2F;volumes目录，其他非docker进程不能修改该路径下的文件，完全由docker引擎来管理。Volumes支持使用Volumes驱动，可以让用户将数据存储在远程主机或云提供商处等。可以以命名方式或匿名方式挂载卷：</p><ul><li>匿名卷（Anonymous Volumes）:首次挂载容器未指定名称，Docker为其随机指定一个唯一名称。</li><li>命名卷（Named Volumes）:指定明确名称，和匿名卷其他特性相同。</li></ul><p>卷由Docker创建并管理，卷适合以下应用场景。</p><ul><li>在多个正在运行的容器之间共享数据。（数据共享）</li><li>当Docker主机不能保证具有特定目录结构时，卷有助于将Docker主机的配置与容器运行时解耦。（构建新目录与主机不同）</li><li>当需要将容器的数据存储到远程主机或云提供商处，而不是本地时。（可以远程挂载卷，公有云、灾备等场景）</li><li>当需要在两个Docker主机之间备份、恢复或迁移数据时。（主机间备份迁移）</li></ul><h3 id="Bind-mount"><a href="#Bind-mount" class="headerlink" title="Bind mount"></a>Bind mount</h3><p>Bind mount 模式下可以存储在宿主机器任何一个地方，但是会依赖宿主机器的目录结构，不能通过docker CLI 去直接管理，并且非docker进程和docker进程都可以修改该路径下的文件。</p><p>它的特点：</p><ul><li>主机上进程或容器可以随时修改。</li><li>相比Volumes，功能更受限、性能更高。</li><li>绑定挂载运行访问敏感文件。</li></ul><p>绑定挂载适合以下应用场景。</p><ul><li>在主机和容器之间共享配置文件。</li><li>在Docker主机上的开发环境和容器之间共享源代码或构建工件。</li><li>当Docker主机上的目录结构保证与容器要求的绑定挂载一致时。</li></ul><h3 id="tmpfs"><a href="#tmpfs" class="headerlink" title="tmpfs"></a>tmpfs</h3><p>tmpfs挂载仅限于运行Linux操作系统的Docker主机使用，它只存储在主机的内存中，不会被写到主机的文件系统中，因此不能持久保存容器的应用数据。<br>在不需要将数据持久保存到主机或容器中时，tmpfs挂载最合适。<br>如果容器产生了非持久化数据，那么可以考虑使用tmpfs挂载避免将数据永久存储到任何位置，并且通过避免写入容器的可写层来提高容器的性能。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="https://yeasy.gitbook.io/docker_practice/introduction/what">Docker — 从入门到实践</a></li><li><a href="ttps://cloud.tencent.com/developer/article/1492076">万字详解Docker架构原理、功能及使用</a></li><li><a href="https://www.zhihu.com/question/268288911/answer/335458760">请问docker与lxc是什么关系，有什么区别。？</a></li><li><a href="https://www.quanxiaoha.com/docker/why-use-docker.html">Docker 和虚拟机的区别是什么？</a></li><li><a href="https://aws.amazon.com/cn/compare/the-difference-between-docker-vm/">Docker 与虚拟机之间有什么区别？</a></li><li><a href="https://zhuanlan.zhihu.com/p/374924046">手撕docker文件结构 —— overlayFS，image，container文件结构详解</a></li><li><a href="https://www.lixueduan.com/posts/docker/06-cgroups-1/#2-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-cgroups">Docker教程(六)—初探 Linux Cgroups：资源控制的奇妙世界</a></li><li><a href="https://juejin.cn/post/7041923410649153543#heading-7">全网最详细的Docker网络教程详解</a></li><li><a href="https://www.qikqiak.com/k8s-book/docs/7.Docker%E7%9A%84%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%BC%8F.html">Docker 的网络模式</a></li><li><a href="https://juejin.cn/post/6844904095615025159">8讲Docker | 容器数据存储方式</a></li><li><a href="https://zhuanlan.zhihu.com/p/622614825">Docker存储管理</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>Docker</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Docker</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【MIT6.824】lab3 Fault-tolerant Key/Value Service 实现笔记</title>
    <link href="/2024/04/08/MIT6-824lab3A3B/"/>
    <url>/2024/04/08/MIT6-824lab3A3B/</url>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>lab3A的实验要求如下：</p><p>Your first task is to implement a solution that works when there are no dropped messages, and no failed servers.</p><p>You’ll need to add RPC-sending code to the Clerk Put&#x2F;Append&#x2F;Get methods in client.go, and implement PutAppend() and Get() RPC handlers in server.go. These handlers should enter an Op in the Raft log using Start(); you should fill in the Op struct definition in server.go so that it describes a Put&#x2F;Append&#x2F;Get operation. Each server should execute Op commands as Raft commits them, i.e. as they appear on the applyCh. An RPC handler should notice when Raft commits its Op, and then reply to the RPC.</p><p>You have completed this task when you reliably pass the first test in the test suite: “One client”.</p><p>Add code to handle failures, and to cope with duplicate Clerk requests, including situations where the Clerk sends a request to a kvserver leader in one term, times out waiting for a reply, and re-sends the request to a new leader in another term. The request should execute just once. These notes include guidance on duplicate detection. Your code should pass the go test -run 3A tests.</p><p>lab3B的实验要求如下：</p><p>Modify your kvserver so that it detects when the persisted Raft state grows too large, and then hands a snapshot to Raft. When a kvserver server restarts, it should read the snapshot from persister and restore its state from the snapshot.</p><p>总体而言，我们需要在lab2所实现的raft系统上构建一个简单的key-value存储系统，这个系统需要支持客户端的Put&#x2F;Append&#x2F;Get操作，同时需要支持Raft的持久化和快照功能。本系统的要求是线性一致的，即每个动作都能被当做是在一个唯一的时刻进行原子执行的，具体一致性相关的内容，可查看之前的文章：<a href="https://slipegg.github.io/2024/03/28/linearizability/">分布式系统中的线性一致性</a>。<br>代码可以在<a href="https://github.com/slipegg/MIT6.824">https://github.com/slipegg/MIT6.824</a>中得到。所有代码均通过了1千次的测试。</p><h1 id="lab3A-实现"><a href="#lab3A-实现" class="headerlink" title="lab3A 实现"></a>lab3A 实现</h1><p>lab3A不涉及到Raft的快照功能，主要是要完成整个系统功能的构建。在实验时测试3A时，测试代码将会不断调用客户端的Put&#x2F;Append&#x2F;Get操作，然后检查是否所有的操作都被正确执行。</p><p>首先通过一个map来存储key-value，如下中的KVMachine所示：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> KVMachine <span class="hljs-keyword">struct</span> &#123;<br>KV <span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-type">string</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(kv *KVMachine)</span></span> Get(key <span class="hljs-type">string</span>) (<span class="hljs-type">string</span>, Err) &#123;<br>value, ok := kv.KV[key]<br><span class="hljs-keyword">if</span> !ok &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>, ErrNoKey<br>&#125;<br><span class="hljs-keyword">return</span> value, OK<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(kv *KVMachine)</span></span> Put(key <span class="hljs-type">string</span>, value <span class="hljs-type">string</span>) Err &#123;<br>kv.KV[key] = value<br><span class="hljs-keyword">return</span> OK<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(kv *KVMachine)</span></span> Append(key <span class="hljs-type">string</span>, value <span class="hljs-type">string</span>) Err &#123;<br>oldValue, ok := kv.KV[key]<br><span class="hljs-keyword">if</span> !ok &#123;<br>kv.KV[key] = value<br><span class="hljs-keyword">return</span> OK<br>&#125;<br>kv.KV[key] = oldValue + value<br><span class="hljs-keyword">return</span> OK<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">newKVMachine</span><span class="hljs-params">()</span></span> *KVMachine &#123;<br><span class="hljs-keyword">return</span> &amp;KVMachine&#123;<span class="hljs-built_in">make</span>(<span class="hljs-keyword">map</span>[<span class="hljs-type">string</span>]<span class="hljs-type">string</span>)&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>然后是Client端的实现，首先Client在初始化时会随机生成一个数字当做自己的id，同时它也专门维护每个请求的唯一id。Client的Put&#x2F;Append&#x2F;Get操作都是通过RPC调用Server端的Put&#x2F;Append&#x2F;Get操作来实现的，如果Server端返回了错误，告诉当前Server不是leader，那么Client就会重新发送请求到下一个Server去，直到找到leader并执行请求成功了为止。Client端的PutAppend&#x2F;Get操作的实现如下，Get也是类似，就是错误处理稍微不同，不再赘述：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(ck *Clerk)</span></span> PutAppend(key <span class="hljs-type">string</span>, value <span class="hljs-type">string</span>, op <span class="hljs-type">string</span>) &#123;<br>DPrintf(<span class="hljs-string">&quot;&#123;Clinetn-%d&#125; try to %s &#123;&#x27;%v&#x27;: &#x27;%v&#x27;&#125;\n&quot;</span>, ck.clientId, op, key, value)<br>args := PutAppendArgs&#123;Key: key, Value: value, Op: op, ClientId: ck.clientId, RequestId: ck.requestId&#125;<br><span class="hljs-keyword">for</span> &#123;<br><span class="hljs-keyword">var</span> reply PutAppendReply<br><span class="hljs-keyword">if</span> ck.servers[ck.leaderId].Call(<span class="hljs-string">&quot;KVServer.PutAppend&quot;</span>, &amp;args, &amp;reply) &amp;&amp; reply.Err == OK &#123;<br>DPrintf(<span class="hljs-string">&quot;&#123;Clinetn-%d&#125; %s &#123;&#x27;%v&#x27;: &#x27;%v&#x27;&#125; success\n&quot;</span>, ck.clientId, op, key, value)<br>ck.requestId++<br><span class="hljs-keyword">break</span><br>&#125; <span class="hljs-keyword">else</span> &#123;<br>ck.leaderId = (ck.leaderId + <span class="hljs-number">1</span>) % <span class="hljs-type">int64</span>(<span class="hljs-built_in">len</span>(ck.servers))<br>time.Sleep(<span class="hljs-number">100</span> * time.Millisecond)<br>&#125;<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>每个Server端都会维护一个KVMachine，并且也连接到一个专门的raft节点，它的主要作用就是将客户端的请求转化为raft节点的日志，然后等待raft节点将日志提交后接收到raft节点的信息，将日志应用到自己的KVMachine中，然后返回给客户端。</p><p>将客户端请求转化为日志传递给raft部分的代码如下,Get请求也是类似的。注意这里对于重复执行过的Put、Append会直接进行返回，因为运行结果只会是OK，所以直接返回OK即可，而Get请求不需要判断是否重复执行，因为Get请求需要获取的实最新的数据，来一次就执行一次即可。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(kv *KVServer)</span></span> PutAppend(args *PutAppendArgs, reply *PutAppendReply) &#123;<br><span class="hljs-comment">// Your code here.</span><br><span class="hljs-keyword">defer</span> DPrintf(<span class="hljs-string">&quot;&#123;KVServer-%d&#125; finishes %s &#123;%s: %s&#125;, the reply is %v\n&quot;</span>, kv.me, args.Op, args.Key, args.Value, reply)<br>kv.mu.RLock()<br><span class="hljs-keyword">if</span> kv.isDuplicate(args.ClientId, args.RequestId) &#123;<br>kv.mu.RUnlock()<br>reply.Err = OK<br><span class="hljs-keyword">return</span><br>&#125;<br>kv.mu.RUnlock()<br><br>logId, _, isLeader := kv.rf.Start(Op&#123;PutAppendArgs: args&#125;)<br><br><span class="hljs-keyword">if</span> !isLeader &#123;<br>reply.Err = ErrWrongLeader<br><span class="hljs-keyword">return</span><br>&#125;<br><br>DPrintf(<span class="hljs-string">&quot;&#123;KVServer-%d&#125; try to %s &#123;%s: %s&#125; with logId: %d\n&quot;</span>, kv.me, args.Op, args.Key, args.Value, logId)<br><br>kv.mu.Lock()<br>ch_putAppend := kv.getNotifyCh_PutAppend(logId)<br>kv.mu.Unlock()<br><br><span class="hljs-keyword">select</span> &#123;<br><span class="hljs-keyword">case</span> result := &lt;-ch_putAppend:<br>reply.Err = result.Err<br><span class="hljs-keyword">case</span> &lt;-time.After(MaxWaitTime):<br>reply.Err = ErrTimeout<br>&#125;<br><br><span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br>kv.mu.Lock()<br><span class="hljs-built_in">delete</span>(kv.notifyChs_PutAppend, logId)<br>kv.mu.Unlock()<br>&#125;()<br>&#125;<br></code></pre></td></tr></table></figure><p>当raft节点将日志分发给了大部分的节点后，就可以将日志提交，然后提醒Server端将日志应用到自己的KVMachine中。代码如下所示。注意对于Get请求，需要判断这时候节点是不是leader，Term是否还相同，以防止由于applyCh传递时间过长，这时候节点已经不是leader，没有最新的数据了。对于Put、Append操作需要判断是否已经是重复执行过的操作，如果是，直接标记为OK即可，不需要再次执行，同样也需要判断当前还是不是leader，如果是才有权限返回给客户端执行结果。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(kv *KVServer)</span></span> applier() &#123;<br><span class="hljs-keyword">for</span> !kv.killed() &#123;<br><span class="hljs-keyword">select</span> &#123;<br><span class="hljs-keyword">case</span> msg := &lt;-kv.applyCh:<br><span class="hljs-keyword">if</span> msg.CommandValid &#123;<br>kv.mu.Lock()<br><span class="hljs-keyword">if</span> msg.CommandIndex &lt;= kv.lastApplied &#123;<br>DPrintf(<span class="hljs-string">&quot;&#123;KVServer-%d&#125; reveives applied log&#123;%v&#125;&quot;</span>, kv.me, msg)<br>kv.mu.Unlock()<br><span class="hljs-keyword">continue</span><br>&#125;<br>kv.lastApplied = msg.CommandIndex<br><br>op := msg.Command.(Op)<br><span class="hljs-keyword">if</span> op.GetArgs != <span class="hljs-literal">nil</span> &#123;<br>DPrintf(<span class="hljs-string">&quot;&#123;KVServer-%d&#125; apply get %v.&quot;</span>, kv.me, op.GetArgs.Key)<br>value, err := kv.kvMachine.Get(op.GetArgs.Key)<br>reply := GetReply&#123;Err: err, Value: value&#125;<br><br><span class="hljs-keyword">if</span> currentTerm, isLeader := kv.rf.GetState(); isLeader &amp;&amp; currentTerm == msg.CommandTerm &#123;<br><span class="hljs-keyword">if</span> ch, ok := kv.notifyChs_Get[msg.CommandIndex]; ok &#123;<br>ch &lt;- reply<br>&#125;<br>&#125;<br>&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> op.PutAppendArgs != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">var</span> reply PutAppendReply<br><span class="hljs-keyword">if</span> kv.isDuplicate(op.PutAppendArgs.ClientId, op.PutAppendArgs.RequestId) &#123;<br>DPrintf(<span class="hljs-string">&quot;&#123;KVServer-%d&#125; receives duplicated request&#123;%v&#125;\n&quot;</span>, kv.me, msg)<br>reply.Err = OK<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>DPrintf(<span class="hljs-string">&quot;&#123;KVServer-%d&#125; apply %s &#123;%s: %s&#125;.\n&quot;</span>, kv.me, op.PutAppendArgs.Op, op.PutAppendArgs.Key, op.PutAppendArgs.Value)<br><span class="hljs-keyword">if</span> op.PutAppendArgs.Op == <span class="hljs-string">&quot;Put&quot;</span> &#123;<br>reply.Err = kv.kvMachine.Put(op.PutAppendArgs.Key, op.PutAppendArgs.Value)<br>&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> op.PutAppendArgs.Op == <span class="hljs-string">&quot;Append&quot;</span> &#123;<br>reply.Err = kv.kvMachine.Append(op.PutAppendArgs.Key, op.PutAppendArgs.Value)<br>&#125;<br>kv.lastPutAppendId[op.PutAppendArgs.ClientId] = op.PutAppendArgs.RequestId<br>&#125;<br><br><span class="hljs-keyword">if</span> _, isLeader := kv.rf.GetState(); isLeader &#123;<br><span class="hljs-keyword">if</span> ch, ok := kv.notifyChs_PutAppend[msg.CommandIndex]; ok &#123;<br>ch &lt;- reply<br>&#125;<br>&#125;<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>DPrintf(<span class="hljs-string">&quot;&#123;KVServer-%d&#125; receives unknown command&#123;%v&#125;&quot;</span>, kv.me, msg)<br>&#125;<br><br><span class="hljs-keyword">if</span> kv.isNeedSnapshot() &#123;<br>DPrintf(<span class="hljs-string">&quot;&#123;KVServer-%d&#125; needs snapshot\n&quot;</span>, kv.me)<br>kv.snapshot(msg.CommandIndex)<br>&#125;<br>kv.mu.Unlock()<br>&#125; <br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="lab3B-实现"><a href="#lab3B-实现" class="headerlink" title="lab3B 实现"></a>lab3B 实现</h1><p>这里主要需要实现Server的持久化和快照功能，每个Server有一个自己的persister，其结构如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> Persister <span class="hljs-keyword">struct</span> &#123;<br>mu        sync.Mutex<br>raftstate []<span class="hljs-type">byte</span><br>snapshot  []<span class="hljs-type">byte</span><br>&#125;<br></code></pre></td></tr></table></figure><p>其中raftstate部分是raft节点存储自身持久化状态用的，而snapshot节点是用来给Server存储自身状态用的，包括了Server的KVMachine状态以及lastPutAppendId。在Server启动时，会从persister中读取raftstate和snapshot，然后根据raftstate来初始化raft节点，根据snapshot来初始化KVMachine和lastPutAppendId。代码如下所示：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(kv *KVServer)</span></span> reloadBySnapshot(snapshot []<span class="hljs-type">byte</span>) &#123;<br><span class="hljs-keyword">if</span> snapshot == <span class="hljs-literal">nil</span> || <span class="hljs-built_in">len</span>(snapshot) &lt; <span class="hljs-number">1</span> &#123;<br><span class="hljs-keyword">return</span><br>&#125;<br><br><span class="hljs-keyword">var</span> kvMachine KVMachine<br><span class="hljs-keyword">var</span> lastPutAppendId <span class="hljs-keyword">map</span>[<span class="hljs-type">int64</span>]<span class="hljs-type">int64</span><br><br>r := bytes.NewBuffer(snapshot)<br>d := labgob.NewDecoder(r)<br><span class="hljs-keyword">if</span> d.Decode(&amp;kvMachine) != <span class="hljs-literal">nil</span> ||<br>d.Decode(&amp;lastPutAppendId) != <span class="hljs-literal">nil</span> &#123;<br>DPrintf(<span class="hljs-string">&quot;&#123;KVServer-%d&#125; reloadBySnapshot failed\n&quot;</span>, kv.me)<br>&#125;<br><br>DPrintf(<span class="hljs-string">&quot;&#123;KVServer-%d&#125; reloadBySnapshot succeeded\n&quot;</span>, kv.me)<br>kv.lastPutAppendId = lastPutAppendId<br>kv.kvMachine = kvMachine<br>&#125;<br></code></pre></td></tr></table></figure><p>当Server在apply节点时，按照要求，如果raft的日志信息过大，就触发快照功能，将Server的状态保存到snapshot中，同时让raft节点生成快照。如下所示：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(kv *KVServer)</span></span> snapshot(lastAppliedLogId <span class="hljs-type">int</span>) &#123;<br>w := <span class="hljs-built_in">new</span>(bytes.Buffer)<br>e := labgob.NewEncoder(w)<br><span class="hljs-keyword">if</span> mr, lr := e.Encode(kv.kvMachine), e.Encode(kv.lastPutAppendId); mr != <span class="hljs-literal">nil</span> ||<br>lr != <span class="hljs-literal">nil</span> &#123;<br>DPrintf(<span class="hljs-string">&quot;&#123;KVServer-%d&#125; snapshot failed. kvMachine length: %v, result: &#123;%v&#125;, lastPutAppendId: &#123;%v&#125;, result: &#123;%v&#125;,&quot;</span>,<br>kv.me, <span class="hljs-built_in">len</span>(kv.kvMachine.KV), mr, kv.lastPutAppendId, lr)<br><span class="hljs-keyword">return</span><br>&#125;<br><br>data := w.Bytes()<br>kv.rf.Snapshot(lastAppliedLogId, data)<br>DPrintf(<span class="hljs-string">&quot;&#123;KVServer-%d&#125; snapshot succeeded\n&quot;</span>, kv.me)<br>&#125;<br></code></pre></td></tr></table></figure><p>由于快照的引入，Server也可能需要apply快照，即对上述的applier函数再多加一个msg类型的判断，如下所示：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> msg.SnapshotValid &#123;<br>kv.mu.Lock()<br>kv.reloadBySnapshot(msg.Snapshot)<br>kv.lastApplied = msg.CommandIndex<br>kv.mu.Unlock()<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="相关问题"><a href="#相关问题" class="headerlink" title="相关问题"></a>相关问题</h1><h2 id="为什么Get操作不能直接读leader的本地数据？"><a href="#为什么Get操作不能直接读leader的本地数据？" class="headerlink" title="为什么Get操作不能直接读leader的本地数据？"></a>为什么Get操作不能直接读leader的本地数据？</h2><p>在Raft系统中，当面临网络分区情况时，原本的leader如果位于一个小分区，那么他就不知道其实大分区中已经有了一个新leader了，这样如果client还是连接的原本的leader，并且是直接读取该leader的本地数据，那么就会面临读取到过时数据的问题，导致系统线性不一致。</p><p>所以解决这个问题的关键在于确定节点真的是leader，这里采取的是一个简单的方法，即将这个Get操作作为一个log日志放入raft系统中，直到raft系统将这个log日志提交后，才返回。实际上还有优化的空间，一个方法是在raft接受到了一个Get操作后，立刻执行心跳，如果接收到了过半的节点的心跳回复，那么就证明了这个节点是真的leader，这样就可以直接返回数据了，这就避免了将Get操作放入raft系统中的开销。还有一种方法是叫做Lease Read，它的吞吐更大，详情可参考<a href="https://blog.mrcroxx.com/posts/code-reading/etcdraft-made-simple/6-readonly/">深入浅出etcd&#x2F;raft —— 0x06 只读请求优化</a>。</p><h2 id="applier中是否有机会出现重复执行的put、append操作？"><a href="#applier中是否有机会出现重复执行的put、append操作？" class="headerlink" title="applier中是否有机会出现重复执行的put、append操作？"></a>applier中是否有机会出现重复执行的put、append操作？</h2><p>有机会出现。例如当客户端发送后，Server将其提交给了Raft，但是Raft没有在规定时间内返回，那么就会返回超时，然后客户端再去循环提交一轮，再一次提交给这个节点的时候，节点此时可能还是没有收到Raft的返回，所以会再次提交给Raft，这样就会出现重复提交的情况。而在applier中就会只执行第一次提交的操作，后续的提交都会被忽略。</p><h2 id="只用lastPutAppendId记录最后一次的Put、Append操作的id是否可行？"><a href="#只用lastPutAppendId记录最后一次的Put、Append操作的id是否可行？" class="headerlink" title="只用lastPutAppendId记录最后一次的Put、Append操作的id是否可行？"></a>只用lastPutAppendId记录最后一次的Put、Append操作的id是否可行？</h2><p>可行。因为系统中Put、Append操作的结果只会是ok，所以不需要记录每次的Put、Append操作的id，同时由于raft系统中一旦apply了就是永久apply了，并且前面的操作也都apply了，不存在回退的情况，所以如果当前操作的id小于最新一次Put、Append操作的id，那么就说明是重复执行了，直接返回ok即可。</p><h1 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h1><p>代码通过了1k次的测试，如下图所示。</p><p><img src="/2024/04/08/MIT6-824lab3A3B/result.jpg" alt="测试结果"></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://blog.mrcroxx.com/posts/code-reading/etcdraft-made-simple/6-readonly/">深入浅出etcd&#x2F;raft —— 0x06 只读请求优化</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>MIT6.824</category>
      
      <category>Lab</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式</tag>
      
      <tag>MIT6.824</tag>
      
      <tag>Go</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【MIT6.824】lab2C-persistence, lab2D-log compaction 实现笔记</title>
    <link href="/2024/04/08/MIT6-824lab2C2D/"/>
    <url>/2024/04/08/MIT6-824lab2C2D/</url>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>lab2C的实验要求如下</p><p>Complete the functions persist() and readPersist() in raft.go by adding code to save and restore persistent state. You will need to encode (or “serialize”) the state as an array of bytes in order to pass it to the Persister. Use the labgob encoder; see the comments in persist() and readPersist(). labgob is like Go’s gob encoder but prints error messages if you try to encode structures with lower-case field names. For now, pass nil as the second argument to persister.Save(). Insert calls to persist() at the points where your implementation changes persistent state. Once you’ve done this, and if the rest of your implementation is correct, you should pass all of the 2C tests.</p><p>lab2D的实验要求如下</p><p>Implement Snapshot() and the InstallSnapshot RPC, as well as the changes to Raft to support these (e.g, operation with a trimmed log). Your solution is complete when it passes the 2D tests (and all the previous Lab 2 tests).</p><p>总体而言， lab2C需要我们实现关键数据的持久化，lab2D需要我们通过快照实现日志的压缩。代码可以在<a href="https://github.com/slipegg/MIT6.824">https://github.com/slipegg/MIT6.824</a>中得到。所有代码均通过了1千次的测试。</p><h1 id="lab2C-实现"><a href="#lab2C-实现" class="headerlink" title="lab2C 实现"></a>lab2C 实现</h1><p>在实验时测试2C时，测试代码将会尝试将某些节点从网络中断开，然后一段时间后再依据这些断开的节点的持久化的信息重新生成一个新的节点并加入到网络中，测试代码将会检测加入这个节点后是否与预期相同。</p><p>在初始化节点的时候，会传入一个Persister对象，这个对象充当一个硬盘的角色，用于持久化数据，后续在测试重新生成节点时，就需要传入旧节点的Persister对象，以便新节点能够从硬盘中读取旧节点的数据进行复原。</p><p>参考raft论文，我们需要持久化的数据有：</p><ul><li>currentTerm</li><li>votedFor</li><li>log entries</li></ul><p>在raft.go中，我们需要实现persist和readPersist函数，用于持久化和读取数据。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// persist saves Raft&#x27;s persistent state to stable storage,</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> persist() &#123;<br>rf.persister.Save(rf.encodeState(), rf.persister.snapshot)<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> encodeState() []<span class="hljs-type">byte</span> &#123;<br>w := <span class="hljs-built_in">new</span>(bytes.Buffer)<br>e := labgob.NewEncoder(w)<br>e.Encode(rf.currentTerm)<br>e.Encode(rf.votedFor)<br>e.Encode(rf.logs)<br><span class="hljs-keyword">return</span> w.Bytes()<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// readPersist restores previously persisted state.</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> readPersist(data []<span class="hljs-type">byte</span>) &#123;<br><span class="hljs-keyword">if</span> data == <span class="hljs-literal">nil</span> || <span class="hljs-built_in">len</span>(data) &lt; <span class="hljs-number">1</span> &#123; <span class="hljs-comment">// bootstrap without any state?</span><br><span class="hljs-keyword">return</span><br>&#125;<br><span class="hljs-keyword">var</span> currentTerm <span class="hljs-type">int</span><br><span class="hljs-keyword">var</span> votedFor <span class="hljs-type">int</span><br><span class="hljs-keyword">var</span> logs []LogEntry<br>r := bytes.NewBuffer(data)<br>d := labgob.NewDecoder(r)<br><span class="hljs-keyword">if</span> d.Decode(&amp;currentTerm) != <span class="hljs-literal">nil</span> ||<br>d.Decode(&amp;votedFor) != <span class="hljs-literal">nil</span> ||<br>d.Decode(&amp;logs) != <span class="hljs-literal">nil</span> &#123;<br>Debug(dError, <span class="hljs-string">&quot;S%v failed to read persist&quot;</span>, rf.me)<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>Debug(dInfo, <span class="hljs-string">&quot;S%v read persist successfully&quot;</span>, rf.me)<br>rf.currentTerm = currentTerm<br>rf.votedFor = votedFor<br>rf.logs = logs<br>rf.lastApplied = rf.getFirstIndex()<br>rf.commitIndex = rf.getFirstIndex()<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>然后我们需要在每次修改了持久化数据的地方调用persist函数，然后在初始化节点时调用readPersist函数来读取持久化数据，整体难度不大。</p><h1 id="lab2D-实现"><a href="#lab2D-实现" class="headerlink" title="lab2D 实现"></a>lab2D 实现</h1><p>在实验时测试2D时，测试代码在接收到apply的命令id为9结尾时，就会调用节点的Snapshot函数进行快照，将日志压缩。代码需要做到在压缩日志后，仍然能够准确地运行。</p><p>首先需要完成快照生成的函数，如下所示,每次会传入需要快照到的日志index，以及当这个节点为止的状态机的快照数据，系统保证传入的日志index一定是已经apply过的。由于已经将状态机的内容放入到了snapshot中，所以其实包括index在内的前面的所有日志都可以删除了，但是由于在同步日志信息时，需要上一个日志的term信息，所以我们会单独保留id为index的日志的id和term信息，放在logs的第一位。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> Snapshot(index <span class="hljs-type">int</span>, snapshot []<span class="hljs-type">byte</span>) &#123;<br>rf.mu.Lock()<br><span class="hljs-keyword">defer</span> rf.mu.Unlock()<br><span class="hljs-keyword">if</span> index &lt;= rf.getFirstIndex() &#123;<br>Debug(dSnap, <span class="hljs-string">&quot;S%v ignores the snapshot request with end index %v, because the index is not bigger than the first index %v&quot;</span>, rf.me, index, rf.getFirstIndex())<br><span class="hljs-keyword">return</span><br>&#125;<br><br>rf.logs = <span class="hljs-built_in">append</span>([]LogEntry&#123;&#123;index, rf.logs[index-rf.getFirstIndex()].Term, <span class="hljs-literal">nil</span>&#125;&#125;, rf.logs[index-rf.getFirstIndex()+<span class="hljs-number">1</span>:]...)<br>rf.persister.Save(rf.encodeState(), snapshot)<br>Debug(dSnap, <span class="hljs-string">&quot;S%v applies the snapshot with end index %v, now the len(logs)=%v&quot;</span>, rf.me, index, <span class="hljs-built_in">len</span>(rf.logs))<br>&#125;<br></code></pre></td></tr></table></figure><p>由于快照的引入，现在logs中的第一个日志可能不再是0了，所以之前代码中所有从logs中依据日志index获取日志的代码都要修改为：<code>rf.logs[index-rf.getFirstIndex()]</code>。</p><p>同时快照的引入还会导致在leader与follower进行日志同步时，需要的同步的日志可能已经没有了，所以这时候需要直接将整个日志发送给对方。</p><p>需要发送的快照请求如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> genInstallSnapshotRequest() *InstallSnapshotRequest &#123;<br><span class="hljs-keyword">return</span> &amp;InstallSnapshotRequest&#123;<br>Term:             rf.currentTerm,<br>LeaderId:         rf.me,<br>LastIncludeIndex: rf.getFirstIndex(),<br>LastIncludeTerm:  rf.logs[<span class="hljs-number">0</span>].Term,<br>Data:             rf.persister.ReadSnapshot(),<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>follower接收到快照请求后，需要进行如下处理,主要就是检查这个快照有没有过期，是不是真的比自己当前commit的日志还要新，如果是的话，就将自己的日志全部删除，只保留快照中给的最后一个日志，作为logs中的第一个日志，然后再唤起applyCond进行快照的apply。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> InstallSnapshot(request *InstallSnapshotRequest, reply *InstallSnapshotReply) &#123;<br>rf.mu.Lock()<br>Debug(dSnap, <span class="hljs-string">&quot;S%v &#123;term: %v, commitIndex: %v&#125;, received from S%v with InstallSnapshotRequest &#123;%v&#125; &quot;</span>, rf.me, rf.currentTerm, rf.commitIndex, request.LeaderId, request)<br><span class="hljs-keyword">defer</span> rf.mu.Unlock()<br><br>reply.Term = rf.currentTerm<br><span class="hljs-keyword">if</span> request.Term &lt; rf.currentTerm &#123;<br><span class="hljs-keyword">return</span><br>&#125;<br><span class="hljs-keyword">if</span> request.Term &gt; rf.currentTerm &#123;<br>rf.currentTerm = request.Term<br>rf.votedFor = <span class="hljs-number">-1</span><br>rf.persist()<br>&#125;<br>rf.changeState(Follower)<br><br><span class="hljs-keyword">if</span> request.LastIncludeIndex &lt;= rf.commitIndex &#123;<br><span class="hljs-keyword">return</span><br>&#125;<br><br>rf.persister.Save(rf.encodeState(), request.Data)<br>rf.commitIndex = request.LastIncludeIndex<br>rf.logs = []LogEntry&#123;&#123;request.LastIncludeIndex, request.LastIncludeTerm, <span class="hljs-literal">nil</span>&#125;&#125; <span class="hljs-comment">//2D遇到的bug所在</span><br>Debug(dSnap, <span class="hljs-string">&quot;S%v installs snapshot from S%v, now the commitIndex is %v&quot;</span>, rf.me, request.LeaderId, rf.commitIndex)<br><br>rf.waitApplySnapshotRequest = *request<br>rf.applyCond.Signal()<br>&#125;<br></code></pre></td></tr></table></figure><p>如果leader接收到回复表示快照已经更新成功了，那么就更新这个节点的nextIndex和matchIndex。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> handleInstallSnapshotReply(peer <span class="hljs-type">int</span>, request *InstallSnapshotRequest, reply *InstallSnapshotReply) &#123;<br><span class="hljs-keyword">if</span> reply.Term &gt; rf.currentTerm &#123;<br>rf.changeState(Follower)<br>rf.currentTerm = reply.Term<br>rf.votedFor = <span class="hljs-number">-1</span><br>rf.persist()<br>Debug(dWarn, <span class="hljs-string">&quot;S%v found higher term %v in InstallSnapshotReply %v from S%v, changes to follower&quot;</span>, rf.me, reply.Term, reply, peer)<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>rf.nextIndex[peer] = request.LastIncludeIndex + <span class="hljs-number">1</span><br>rf.matchIndex[peer] = request.LastIncludeIndex<br>Debug(dLog, <span class="hljs-string">&quot;S%v has installed snapshot to S%v, now the S%v&#x27;s nextIndex is %v&quot;</span>, rf.me, peer, peer, rf.nextIndex[peer])<br>rf.updateCommitIndexForLeader()<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>注意为了能够有序地进行快照的apply，对原本的applier函数进行了修改，同时增加了waitApplySnapshotRequest来记录最新需要apply的快照请求。</p><p>其主要思想是每次唤起applyCond时，先检查是否有新的快照请求，即waitApplySnapshotRequest的Term是否为-1，如果不为-1，那么就进行快照的apply，快照apply了之后再把waitApplySnapshotRequest的Term设置为-1。如果没有新的快照请求，那么就进行日志的apply。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> applier() &#123;<br><span class="hljs-keyword">for</span> !rf.killed() &#123;<br>rf.mu.Lock()<br><span class="hljs-keyword">for</span> rf.lastApplied &gt;= rf.commitIndex &#123;<br>rf.applyCond.Wait()<br>&#125;<br><br><span class="hljs-keyword">if</span> rf.waitApplySnapshotRequest.Term != <span class="hljs-number">-1</span> &#123;<br><span class="hljs-keyword">if</span> rf.lastApplied &lt; rf.waitApplySnapshotRequest.LastIncludeIndex &#123;<br>rf.mu.Unlock()<br><br>rf.applyCh &lt;- ApplyMsg&#123; <span class="hljs-comment">//Question: two applyCh update way, how to update orderly?</span><br>SnapshotValid: <span class="hljs-literal">true</span>,<br>Snapshot:      rf.waitApplySnapshotRequest.Data,<br>SnapshotTerm:  rf.waitApplySnapshotRequest.LastIncludeTerm,<br>SnapshotIndex: rf.waitApplySnapshotRequest.LastIncludeIndex,<br>&#125;<br><br>rf.mu.Lock()<br>rf.lastApplied = rf.waitApplySnapshotRequest.LastIncludeIndex<br>Debug(dSnap, <span class="hljs-string">&quot;S%v applies snapshot from S%v, now the lastApplied is %v&quot;</span>, rf.me, rf.waitApplySnapshotRequest.LeaderId, rf.lastApplied)<br><br>&#125;<br>rf.waitApplySnapshotRequest = InstallSnapshotRequest&#123;Term: <span class="hljs-number">-1</span>&#125;<br>rf.mu.Unlock()<br>&#125; <span class="hljs-keyword">else</span> &#123;<br>commitIndex, lastApplied := rf.commitIndex, rf.lastApplied<br><span class="hljs-keyword">if</span> rf.getFirstIndex() != <span class="hljs-number">0</span> &amp;&amp; lastApplied+<span class="hljs-number">1</span>-rf.getFirstIndex() &lt;= <span class="hljs-number">0</span> &#123;<br>Debug(dWarn, <span class="hljs-string">&quot;S%v has no log to apply, because lastApplied %v &lt; firstIndex %v&quot;</span>, rf.me, lastApplied, rf.getFirstIndex())<br>rf.mu.Unlock()<br><span class="hljs-keyword">continue</span><br>&#125;<br>entries := <span class="hljs-built_in">make</span>([]LogEntry, commitIndex-lastApplied)<br>Debug(dInfo, <span class="hljs-string">&quot;S%v pre to apply log entries. LastApplied: %v, FirstIndex: %v, commitIndex: %v)&quot;</span>,<br>rf.me, lastApplied, rf.getFirstIndex(), commitIndex)<br><span class="hljs-built_in">copy</span>(entries, rf.logs[lastApplied+<span class="hljs-number">1</span>-rf.getFirstIndex():commitIndex+<span class="hljs-number">1</span>-rf.getFirstIndex()])<br>rf.mu.Unlock()<br><br><span class="hljs-keyword">for</span> _, entry := <span class="hljs-keyword">range</span> entries &#123;<br>rf.applyCh &lt;- ApplyMsg&#123;<br>CommandValid: <span class="hljs-literal">true</span>,<br>Command:      entry.Command,<br>CommandIndex: entry.Index,<br>CommandTerm:  entry.Term,<br>&#125;<br>&#125;<br><br>rf.mu.Lock()<br>Debug(dInfo, <span class="hljs-string">&quot;S%v finishes applying log entries(startId: %v, length: %v), now rf.lastApplied = %v&quot;</span>,<br>rf.me, lastApplied+<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(entries), rf.lastApplied)<br>rf.lastApplied = commitIndex<br>rf.mu.Unlock()<br>&#125;<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="问题记录"><a href="#问题记录" class="headerlink" title="问题记录"></a>问题记录</h2><p>当时写的时候也感觉不是特别复杂，但是后面测试的时候发现这里还是有很多需要注意的点，容易导致错误。快照的引入导致的一个重要的问题是我们现在有两种方式来更新状态机的数据，一种是通过日志的apply，一种是通过快照的apply。</p><p>一开始的写法是在接收到快照请求进行InstallSnapshot的处理的时候新起了一个go协程来直接对快照进行apply，但是这会导致一系列的问题。</p><p>一开始我们对这两者的并发做什么限制，那么这就有可能出现下面这种情况：</p><ol><li>follower节点接受到快照同步请求，并且开启一个协程开始进行快照的apply</li><li>在快照的apply之前，follower节点接收到下一个日志的同步的请求，开始进行日志的apply</li></ol><p>这两个apply的顺序其实是不确定的，很有可能就会出现先进行日志的apply，然后再进行快照的apply，这样就会导致状态机的数据不一致，所以需要控制在快照进行apply的时候，不允许进行日志的apply。</p><p>然后我采用的方法是控制节点的lastApplied值，即在开启协程进行快照的apply前将lastApplied值设置为-1，然后在快照的apply结束后再将lastApplied设置为快照的index值，然后在日志进行apply的时候，对lastApplied进行判断，如果lastApplied值为-1，那么就进行锁等待，直到lastApplied值不为-1，然后再进行日志的apply。但是这种方法在测试的时候会发现，进行1000次测试大约会有0~3次的可能出现错误，错误的原因是在进行日志的apply的时候，需要apply的日志已经在logs中没有了，导致了取值的错误，也就是并发控制没有成功，在进行了快照的apply后，日志的apply依旧在进行。</p><p>经过debug发现这是由于出现了如下这种情况：</p><ol><li>followe节点接收到日志同步的请求，开启一个协程进行日志的apply</li><li>leader节点已经进行了快照，然后由于超时又给该follower节点发送了日志同步的请求</li><li>follower节点接收到快照同步的请求，设置lastApplied为-1，然后开启一个协程进行快照的apply</li><li>follower节点结束了日志的apply，将lastApplied设置为日志的index，然后follower节点继续检查，发现lastApplied不为-1，且lastApplied小于commitIndex，所以继续进行日志的apply,然后在logs中取日志时发现该日志已经没有了，导致错误。</li></ol><p>所以通过lastApplied进行并发控制并不可行，最后采用的方法是添加了snapApplyCount变量，每次在进行快照的apply时，将snapApplyCount加1，快照的apply结束后将snapApplyCount减1，然后在进行日志的apply时，如果snapApplyCount不为0，那么就进入锁等待。</p><p>注意在完成快照的apply后，有可能节点已经接收到了leader同步来的其他日志，所以需要在结束后检查是否有新的日志需要apply，如果需要就唤起日志的apply。最后处理快照同步请求的代码如上述的InstallSnapshot所示，日志apply的代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> applier() &#123;<br><span class="hljs-keyword">for</span> !rf.killed() &#123;<br>rf.mu.Lock()<br><span class="hljs-keyword">for</span> rf.snapApplyCount != <span class="hljs-number">0</span> || rf.lastApplied &gt;= rf.commitIndex &#123;<br>rf.applyCond.Wait()<br>&#125;<br><br>commitIndex, lastApplied := rf.commitIndex, rf.lastApplied<br><span class="hljs-keyword">if</span> rf.getFirstIndex() != <span class="hljs-number">0</span> &amp;&amp; lastApplied+<span class="hljs-number">1</span>-rf.getFirstIndex() &lt;= <span class="hljs-number">0</span> &#123;<br>rf.mu.Unlock()<br><span class="hljs-keyword">continue</span><br>&#125;<br>entries := <span class="hljs-built_in">make</span>([]LogEntry, commitIndex-lastApplied)<br>Debug(dInfo, <span class="hljs-string">&quot;S%v pre to apply log entries. LastApplied: %v, FirstIndex: %v, commitIndex: %v)&quot;</span>,<br>rf.me, lastApplied, rf.getFirstIndex(), commitIndex)<br><span class="hljs-built_in">copy</span>(entries, rf.logs[lastApplied+<span class="hljs-number">1</span>-rf.getFirstIndex():commitIndex+<span class="hljs-number">1</span>-rf.getFirstIndex()])<br>rf.mu.Unlock()<br><br><span class="hljs-keyword">for</span> _, entry := <span class="hljs-keyword">range</span> entries &#123;<br>rf.applyCh &lt;- ApplyMsg&#123;<br>CommandValid: <span class="hljs-literal">true</span>,<br>Command:      entry.Command,<br>CommandIndex: entry.Index,<br>CommandTerm:  entry.Term,<br>&#125;<br>&#125;<br><br>rf.mu.Lock()<br>Debug(dInfo, <span class="hljs-string">&quot;S%v finishes applying log entries(startId: %v, length: %v), now rf.lastApplied = %v&quot;</span>,<br>rf.me, lastApplied+<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(entries), rf.lastApplied)<br>rf.lastApplied = commitIndex<br>rf.mu.Unlock()<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>但是上述方法后面经过测试发现也还是有少量的bug，bug的主要原因在于如下这种情况：</p><ol><li>follower节点接收到最后日志为x的快照同步请求，开启一个协程进行快照的apply</li><li>follower节点又接收到最后日志为x+10的快照同步请求，开启一个协程进行快照的apply</li><li>follower先完成了x+10的快照的apply，然后才完成了x的快照的apply，但是这时候它会将lastApplied设置为x，同时apply的顺序也出现了错误。</li></ol><p>纵观上面的问题的一大根源在于我们出现了多个apply的协程，而没有对协程进行很好的并发控制，所以最后采取了上述的发型，将所有的apply都放在一个协程中进行，优先进行快照的apply，进测试可以准确地通过。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>最终对lab2中所有的测试进行了1000次的测试，全部通过。</p><p><img src="/2024/04/08/MIT6-824lab2C2D/result.jpg" alt="测试结果"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>整个lab2中感觉难度最大的还是lab2B，因为需要实现的功能比较多，需要多多参考raft论文中的论文，最为印象深刻的就是lab2D中的并发问题了，这种问题确实在一开始实现的时候比较难想到，需要通过实验发现，而这种1000次测试才出现一两次错误的问题就更加难发现了，需要有全面的日志记录和多次重复实验的系统才行，后面有机会也分享一下有关日志记录和重复实验相关的内容。</p>]]></content>
    
    
    <categories>
      
      <category>MIT6.824</category>
      
      <category>Lab</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式</tag>
      
      <tag>MIT6.824</tag>
      
      <tag>Go</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes 架构及部署、调度、状态管理流程简介</title>
    <link href="/2024/04/02/k8sArchitecture/"/>
    <url>/2024/04/02/k8sArchitecture/</url>
    
    <content type="html"><![CDATA[<p>Kubernetes简称k8s，是用于自动部署、扩展和管理“容器化应用程序”的开源系统。该系统由Google设计并捐赠给Cloud Native Computing Foundation来使用。 它旨在提供“跨主机集群的自动部署、扩展以及运行应用程序容器的平台”。 它支持一系列容器工具，包括Docker等。它是当前绝对主流的容器管理平台。目前阿里（ACK）、字节（ Gödel ）、美团（LAR）内部的资源管理系统都基于k8s。</p><h1 id="K8s-架构"><a href="#K8s-架构" class="headerlink" title="K8s 架构"></a>K8s 架构</h1><p><img src="/2024/04/02/k8sArchitecture/arc.png" alt="k8s架构图"></p><p>K8s中的一些重要组件的简介如下：</p><ul><li>Etcd：基于Raft的分布式键值存储系统，保存了整个集群的状态。</li><li>Pod：集群中运行部署服务的最小单元，一个Pod可由多个Docker及网络、存储组件组成。</li><li>API Server：所有资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制</li><li>Controller manager：负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；</li><li>Scheduler：负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上；</li><li>Kubelet：负责维护节点内的Pods和他们上面的容器，同时也负责Volume（CVI）和网络（CNI）的管理；</li><li>Kube-Proxy：负责为 Service 提供 cluster 内部的服务发现和负载均衡</li></ul><h2 id="Etcd"><a href="#Etcd" class="headerlink" title="Etcd"></a>Etcd</h2><p>Etcd主要用于保存集群所有的网络配置和对象的状态信息。整个 Kubernetes 系统中一共有两个服务需要用到 etcd 用来协同和存储配置，分别是：</p><ul><li>网络插件 flannel、对于其它网络插件也需要用到 etcd 存储网络的配置信息</li><li>Kubernetes 本身，包括各种对象的状态和元信息配置</li></ul><p>Etcd是基于Raft的分布式键值存储系统，可以查考我之前的<a href="https://slipegg.github.io/2023/12/26/RaftPaperRead/">关于Raft的文章</a>。</p><p>我们在安装 Flannel 的时候配置了 FLANNEL_ETCD_PREFIX&#x3D;”&#x2F;kube-centos&#x2F;network” 参数，这是 Flannel 查询 etcd 的目录地址。</p><p>查看 Etcd 中存储的 flannel 网络信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem <span class="hljs-built_in">ls</span> /kube-centos/network -r<br>2018-01-19 18:38:22.768145 I | warning: ignoring ServerName <span class="hljs-keyword">for</span> user-provided CA <span class="hljs-keyword">for</span> backwards compatibility is deprecated<br>/kube-centos/network/config<br>/kube-centos/network/subnets<br>/kube-centos/network/subnets/172.30.31.0-24<br>/kube-centos/network/subnets/172.30.20.0-24<br>/kube-centos/network/subnets/172.30.23.0-24<br>```查看 flannel 的配置：```bash<br>$ etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/kubernetes.pem --key-file=/etc/kubernetes/ssl/kubernetes-key.pem get /kube-centos/network/config<br>2018-01-19 18:38:22.768145 I | warning: ignoring ServerName <span class="hljs-keyword">for</span> user-provided CA <span class="hljs-keyword">for</span> backwards compatibility is deprecated<br>&#123;<span class="hljs-string">&quot;Network&quot;</span>: <span class="hljs-string">&quot;172.30.0.0/16&quot;</span>, <span class="hljs-string">&quot;SubnetLen&quot;</span>: 24, <span class="hljs-string">&quot;Backend&quot;</span>: &#123; <span class="hljs-string">&quot;Type&quot;</span>: <span class="hljs-string">&quot;host-gw&quot;</span>&#125; &#125;<br></code></pre></td></tr></table></figure><p>Kubernetes 使用 etcd v3 的 API 操作 etcd 中的数据。所有的资源对象都保存在 &#x2F;registry 路径下，如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs stylus">ThirdPartyResourceData<br>apiextensions<span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span><br>apiregistration<span class="hljs-selector-class">.k8s</span><span class="hljs-selector-class">.io</span><br>certificatesigningrequests<br>clusterrolebindings<br>clusterroles<br>configmaps<br>controllerrevisions<br>controllers<br>daemonsets<br>deployments<br>events<br>horizontalpodautoscalers<br>ingress<br>limitranges<br>minions<br>monitoring<span class="hljs-selector-class">.coreos</span><span class="hljs-selector-class">.com</span><br>namespaces<br>persistentvolumeclaims<br>persistentvolumes<br>poddisruptionbudgets<br>pods<br>ranges<br>replicasets<br>resourcequotas<br>rolebindings<br>roles<br>secrets<br>serviceaccounts<br>services<br>statefulsets<br>storageclasses<br>thirdpartyresources<br></code></pre></td></tr></table></figure><p>如果你还创建了 CRD（自定义资源定义），则在此会出现 CRD 的 API。</p><h2 id="API-Server"><a href="#API-Server" class="headerlink" title="API Server"></a>API Server</h2><p>API Server是K8s的核心组件之一，它类似于linux系统中的系统调用。一个核心的API设计原则是所有的API都应该是声明式的，即用户只需要告诉K8s自己想要的状态，而不需要告诉K8s如何去做。例如告诉K8s我需要3个Pod的副本，K8s会自动去创建这3个Pod的副本，而不去告诉K8s我要再新建一个副本，因为API是有可能被丢弃或者重复执行的，但是声明式的API是幂等的，即重复执行的结果是一样的。</p><h2 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h2><p>Pod 是 Kubernetes 中最小的调度单元，它是一个或多个容器的集合（目前K8s也支持了其他类型的虚拟化产品），同时也可以在Pod中包含存储、网络配置等共享资源。Pod 是 Kubernetes 中的原子调度单位，Kubernetes 会将 Pod 中的容器一起调度到同一个节点上，确保它们能够正常运行。</p><p>目前 Kubernetes 中的业务主要可以分为长期伺服型（long-running）、批处理型（batch）、节点后台支撑型（node-daemon）和有状态应用型（stateful application）；分别对应的小机器人控制器为 Deployment、Job、DaemonSet 和 StatefulSet</p><h2 id="Controller-manager"><a href="#Controller-manager" class="headerlink" title="Controller manager"></a>Controller manager</h2><p>Controller Manager 就是集群内部的管理控制中心，由负责不同资源的多个 Controller 构成，共同负责集群内的 Node、Pod 等所有资源的管理，比如当通过 Deployment 创建的某个 Pod 发生异常退出时，RS Controller 便会接受并处理该退出事件，并创建新的 Pod 来维持预期副本数。</p><p>几乎每种特定资源都有特定的 Controller 维护管理以保持预期状态，而 Controller Manager 的职责便是把所有的 Controller 聚合起来：</p><ul><li>提供基础设施降低 Controller 的实现复杂度</li><li>启动和维持 Controller 的正常运行</li></ul><p>Kubernetes 中常见的几种类型的 Controller有如下这些：</p><ul><li>Replication Controller 用于确保在集群中始终运行指定数量的 Pod 实例。如果由于某种原因导致 Pod 实例数低于预期值，Replication Controller 会自动启动新的 Pod 实例，以满足配置的副本数目。</li><li>ReplicaSet 是 Replication Controller 的升级版本，它支持更灵活的 Pod 选择方式，并提供了更强大的标签选择器功能。在新的 Kubernetes 集群中，建议使用 ReplicaSet 而不是 Replication Controller，它一般不单独使用，而是作为 Deployment 的理想状态参数使用。</li><li>Deployment 用于管理应用程序的发布和更新。它可以创建 ReplicaSet，并在需要时启动新的 Pod 实例，以确保应用程序的副本数量符合所需的状态。Deployment 还支持滚动更新、版本回滚等功能，使得应用程序的部署和更新变得更加灵活和可控。</li><li>StatefulSet 用于管理有状态应用程序的部署，例如数据库。与 ReplicaSet 不同，StatefulSet 会为每个 Pod 实例分配稳定的网络标识符和持久化存储，确保在 Pod 重启或迁移时能够保持状态。</li><li>DaemonSet 用于在集群中的每个节点上运行一个副本（或者根据节点标签进行选择）。它通常用于部署一些系统级别的后台服务，如日志收集器、监控代理等。</li><li>Job 用于一次性任务的管理，例如批处理作业。CronJob 则是定时任务的管理器，它可以周期性地执行指定的任务，类似于 Linux 系统中的 cron 任务。</li></ul><h2 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h2><p>kube-scheduler 是 Kubernetes 的调度器，主要负责整个集群资源的调度功能，根据特定的调度算法和策略，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源。</p><p>它的调度流程可以看下面的<a href="#schedule">K8s调度流程:</a>里的内容。</p><p>值得注意的是，由于K8s原生的调度器是只支持一个一个单独调度，所以对于批处理作业调度的场景不受用，例如需要gang调度，即需要同时多个Pod一起上台才能运行，这时一个个调度就有死锁以及浪费资源的问题，所以目前K8s社区退出了<a href="https://volcano.sh/zh/">Volcano</a>调度器，值得去关注一下。</p><h2 id="Kubelet"><a href="#Kubelet" class="headerlink" title="Kubelet"></a>Kubelet</h2><p>Kubelet 是 Kubernetes 集群中每个节点上的代理，负责维护容器的生命周期，同时与容器运行时（如 Docker）进行交互，确保容器正常运行。Kubelet 会定期从 API Server 获取 Pod 的配置信息，然后创建和管理 Pod 中的容器，同时监控容器的状态，并上报给 API Server。</p><h2 id="Kube-Proxy"><a href="#Kube-Proxy" class="headerlink" title="Kube-Proxy"></a>Kube-Proxy</h2><p>Kube-proxy 是 kubernetes 工作节点上的一个网络代理组件，运行在每个节点上。Kube-proxy维护节点上的网络规则，实现了Kubernetes Service 概念的一部分 。它的作用是使发往 Service 的流量（通过ClusterIP和端口）负载均衡到正确的后端Pod。</p><p>kube-proxy 监听 API server 中 资源对象的变化情况，包括以下三种：</p><ul><li>service</li><li>endpoint&#x2F;endpointslices</li><li>node</li><li>然后根据监听资源变化操作代理后端来为服务配置负载均衡。</li></ul><h1 id="K8s-部署流程"><a href="#K8s-部署流程" class="headerlink" title="K8s 部署流程"></a>K8s 部署流程</h1><p>在 Kubernetes 中，一个控制器至少追踪一种类型的 Kubernetes 资源。这些资源对象有一个代表期望状态的 spec 字段。 该资源的控制器负责所属对象当前状态接近期望状态，如下是一个部署nginx的例子：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs yml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-deployment</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span>  <span class="hljs-comment"># 副本数量</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">matchLabels:</span><br>      <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span><br>  <span class="hljs-attr">template:</span><br>    <span class="hljs-attr">metadata:</span><br>      <span class="hljs-attr">labels:</span><br>        <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">containers:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span><br>          <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:latest</span>  <span class="hljs-comment"># Docker版本</span><br>          <span class="hljs-attr">ports:</span><br>            <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span><br>          <span class="hljs-attr">resources:</span><br>            <span class="hljs-attr">requests:</span><br>              <span class="hljs-attr">cpu:</span> <span class="hljs-string">&quot;100m&quot;</span>    <span class="hljs-comment"># 请求 100 毫核 CPU</span><br>              <span class="hljs-attr">memory:</span> <span class="hljs-string">&quot;128Mi&quot;</span>    <span class="hljs-comment"># 请求 128 兆字节内存</span><br>            <span class="hljs-attr">limits:</span><br>              <span class="hljs-attr">cpu:</span> <span class="hljs-string">&quot;200m&quot;</span>    <span class="hljs-comment"># 最大限制 200 毫核 CPU</span><br>              <span class="hljs-attr">memory:</span> <span class="hljs-string">&quot;256Mi&quot;</span>    <span class="hljs-comment"># 最大限制 256 兆字节内存</span><br><br></code></pre></td></tr></table></figure><p>控制器需要保证所属对象一直处于期望状态，但是这就有一个问题，如何探测到变化。因为如何持续通过API Server去查询Etcd中资源的状态，会导致API Server的压力过大，所以K8s采用了Informer的机制。具体如下：</p><ol><li>提前Cache住Etcd中的内容，减少API Server调用。使用 Informer 实例的 Lister() 方法， List&#x2F;Get Kubernetes 中的 Object 时，Informer 不会去请求 Kubernetes API，而是直接查找缓存在本地内存中的数据，依赖Etcd的List&amp;Watch机制，客户端及时获知这些对象的状态变化，然后更新本地缓存，这样就在客户端为这些API对象维护了一份和Etcd数据库中几乎一致的数据，然后控制器等客户端就可以直接访问缓存获取对象的信息，而不用去直接访问apiserver。通过这种方式，Informer 既可以更快地返回结果，又能减少对 Kubernetes API 的直接调用。</li><li>Watch机制及时通知变化。Informer 通过 Kubernetes Watch API 监听某种 resource 下的所有事件。Watch API 本质上就是一种 APIServer 主动向控制器等客户端推送 Kubernetes 资源修改、创建的一种机制。这样我们就可以获取到资源的变更，及时更新对象状态。</li></ol><p><img src="/2024/04/02/k8sArchitecture/deploy.jpg" alt="k8s部署流程图"></p><p>如上图所示，K8s的部署流程主要分为以下几个步骤：</p><ol><li>kubectl向apiserver发送部署请求（例如使用 kubectl create -f deployment.yml）</li><li>apiserver将 Deployment 持久化到etcd；etcd与apiserver进行一次http通信。</li><li>controller manager通过watch api监听 apiserver ，deployment controller看到了一个新创建的deplayment对象更后，将其从队列中拉出，根据deployment的描述创建一个ReplicaSet并将 ReplicaSet 对象返回apiserver并持久化回etcd。以此类推，当replicaset控制器看到新创建的replicaset对象，将其从队列中拉出，根据描述创建pod对象。</li><li>接着scheduler调度器看到未调度的pod对象，根据调度规则选择一个可调度的节点，加载到pod描述中nodeName字段，并将pod对象返回apiserver并写入etcd。</li><li>kubelet在看到有pod对象中nodeName字段属于本节点，将其从队列中拉出，通过容器运行时创建pod中描述的容器。</li></ol><p><a id="schedule"></a></p><h1 id="K8s-调度流程"><a href="#K8s-调度流程" class="headerlink" title="K8s 调度流程"></a>K8s 调度流程</h1><p><img src="/2024/04/02/k8sArchitecture/schedule.png" alt="k8s调度流程图"></p><p>Scheduler 的调度策略启动配置目前支持三种方式，配置文件 &#x2F; 命令行参数 &#x2F; ConfigMap。调度策略可以配置指定调度主流程中要用哪些过滤器 (Predicates)、打分器 (Priorities) 、外部扩展的调度器 (Extenders)，以及最新支持的 SchedulerFramwork 的自定义扩展点 (Plugins)。</p><p>Scheduler 在启动的时候通过 K8s 的 informer 机制以 List+Watch 从 kube-apiserver 及时感知获取调度需要的数据例如：Pods、Nodes、Persistant Volume(PV), Persistant Volume Claim(PVC) 等等，并将这些数据做一定的预处理作为调度器的的 Cache。</p><p>通过 Informer 将需要调度的 Pod 插入 Queue 中，Pipeline 会循环从 Queue Pop 等待调度的 Pod 放入 Pipeline 执行。调度流水线 (Schedule Pipeline) 主要有三个阶段。</p><center>Scheduler Thread→Wait Thread →Bind Thread</center><p>在Scheduler Thread阶段，对Pod一个个串行调度，流程为 Filter -&gt; Score -&gt; Reserve：</p><ol><li>Filter：筛选出符合 Pod Spec 描述的 Nodes</li><li>Score：对筛选出的 Nodes 进行打分和排序</li><li>Reserve：将Pod调度到得分最高的Node中，并更新自己的NodeCache</li></ol><p>在Wait Thread（异步并行）阶段：等待 Pod 关联的资源的就绪。例如等待 PVC 的 PV 创建成功，或者 Gang 调度中等待关联的 Pod 调度成功等等。<br>在Bind Thread（异步并行）阶段：将 Pod 和 Node 的关联持久化 Kube APIServer，如果失败则重新调度。</p><h1 id="K8s-状态管理"><a href="#K8s-状态管理" class="headerlink" title="K8s 状态管理"></a>K8s 状态管理</h1><p>K8s中对Node节点的状态管理主要有两种方式，一种是通过Lease（租约）的方式，一种是通过NodeStatus上报的方式。</p><p>节点中的kubelet通过Lease更新维持存活状态（2019年 k8s v1.17 正式加入）。具体来说，每个节点有一个lease对象，各节点的kubelet定期更新自己的lease对象（默认10s一次），每次更新的内容较少，比较轻量。如果没有及时收到更新，就可以怀疑Node损坏，开始进一步处理。</p><p>同时kubelet也会定期（默认10秒）计算一次NodeStatus（时间独立计算），只有发生有意义的变化或者不上报持续时间超过了参数node-status-update-period（默认5m）时，kubelet才上报NodeStatus。NodeStatus上报数据大，一般包含节点的资源状态、运行的Pod信息、节点Ip、节点版本等。</p><p>注意Lease机制的提出主要是为了解决大规模场景下频繁进行大型NodeStatus上报导致的性能问题，同时也可以更快的发现节点的异常。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="https://jimmysong.io/kubernetes-handbook/concepts/">Kubernetes 架构</a></li><li><a href="https://blog.ihypo.net/15763910382218.html">Kubernetes Controller Manager 工作原理</a></li><li><a href="Volcano">Volcano</a></li><li><a href="https://zhuanlan.zhihu.com/p/338462784">一文看懂 Kubelet</a></li><li><a href="https://zhuanlan.zhihu.com/p/337806843">一文看懂 Kube-proxy</a></li><li><a href="https://zhuanlan.zhihu.com/p/410211543">K8s deployment部署一个pod的流程</a></li><li><a href="https://juejin.cn/post/6844904041097461774">从零开始入门 K8s | 调度器的调度流程和算法介绍</a></li><li><a href="https://www.cnblogs.com/WJQ2017/p/17090603.html">kubelet上报心跳机制</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>k8s</category>
      
      <category>基础知识</category>
      
    </categories>
    
    
    <tags>
      
      <tag>k8s</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>分布式系统中的线性一致性</title>
    <link href="/2024/03/28/linearizability/"/>
    <url>/2024/03/28/linearizability/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><strong>分布式一致性</strong>指的是在同一个系统中，不同客户端操作看到的数据之间的关系。<br>这里重点关注的是网络键值存储系统中的一致性模型。</p><p>首先为什么这有可能出现问题呢？主要是来源于分布式系统中特有的问题，包括：</p><ul><li>并发读&#x2F;写</li><li>副本</li><li>缓存</li><li>故障、恢复</li><li>重传</li><li>…</li></ul><p>这些问题就有可能导致系统出现意想不到的场景，比如如下是一个简单的生产者代码：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">put</span><span class="hljs-params">(<span class="hljs-string">&quot;result&quot;</span>, <span class="hljs-number">27</span>)</span></span><br><span class="hljs-function"><span class="hljs-title">put</span><span class="hljs-params">(<span class="hljs-string">&quot;done&quot;</span>, true)</span></span><br></code></pre></td></tr></table></figure><p>下面是一个消费者代码：</p><figure class="highlight isbl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs isbl"><span class="hljs-variable"><span class="hljs-keyword">while</span></span> <span class="hljs-function"><span class="hljs-title">get</span>(<span class="hljs-string">&quot;done&quot;</span>) <span class="hljs-variable">!</span>= <span class="hljs-variable"><span class="hljs-literal">true</span></span></span><br><span class="hljs-function">    <span class="hljs-variable">pause</span></span><br><span class="hljs-function"><span class="hljs-variable">v</span> = <span class="hljs-title">get</span>(<span class="hljs-variable"><span class="hljs-class">result</span></span>)</span><br></code></pre></td></tr></table></figure><p>在分布式系统中，如果不清楚系统的一致性模型，那么v的值实际上也是难以确认的，因为消费者在读取的时候可能是在一个副本上读取，而生产者在写入的时候可能是在另一个副本上写入，这样就有可能出现v的值不是27的情况。</p><h1 id="线性一致性（linearizability）"><a href="#线性一致性（linearizability）" class="headerlink" title="线性一致性（linearizability）"></a>线性一致性（linearizability）</h1><p>在线性一致性模型中每个操作似乎都在调用和响应之间的某个时刻以原子方式即时执行。<br>它是一种强一致性模型，它符合程序员的直觉，但是也会导致排除掉一些可能的优化机会。</p><p>如图1所示是一个多客户端并发操作的例子，注意每个方框的最左边是操作开始的时刻，最右边是操作成功获得相应的时刻。</p><p><img src="/2024/03/28/linearizability/history-1.svg" alt="图1"></p><p>这段历史是线性一致的，因为如图2中的橙色所示，我们可以在每个操作中找到一个可能执行的时间点，组成这样的正确的顺序历史：Put(“x”, “0”), Get(“x”) -&gt; “0”, Put(“x”, “1”), Get(“x”) -&gt; “1”。</p><p><img src="/2024/03/28/linearizability/history-1-linearization.svg" alt="图2"></p><p>而相反的下面图3这个就不是一个线性一致的历史，因为无法找到一个正确的顺序历史。</p><p><img src="/2024/03/28/linearizability/history-2.svg" alt="图3"></p><p>对于由于网络等因素导致的重传，如下：</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs gherkin">C1: |<span class="hljs-string">--------Wx1---------</span>|<span class="hljs-string"> （由于重传）</span><br><span class="hljs-string">C2：</span>|<span class="hljs-string">-Wx2-</span>|<span class="hljs-string"> </span><br><span class="hljs-string">C3：</span>|<span class="hljs-string">-Rx1-</span>|<span class="hljs-string"> </span>|<span class="hljs-string">-Rx2-</span>|<span class="hljs-string"> </span>|<span class="hljs-string">-Rx1-</span>|<br></code></pre></td></tr></table></figure><p>这理论上是可能存在的，但是如果系统表现成这样，那么这个系统就不是线性一致的。所以这也就要求了线性一致性系统中必须要处理重传的重复请求。</p><p>线性一致性系统的主要优点在于：</p><ol><li>客户端读取到的都是最新的值</li><li>所有的客户端看到的都是相同的数据</li><li>所有客户端以相同的顺序看到数据的更改</li></ol><p>如何实现线性一致性呢？主要是需要依赖于一个不会奔溃的串行服务器，让它来为所有童虎到达的客户端请求选择执行的顺序，并按顺序依次执行，一次一个，在开始下一个之前回复每一个请求。</p><p>但是如果想要容错就需要备份了，所有请求都发送到主服务器，选择串行顺序，然后转发到备份，备份以相同的顺序执行，主服务器仅在备份都执行后才回复客户端。</p><p>事实上由于请求都发往主服务器，而不能利用备份服务器的能量，所以主服务器可能会成为瓶颈。</p><h1 id="线性一致性测试"><a href="#线性一致性测试" class="headerlink" title="线性一致性测试"></a>线性一致性测试</h1><p>对系统进行线性一致性的检查是一个NP难的问题。<br>对其NP难的证明可以从一个子集问题出发进行说明，如下是一个系统的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Adder</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self._total = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">add</span>(<span class="hljs-params">self, value</span>):<br>        self._total += value<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self._total<br></code></pre></td></tr></table></figure><p>考虑如图4所示的历史：</p><p><img src="/2024/03/28/linearizability/subset-sum.svg" alt="图4"></p><p>那么这个线性检测的问题就转化为了一个集合S&#x3D;{s<sub>1</sub>, s<sub>2</sub>, …, s<sub>n</sub>}，是否存在一个子集S’，使得S’的和等于目标值t。<br>当且仅当子集和问题的答案为“是”时，该历史才可线性化。即系统中存在于这个子集中的Add操作在Get操作之前执行，其余的Add操作在Get操作之后执行。</p><p>由于存在这种NP难的特性，对于一个分布式系统中的测试往往是通过多样化的执行、故障注入来完成的。即在进行了一系列的操作后，记录历史日志，然后对历史日志进行线性化检测，看是否存在一个合理的操作顺序。</p><h1 id="其他一致性模型"><a href="#其他一致性模型" class="headerlink" title="其他一致性模型"></a>其他一致性模型</h1><p>除了线性一致性外，还有其他一致性模型，比如：</p><ol><li><strong>Eventual Consistency（最终一致性）：</strong><br> 在最终一致性模型中，系统保证如果没有新的更新操作发生，那么最终所有节点都会收敛到相同的值。这意味着系统可能会在一段时间内出现不一致状态，但最终会达到一致状态。最终一致性通常用于分布式系统中为了提高性能而牺牲一致性的情况。</li><li><strong>Causal Consistency（因果一致性）：</strong><br> 因果一致性是相对于事件发生的因果关系来保证的一致性模型。如果事件 A 在事件 B 之前发生，那么任何观察到事件 B 的节点都必须也能够观察到事件 A。因此，因果一致性要求对于有因果关系的事件必须保持一致性，而对于无因果关系的事件则允许并发。</li><li><strong>Fork Consistency（分支一致性）：</strong><br> 分支一致性是一种介于最终一致性和因果一致性之间的模型。它允许系统中的分支出现，即允许有多个不同的历史状态。在分支一致性中，系统保证对于单个分支内部的一致性，并且最终所有分支都会收敛到一致状态。</li><li><strong>Serializability（串行化一致性）：</strong><br> 串行化一致性是指系统保证所有并发执行的操作按照某种顺序执行时，产生的结果与它们依次顺序执行时的结果相同。这意味着系统能够模拟所有操作按照某个全局顺序执行的效果，从而保证了强一致性。</li><li><strong>Sequential Consistency（顺序一致性）：</strong><br> 顺序一致性是一种强一致性模型，要求系统中的所有操作必须按照它们发生的顺序进行执行。即使是在分布式系统中，对于每个节点来说，所有操作的执行顺序也必须与它们在全局中发生的顺序相一致。</li></ol><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="https://anishathalye.com/testing-distributed-systems-for-linearizability/">测试分布式系统的线性化</a></li><li><a href="http://nil.csail.mit.edu/6.5840/2023/notes/l-linearizability.txt">6.5840 2023 Lecture 9: Consistency, Linearizability</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>MIT6.824</category>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式</tag>
      
      <tag>MIT6.824</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【MIT6.824】lab2B-log replication 实现笔记</title>
    <link href="/2024/03/19/MIT6-824lab2B/"/>
    <url>/2024/03/19/MIT6-824lab2B/</url>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>lab2B的实验要求如下：</p><p>Implement the leader and follower code to append new log entries, so that the go test -run 2B tests pass.</p><ul><li><strong>Hint:</strong> Run git pull to get the latest lab software.</li><li><strong>Hint:</strong> Your first goal should be to pass TestBasicAgree2B(). Start by implementing Start(), then write the code to send and receive new log entries via AppendEntries RPCs, following Figure 2. Send each newly committed entry on applyCh on each peer.</li><li><strong>Hint:</strong> You will need to implement the election restriction (section 5.4.1 in the paper).</li><li><strong>Hint:</strong> One way to fail to reach agreement in the early Lab 2B tests is to hold repeated elections even though the leader is alive. Look for bugs in election timer management, or not sending out heartbeats immediately after winning an election.</li><li><strong>Hint:</strong> Your code may have loops that repeatedly check for certain events. Don’t have these loops execute continuously without pausing, since that will slow your implementation enough that it fails tests. Use Go’s condition variables, or insert a time.Sleep(10 * time.Millisecond) in each loop iteration.</li><li><strong>Hint:</strong> Do yourself a favor for future labs and write (or re-write) code that’s clean and clear. For ideas, re-visit our the Guidance page with tips on how to develop and debug your code.</li><li><strong>Hint:</strong> If you fail a test, look over the code for the test in config.go and test_test.go to get a better understanding what the test is testing. config.go also illustrates how the tester uses the Raft API.</li></ul><p>主要的要求就是在lab2A完成领导者选举的基础上实现日志的复制，代码可以在<a href="https://github.com/slipegg/MIT6.824">https://github.com/slipegg/MIT6.824</a>中得到。</p><h1 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a>测试环境</h1><p>由于整个lab是在模拟环境中进行的，所以我们需要先简单连接一下实验是如何测试和运行的。</p><p>查看测试脚本可知，每次客户端提交用户请求都是通过调用leader的Start函数来实现的，故Start函数在接收到了日志后就需要主动开始日志复制的过程。</p><p>而当leader将日志复制到大多数节点后，除了各个节点自己需要标定这个日志已经提交了外，还需要将这个日志已经提交了的信息返回给客户端，这个信息的结构为ApplyMsg，它通过applyCh这个channel来实现的，实际代码中我们可能需要启用一个go协程来在需要时进行异步执行，如下所示。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> ApplyMsg <span class="hljs-keyword">struct</span> &#123;<br>CommandValid <span class="hljs-type">bool</span><br>Command      <span class="hljs-keyword">interface</span>&#123;&#125;<br>CommandIndex <span class="hljs-type">int</span><br><br><span class="hljs-comment">// For 2D:</span><br>SnapshotValid <span class="hljs-type">bool</span><br>Snapshot      []<span class="hljs-type">byte</span><br>SnapshotTerm  <span class="hljs-type">int</span><br>SnapshotIndex <span class="hljs-type">int</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> applier() &#123;<br><span class="hljs-keyword">for</span> !rf.killed() &#123;<br>rf.mu.Lock()<br><span class="hljs-keyword">for</span> rf.lastApplied &gt;= rf.commitIndex &#123;<br>rf.applyCond.Wait()<br>&#125;<br><br>commitIndex, lastApplied := rf.commitIndex, rf.lastApplied<br>entries := <span class="hljs-built_in">make</span>([]LogEntry, commitIndex-lastApplied)<br><span class="hljs-built_in">copy</span>(entries, rf.logs[lastApplied+<span class="hljs-number">1</span>:commitIndex+<span class="hljs-number">1</span>])<br>Debug(dTest, <span class="hljs-string">&quot;S%v commitIndex: %v, lastApplied: %v, entries: %v&quot;</span>, rf.me, commitIndex, lastApplied, entries)<br>rf.mu.Unlock()<br><br><span class="hljs-keyword">for</span> _, entry := <span class="hljs-keyword">range</span> entries &#123;<br><span class="hljs-comment">// entry: committed log entry</span><br>rf.applyCh &lt;- ApplyMsg&#123;<br>CommandValid: <span class="hljs-literal">true</span>,<br>Command:      entry.Command,<br>CommandIndex: entry.Index,<br>&#125;<br>&#125;<br><br>rf.mu.Lock()<br>Debug(dCommit, <span class="hljs-string">&quot;S%v applies log entries(startId: %v, length: %v)&quot;</span>, rf.me, lastApplied+<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(entries))<br>rf.lastApplied = commitIndex<br>rf.mu.Unlock()<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="日志复制代码"><a href="#日志复制代码" class="headerlink" title="日志复制代码"></a>日志复制代码</h1><p>在具体实现时，发现Raft论文中的代码结构图对于编写整个代码非常有帮助，所以在这里也贴出来。</p><p><img src="/2024/03/19/MIT6-824lab2B/RaftStruct.jpg" alt="Raft论文中的代码结构图"></p><p>项目的整体流程及思路我整理成了如下的过程.</p><p><img src="/2024/03/19/MIT6-824lab2B/p1.jpg" alt="客户端发送日志后触发日志复制"></p><ul><li><strong>Start:</strong> leader节点接受新的命令，将命令添加到自己的日志中，并向其他所有节点发送日志复制请求。</li><li><strong>ReplicateLog:</strong> 由于日志复制可能会失败，所以需要一个循环来不断重试，直到日志复制成功。复制时通过RPC来调度其他节点的AppendEntires函数，并在得到返回结果后调用handleAppendResponse进行处理.</li></ul><p><img src="/2024/03/19/MIT6-824lab2B/p2.jpg" alt="节点处理日志添加请求及处理日志添加返回结果"></p><ul><li><strong>AppendEntires:</strong> 节点接受到日志复制请求后，必须先要判定对方是不是term大于等于自己的term，如果不是就是过时的leader，直接拒绝。由于这个函数也会被当做心跳包来使用，所以收到后还需要自动转变为follower。然后再判断leader中记录的要复制的前一个日志preLog是不是和自己的一致，如果不一致，有两种可能：<ol><li>自己的日志更短，都没复制到preLog这一步，所以直接拒绝，并主动在reply中返回自己的最后一个日志的id。</li><li>有preLog，但是日志的term不一致，也就是自己有冗余的错误的日志，这里是可删可不删后面的日志，论文中采取的方法是返回错误，然后一个一个往前找对得上的日志，而由于这个速度太慢了，会导致测试不通过，所以这里采取的方法是返回本节点的preLog的term，这个term肯定是比leader更小的，然后等待leader找到这个term的最后一个日志，尝试全部复制过来，这样可以加速日志的复制。<br>如果都一致了，就加入到自己的日志中，然后再去对比参看leader有没有新commit日志，如果有就更新自己的commitIndex，并将他们apply，返回给客户端。</li></ol></li><li><strong>handleAppendResponse:</strong> 这个函数是用来处理AppendEntires的返回结果的，首先需要判定自己还是不是leader，term有没有变，如果不是leader或者term变了，就不继续处理。如果返回成功，那么就更新自身记录的对应节点的matchId和nextId，如果返回失败,则对应有几种可能：<ol><li>发现比自己term还高的节点，说明自己的term过时了，需要转变为follower。</li><li>对方的日志比自己以为的要短，需要将该节点nextId往前移动到对应的位置。</li><li>对方的preLog的term不一致，那么就需要找到perLog的term对应的最后一个日志，然后将nextId移动到这个位置的下一个位置。</li></ol></li></ul><p><img src="/2024/03/19/MIT6-824lab2B/p3.jpg" alt="leader尝试更新日志及新的投票规则"></p><ul><li><strong>TryApplyForLeader:</strong> 这里把所有的matchId倒序排序，然后查看中间这个位置的matchId是不是变化了，如果变化了，就去查看这个最新的newCommitId对应的term是不是等于自己的term，如果不是，还是不处理，这对应了论文中的5.4.2节中的规则；如果是，就更新自己的commitIndex，并将这个日志apply。</li><li><strong>Vote:</strong> 如果投票的请求的term比自己的term小，那么就是过时的请求，直接拒绝。如果相同，同时自己已经投票给了其他节点，那么也拒绝。如果请求的term比自己的term大，那么就转变为follower，并更新term。（这一步是为了处理有些时候有的节点进入了网络分区，term不断变大，新接入之后发起投票，这是否应该大伙都转为follower，但是可能受限于日志不符合，不能成为leader，然后大家在新的term中重新选举，统一得到一个leader。）然后检查请求的前一个日志是否是最新的，最新的规则是要么这个前一个日志的term比自己的term大，要么term相同但是index相同或者更大，如果不是，就拒绝。最后如果都通过了，就投票给对方。</li></ul><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>使用了之前写的自动测试实验脚本的测试结果如下，可以看到它通过了1000次的测试。</p><p><img src="/2024/03/19/MIT6-824lab2B/result.jpg" alt="实验结果"></p><h1 id="测试实例-TestBackup2B分享"><a href="#测试实例-TestBackup2B分享" class="headerlink" title="测试实例-TestBackup2B分享"></a>测试实例-TestBackup2B分享</h1><p>测试过程中被TestBackup2B困扰了很久这里分享一下这个测试示例。</p><p><img src="/2024/03/19/MIT6-824lab2B/p4.jpg" alt="TestBackup2B"></p><p>可以看到测试主要是通过网络中断来进行测试的。一开始这个测试没通过，经过查看日志得知是在T5时刻没有选举出正确的leader，两个原因：</p><ol><li>在选举时判断每个节点最后的日志是不是最新的时候写错了，只比较自己的最后的日志相较于Candidate的最后的日志是不是一样长，而没有比较Candidate的最后的日志的term是不是比自己的大，这就导致了在T5时刻本来应该只有S2才能被选举出来，但是我确认S0和S1也可以被选举出来。</li><li>在比较最后的日志前没有提前先判断对方的term是不是比自己的term大，由于S1在disconnect阶段一直在尝试重新选举，所以其term会很大，如果其他节点在接收到S1投票请求后没有转变为follower，并更新term会导致S1一直拒绝其他人的选举投票，而自己又由于日志不够新，被S2拒绝，导致选举一直无法完成。</li></ol>]]></content>
    
    
    <categories>
      
      <category>MIT6.824</category>
      
      <category>Lab</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式</tag>
      
      <tag>MIT6.824</tag>
      
      <tag>Go</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读】Not All Resources are Visible:Exploiting Fragmented Shadow Resources in Shared-State Scheduler Architecture</title>
    <link href="/2024/03/14/ShadowResources/"/>
    <url>/2024/03/14/ShadowResources/</url>
    
    <content type="html"><![CDATA[<h1 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h1><p><strong>论文地址：</strong> <a href="https://dl.acm.org/doi/10.1145/3620678.3624650">Not All Resources are Visible: Exploiting Fragmented Shadow Resources in Shared-State Scheduler Architecture</a></p><p><strong>收录会议：</strong> 云计算顶会-ACM Symposium on Cloud Computing(SoCC2023)</p><p><strong>作者：</strong> 交通大学-李超教授团队</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p><strong>资源</strong>：一个集群中有成千上万台机器。</p><p><strong>请求：</strong> 百万级的请求并发，请求需求更低，运行时间更短，到达秒级甚至毫秒级的运行时间，如下图所示。</p><p><img src="/2024/03/14/ShadowResources/requestTrace.png" alt="图1. a为谷歌集群数据集中任务的cpu和内存占用率的分布，b为阿里巴巴集群数据集中任务运行时间的分布"></p><p><strong>调度：</strong> 主要有三种集群调度架构，如图2所示，单体式架构由于拓展性和灵活性不足，不适合大规模集群，两级式架构资源利用率低，共享状态架构具有高拓展性，更为流行。</p><p><img src="/2024/03/14/ShadowResources/scheduleArc.png" alt="图2. a为单体式架构，b为两级式架构，c为共享状态调度架构"></p><p>共享状态架构的具体介绍：</p><ul><li>有一个管理员维护了中央状态视图CSV。</li><li>存在多个并行调度器，调度范围是全局，调度依据是本地状态视图LSV，并将调度决策提交到CSV中，以避免与其他调度器发生冲突。</li><li>CSV 定期更新每个调度器拥有的本地状态视图 (LSV)，并具有固定的更新延迟。</li><li>最初的共享状态设计会在每次成功的资源分配操作时更新，但在实际集群中，更新延迟通常为秒级，以减少更新带来的系统开销。</li></ul><p>对低开销的追求使得每个调度器的LSV间歇性地过时，因为它们在更新延迟内对于已释放资源的最新状态是不可见的，本文将这些资源称为“影子资源”。<br>影子资源的数量与同步时间间隔和任务运行时间有关，根据理论和实验分析，它会占到已分配资源的2~13%，同时减少同步时间间隔会给调度系统带来较高的同步开销，难以实现，同时云中任务粒度小，会导致影子分散，所以考虑对其的利用是必要的。</p><p>但是之前的相关研究主要集中在优化调度策略来有效管理可见资源，如更高的利用率，更低的延迟，而忽略了对于影子资源利用的研究。</p><h1 id="创新与贡献"><a href="#创新与贡献" class="headerlink" title="创新与贡献"></a>创新与贡献</h1><p>考虑利用影子资源有两点需要注意：</p><ol><li>由于影子资源存在时间短，所以需要敏捷、高效地利用。</li><li>要灵活、透明，避免干扰正常的调度。</li></ol><p>故本文提出了RMiner机制来对影子资源进行利用，它包含三个协作组件：</p><ol><li>shadow resource manager：负责收集影子资源。</li><li>RM filter：筛选适合给影子资源利用的任务。</li><li>RM scheduler：负责将筛选出的任务分配给影子资源。</li></ol><p>针对不同的集群管理目标，RMiner 提供 SafeRM 和 SmartRM 两种资源挖掘模式，以平衡资源利用率最大化和冲突最小化</p><p>总体创新点如下：</p><ol><li>发现了共享状态调度架构中的影子资源，并从理论上和实验上对其进行了分析。</li><li>提出了RMiner机制，对影子资源进行了利用。</li><li>构建了RMiner的模拟器，实验证明了RMiner可以以较小的开销极大地提高集群性能。</li></ol><h1 id="理论分析"><a href="#理论分析" class="headerlink" title="理论分析"></a>理论分析</h1><p>经过理论分析，本文得出了瞬时影子资源的期望式：</p><p><img src="/2024/03/14/ShadowResources/expectation.jpg" alt="影子资源期望式"></p><p>即影子资源总量主要与状态更新延迟d<sub>u</sub>和集群中分配的资源数量r<sub>run</sub>成正比，而与任务的平均执行时间η成反比。</p><p>根据谷歌数据集、fuxi2.0、Borg的数据，假设任务的执行时间为4s<del>5s,状态更新延迟为0.3\</del>1.0s，那么影子资源占已分配资源的3~12.5%。<br>这是值得被考虑的，同时随着轻量云任务的增多，影子资源的利用也会变得更加重要。</p><p>通过对阿里巴巴集群数据集的随机采样，并进行了10中不同配置的实验，记录了影子资源的分布情况，与实验分析基本一致，如图3所示。</p><p><img src="/2024/03/14/ShadowResources/fig3.jpg" alt="图3. 影子资源理论与实验结果对比"></p><h1 id="RMiner机制设计"><a href="#RMiner机制设计" class="headerlink" title="RMiner机制设计"></a>RMiner机制设计</h1><p><img src="/2024/03/14/ShadowResources/fig4.jpg" alt="图4. RMiner机制"></p><p>RMiner机制包含三个协作组件，如图4中的蓝色所示：</p><ol><li>Shadow Resource Manager 通过新设计的索引，探测并管理最新的影子资源。</li><li>RM Filter选择适合影子资源的任务（RM Tasks）到任务队列中</li><li>RM Scheduler负责灵活地将影子资源分配给RM任务。</li></ol><p>RMiner的两大设计原则：</p><ol><li>避免入侵：不对原始调度系统做侵入式修改，原始调度系统依旧不需要知道影子资源的存在，并且不会和影子资源的分配发生冲突。</li><li>平衡性能：RMiner 面临着最大化利用不可见资源和最小化与可见调度冲突的权衡。过度利用影子资源可能会导致大量抢占式执行，从而降低集群的整体性能，反之亦然。</li></ol><h2 id="Shadow-Resource-Manager"><a href="#Shadow-Resource-Manager" class="headerlink" title="Shadow Resource Manager"></a>Shadow Resource Manager</h2><p><img src="/2024/03/14/ShadowResources/fig5.jpg" alt="图5. Shadow Resource Manager"></p><p>Shadow Resource Manager 通过图5所示的6种索引来探测并管理最新的影子资源：</p><ol><li>Shadow Resource Id: 用于标识影子资源的唯一ID。</li><li>Survival Time: 用于标识影子资源的存活时间。</li><li>Machine Id: 用于标识影子资源所在的机器。(注意同一机器的影子资源会被合并)</li><li>Available Resource: 用于标识影子资源的可用资源。</li><li>Occupied Resource: 用于标识影子资源的已占用资源。</li><li>Allocated Tasks: 用于标识影子资源已分配的任务。</li></ol><p>当中央状态视图同步集群状态并监控资源R的释放时，影子状态视图立即通过Echo State机制感知到该信息，并通过Echo State机制将新发现的影子资源合并到影子状态视图中。</p><p>注意在每次状态更新时，Shadow Resource Manager 会将所有空闲的影子资源释放，而只会继续管理已经被占用了的影子资源。</p><p>RM任务分配和运行结束释放都不用更新CSV，即CSV还是会一直认为这时候这些资源是空闲的，这样做是为了不影响原本的整个系统。</p><p>具体来说，它通过Echo State机制来迅速侦测新的影子资源，这种机制使得影子资源视图能够与CSV同步，但是主机影子资源的状态更新信息会被CSV忽略，而只会被Shadow Resource Manager处理，以避免影响原本的系统</p><pre><code class="hljs">问题：相当于是一种增量式的更新？但是问题在于这种大规模增量式的更新是否也会导致manager的状态更新延迟？延迟会有多严重？而且如果这里假设了这样子可以与最新的资源状态保持同步，那么这种方法能不能用到普通的调度器的同步上。或许直接预测可能的影子资源状态会更好？</code></pre><h2 id="RM-Filter"><a href="#RM-Filter" class="headerlink" title="RM Filter"></a>RM Filter</h2><p>RM Filter 是会在任务分配给各个调度器之前提前进行，任务选取有以下3个原则：</p><ol><li>过滤的任务应该与影子资源的短暂和碎片化属性相匹配。</li><li>其次，过滤后的任务应该能被抢先杀死或迁移，因为为了避免入侵，我们优先执行普通任务而不是 RM 任务，这样可以避免影响原始调度系统。</li><li>不选取太多任务，以免形成性能瓶颈。具体来说会考虑5个因素：<ol><li>RM任务队列的长度，表示RM调度器的工作负载；（负相关）</li><li>来自影子资源管理器的当前影子资源量； （正相关）</li><li>当前任务提交率，表示集群的工作负载；（正相关）</li><li>RM任务调度的成功率；（正相关）</li><li>调度系统当前的更新延迟。（正相关）</li></ol></li></ol><p>实际上还可以通过强化学习来进行任务的筛选，但是这不是本文的重点。</p><h2 id="RM-Scheduler"><a href="#RM-Scheduler" class="headerlink" title="RM Scheduler"></a>RM Scheduler</h2><p><img src="/2024/03/14/ShadowResources/fig6.jpg" alt="图6. RM Scheduler"></p><p>RM Scheduler 的执行如图6所示。它有两种调度模式：</p><ol><li><strong>虚线：</strong> 调度足够在影子资源上执行的任务，直接分配给影子资源，对CSV透明。（这种才叫RM任务）实际上就是将影子资源倒序排列，遍历选取合适的资源进行分配。</li><li><strong>实线：</strong> 调度不能在影子资源上执行的任务，当做是普通的调度器进行调度。流程如下：<ol><li>提交资源分配决定给影子状态管理视图；</li><li>影子资源管理视图返回信号没有足够的影子资源；</li><li>提交调度决策给CSV，就像普通的调度器一样；</li><li>如果调度成功就直接执行。</li></ol></li></ol><p>可以看出实际上RM Scheduler在执行时既需要拥有对影子资源的视图，也需要拥有对CSV的视图。</p><h2 id="Resource-Miner-优化"><a href="#Resource-Miner-优化" class="headerlink" title="Resource Miner 优化"></a>Resource Miner 优化</h2><p><strong>影子资源等待延迟：</strong> 当通过CSV更新，调度器对影子资源可见到这部分影子资源实际上被分配出去的这段时间。</p><p>可以探索RMiner去利用这段时间的资源。</p><p>举例，如图7所示，T0时刻R释放，成为了一种影子资源，CSV通过echo state更新给了RMiner，RMiner在T1时刻将任务放置到了这部分资源上（注意这时候不需要更新CSV），T2时刻进行了CSV更新，注意这时候调度器也认为原本被分配的影子资源也还是可以利用的，在T3时刻，调度器进行了一次普通的任务分配，没有冲突，但是在T4时刻，调度器将任务发送给了影子资源，这时候和影子资源的分配产生了冲突。其中T2~T4这段时间就是影子资源的<strong>等待延迟</strong>。</p><p>   <strong>问题：</strong> 为什么不告诉CSV这部分资源已经被分配了？这样就可以避免后续的冲突了。</p><p><img src="/2024/03/14/ShadowResources/fig7.jpg" alt="图7. 影子资源的等待延迟"></p><p>对影子资源的高利用率和低冲突率的权衡，所导致的对等待延迟时刻的影子资源的利用方式，使得本文提出了两种资源挖掘模式：</p><p><img src="/2024/03/14/ShadowResources/fig8.jpg" alt="图8. RMiner的两种资源挖掘模式对比"></p><h3 id="SafeRM-Mode"><a href="#SafeRM-Mode" class="headerlink" title="SafeRM Mode"></a>SafeRM Mode</h3><p>RMiner只在影子资源的存在时间对其进行利用，而不再等待延迟时间内对其进行利用，以最大程度避免冲突。在SafeRM模式下，RM Filter会优先考虑任务运行时间短的任务。RM调度器主要将短任务给小的影子资源。</p><p>当然由于RM任务的工作时长难以确定，所以RM任务也可能会超过影子资源存活时间，这是会尝试先迁移，如果不行再杀死。</p><h3 id="SmartRM-Mode"><a href="#SmartRM-Mode" class="headerlink" title="SmartRM Mode"></a>SmartRM Mode</h3><p>在影子资源的存在时间和等待延迟时间内对其进行利用，以最大程度提高资源利用率。在SmartRM模式下，RM Filter首先考虑任务的资源需求和驱逐成本，并优先考虑低优先的任务。RM调度器主要讲资源需求大的任务分配给尽量多空闲资源的合适的影子资源。</p><p>当冲突发生时，RM调度器会先杀死低优先级的RM任务，然后再尝试为其迁移。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>实验时，在谷歌集群模拟系统上添加了RM相关的组件。实验任务为2k+。模拟的节点为1500个同构节点，每个节点64个CPU，16个内存。</p><p>实验的任务主要来自阿里巴巴集群数据集。更新延迟为0.5s，调度器调度速率为每秒1000个任务，调度器数量分别设置为8和16，任务平均执行时间为5s，指数分布，平均一个作业包含12个任务，任务达到速率有1.43和0.7，前者在300秒内生成了200多个作业，即约2400个任务。</p><p>实验主要想回答3个问题：</p><ol><li>RMiner 能为共享状态架构带来哪些性能提升？ </li><li>在当前的共享状态调度器中采用RMiner的成本是多少？ </li><li>引入的优化如何有助于性能改进？</li></ol><h2 id="资源利用率"><a href="#资源利用率" class="headerlink" title="资源利用率"></a>资源利用率</h2><p><img src="/2024/03/14/ShadowResources/fig9.jpg" alt="图9. 资源利用率"></p><p>   图是真好看啊！</p><p>条形图表示 CPU 利用率的改进。显然，RMiner 通过挖掘影子资源来提高集群 CPU 利用率。不同场景下，影子资源占用集群资源的1.5%~5.0%。通过利用资源等待延迟方面的资源，SafeRM 的性能优于 NoRM 1.5% 至 4%，SmartRM 的性能优于 NoRM 1.6% 至 5.8%。</p><p>更具体地说，RMiner 在 8 个调度程序场景（平均利用率为 36.9%）下比在 16 个调度程序（平均利用率为 71.8%）下工作得更好，因为更少的调度程序可以更容易地为影子资源找到合适的 RM 任务（因为资源利用率低） 。此外，RMiner 在较高的任务提交率（2 倍）下表现更好，因为更多的任务提供了更多可供利用的已释放资源。</p><p>我们还将每个设置下的影子资源利用率报告为标记线。通过记录总体影子资源和分配的影子资源，SafeRM 使用了 26% 到 82% 的影子资源，SmartRM 使用了 58% 到 112% 的影子资源。 SafeRM 更加保守，仅限制影子资源生存时间的任务，而 SmartRM 则更加激进，在资源等待延迟中利用影子资源，甚至超过了不可见影子资源的上限。</p><h2 id="任务吞吐量"><a href="#任务吞吐量" class="headerlink" title="任务吞吐量"></a>任务吞吐量</h2><p><img src="/2024/03/14/ShadowResources/fig10.jpg" alt="图10. 任务吞吐量"></p><p>图 10 (a) 显示了阿里巴巴跟踪的改进，其中 SafeRM 比 NoRM 实现了高达 10% 的吞吐量提升，SmartRM 实现了高达 28% 的吞吐量提升在高工作负载（任务提交率）下，RMiner 比低工作负载表现更好，因为更多已完成的任务产生更多的影子资源。</p><p>此外，我们在图 10(b) 中比较了 Google 跟踪上三种方案的吞吐量。结果表明，SafeRM 实现了 2% 到 9% 的改进，SmartRM 实现了 10% 到 28% 的改进，这与阿里巴巴的结果类似，进一步验证了 RMiner 的性能。</p><h2 id="任务等待延迟"><a href="#任务等待延迟" class="headerlink" title="任务等待延迟"></a>任务等待延迟</h2><p><img src="/2024/03/14/ShadowResources/fig11.jpg" alt="图11. 任务等待延迟"></p><p>我们在图 11 中展示了作业等待时间的结果。它表明 RMiner 在较低工作负载下的表现与 NoRM 类似，因为在这种情况下任务不需要在队列中等待。然而，在更高的工作负载（1.75x和2x）下，并行提交的任务更多，普通调度器几乎已经达到了调度能力。 RMiner 的表现非常出色，因为它利用了更多短期任务来减少整体排队延迟。 RMiner 在 8 个调度程序下将作业等待时间缩短高达 25.4%，在 16 个调度程序下将作业等待时间缩短高达 10.4%。更多的调度器减少了并发调度任务的压力，但会导致更多的调度冲突。此外，我们进一步在 Google 的跟踪上验证了改进，发现 RMiner 在 8 个调度器下实现了 59.9% 的改进，在 16 个调度器下实现了 24.9% 的改进。</p><h2 id="任务冲突"><a href="#任务冲突" class="headerlink" title="任务冲突"></a>任务冲突</h2><p><img src="/2024/03/14/ShadowResources/fig12.jpg" alt="图12. 任务冲突"></p><p>我们记录了 16 个调度程序设置的不同工作负载级别下的冲突。图 12 报告了性能改进和引发的冲突之间的关系。图 12（a）显示了基线和 SafeRM 之间的比较，这表明 SafeRM 在最坏情况下导致冲突增加不到 3%，从而将资源利用率和任务吞吐量提高了 4%。平均而言，与当前的共享状态调度程序相比，SafeRM 造成的冲突多了 0.5%，这与性能收益相比是可以接受的。</p><p>此外，我们在图 12 (b) 中报告了 SmartRM 的结果。同样，SmartRM 在最坏的情况下会导致冲突增加 3%，资源利用率提高 6%，吞吐量提高 13%。 SmartRM 的平均冲突成本为 0.73%，由于挖矿策略更加激进，比 SafeRM 略高。我们还发现，在较高工作负载下，冲突成本更加严重，因为更多并发任务提交使 SmartRM 更容易与正常调度程序发生冲突。综上所述，从工作冲突的角度来看，成本与绩效的提升相比可以忽略不计。</p><h2 id="总体分析"><a href="#总体分析" class="headerlink" title="总体分析"></a>总体分析</h2><p>RMiner 的额外开销也是回答问题 3 的一个重要方面。不幸的是，工业模拟器没有对调度开销进行建模，因此我们进行了全面的理论分析。 RMiner的开销包括影子资源管理开销和RM调度开销。影子资源管理器占用额外的内存空间来存储和更新影子资源状态索引，大约是CSV空间的3%-12.5%。在较高工作负载下，管理算法的频率很频繁，但通过哈希映射，操作的复杂度为 O(1)，从而导致可接受的计算开销。总体而言，影子资源的管理所产生的开销可以忽略不计。</p><p>至于额外的调度开销，当前的共享状态调度器设计配备了数十个具有全局状态视图的并行分布式调度器。 RMiner 增加了一个 RM 调度器，大大增强了当前设计的可见性，并且 RM 调度器的调度成本比传统的并行调度器要低，因为调度范围和实体都比以前更小。因此，RMiner 的调度开销比当前共享状态设计大约增加了个位数。总体而言，与集群性能的显着提升相比，在当前共享状态调度器中采用 RMiner 的成本可以忽略不计。</p><h2 id="RM模式对比"><a href="#RM模式对比" class="headerlink" title="RM模式对比"></a>RM模式对比</h2><p>实验中，SafeRM保留更新延迟的过滤阈值以保证最小化冲突。 SmartRM的默认阈值也是更新延迟，我们将过滤器阈值调整为2x更新延迟和4x更新延迟来比较性能。前者发生冲突的可能性较低，被定义为保守派SmartRM（SmartRM-C）。相反，后者被定义为激进的SmartRM（SmartRM-A）。此外，我们改变了 0.5 秒的默认更新延迟来调查对结果的影响。实验在 1x 工作负载级别和 16 个并行调度器下进行。</p><p><img src="/2024/03/14/ShadowResources/fig13.jpg" alt="图13. RM模式对比-任务吞吐率和利用率"></p><p><img src="/2024/03/14/ShadowResources/fig14.jpg" alt="图14. RM模式对比-任务冲突"></p><p>查看图13，与直觉相反，将更多任务过滤到 RMiner (SmartRM-A) 会提高资源利用率，同时降低任务吞吐量。这是因为此设置留给普通调度程序的短期任务较少，而普通调度程序往往会将重量级任务调度到集群，集群会同时占用更多资源，但总共完成的任务较少。因此，我们需要仔细控制过滤原则，以避免 RMiner 中的过度过滤和欠过滤任务。更新延迟也会影响 RMiner 的性能。更新延迟越大，资源浪费越大，导致任务吞吐量和资源利用率降低。在较高的更新延迟下，SmartRM-A 在利用率方面的表现比在较低情况下更差，因为 RMiner 的改进被正常调度程序的退化所掩盖，但它在利用率方面的表现几乎相同，因为在这种情况下执行了更多的重量级任务。</p><p>此外，我们在图 14 中报告了冲突的详细信息。我们记录了由于冲突而终止的任务，并将数量标准化为 RM 任务。该指标的值越低意味着与正常调度发生冲突的 RM 任务就越少。显然，SafeRM 很少与正常调度发生冲突。对于SmartRM来说，过滤器阈值越高，杀死RM任务的比例就越高，导致与并行调度器的冲突更多。性能改进和冲突成本之间存在权衡，权衡的两侧代表了RMiner的不同设计目标：最高的性能改进或对正常系统的最低侵入。总体而言，不同的RMiner以可接受的成本实现了相当大的性能提升，并且可以针对不同的目标进行灵活配置。</p>]]></content>
    
    
    <categories>
      
      <category>论文阅读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文阅读</tag>
      
      <tag>集群调度</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux控制台输出多种样式彩色字符及原理解析</title>
    <link href="/2024/03/02/colorfulEcho/"/>
    <url>/2024/03/02/colorfulEcho/</url>
    
    <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>之前在做MIT6.824的实验的时候，有<a href="https://blog.josejg.com/debugging-pretty/">助教资料</a>在说明如何输出彩色的字符来让日志更加清晰。所以对Linux控制台如何输出多种样式的彩色字符以及它的原理产生了兴趣，学习了之后在这里记录一下。</p><h1 id="2-原理"><a href="#2-原理" class="headerlink" title="2. 原理"></a>2. 原理</h1><p>Linux控制台输出彩色字符的原理是通过ANSI转义码来实现的。ANSI转义码是一种控制字符，用于控制文本终端的行为。包括但不限于控制光标位置、颜色、清屏等。</p><p>下面这是一段输出蓝色字符的控制台代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[34mHello\033[0m&quot;</span><br></code></pre></td></tr></table></figure><p>在vscode的终端显示结果如下：</p><p><img src="/2024/03/02/colorfulEcho/eg1.jpg" alt="输出结果"></p><p>下面对这段代码逐个进行解析：</p><ul><li>-e：表示开启转义字符的解析，如果不加这个参数，\033会被当做普通字符输出。</li><li>\033：表示转义字符的开始。在ASCII字符集中，十进制的33代表了Escape字符（也可以写为\033或\x1B），它通常用于表示控制序列的开始。</li><li>[34m：表示设置颜色。34代表蓝色，m表示设置颜色的转义序列的结束。</li><li>[0m：表示重置为默认设置。0代表默认设置，m表示设置颜色的转义序列的结束。如果不设置为默认设置，后续的字符都会被设置为蓝色。</li></ul><h1 id="3-转义代码"><a href="#3-转义代码" class="headerlink" title="3. 转义代码"></a>3. 转义代码</h1><p>主要与输出字符格式相关的转义代码的格式如下，可以单独使用也可以利用;来混合使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">\033[显示方式;前景色;背景色m<br></code></pre></td></tr></table></figure><p>下面是一些常用的转义代码：</p><h2 id="1-显示方式"><a href="#1-显示方式" class="headerlink" title="1. 显示方式"></a>1. 显示方式</h2><p>代码及对应显示方式如下：</p><ul><li>0：所有属性关闭，恢复到默认值。</li><li>1：粗体或者高亮。</li><li>2：弱化（低亮）。(不是所有终端都支持)</li><li>3：斜体。(不是所有终端都支持)</li><li>4：下划线。</li><li>5,6：闪烁。(不是所有终端都支持)</li><li>7：反显，即前景色和背景色交换。</li><li>8：隐藏字符。</li><li>9：删除线。(不是所有终端都支持)</li><li>10：将文本的前景色设置为默认颜色。</li><li>21：双下划线。(不是所有终端都支持)</li></ul><p>在vscode的终端显示结果如下：</p><p><img src="/2024/03/02/colorfulEcho/eg2.jpg" alt="显示方式"></p><h2 id="2-前景色"><a href="#2-前景色" class="headerlink" title="2. 前景色"></a>2. 前景色</h2><p>代码及对应颜色如下：</p><ul><li>30：黑色。</li><li>31：红色。</li><li>32：绿色。</li><li>33：黄色。</li><li>34：蓝色。</li><li>35：洋红。</li><li>36：青色。</li><li>37：白色。</li></ul><p>在vscode的终端显示结果如下（注意37号白色被终端自动修改了以能够显示清楚）：</p><p><img src="/2024/03/02/colorfulEcho/eg3.jpg" alt="前景色"></p><p>而38号前景色是用于设置前景色的其他模式，包括两种：</p><ol><li>使用 ANSI 256 色模式设置前景色，例如：\033[38;5;196m。这里的5表示使用 ANSI 256 色模式，196表示使用ANSI 256 色模式中的第 196 种颜色</li><li>使用 TrueColor 模式设置前景色，例如：\033[38;2;255;0;0m。这里的2表示使用 TrueColor 模式，255;0;0表示RGB颜色值</li></ol><p>39号表示重置前景色为默认颜色。</p><h2 id="3-背景色"><a href="#3-背景色" class="headerlink" title="3. 背景色"></a>3. 背景色</h2><p>代码及对应颜色如下：</p><ul><li>40：黑色。</li><li>41：红色。</li><li>42：绿色。</li><li>43：黄色。</li><li>44：蓝色。</li><li>45：洋红。</li><li>46：青色。</li><li>47：白色。</li></ul><p>在vscode的终端显示结果如下（注意47号白色被终端自动修改了以能够显示清楚）：</p><p><img src="/2024/03/02/colorfulEcho/eg4.jpg" alt="背景色"></p><p>同样的，48号背景色是用于设置背景色的其他模式，包括使用 ANSI 256 色模式设置背景色和使用 TrueColor 模式设置背景色。<br>49号表示重置背景色为默认颜色。</p><h2 id="4-其他"><a href="#4-其他" class="headerlink" title="4. 其他"></a>4. 其他</h2><p>还有一些其他比较有意思的转义代码，不过格式就不是\033[显示方式;前景色;背景色m了，如下：</p><ul><li><code>\033[n*A</code> :光标上移n行 </li><li><code>\033[nB</code>:光标下移n行 </li><li><code>\033[nC</code>:光标右移n行 </li><li><code>\033[nD</code>:光标左移n行 </li><li><code>\033[y</code>;xH :设置光标位置 </li><li><code>\033[2J</code> :清屏 </li><li><code>\033[K</code>:清除从光标到行尾的内容 </li><li><code>\033[s</code>:保存光标位置 </li><li><code>\033[u</code>:恢复光标位置 </li><li><code>\033[?25l</code>:隐藏光标 </li><li><code>\033[?25h</code>:显示光标</li></ul><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="https://blog.csdn.net/TomorrowAndTuture/article/details/116448273">Linux 命令行输出不同颜色的文本</a></li><li><a href="https://www.linuxquestions.org/questions/linux-software-2/adding-colors-to-your-motd-105038/">Adding colors to your motd?</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Linux</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>在Unbuntu上安装Go以及解决Vscode上Go插件无法安装的问题</title>
    <link href="/2024/03/01/ubuntuInstallGo/"/>
    <url>/2024/03/01/ubuntuInstallGo/</url>
    
    <content type="html"><![CDATA[<h1 id="1-下载-Go-压缩包"><a href="#1-下载-Go-压缩包" class="headerlink" title="1. 下载 Go 压缩包"></a>1. 下载 Go 压缩包</h1><p>在写这篇文章的时候，Go 的最新版为 1.22.0。在我们下载安装包时，请浏览Go 官方下载页面,并且检查一下是否有新的版本可用。</p><p>以 root 或者其他 sudo 用户身份运行下面的命令，下载并且解压 Go 二进制文件到&#x2F;usr&#x2F;local目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">wget -c https://dl.google.com/go/go1.22.0.linux-amd64.tar.gz -O - | tar -xz -C /usr/local<br></code></pre></td></tr></table></figure><h1 id="2-调整环境变量"><a href="#2-调整环境变量" class="headerlink" title="2. 调整环境变量"></a>2. 调整环境变量</h1><p>通过将 Go 目录添加到$PATH环境变量，系统将会知道在哪里可以找到 Go 可执行文件。</p><p>执行下面的命令将Go和GOPATH环境变量添加到~&#x2F;.bashrc文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;# Set Go environment variables&#x27;</span> &gt;&gt; ~/.bashrc<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;export GOROOT=/usr/local/go&#x27;</span> &gt;&gt; ~/.bashrc<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;export GOPATH=$HOME/go&#x27;</span> &gt;&gt; ~/.bashrc<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&#x27;export PATH=$PATH:$GOROOT/bin:$GOPATH/bin&#x27;</span> &gt;&gt; ~/.bashrc<br></code></pre></td></tr></table></figure><p>为了让这些更改生效，需要运行以下命令来重新加载.bashrc配置：：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure><h1 id="3-验证-Go-安装过程"><a href="#3-验证-Go-安装过程" class="headerlink" title="3. 验证 Go 安装过程"></a>3. 验证 Go 安装过程</h1><p>通过打印 Go 版本号，验证安装过程。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">go version<br></code></pre></td></tr></table></figure><p>输出应该像下面这样：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">go</span> version go1.<span class="hljs-number">22</span>.<span class="hljs-number">0</span> linux/amd64<br></code></pre></td></tr></table></figure><h1 id="4-安装Vscode插件"><a href="#4-安装Vscode插件" class="headerlink" title="4. 安装Vscode插件"></a>4. 安装Vscode插件</h1><p>首先在vscode中搜索安装Go插件，点击第一个名称为Go的插件进行安装。<br>然后由于网络防火墙的原因，有部分组件无法下载。<br>需要在命令行中输入以下命令替换go的下载源：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">go <span class="hljs-built_in">env</span> -w GO111MODULE=on<br>go <span class="hljs-built_in">env</span> -w GOPROXY=https://goproxy.io,direct<br></code></pre></td></tr></table></figure><p>替换好关闭vscode重新打开，会弹出install all，点击等待安装即可。<br>如果没有弹出就按crtl+shift+p，输入go install&#x2F;update tools，点击等待安装即可。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="https://cloud.tencent.com/developer/article/1623121">如何在 Ubuntu 20.04 上安装 Go</a></li><li><a href="https://zhuanlan.zhihu.com/p/387853200">解决vscode安装go插件失败的问题</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
      <tag>Go</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>在CentOS上使用源码安装Python3.7，不与系统Python2.7冲突，同时支持pip3（脚本安装，亲测有效）</title>
    <link href="/2024/03/01/centosInstallPython/"/>
    <url>/2024/03/01/centosInstallPython/</url>
    
    <content type="html"><![CDATA[<p>该脚本主要是在Centos系统上使用源码安装Python3.7，安装后可以调用python3和pip3来进行使用，同时不与系统Python2.7冲突，还额外加入了腾讯的pip源来加速pip3下载包。</p><p>脚本使用方法如下：</p><ol><li>创建文件 <code>install_py37.sh</code>，写入以下 shell 脚本</li><li>赋予执行权限，<code>chmox +x install_py37.sh</code></li><li>执行脚本，<code>./install_py37.sh</code></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/env bash</span><br><br><span class="hljs-comment">## 下载 Python 源码，如果已下载源码在脚本当前目录下，可注释跳过下载步骤</span><br>wget https://www.python.org/ftp/python/3.7.12/Python-3.7.12.tgz<br><br><span class="hljs-comment">## 安装编译依赖组件</span><br>yum -y install wget zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make libffi-devel xz-devel<br><br><span class="hljs-comment">## 解压安装</span><br><span class="hljs-comment"># 解压到/usr/local/src目录</span><br>tar zvxf Python-3.7.12.tgz -C /usr/local/src<br><span class="hljs-built_in">cd</span> /usr/local/src/Python-3.7.12<br><span class="hljs-comment"># 编译前配置</span><br>./configure prefix=/usr/local/python3 --enable-shared<br><span class="hljs-comment"># 编译构建</span><br>make -j8<br><span class="hljs-comment"># 安装Python</span><br>make install<br><span class="hljs-comment"># 清理编译产出的中间文件</span><br>make clean<br><span class="hljs-comment"># 链接构建产出的Python可执行文件到/usr/local/bin目录</span><br><span class="hljs-built_in">ln</span> -s /usr/local/python3/bin/python3 /usr/local/bin/python3<br><span class="hljs-comment"># 链接构建产出的pip3可执行文件到/usr/local/bin目录</span><br><span class="hljs-built_in">ln</span> -s /usr/local/python3/bin/pip3 /usr/local/bin/pip3<br><span class="hljs-comment"># 链接构建产出的Python动态库</span><br><span class="hljs-built_in">ln</span> -s /usr/local/python3/lib/libpython3.7m.so.1.0 /usr/lib/libpython3.7m.so.1.0<br><span class="hljs-comment"># 配置动态库</span><br>ldconfig<br><br><span class="hljs-comment">## 检查Python版本是否安装成功</span><br><span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[1;42;37m[<span class="hljs-subst">$(date <span class="hljs-string">&quot;+%Y/%m/%d %H:%M:%S&quot;</span>)</span>] [Check]: 检查Python版本\033[0m&quot;</span><br>python3 --version<br><span class="hljs-built_in">echo</span> -e <span class="hljs-string">&quot;\033[1;42;37m[<span class="hljs-subst">$(date <span class="hljs-string">&quot;+%Y/%m/%d %H:%M:%S&quot;</span>)</span>] [Check]: 检查Python版本\033[0m&quot;</span><br><br><span class="hljs-comment">## pypi下载源配置</span><br><span class="hljs-built_in">mkdir</span> ~/.pip3/<br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;extra-index-url = https://mirrors.cloud.tencent.com/pypi/simple&quot;</span> &gt;&gt; ~/.pip3/pip.conf<br></code></pre></td></tr></table></figure><p>主要是参考了这篇文章：<a href="https://tencent.github.io/CodeAnalysis/zh/advanced/install_python37_on_centos.html">CentOS 7 安装 Python 3.7</a><br>不同点在于将原本的链接路径和安装结果改为了python3和pip3。</p>]]></content>
    
    
    <categories>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
      <tag>Python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【MIT6.824】lab2A实现笔记</title>
    <link href="/2024/02/27/MIT6-824lab2A/"/>
    <url>/2024/02/27/MIT6-824lab2A/</url>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>实现了MIT6.824中的lab2A，即leader选举的部分。</p><h1 id="Raft结构及初始化"><a href="#Raft结构及初始化" class="headerlink" title="Raft结构及初始化"></a>Raft结构及初始化</h1><p>为一个Raft中的节点增加的变量主要有：</p><ul><li>currentTerm: 当前任期</li><li>votedFor: 为谁投票, -1表示没有投票，注意一个任期只能投一次票</li><li>state: 当前节点的状态</li><li>heartbeatTimeout: 心跳超时计数器</li><li>electionTimeout: 选举超时计数器</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// A Go object implementing a single Raft peer.</span><br><span class="hljs-keyword">type</span> Raft <span class="hljs-keyword">struct</span> &#123;<br>mu               sync.RWMutex        <span class="hljs-comment">// Lock to protect shared access to this peer&#x27;s state</span><br>peers            []*labrpc.ClientEnd <span class="hljs-comment">// RPC end points of all peers</span><br>persister        *Persister          <span class="hljs-comment">// Object to hold this peer&#x27;s persisted state</span><br>me               <span class="hljs-type">int</span>                 <span class="hljs-comment">// this peer&#x27;s index into peers[]</span><br>dead             <span class="hljs-type">int32</span>               <span class="hljs-comment">// set by Kill()</span><br>currentTerm      <span class="hljs-type">int</span><br>votedFor         <span class="hljs-type">int</span><br>state            NodeState<br>heartbeatTimeout *time.Timer<br>electionTimeout  *time.Timer<br>&#125;<br><br><span class="hljs-comment">// NodeState represents the state of a node in the raft protocol</span><br><span class="hljs-keyword">type</span> NodeState <span class="hljs-type">uint8</span><br><br><span class="hljs-keyword">const</span> (<br>Follower NodeState = <span class="hljs-literal">iota</span><br>Candidate<br>Leader<br>)<br></code></pre></td></tr></table></figure><p>初始化Make函数如下，注意为新添加的变量进行初始化，可以看到初始化之后就会启动一个ticker的goroutine来让节点不断运行。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">Make</span><span class="hljs-params">(peers []*labrpc.ClientEnd, me <span class="hljs-type">int</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">persister *Persister, applyCh <span class="hljs-keyword">chan</span> ApplyMsg)</span></span> *Raft &#123;<br>rf := &amp;Raft&#123;<br>peers:            peers,<br>persister:        persister,<br>me:               me,<br>dead:             <span class="hljs-number">0</span>,<br>currentTerm:      <span class="hljs-number">0</span>,<br>votedFor:         <span class="hljs-number">-1</span>,<br>state:            Follower,<br>heartbeatTimeout: time.NewTimer(time.Duration(StableHeartbeatTimeout())),<br>electionTimeout:  time.NewTimer(time.Duration(RandomizedElectionTimeout())),<br>&#125;<br><br><span class="hljs-comment">// Your initialization code here (2A, 2B, 2C).</span><br><br><span class="hljs-comment">// initialize from state persisted before a crash</span><br>rf.readPersist(persister.ReadRaftState())<br><br><span class="hljs-comment">// start ticker goroutine to start elections</span><br><span class="hljs-keyword">go</span> rf.ticker()<br><br><span class="hljs-keyword">return</span> rf<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="计时函数"><a href="#计时函数" class="headerlink" title="计时函数"></a>计时函数</h1><p>计时函数如上所述有两个：</p><ul><li>heartbeatTimeout: 倒计时结束时，需要向其他节点发送心跳，以维持自己的leader地位</li><li>electionTimeout: 倒计时结束时，需要转化为candidate开始选举，如果在倒计时结束前收到了leader的心跳，则重置倒计时。</li></ul><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> ticker() &#123;<br><span class="hljs-keyword">for</span> rf.killed() == <span class="hljs-literal">false</span> &#123;<br><span class="hljs-keyword">select</span> &#123;<br><span class="hljs-keyword">case</span> &lt;-rf.heartbeatTimeout.C:<br>rf.mu.Lock()<br><span class="hljs-keyword">if</span> rf.state == Leader &#123;<br>rf.broadcastHeartbeat()<br>rf.heartbeatTimeout.Reset(StableHeartbeatTimeout())<br>&#125;<br>rf.mu.Unlock()<br><span class="hljs-keyword">case</span> &lt;-rf.electionTimeout.C:<br>rf.mu.Lock()<br>rf.changeState(Candidate)<br>rf.currentTerm++<br>rf.startElection()<br>rf.electionTimeout.Reset(RandomizedElectionTimeout())<br>rf.mu.Unlock()<br>&#125;<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="选举leader"><a href="#选举leader" class="headerlink" title="选举leader"></a>选举leader</h1><p>选举leader主要依靠发送RequestVote RPC来进行，选举的过程如下：</p><ol><li>electionTimeout计时器到期，节点转化为candidate状态，增加currentTerm并开始选举</li><li>发送RequestVote RPC给其他节点，请求投票</li><li>接收到其他节点的投票结果。</li></ol><p>按照Raft论文的描述可以将选举结果分为三种：</p><ol><li>得到了大多数节点的投票，成为leader</li><li>有其他节点成为了leader，自己转化为follower。如何感知到其他节点成为了leader呢？有两种手段：<ol><li>通过RequestVoteReply中的Term字段，如果Term比自己的大，则说明有其他节点成为了leader</li><li>接受到了其他节点的心跳，说明有其他节点成为了leader</li></ol></li><li>大家平分选票，没有leader产生，等待electionTimeout计时器到期，重新开始选举</li></ol><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> startElection() &#123;<br>request := rf.genRequestVoteRequest()<br>DPrintf(<span class="hljs-string">&quot;&#123;Node %v&#125; starts election with RequestVoteRequest %v&quot;</span>, rf.me, request)<br>rf.votedFor = rf.me<br>grantedVoteNum := <span class="hljs-number">1</span><br><br><span class="hljs-comment">// Your code here (2A, 2B).</span><br><span class="hljs-keyword">for</span> peer := <span class="hljs-keyword">range</span> rf.peers &#123;<br><span class="hljs-keyword">if</span> peer != rf.me &#123;<br><span class="hljs-keyword">if</span> peer == rf.me &#123;<br><span class="hljs-keyword">continue</span><br>&#125;<br><br><span class="hljs-keyword">go</span> rf.electionRequestOnce(peer, &amp;grantedVoteNum, request)<br>&#125;<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> electionRequestOnce(peer <span class="hljs-type">int</span>, grantedVoteNum *<span class="hljs-type">int</span>, request *RequestVoteArgs) &#123;<br>reply := <span class="hljs-built_in">new</span>(RequestVoteReply)<br><span class="hljs-keyword">if</span> rf.sendRequestVote(peer, request, reply) &#123;<br>rf.mu.Lock()<br><span class="hljs-keyword">defer</span> rf.mu.Unlock()<br>DPrintf(<span class="hljs-string">&quot;&#123;Node %v&#125; received RequestVoteReply &#123;%v&#125; from &#123;Node %v&#125;&quot;</span>, rf.me, reply, peer)<br><span class="hljs-keyword">if</span> rf.currentTerm == request.Term &amp;&amp; rf.state == Candidate &#123;<br><span class="hljs-keyword">if</span> reply.VoteGranted &#123;<br>*grantedVoteNum++<br><span class="hljs-keyword">if</span> *grantedVoteNum &gt; <span class="hljs-built_in">len</span>(rf.peers)/<span class="hljs-number">2</span> &#123;<br>rf.changeState(Leader)<br>rf.broadcastHeartbeat()<br>&#125;<br>&#125;<br>&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> reply.Term &gt; rf.currentTerm &#123;<br>DPrintf(<span class="hljs-string">&quot;&#123;Node %v&#125; found higher term %v in RequestVoteReply %v from &#123;Node %v&#125;&quot;</span>, rf.me, reply.Term, reply, peer)<br>rf.currentTerm = reply.Term<br>rf.votedFor = <span class="hljs-number">-1</span><br>rf.changeState(Follower)<br>&#125;<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>节点在进行投票时的规则如下：</p><ol><li>如果自己的term比对方大，则拒绝投票</li><li>如果在当前term中已经投过票给其他candidate，则拒绝投票</li><li>其余情况下投票给对方，并更新自己的term与votedFor，并直接转化为follower状态</li></ol><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) &#123;<br><span class="hljs-comment">// Your code here (2A, 2B).</span><br>rf.mu.Lock()<br><span class="hljs-keyword">defer</span> rf.mu.Unlock()<br><span class="hljs-keyword">defer</span> DPrintf(<span class="hljs-string">&quot;&#123;Node %v&#125;&#x27;s state is &#123;state: %v, term: %v&#125;, the RequestVoteReply is &#123;%v&#125;&quot;</span>, rf.me, rf.state, rf.currentTerm, reply)<br><br><span class="hljs-keyword">if</span> args.Term &lt; rf.currentTerm || (args.Term == rf.currentTerm &amp;&amp; rf.votedFor != <span class="hljs-number">-1</span> &amp;&amp; rf.votedFor != args.CandidateId) &#123;<br>reply.Term, reply.VoteGranted = rf.currentTerm, <span class="hljs-literal">false</span><br><span class="hljs-keyword">return</span><br>&#125;<br><span class="hljs-keyword">if</span> args.Term &gt; rf.currentTerm &#123;<br>rf.currentTerm, rf.votedFor = args.Term, <span class="hljs-number">-1</span><br>rf.changeState(Follower)<br>&#125;<br><span class="hljs-keyword">if</span> !rf.isLogUpToDate(args.LastLogIndex, args.LastLogTerm) &#123;<br>reply.Term, reply.VoteGranted = rf.currentTerm, <span class="hljs-literal">false</span><br><span class="hljs-keyword">return</span><br>&#125;<br><br>rf.votedFor = args.CandidateId<br><span class="hljs-comment">// now the term of the candidate must equal to the current term of the rf</span><br>reply.Term, reply.VoteGranted = rf.currentTerm, <span class="hljs-literal">true</span><br>&#125;<br><br><span class="hljs-keyword">type</span> RequestVoteArgs <span class="hljs-keyword">struct</span> &#123;<br>Term         <span class="hljs-type">int</span><br>CandidateId  <span class="hljs-type">int</span><br>LastLogIndex <span class="hljs-type">int</span><br>LastLogTerm  <span class="hljs-type">int</span><br>&#125;<br><span class="hljs-keyword">type</span> RequestVoteReply <span class="hljs-keyword">struct</span> &#123;<br>Term        <span class="hljs-type">int</span><br>VoteGranted <span class="hljs-type">bool</span><br>&#125;<br><br></code></pre></td></tr></table></figure><p>注意在状态转化时需要对计时器进行相应的修改，如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> changeState(newState NodeState) &#123;<br><span class="hljs-keyword">if</span> rf.state == newState &#123;<br><span class="hljs-keyword">return</span><br>&#125;<br>DPrintf(<span class="hljs-string">&quot;&#123;Node %v&#125; changes state from %s to %s&quot;</span>, rf.me, rf.state, newState)<br>rf.state = newState<br><br><span class="hljs-keyword">switch</span> newState &#123;<br><span class="hljs-keyword">case</span> Follower:<br>rf.heartbeatTimeout.Stop()<br>rf.electionTimeout.Reset(RandomizedElectionTimeout())<br><span class="hljs-keyword">case</span> Candidate:<br><span class="hljs-keyword">case</span> Leader:<br>rf.broadcastHeartbeat()<br>rf.heartbeatTimeout.Reset(StableHeartbeatTimeout())<br>rf.electionTimeout.Stop()<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="心跳广播"><a href="#心跳广播" class="headerlink" title="心跳广播"></a>心跳广播</h1><p>理论上心跳发送应该与日志复制用的是同一种RPC，但是lab2A不需要实现日志复制，所以这里的日志复制进行了简化，能发送心跳来维持自己的leader地位即可。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> replicateOneRound(peer <span class="hljs-type">int</span>) &#123;<br><span class="hljs-keyword">if</span> rf.state != Leader &#123;<br><span class="hljs-keyword">return</span><br>&#125;<br><br>request := rf.genAppendEntriesRequest(peer)<br>reply := <span class="hljs-built_in">new</span>(AppendEntriesReply)<br><span class="hljs-keyword">if</span> rf.sendAppendEntries(peer, request, reply) &#123;<br>DPrintf(<span class="hljs-string">&quot;&#123;Node %v&#125; received AppendEntriesReply &#123;%v&#125; from &#123;Node %v&#125;&quot;</span>, rf.me, reply, peer)<br>&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> AppendEntries(args *AppendEntriesRequest, reply *AppendEntriesReply) &#123;<br>DPrintf(<span class="hljs-string">&quot;&#123;Node %v&#125; received AppendEntriesRequest &#123;%v&#125;&quot;</span>, rf.me, args)<br>rf.changeState(Follower)<br>rf.electionTimeout.Reset(RandomizedElectionTimeout())<br>reply.Term, reply.Success = rf.currentTerm, <span class="hljs-literal">true</span><br>&#125;<br><br><span class="hljs-keyword">type</span> AppendEntriesRequest <span class="hljs-keyword">struct</span> &#123;<br>Term     <span class="hljs-type">int</span><br>LeaderId <span class="hljs-type">int</span><br>&#125;<br><span class="hljs-keyword">type</span> AppendEntriesReply <span class="hljs-keyword">struct</span> &#123;<br>Term    <span class="hljs-type">int</span><br>Success <span class="hljs-type">bool</span><br>&#125;<br></code></pre></td></tr></table></figure><h1 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h1><p>运行结果如下，能通过所有测试：</p><p><img src="/2024/02/27/MIT6-824lab2A/result.jpg" alt="运行结果"></p>]]></content>
    
    
    <categories>
      
      <category>MIT6.824</category>
      
      <category>Lab</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式</tag>
      
      <tag>MIT6.824</tag>
      
      <tag>Go</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读】In Search of an Understandable Consensus Algorithm</title>
    <link href="/2023/12/26/RaftPaperRead/"/>
    <url>/2023/12/26/RaftPaperRead/</url>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>共识一致性算法常用在分布式系统中，一个系统会有一个领导者，如GFS，我们需要有多个领导者副本来提高系统的容错性。但是之前的共识性算法经常采用Paxos，但是该算法很难理解。所以本文的作者重点面向可理解性提出了一个新的共识性算法Raft。主要做法是将大步骤分解成小步骤，然后尽量降低复杂度。</p><p>在具体关注其实现之前强烈建议去<a href="https://thesecretlivesofdata.com/raft/">raft可视化</a>中去学习一下基本的流程，以对其有个大概的印象，然后还可以参考这部分的介绍来学习动画中的内容：<a href="https://www.cnblogs.com/Finley/p/14467602.html">看动画轻松学会 Raft 算法 </a></p><h1 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>raft系统中各个成员有3种状态：<strong>leader, follower, or candidate</strong></p><p><img src="/2023/12/26/RaftPaperRead/serverState.jpg" alt="server state"></p><p>每一次开始选举leader到下一次为止都是一个term，一个term开始选举时也可能因为平分选票而导致选举leader失败，这样就会进入下一term的选举</p><p><img src="/2023/12/26/RaftPaperRead/term.jpg" alt="term"></p><p>服务器采用RPC进行通信，主要有两种信息：</p><ol><li><strong>RequestVote</strong>，选举投票</li><li><strong>AppendEntries</strong>，复制日志条目（也当做心跳使用）</li></ol><h2 id="leader选举"><a href="#leader选举" class="headerlink" title="leader选举"></a>leader选举</h2><p>每一个服务器在一定范围内随机生成一个等待时间，如果在该时间内没有收到leader的心跳，那么就认为leader下线了，就给自己的term+1，然后发起投票，希望自己成为leader。所以这也就需要保证让等待时间大于心跳信息发送的间隔时间。</p><p>服务器发起投票后有以下3种结果：</p><ol><li>获得包括自己在内超过所有服务器半数的投票，自己成为leader</li><li>收到其他term&gt;&#x3D;自己的leader的信息，说明已经有leader了，自己变为follower</li><li>由于平分选票，谁都不能成为leader，再次等待随机时间后，再发起一轮投票</li></ol><p>为了防止选举到的leader没有全部的commit的日志，规定：</p><ol><li>发起投票的服务器如果没有自己拥有的日志新，则不给它投票。因为已经commit的日志肯定在超过半数的服务器上有留存，那么一个没有全部commit的日志的服务器就必然不能拿到超过半数的同意票。</li><li>新leader上台时，不会去尝试复制旧的日志，然后提交，它只会去专注于提交新的日志，并在将新的日志复制给了大半的服务器后，将之前所有的可能没提交的一并提交。这样子就避免了下图中(d)可能出现的错误情况。</li></ol><p><img src="/2023/12/26/RaftPaperRead/errorStatus.jpg" alt="errorStatus"></p><h2 id="日志复制"><a href="#日志复制" class="headerlink" title="日志复制"></a>日志复制</h2><p>leader会接收用户发来的请求，并生成日志，然后将日志发送给各个follower，如果有包括自己在内超一半的服务器拥有了改日志，就将日志（也包含之前可能未提交的日志）commit，然后返回给用户执行结果。</p><p>raft还通过以下特性来保持日志的一致性：</p><ol><li>相同term相同index的日志内容相同</li><li>如果两个服务器某处日志的term和index都相同，那么他们之前的所有日志也相同</li></ol><p>第一条的保证是来自于每个leader创建了日志之后不会再修改。<br>第二条的保证使用的是归纳法的思想，每次发送的AppendEntries都包含前一个日志的信息，必须要前一个日志信息相同才可以接受，否则就拒绝，然后leader会不断尝试依次递减发送上一个日志（leader会为每一个follower维护一个nextIndex，代表其需要发送的日志id），直到找到相同的为止，然后将往后的日志都发送过去。当然这部分匹配可以进行优化。</p><p>各个主机上的log条目可能千奇百怪，但是注意到只有超过一半的服务器拥有的日志才是可能提交的日志，才需要永久性保存，其他都是没有提交的日志，可以进行删除更新即可分析清楚。</p><p><img src="/2023/12/26/RaftPaperRead/possibleLogStatus.jpg" alt="possible Log Status"></p><h2 id="成员变更"><a href="#成员变更" class="headerlink" title="成员变更"></a>成员变更</h2><p>当集群中成员需要进行改变的，一个方式是停掉集群，然后各自修改配置，这时安全的，但是会导致部分时间集群服务不可用。另一种方法是在线修改，但是直接的在线修改可能会导致如下的问题，即加入server4、5之后，由于部分主机1，,还不知道，所以会导致出现两个leader。</p><p><img src="/2023/12/26/RaftPaperRead/errorMemberChange.jpg" alt="errorMemberChange"></p><p>Raft提出的解决方案是采用两阶段的方法执行成员变更。</p><p>首先集群配置进行更新的时候，会将原本的配置Cold和新配置Cnew联合起来形成Cold，new，leader会将其复制给原本记录的其他人，一旦服务器收到了，就会把最新的配置设置为当前使用的，提交必须要要保证new和old中都有过半的服务器被使用了，然后再将Cnew复制给其他new中的服务器，一旦Cnew也被过半的new中的服务器收到了，就提交，然后整体配置就转为了Cnew。如下图所示，就避免了Cold和Cnew都能同时做决策的情况。</p><p><img src="/2023/12/26/RaftPaperRead/changeConfigure.jpg" alt="changeConfigure"></p><p>但是仍然还有几个问题需要解决：</p><ol><li>新加入的服务器难以快速跟上进度。Raft将其先作为非投票的成为加入到集群中，赶上后再转为正常。</li><li>leader可能不是new中的一部分。在Cnew提交后再进行将leader下线，所以在前面一段时间，其实leader在管理不属于他的集群。</li><li>不在Cnew被删除的服务器可能会影响集群可用性，因为他们不会再收到心跳，然后就会不断发起投票。Raft的解决办法是如果服务器认为leader还存在，即还没有等待超时，就会忽略投票请求，不会更新其term和给他投票。</li></ol><h2 id="日志压缩"><a href="#日志压缩" class="headerlink" title="日志压缩"></a>日志压缩</h2><p>长时间的运行会导致日志的堆积，可以通过生成快照将状态拷贝下来，然后再将不需要的日志删除，如下图所示。</p><p><img src="/2023/12/26/RaftPaperRead/snapshot.jpg" alt="snapshot"></p><p>注意快照需要包含最后一个日志的信息，以让下一个日志生成的时候进行检查。</p><p>每个服务器独立进行快照生成，而不是由leader统一生成和发送，这是为了降低网络带宽消耗，降低系统复杂度。</p><p>新加入的服务器，或者特殊情况下的follower可能会需要leader将快照整个发送给它来初始化状态，follower在接收到到快照后会把快照最后一项之前的所有log删除。</p><p>此外还需要注意快照生成的频率，简单的方法是快照到一定大小后就进行生成。还需要避免写入快照对系统的影响，这可以通过写时复制的方法进行支持。</p><h2 id="客户交互"><a href="#客户交互" class="headerlink" title="客户交互"></a>客户交互</h2><p>客户端启动时，随机选择服务器，如果不是leader，该服务器会返回相关信息来帮助客户找到leader。如果leader宕机，客户请求会超时，然后再次尝试随机选择服务器来连接。</p><p>我们希望提供线性语义，但是raft实际上可能一个操作执行多次，例如leader在提交了之后马上宕机，然后没来得及返回给用户，然后用户可能会再次发送该请求，导致其二次执行。解决方法是让客户的每个请求都分配一个序号，如果接收到已经执行过的序号，就理解响应但是忽略执行。</p><p>对于只读操作，如果返回请求的leader马上被其他服务器替换，那么就面临返回过时信息的问题。所以raft需要保证新上台的leader知道哪些是已经执行了的，所以新上台的leader需要提交一个无操作的条目，来同步。raft也让leader在处理只读请求时与大多数成员交换心跳信息来处理此问题。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="https://raft.github.io/">https://raft.github.io/</a></li><li><a href="https://raft.github.io/raft.pdf">https://raft.github.io/raft.pdf</a></li><li><a href="https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-06-raft1">https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-06-raft1</a></li><li><a href="https://willzhuang.github.io/2018/03/04/Raft%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/">https://willzhuang.github.io/2018/03/04/Raft%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/</a></li><li><a href="https://thesecretlivesofdata.com/raft/">https://thesecretlivesofdata.com/raft/</a></li><li><a href="https://www.cnblogs.com/Finley/p/14467602.html">https://www.cnblogs.com/Finley/p/14467602.html</a></li><li><a href="https://acehi.github.io/thesecretlivesofdata-cn/raft/">https://acehi.github.io/thesecretlivesofdata-cn/raft/</a></li><li><a href="https://zhuanlan.zhihu.com/p/32052223">https://zhuanlan.zhihu.com/p/32052223</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>MIT6.824</category>
      
      <category>论文</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式</tag>
      
      <tag>MIT6.824</tag>
      
      <tag>论文阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读】The Design of a Practical System for Fault-Tolerant Virtual Machines</title>
    <link href="/2023/12/12/Fault-TolerantVirtualMachinesPaperRead/"/>
    <url>/2023/12/12/Fault-TolerantVirtualMachinesPaperRead/</url>
    
    <content type="html"><![CDATA[<p><code>为了更有效的做论文阅读笔记，之后都打算将每篇论文笔记的内容控制在较少的字数范围内，毕竟原论文摆在那里，将其翻译照抄过来也没什么意思，将论文读薄才是最重要的。( •̀ ω •́ )✧</code></p><p>“The Design of a Practical System for Fault-Tolerant Virtual Machines”是MIT6.824推荐阅读的论文之一，它介绍了一种通过主备机制来进行单核虚拟机级别的容错方法。</p><h1 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h1><p>本文考虑的主要是fail-stop故障，例如电源线拔掉了，爆炸了，网络离线了等情况，而这也才能用复制的方法容错，普通的程序错误导致的故障也无法通过复制来解决。</p><p>容错一般有两种方法：</p><ol><li><strong>状态转移：</strong>拷贝主虚拟机的所有状态到另一个虚拟机上</li><li><strong>复制状态机：</strong>将虚拟机认为是一个状态机，只拷贝具体的操作</li></ol><p>明显复杂状态机对宽带要求更低，但是其设计更为复杂，本文采用的是复制状态机的方法。但是后面VMWare团队有推出多核虚拟机级别的容错，该方法采用的是类似状态转移的方法。</p><p>容错一般还可以分为应用层容错和主机层容错，<strong>本文是主机层</strong>，在这有容错的虚拟机上可以运行任何应用。</p><h1 id="设计概述"><a href="#设计概述" class="headerlink" title="设计概述"></a>设计概述</h1><p><img src="/2023/12/12/Fault-TolerantVirtualMachinesPaperRead/arc.jpg" alt="架构"></p><p>设计主要解决的问题是如何保证两个VM的状态一直保持一致。两个主副VM之间会通过Logging channel进行连接，主VM会将其<strong>任何会导致两者状态不一致的命令或者非确定性事件命令</strong>都通过Logging channel发送给副VM，副VM会读取该channel来执行相同的操作，但是该执行的输出会被忽略掉。</p><p>传递指令需要特别注意的是一些<strong>非确定性事件</strong>，该事件主要有两类分别是随时到达的<strong>客户端输入</strong>和在不同时刻不同的VM上会产生不同的结果的<strong>怪异指令</strong>，例如生成随机数、获取当前时间、获取主机id等。</p><pre><code class="hljs">非确定性事件还包括CPU并发，因为指令交织的顺序难以保证，例如两个并发的线程同时向一块数据加锁，那么主副VM上哪个线程能拿到锁其实是不确定的，但是本文是针对单CPU的，没有提及这个问题</code></pre><p>可以猜测传递的日志中主要有三样东西：</p><ol><li><strong>事件发生时的指令序号</strong>，即自机器启动以来指令的相对序号</li><li><strong>日志类型</strong></li><li><strong>数据</strong>，如果是网络数据包日志，那么就包含对应的数据，如果是怪异指令，那么就是其在主虚拟机上执行的结果</li></ol><p>需要注意的是为了保证副VM的执行不会超过主VM，副VM只有的channel里面有指令的时候才会继续运行，即<strong>副VM永远会落后主VM一个指令</strong>，不然就会一直停止等待，或者检测到主VM挂了，自己上台当主VM</p><h1 id="输出控制"><a href="#输出控制" class="headerlink" title="输出控制"></a>输出控制</h1><p>系统通过网络数据包来与用户进行交互，文章的目标是让用户接收到返回信息时该指令一定是在两个VM上都能执行了的，它避免的是如下的场景：</p><ul><li>主虚拟机给了用户返回，但是由于其马上crash了，没有将指令及时传给副VM，那么后面通过副VM上台时，该命令其实是没执行的，但是用户会以为其已经执行了</li></ul><p><img src="/2023/12/12/Fault-TolerantVirtualMachinesPaperRead/output.jpg" alt="输出控制"></p><p>解决方法是：<strong>主VM输出返回必须要在发送了日志且副VM返回了确认接收之后</strong></p><p>当然这也有可能会导致重复输出，因为主VM输出后马上奔溃，而副VM上台后还没有执行这个命令，那么后面再执行时就会导致重复输出，而文中提到由于有TCP的规则在，由于输出的是完全一致的数据包，该重复输出会被TCP的协议解决掉。</p><h1 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h1><p>主副VM之间需要知道对方有没有存活，文中使用了UDP心跳来检测服务器是否奔溃，此外也通过监控日志流量（因为定时器中断的存在，日志流量应该是有规律的）来探查，如果超过特点时间，就可能发生故障了，但是这依然会存在<strong>脑裂</strong>的问题，如果只是两个VM之间的网络出问题了，那么副VM如果这时上台就会出现两个主VM。文中采用的解决方案是通过<strong>Test-and-Set</strong>方案，它会在共享存储中执行一个原子性的测试设置操作。如果操作成功，VM就会被允许上线，如果不成功就说明另外一个还在运行。如果采用的不是共享存储，那么也会引入一个第三方的决策者来进行判断。</p><p>如果是副VM奔溃了，则会重新起一个副VM，该VM来自对主VM的完全拷贝。</p><p>同时为了保证容错的副VM上台后，不会需要太长时间才能把剩余的命令消费掉，已经为了防止channel的缓冲区被填满，<strong>副VM会和主VM保持一定的指令数间隔</strong>，文中提到执行延迟应不小于100ms，如果副VM跟不上主VM的处理速度，系统会分配给主VM更少的Cpu周期数来平衡两者的速度。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://pdos.csail.mit.edu/6.824/papers/vm-ft.pdf">https://pdos.csail.mit.edu/6.824/papers/vm-ft.pdf</a></li><li><a href="https://zhuanlan.zhihu.com/p/523109983">https://zhuanlan.zhihu.com/p/523109983</a></li><li><a href="https://pdos.csail.mit.edu/6.824/notes/l-vm-ft.txt">https://pdos.csail.mit.edu/6.824/notes/l-vm-ft.txt</a></li><li><a href="https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-04-vmware-ft">https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-04-vmware-ft</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>MIT6.824</category>
      
      <category>论文</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式</tag>
      
      <tag>MIT6.824</tag>
      
      <tag>论文阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读】The Google file System</title>
    <link href="/2023/12/05/TheGoogleFileSystemPaperRead/"/>
    <url>/2023/12/05/TheGoogleFileSystemPaperRead/</url>
    
    <content type="html"><![CDATA[<p>The Google file System论文是MIT6.824中推荐阅读的论文，他是Google早期的三大论文之一，由于课程并不需要实现这个系统，所以就对整部论文中的关键点进行介绍总结。</p><h1 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h1><p>为了满足Google快速增长的数据处理需求，Google需要自己构建一套文件系统——The Google file System(GFS)。而这套文件系统也必须是分布式的文件系统才足以满足要求，但是我们都知道这面临着很多困难：</p><ul><li>分布式的文件系统会使用大量的主机，而这会使得主机出错成为常态</li><li>为了解决时不时的一部分主机出错带来的影响，我们需要对文件进行拷贝放置到多个主机上</li><li>一旦文件有多份，我们就需要就其产生的一致性问题进行解决</li><li>而保证一致性就又会导致系统的低性能</li><li>一致性和低性能的取舍一直是一个研究的重点问题</li></ul><p>GFS就在解决上述问题，同时他也着重于解决自己场景下的问题，其设计的特点如下：</p><ul><li>故障设备经常发生</li><li>文件比传统标准更大，数GB大小的文件是十分常见的</li><li>系统负载主要来自两种读操作：大规模的流式读取和小规模的随机读取</li><li>系统负载还来自很多对文件的大规模追加写入</li><li>同时设计应用程序和文件系统API便于提高整个系统的灵活性</li></ul><h1 id="设计概述"><a href="#设计概述" class="headerlink" title="设计概述"></a>设计概述</h1><p><img src="/2023/12/05/TheGoogleFileSystemPaperRead/GFSarch.png" alt="图1 GFS架构"></p><p>如图1所示，一个GFS集群包括单个master（主服务器）和多个chunkserver（块服务器），并被多个client（客户端）访问。每个节点通常为一个运行着用户级服务进程的Linux主机。</p><h2 id="chunk"><a href="#chunk" class="headerlink" title="chunk"></a>chunk</h2><p>文件被划分为若干个64MB大小的chunk（块）。每个chunk被一个不可变的全局唯一的64位chunk handle（块标识符）唯一标识，chunk handle在chunk被创建时由主节点分配。chunkserver将chunk作为Linux文件存储到本地磁盘中，通过chunk handle和byte range（字节范围）来确定需要被读写的chunk和chunk中的数据。为了可靠性考虑，每个chunk会在多个chunkserver中有副本。默认存储三份副本，用户也可以为不同的命名空间的域指定不同的副本级别。</p><p>64MB大小的chunk其实并没有带来内部碎片，因为每个chunk的副本被作为普通的Linux文件存储在chunkserver上，linux文件上是以几KB为单位进行存储空间分配的，其仅在需要时扩展。懒式空间分配（lazy space allocation）避免了内部碎片（internal fragmentation）带来的空间浪费。</p><p>同时一个较大的chunk带来了以下的优势：</p><ol><li>减少了client与master交互的次数，</li><li>使得client可以在一个chunk上执行更多的操作</li><li>减小了master中存储的云数据的大小</li></ol><p>需要注意的是如果多个client访问同一个文件，那么存储这这些文件的chunkserver会成为hot spot（热点）。但是因为应用程序大部分都顺序地读取包含很多chunk的大文件，所以hot spot不是主要问题。而如果出现这个问题，一个潜在的长期解决方案是在让client在这种场景下从其他client读取数据。</p><h2 id="master"><a href="#master" class="headerlink" title="master"></a>master</h2><p>master维护系统所有的元数据。元数据包括命名空间（namespace）、访问控制（access control）信息、文件到chunk的映射和chunk当前的位置。master还控制系统级活动如chunk租约（chunk lease）管理、孤儿chunk垃圾回收（garbage collection of orphaned chunks）和chunkserver间的chunk迁移（migration）。master周期性地通过心跳（HeartBeat）消息与每个chunkserver通信，向其下达指令并采集其状态信息。</p><p>文件和块的命名空间、文件到chunk的映射这两种类型还通过将变更记录到一个操作日志（operation log）的方式持久化存储在master的磁盘上。具体来说需要将操作日志备份到多台远程主机上，且只有当当前操作记录条目被本地和远程主机均写入到了磁盘后才能向客户端发出响应。master会在操作记录被写入前批量合并一些操作记录来减少写入和备份操作对整个系统吞吐量的影响。master会对其状态创建一个检查点（checkpoint），这样master就可以从磁盘加载最后一个检查点并重放该检查点后的日志来恢复状态。因为创建一个检查点需要一段时间，所以master被设计为可以在不推迟新到来的变更的情况下创建检查点。创建检查点时，master会切换到一个新的日志文件并在一个独立的线程中创建检查点，这个新的检查点包含了在切换前的所有变更。</p><p>master不会持久化存储chunk的位置信息，而是在启动时和当chunkserver加入集群时向chunkserver询问其存储的chunk信息。这样相比于持久化，消除了当chunkserver加入或离开集群、更改名称、故障、重启等问题时，保持master和chunkserver同步的问题。</p><h2 id="chunk-server"><a href="#chunk-server" class="headerlink" title="chunk server"></a>chunk server</h2><p>chunk副本分配到chunk server策略有两个目标：最大化数据<strong>可靠性</strong>和<strong>可用性</strong>、<strong>最大化网络带宽利用</strong>。对于这两个目标，仅将副本分散在所有机器上是不够的，这样做只保证了容忍磁盘或机器故障且只充分利用了每台机器的网络带宽。我们必须<strong>在机架间分散chunk的副本</strong>。这样可以保证在一整个机架都被损坏或离线时（例如，由交换机、电源电路等共享资源问题引起的故障），chunk的一些副本仍存在并保持可用状态。除此之外，这样还使对chunk的流量（特别是读流量）<strong>能够充分利用多个机架的总带宽</strong>。而另一方面，其代价就是<strong>写流量必须流经多个机架</strong>，导致一定程度的速度损耗。</p><p>chunk副本的创建可能由三个原因引起：chunk创建、重做副本（re-replication）和重均衡（rebalance）。</p><p><strong>chunk创建</strong>时，选择chunk server主要需要考虑以下因素：</p><ol><li>希望在磁盘利用率低于平均值的chunkserver上放置副本，以平衡chunkserver间的磁盘利用率</li><li>希望限制每台chunkserver上最近创建的chunk的数量。尽管创建chunk本身开销很小，但由于创建后一般会接着马上大量写，所以需要平衡限制写入流量</li><li>希望将chunk的副本跨机架分散。</li></ol><p><strong>重做副本</strong>一般是因为chunk被损坏了或者chunk server不可用了，或者目标副本数增加了，重做副本需要有优先级：</p><ol><li>当前chunk副本数与目标副本数之差越大优先级就越高</li><li>更倾向于优先为还存在的文件的chunk重做副本，而不是优先为最近被删除的文件重做。</li><li>为了最小化故障对正在运行的应用程序的影响，我们提高了所有正在阻塞client进程的chunk的优先级。</li></ol><p>重做副本选择chunk server所考虑的因素与chunk 创建一样，但是为了防止克隆操作的流量远高于client流量的情况发生，master需要对整个集群中活动的克隆操作数和每个chunkserver上活动的<strong>克隆操作数进行限制</strong>。除此之外，在克隆操作中，每个chunkserver还会<strong>限制对源chunkserver的读请求</strong>，以限制每个克隆操作占用的总带宽。</p><p>每隔一段时间master会对副本进行<strong>重均衡</strong>：master会检测当前的副本分布并移动副本位置，使磁盘空间和负载更加均衡。同样，在这个过程中，master会逐渐填充一个新的chunkserver，而不会立刻让来自新chunk的高负荷的写入流量压垮新的chunkserver。新副本放置位置的选择方法与上文中讨论过的类似。此外，master必须删除一个已有副本。通常，master会选择删除空闲磁盘空间低于平均的chunkserver上的副本，以均衡磁盘空间的使用。</p><h2 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h2><p>被链接到应用程序中的GFS client的代码实现了文件系统API并与master和chunkserver通信，代表应用程序来读写数据。</p><p>GFS支持如创建（create）、删除（delete）、打开（open）、关闭（close）、读（read）、写（write）文件等常用操作。此外，GFS还支持快照（snapshot）和追加记录（record append）操作。</p><h2 id="垃圾回收"><a href="#垃圾回收" class="headerlink" title="垃圾回收"></a>垃圾回收</h2><p>在文件被删除后，GFS不会立刻回收可用的物理存储空间。master仅在周期性执行懒式垃圾回收时回收物理存储空间，其中垃圾回收分为文件级垃圾回收和chunk级垃圾回收。</p><h3 id="文件级垃圾回收"><a href="#文件级垃圾回收" class="headerlink" title="文件级垃圾回收"></a>文件级垃圾回收</h3><p>当一个文件被应用程序删除时，master会像执行其他操作时一样立刻将删除操作写入日志。但是master不会立刻对资源进行回收，而是将待删除的文件重命名为一个带有删除时间戳的隐藏文件名。当master周期性地扫描文件系统命名空间时，它会删除已经存在超过三天（用户可以配置这个间隔时间）的这种隐藏文件。在文件被彻底删除之前，仍可通过该文件被重命名后的特殊的新文件名对其进行访问，也可以通过将其重命名为正常文件的方式撤销删除。当隐藏文件被从命名空间中移除时，其在内存中的元数据也会被删除。这种方式可以有效地切断文件和其对应的chunk的链接。</p><h3 id="chunk级垃圾回收"><a href="#chunk级垃圾回收" class="headerlink" title="chunk级垃圾回收"></a>chunk级垃圾回收</h3><p>chunk成为无法被任何文件访问到的孤儿chunk的原因可能是chunk的创建可能仅在部分chunkserver上成功而在其他chunkserver上失败，或者chunk在进行删除时某些chunk server没有收到相应的消息。</p><p>在进行chunk级垃圾回收时，master会周期性扫描chunk命名空间，并找出孤儿chunk，删除这些chunk的元数据。在chunkserver周期性地与master进行心跳消息交换时，chunkserver会报告其拥有的chunk的子集，而master会回复这些chunk中元数据已经不存在的chunk的标识。chunkserver可以自由地删除这些元数据已经不存在的chunk的副本。</p><p>该垃圾回收机制的优点：</p><ol><li>这种方法在设备经常出现故障的大规模可伸缩分布式系统中非常简单可靠。chunk的创建可能仅在部分chunkserver上成功而在其他chunkserver上失败，这样会导致系统中出现master不知道的副本。且副本删除消息可能会丢失，这样master在其自身和chunkserver故障时都必须重新发送该消息。垃圾回收机制为清理那些不知道是否有用的副本提供了一个统一且可靠的方法。</li><li>垃圾回收机制将对存储空间的回收操作合并为master的后台活动，如周期性扫描命名空间和周期性地与chunkserver握手。因此，垃圾回收机制可以分批回收存储空间并平摊回收的开销。另外，垃圾回收仅在master相对空闲时执行。这样，master可以更迅速的相应需要及时响应的来自client的请求。</li><li>延迟回收存储空间可以防止意外的不可逆删除操作。</li></ol><p>该垃圾回收机制的缺点：</p><ol><li>当用户存储空间紧张时，延迟回收会让用户难以释放存储空间。</li><li>快速创建并删除临时文件的应用程序可能无法立刻重用存储空间。</li></ol><p>为了解决这个问题，用户可以再次显示删除已删除文件时，加快了对存储空间的回收。同时，允许用户对不同的命名空间应用不同的副本与回收策略。例如，用户可以指定某个目录树下的所有文件都不需要副本，且当这个目录树下的文件被删除时立刻且无法撤销地将其从文件系统中移除。</p><h1 id="读操作"><a href="#读操作" class="headerlink" title="读操作"></a>读操作</h1><p>如图1所示，具体读操作在流程如下：</p><ol><li>client会将读取的文件名，读取的是第几个chunk发送给master</li><li>master返回给client该chunk的chunk handle以及其所在的所有chunkserver</li><li>client接收信息，并进行缓存，然后选择距离其最近的chunkserver（可以直接通过ip规律决定）来发起询问，请求对应chunk handle中byte范围的内容</li><li>被询问的chunkserver根据自己保存的chunk handle与具体linux文件的对应关系来读取文件（实际应该就是以chunk handle命名的文件），并返回对应的内容</li><li>client获取到对应的内容，如果还需要接着读取，可以依据缓存信息直接向client发起读取请求</li></ol><h1 id="写操作"><a href="#写操作" class="headerlink" title="写操作"></a>写操作</h1><p><img src="/2023/12/05/TheGoogleFileSystemPaperRead/write.png" alt="图2 写操作"></p><h2 id="租约"><a href="#租约" class="headerlink" title="租约"></a>租约</h2><p>虽然每个chunk有多个副本，但是为了保证副本间变更的一致性，master向其中一份副本授权一个变更的租约，称这个副本为primary（有时也可代指primary副本所在的chunkserver），其余的副本称为Secondary。租约的时间为60秒。然而，一旦chunk被变更，primary就可以向master请求延长租约时间，或者（通常为）接受来自master的租约时间延长操作。这些租约延长请求和租约授权请求依赖master与chunkserver间周期性地心跳消息来实现。即使master与一个primary的通信丢失，master仍可以在旧租约过期后安全地向另一个副本授权新的租约，以此来避免同时产生多个primary。</p><h2 id="数据流"><a href="#数据流" class="headerlink" title="数据流"></a>数据流</h2><p>在进行写时，需要将数据传递给每个chunk server，为了高效地利用网络，其对数据流与控制流进行了解耦。为了充分利用机器的网络带宽，数据会沿着chunkserver链线性地推送。假设client正准备将数据推送给S1~S4。client会将数据发送给最近的chunkserver，比如S1。S1会将数据传递给S2至S4中离它最近的chunkserver，比如S2。同样，S2会将数据传递给S3至S4中离它最近的chunkserver，以此类推。由于其网络拓扑非常简单，所以可以通过IP地址来准确地估算出网络拓扑中的“距离”。</p><p>当chunkserver收到一部分数据时，它会立刻开始将数据传递给其他chunkserver。因为我们使用全双工的交换网络，所以流水线可以大幅减少时延。发送数据不会减少接受数据的速度。如果没有网络拥塞，理论上将$B$个字节传输给$R$个副本所需的时间为$B&#x2F;T+RL$，其中$T$是网络的吞吐量，$L$是两台机器间的传输时延。通常，我们的网络连接吐吞量$T$为$100Mbps$，传输时延$L$远小于$1ms$。</p><h2 id="写流程"><a href="#写流程" class="headerlink" title="写流程"></a>写流程</h2><p>如图2所示，写流程如下：</p><ol><li>client向master询问哪个chunkserver持有指定chunk的租约及该chunk的其他副本的位置。如果没有chunkserver持有租约，那么master会选择一个副本对其授权（这一步在图中没有展示）</li><li>master回复primary副本的标识符和其他副本（也称secondary）的位置。client也对其进行缓存</li><li>client将数据通过数据流的方式推送到所有副本</li><li>一旦所有副本都确认收到了数据，client会向primary发送一个write请求。primary会为其收到的所有的变更（可能来自多个client）分配连续的编号，这一步提供了重要的顺序。primary对在本地按照该顺序应用变更</li><li>primary将write请求继续传递给其他secondary副本。每个secondary副本都按照primary分配的顺序来应用变更。</li><li>所有的secondary副本通知primary其完成了变更操作。</li><li>primary回复client。任意副本遇到的任何错误都会被报告给client。即使错误发生，write操作可能已经在primary或secondary的任意子集中被成功执行。（如果错误在primary中发生，那么操作将不会被分配顺序，也不会被继续下发到其他副本。）只要错误发生，该请求都会被认为是失败的，且被修改的区域的状态为inconsistent。client中的代码会通过重试失败的变更来处理这种错误。首先它会重试几次步骤（3）到步骤（7），如果还没有成功，再从write请求的初始操作开始重试。成功后该区域会被修正为consistent状态</li></ol><p>如果应用程序发出的一次write请求过大或跨多个chunk，GFS的client代码会将其拆分成多个write操作。拆分后的write请求都按照上文中的控制流执行，但是可能存在与其他client的并发的请求交叉或被其他client的并发请求覆盖的情况。因此，共享的文件区域最终可能包含来自不同client的片段。但共享的文件区域中的内容最终是相同的，因为每个操作在所有副本上都会以相同的顺序被成功执行。</p><h2 id="原子性record-append"><a href="#原子性record-append" class="headerlink" title="原子性record append"></a>原子性record append</h2><p>在传统的write操作中，client会指定数据写入的偏移量。然而在record append中，client仅需指定待追加的数据。GFS会为其选择一个偏移量，在该偏移量处至少一次地原子性地将数据作为一个连续的字节序列追加到文件，并将该偏移量返回给client。</p><p>record append被大量应用在的有多个来自不同机器的client向同一个文件并发append数据的分布式应用程序中。如果通过传统的write操作，那么client还需要额外的复杂且开销很高的同步操作（例如分布式锁管理）。</p><p>record append仅在primary端稍有点额外的逻辑。在client将数据推送到文件中最后一个chunk的所有chunk server之后，client会向primary发送一个请求。primary会检查当新记录追加到该chunk之后，是否会导致该chunk超过其最大大小限制（64MB）。如果会超，primary会将该chunk填充到最大的大小，并通知secondary也做相同的填充操作，再回复客户端，使其在下一个chunk上重试该操作。record append操作限制了每次最多写入最大chunk大小的四分之一的数据，以保证在最坏的情况下产生的碎片在可接受的范围内。在一般情况下，添加的记录大小都在不会超过chunk的最大限制，这样primary会向数据追加到它的副本中，并通知secondary在与其追加的偏移量相同的位置处写入数据，并将最终成功操作的结果返回给client。</p><p>如果record append操作在任何一个副本中失败，就会返回失败，使得client会重试操作。这样会导致同一个chunk的<strong>不同副本中可能包含不同的数据</strong>，这些数据可能是同一条记录的部分或完整的副本。GFS不保证所有副本在字节级别一致，其只保证record append的数据<strong>作为一个单元被原子性地至少写入一次</strong>。这一点很容易证明，因为数据必须在某个chunk的所有副本的相同偏移位置处写入。此外，在record append之后，每个副本都至少与最后一条记录一样长。这样，任何未来的新记录都会被分配到一个更高的偏移位置或者一个新chunk，即使另一个副本成为了primary也能保证这个性质。这样，被record append操作<strong>成功写入的区域</strong>在一致性方面都将是<strong>defined状态</strong>（因此也是consistent的），而这些<strong>defined区域间的文件区域是inconsistent的（因此也是undefined的）</strong>。我们应用程序会通过章节2.7.2中讨论的方式处理inconsistent的区域。</p><h2 id="一致性"><a href="#一致性" class="headerlink" title="一致性"></a>一致性</h2><p><img src="/2023/12/05/TheGoogleFileSystemPaperRead/consistent.jpg" alt="图3 一致性"></p><p><strong>一致性：</strong>一个文件区域的任意一个副本被任何client读取总能得到相同的数据<br><strong>确定性：</strong>client总能读取到其写入的信息</p><p>在没有并发的情况下写入成功时，写入的内容是一致且确定的<br>在有并发的情况下写入成功时，写入的内容是一致但非确定的，因为写入的内容可能混合了多个请求</p><p>写入操作操作可能为write或record append，其中对于record append，GFS可能会在记录的中间插入填充（padding）和或重复的记录。它们占用的区域状态为inconsistent的，通常情况下，它们的数量远少于用户数据。</p><p>GFS会保证被写入成功的的区域各个副本都是一致的，主要通过以下手段：</p><ol><li>chunk执行变更时，其所有副本按照相同的顺序应用变更</li><li>使用chunk版本号（chunk version）来检测因chunkserver宕机而错过了变更的陈旧的chunk副本。陈旧的chunk副本永远不会在执行变更时被使用，也不会在master返回client请求的chunk的位置时被使用。它们会尽早地被作为垃圾回收。</li><li>即使在变更被成功应用的很长时间后，设备故障仍然可以损坏（corrupt）会销毁（destroy）数据。GFS通过master和所有chunkserver周期性握手的方式来确定故障的chunkserver，并通过校验和（checksunmming）的方式检测数据损坏。一旦出现问题，数据会尽快地从一个合法的副本恢复。一个chunk只有在GFS作出反应前（通常在几分钟内）失去了所有的副本，chunk才会不可逆地丢失。即使在这种情况下，chunk也仅变得不可用而非返回错误的数据。</li></ol><h3 id="应用程序影响"><a href="#应用程序影响" class="headerlink" title="应用程序影响"></a>应用程序影响</h3><p>GFS应用程序可以通过一些简单的技术来使用其宽松的一致性模型，且这些技术已经因其他目标而被使用。</p><p>在实际使用中，我们所有的应用程序都通过append而不是overwrite的方式对文件进行变更。其中一个典型的引用场景是：一个write从头到尾地生成一个文件。它会周期性地为已经写入的文件数据创建检查点，并在所有数据都被写入文件后自动将其重命名为一个永久的文件名。检查点可能包含应用程序级别的校验和。reader会验证文件仅处理跟上最新的检查点的文件区域，这些区域的状态一定的“defined”的。尽管这种方法有一致性和并发问题，它仍很好地满足了我们的需求。append的效率远高于随机写入，且在应用程序故障时更容易恢复。检查点机制允许writer在重启时增量写入，并能够防止reader处理那些虽然已经被成功写入文件但是从应用程序的角度看仍然不完整的文件数据。</p><p>另一种典型的用途是，许多write并发地向同一个文件append数据以获得合并后的结果或文件作为生产者-消费者队列使用。record append的“至少一次追加（append-at-least-once）”语义保证了每个write的输出。</p><p>而reader偶尔需要<strong>处理填充和重复的数据</strong>。每条被writer准备好的记录包含如校验和的额外信息，这样，记录的合法性就可被校验。一个reader通过校验和来识别并丢弃额外的填充和记录。如果rearder无法容忍偶尔发生的重复（如果重复的记录可能触发非幂等（non-idempotent）运算），它可以使用记录中的唯一标识符来对齐进行过滤。通常，在命名应用程序相关的实体时（如web文档），总会使用唯一的标识符。这些记录I &#x2F; O (除去重复)的功能在我们的应用程序共享的库代码中，并适用于Google的其他文件接口实现。通过这些库，带有极少的重复的记录，总会被以相同顺序交付给reader。</p><h1 id="容错处理"><a href="#容错处理" class="headerlink" title="容错处理"></a>容错处理</h1><h2 id="chunk副本"><a href="#chunk副本" class="headerlink" title="chunk副本"></a>chunk副本</h2><p>每个chunk会在不同机架的多个chunkserver上存有副本。用户可以为不同命名空间的文件制定不同的副本级别。副本级别默认为3。当有chunkserver脱机或通过校验和检测到损坏的副本时，master根据需求克隆现有的副本以保证每个chunk的副本数都是饱和的。</p><h2 id="master副本"><a href="#master副本" class="headerlink" title="master副本"></a>master副本</h2><p>master的状态同样有副本，master的操作日志和检查点被<strong>在多台机器上复制</strong>。只有当变更在被日志记录并被写入，master本地和所有master副本的磁盘中后，这个变更才被认为是已提交的。为了简单起见，一个master进程既要负责处理所有变更又要负责处理后台活动，如垃圾回收等从内部改变系统的活动。当master故障时，其几乎可以立刻重启。如果运行master进程的机器故障或其磁盘故障，在GFS之外的负责监控的基础架构会在其它持有master的操作日志副本的机器上启动一个新的master进程。client仅通过一个规范的命名来访问master结点（例如gfs-test），这个规范的命名是一个DNS别名，其可以在master重新被分配到另一台机器时被修改为目标机器。</p><p>此外，“影子”master节点（“shadow” master）<strong>可以提供只读的文件系统访问</strong>，即使在主master结点脱机时它们也可以提供服务。因为这些服务器可能稍稍滞后于主master服务器（通常滞后几分之一秒），所以这些服务器是影子服务器而非镜像服务器。这些影子master服务器增强了那些非正在被变更的文件和不介意读到稍旧数据的应用程序的可用性。实际上，由于文件内容是从chunkserver上读取的，所以应用程序不会读取到陈旧的文件内容。能够在一个很短的时间窗口内被读取到的陈旧的数据只有文件元数据，如目录内容和访问控制信息。</p><p>为了让自己的元数据跟随主master变化，影子master服务器会持续读取不断增长的操作日志副本，并像主master一样按照相同的顺序对其数据结构应用变更。像主master一样，影子master服务器也会在启动时从chunkserver拉取数据来获取chunk副本的位置（启动后便很少拉取数据），并频繁地与chunkserver交换握手信息来监控它们的状态。<strong>只有因主master决定创建或删除副本时，影子master服务器上的副本位置才取决于主master服务器</strong>。</p><h2 id="数据完整性"><a href="#数据完整性" class="headerlink" title="数据完整性"></a>数据完整性</h2><p>每个chunkserver都<strong>使用校验和来检测存储的数据是否损坏</strong>。由于GFS集群通常在数百台机器上有数千chunk磁盘，所以集群中经常会出现磁盘故障，从而导致数据损坏或丢失。我们可以<strong>通过chunk的其他副本来修复损坏的chunk</strong>，但不能通过比较chunkserver间的副本来检测chunk是否损坏。除此之外，即使内容不同的副本中的数据也可能都是合法的：GFS中变更的语义（特别是前文中讨论过的record append）不会保证副本完全相同。因此，<strong>每个chunkserver必须能够通过维护校验和的方式独立的验证副本中数据的完整性</strong>。</p><p><strong>一个chunk被划分为64KB的block</strong>。<strong>每个block有其对应的32位校验和</strong>。就像其他元数据一样，校验和也在内存中保存且会被通过日志的方式<strong>持久化存储</strong>。校验和与用户数据是分开存储的。</p><p>对于读取操作，无论请求来自client还是其他chunkserver，<strong>chunkserver都会在返回任何数据前校验所有包含待读取数据的block的校验和</strong>。因此，chunkserver不会将损坏的数据传给其他机器。如果一个block中数据和记录中低的校验和不匹配，那么chunkserver会给请求者返回一个错误，<strong>并向master报告校验和不匹配</strong>。随后，请求者会从其他副本读取数据，<strong>而master会从该chunk的其他副本克隆这个chunk</strong>。当该chunk新的合法的副本被安置后，master会通知报告了校验和不匹配的chunkserver删除那份损坏的副本。</p><p>校验和对读取性能的影响很小。因为我们的大部分读操作至少会读跨几个block的内容，我们只需要读取并校验相对少量的额外数据。GFS客户端代码通过尝试将读取的数据与需要校验的block边界对其的方式，进一步地减小了校验开销。除此之外，chunkserver上校验和的查找与比较不需要I&#x2F;O操作，且校验和计算操作经常与其他操作在I&#x2F;O上重叠，因此几乎不存在额外的I&#x2F;O开销。</p><p>因为向chunk末尾append数据的操作在我们的工作负载中占主要地位，所以我们对这种写入场景的校验和计算做了大量优化。在<strong>append操作</strong>时，<strong>我们仅增量更新上一个block剩余部分的校验和，并为append的新block计算新校验和</strong>。即使最后一个block已经损坏且目前没被检测到，增量更新后的该block的新校验和也不会与block中存储的数据匹配。在下一次读取该block时，GFS会像往常一样检测到数据损坏。</p><p>相反，如果<strong>write操作</strong>覆盖了一个chunk已存在的范围，那么我们<strong>必须读取并验证这个范围的头一个和最后一个block</strong>，再执行write操作，最后计算并记录新的校验和。如果我们没有在写入前校验头一个和最后一个block，新的校验和可能会掩盖这两个block中没被覆写的区域中存在的数据损坏问题。因为写入会修改头一个和后一个block的部分内容，且会重新计算校验和，如果该内容以损坏，然后又重新计算了校验和，就会掩盖损坏内容。</p><p>chunkserver可以<strong>在空闲期间扫描并验证非活动的chunk的内容</strong>。这样可以让我们检测到很少被读取的chunk中的数据损坏。一旦检测到数据损坏，master可以创建一个新的未损坏的副本并删除损坏的副本。这样可以防止master将chunk的非活动的但是已损坏的副本识别成数据合法的副本。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>总体而言GFS提供了一个大规模分布式存储的一个良好的解决方案，也让我对分布式存储有了更深的影响，其中其数据流与控制流解耦，租约设计，弱一致性换取高性能，具体的分布式读写操作，分布式数据容错方案都给我留下了深刻的印象。</p><p>但是GFS也被提出具有一定的问题，其问题主要来自于master节点保存的内容过多，master节点的容错率不强等。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="http://research.google.com/archive/gfs.html">http://research.google.com/archive/gfs.html</a></li><li><a href="https://pdos.csail.mit.edu/6.824/schedule.html">https://pdos.csail.mit.edu/6.824/schedule.html</a></li><li><a href="https://blog.mrcroxx.com/posts/paper-reading/gfs-sosp2003/">https://blog.mrcroxx.com/posts/paper-reading/gfs-sosp2003/</a></li><li><a href="https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-03-gfs/3.1">https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-03-gfs/3.1</a></li><li><a href="https://www.youtube.com/watch?v=WLad7CCexo8&ab_channel=BitTiger%E5%AE%98%E6%96%B9%E9%A2%91%E9%81%93BitTigerOfficialChannel">https://www.youtube.com/watch?v=WLad7CCexo8&ab_channel&#x3D;BitTiger%E5%AE%98%E6%96%B9%E9%A2%91%E9%81%93BitTigerOfficialChannel</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>MIT6.824</category>
      
      <category>论文</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式</tag>
      
      <tag>MIT6.824</tag>
      
      <tag>论文阅读</tag>
      
      <tag>GFS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>IDEA远程开发选项丢失修复方法</title>
    <link href="/2023/12/01/IDEA%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91%E9%80%89%E9%A1%B9%E4%B8%A2%E5%A4%B1%E4%BF%AE%E5%A4%8D%E6%96%B9%E6%B3%95/"/>
    <url>/2023/12/01/IDEA%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91%E9%80%89%E9%A1%B9%E4%B8%A2%E5%A4%B1%E4%BF%AE%E5%A4%8D%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<p>今天打开IDEA发现远程开发选项没有了：</p><p><img src="/2023/12/01/IDEA%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91%E9%80%89%E9%A1%B9%E4%B8%A2%E5%A4%B1%E4%BF%AE%E5%A4%8D%E6%96%B9%E6%B3%95/%E6%8D%9F%E5%9D%8F%E7%95%8C%E9%9D%A2.jpg" alt="损坏界面"></p><p>发现全网也没有什么提到过这个错误的，所有捣鼓了好久还进行了IDEA的重装也一直都没有解决，最后查看<a href="https://www.jetbrains.com/help/idea/2023.2/jetbrains-gateway.html#plugin_install">IDEA官方的介绍文档</a>才发现了问题所在：<strong>Remote Development Gateway插件被关闭了</strong></p><p>故而解决方法就是点击IDEA的设置选项卡，在插件(plugs)选项中重新勾选<code>Remote Development Gateway</code>来启用该插件即可</p><p><img src="/2023/12/01/IDEA%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91%E9%80%89%E9%A1%B9%E4%B8%A2%E5%A4%B1%E4%BF%AE%E5%A4%8D%E6%96%B9%E6%B3%95/%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95.jpg" alt="解决方法"></p><p>点击启用后可以发现远程开发选项又回来了：</p><p><img src="/2023/12/01/IDEA%E8%BF%9C%E7%A8%8B%E5%BC%80%E5%8F%91%E9%80%89%E9%A1%B9%E4%B8%A2%E5%A4%B1%E4%BF%AE%E5%A4%8D%E6%96%B9%E6%B3%95/%E4%BF%AE%E5%A4%8D%E5%90%8E%E7%95%8C%E9%9D%A2.jpg" alt="修复后界面"></p><p>所以还是要多从官方的信息源找起啊</p>]]></content>
    
    
    <categories>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>IDEA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【MIT6.824】lab 1 MapReduce实现总结</title>
    <link href="/2023/11/22/MIT6-824lab1/"/>
    <url>/2023/11/22/MIT6-824lab1/</url>
    
    <content type="html"><![CDATA[<p>MIT6.824是一门经典的分布式课程，课程链接：<a href="https://pdos.csail.mit.edu/6.824/labs/lab-mr.html">https://pdos.csail.mit.edu/6.824/labs/lab-mr.html</a>，对于lab 1我们需要在提供的代码框架的基础上补充coordinator和worker的代码，以实现分布式的MapReduce程序。</p><p>本人在借鉴了部分其他人的设计思想的基础上，独立完成了所有的代码，最后设计的实现能够通过所有的测试脚本。</p><p>实现的代码厂库：<a href="https://github.com/slipegg/MIT6.824/tree/main/6.5840">https://github.com/slipegg/MIT6.824/tree/main/6.5840</a></p><h1 id="实现目标"><a href="#实现目标" class="headerlink" title="实现目标"></a>实现目标</h1><p>在给定的代码框架中实现一个单词计数的MapReduce程序。原本的框架中已经给了一个在本地串行执行单词计数的独立程序，并提供了一个通过UNIX-domain sockets实现的RPC(<a href="https://slipegg.github.io/2023/11/14/RPC%E4%BB%8B%E7%BB%8D/">RPC介绍</a>)，我们需要完成的部分有：</p><ol><li>设计coordinator和worker之间交流的流程和格式，以方便worker向coordinator申请任务，coordinator将taks发送给worker，worker把task的完成情况返回给coordinator</li><li>coordinator对Map类型的task和Reduce类型的task进行管理，需要初始化这些任务，需要记录任务完成的情况，并生成新的任务，直到全部完成</li><li>worker如何完成Map类型以及Reduce类型task</li></ol><h1 id="总体设计"><a href="#总体设计" class="headerlink" title="总体设计"></a>总体设计</h1><p><strong>worker</strong>会不断向coordinator发送心跳，申请任务，拿到任务后进行map或者renduce类型的task的执行，在执行完毕后发送请求给coordinator以表示该任务完成了。当coordinator告诉其所有任务都完成时，他会结束运行</p><p><strong>coordinator</strong>只维护task的状态不维护各个worker的状态。worker向其发送心跳申请任务时，coordinator会去遍历任务，取出还没有发送的任务或者过了太长时间都没有完成的任务返回回去，如果没有，就返回一个等待任务。coordinator接收到worker的某个任务完成的请求时会改变这个任务的状态，如果当前阶段所有的任务都完成了就转向下一个阶段，知道转到了所有MapReduce任务都完成的阶段。</p><p>整体流程如下图所示：</p><p><img src="/./MIT6-824lab1/MapReduce.png" alt="MapReduce流程"></p><h1 id="rpc信息传递设计"><a href="#rpc信息传递设计" class="headerlink" title="rpc信息传递设计"></a>rpc信息传递设计</h1><h2 id="Heartbeat"><a href="#Heartbeat" class="headerlink" title="Heartbeat"></a>Heartbeat</h2><p>worker通过rpc向coordinator发送心跳（Heartbeat）来申请任务。如下：</p><ul><li><p>关键结构体定义如下，HeartbeatRequest是个空结构，HeartbeatResponse承载了coordinator返回给worker的信息，这里的信息实际上是运行map类型和reduce类型的task所必须的信息的集合。所有的返回都需要JobType来标明其类型，需要id来标明其是哪个作业，<strong>对于map类型作业</strong>，其额外需要FilePath来获取任务的输入，还需要NReduce来决定输出的数量，<strong>对于reduce类型作业</strong>，其额外需要NMap来辅助获取map类型的中间输出。</p>  <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> HeartbeatRequest <span class="hljs-keyword">struct</span> &#123;<br>&#125;<br><br><span class="hljs-keyword">type</span> HeartbeatResponse <span class="hljs-keyword">struct</span> &#123;<br>    FilePath <span class="hljs-type">string</span><br>    JobType  JobType<br>    NReduce  <span class="hljs-type">int</span><br>    NMap     <span class="hljs-type">int</span><br>    Id       <span class="hljs-type">int</span><br>&#125;<br></code></pre></td></tr></table></figure></li><li><p>调用请求如下，它会调用coordinator的heartbeat函数来处理，并将任务返回到response中。</p>  <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs go">call(<span class="hljs-string">&quot;Coordinator.Heartbeat&quot;</span>, &amp;HeartbeatRequest&#123;&#125;, &amp;response)<br></code></pre></td></tr></table></figure></li></ul><h2 id="Report"><a href="#Report" class="headerlink" title="Report"></a>Report</h2><p>worker完成任务后通过rpc向coordinator发送回复。如下：</p><ul><li><p>关键结构体设计如下。ReportRequest通过phase和id来联合表示是哪个任务完成了。</p>  <figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs elm"><span class="hljs-keyword">type</span> <span class="hljs-type">ReportRequest</span> struct &#123;<br>    <span class="hljs-type">Id</span>    int<br>    <span class="hljs-type">Phase</span> <span class="hljs-type">SchedulePhase</span><br>&#125;<br><br><span class="hljs-keyword">type</span> <span class="hljs-type">ReportResponse</span> struct &#123;<br>&#125;<br><br></code></pre></td></tr></table></figure></li><li><p>调用请求如下,它会调用coordinator的Report函数来处理，来将该任务标记为运行结束。</p>  <figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs lisp">call(<span class="hljs-string">&quot;Coordinator.Report&quot;</span>, <span class="hljs-symbol">&amp;ReportRequest</span>&#123;Id: id, Phase: phase&#125;, <span class="hljs-symbol">&amp;ReportResponse</span>&#123;&#125;)<br></code></pre></td></tr></table></figure></li></ul><h1 id="coordinator设计"><a href="#coordinator设计" class="headerlink" title="coordinator设计"></a>coordinator设计</h1><p>coordinator会衍生出2个额外的协程，一个负责给rpc注册，并响应rpc传来的函数调用请求，一个负责给worker选择task生成resopnse</p><h2 id="rpc函数调用处理"><a href="#rpc函数调用处理" class="headerlink" title="rpc函数调用处理"></a>rpc函数调用处理</h2><p>给rpc注册的程序就是原本框架提供的代码，具体代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-comment">// start a thread that listens for RPCs from worker.go</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Coordinator)</span></span> server() &#123;<br>rpc.Register(c)<br>rpc.HandleHTTP()<br><span class="hljs-comment">//l, e := net.Listen(&quot;tcp&quot;, &quot;:1234&quot;)</span><br>sockname := coordinatorSock()<br>os.Remove(sockname)<br>l, e := net.Listen(<span class="hljs-string">&quot;unix&quot;</span>, sockname)<br><span class="hljs-keyword">if</span> e != <span class="hljs-literal">nil</span> &#123;<br>log.Fatal(<span class="hljs-string">&quot;listen error:&quot;</span>, e)<br>&#125;<br><span class="hljs-keyword">go</span> http.Serve(l, <span class="hljs-literal">nil</span>)<br>&#125;<br><br></code></pre></td></tr></table></figure><p>其相应的也是上面提到的hearbeat和report事件。比较有go特色的的地方在于如何等待另一个进程生成对应的回应，采用的是如下的代码：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> heartbeatMsg <span class="hljs-keyword">struct</span> &#123;<br>response *HeartbeatResponse<br>ok       <span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Coordinator)</span></span> Heartbeat(request *HeartbeatRequest, response *HeartbeatResponse) <span class="hljs-type">error</span> &#123;<br>msg := heartbeatMsg&#123;response, <span class="hljs-built_in">make</span>(<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>&#123;&#125;)&#125;<br>c.heartbeatCh &lt;- msg<br>&lt;-msg.ok<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><p>构建msg将信息传递过去给c.heartbeach，然后等待msg.ok准备就绪，也就是response填好了数据，再返回。Report也是同理</p><h2 id="task管理"><a href="#task管理" class="headerlink" title="task管理"></a>task管理</h2><p>在coordinator初始化时会生成一个schedule协程来负责task生成和管理</p><ul><li>task有4种类型Map类型、Reduce类型、等待类型和完成类型，Map类型和Reduce类需要worker进行实际处理，等待类型只需要worker去sleep一段时间就好了，然后再去询问有没有新任务，完成类型的任务发送过来之后worker就可以结束运行了</li><li>task有3个状态，分别为等待、运行、完成，一开始初始化时为等待状态，交给worker运行后为运行状态，worker发送report回来说明自己运行完毕后为完成状态。</li><li>coordinator有三个阶段分别为Map阶段、Reduce阶段和Complete阶段，一开始为Map阶段，其需要处理Map类型的task，当Map类型的task全部完成后需要转变到Reduce阶段，处理Reduce类型的task，当Reduce类型的状态也全部完成后就转为Complete状态，可以结束运行了。</li></ul><p>schedule代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Coordinator)</span></span> schedule() &#123;<br>c.initMapPhase()<br><br><span class="hljs-keyword">for</span> &#123;<br><span class="hljs-keyword">select</span> &#123;<br><span class="hljs-keyword">case</span> msg := &lt;-c.heartbeatCh:<br>isAllTaskDoneInPhase := c.selectANewTask(msg.response)<br><span class="hljs-keyword">if</span> isAllTaskDoneInPhase &#123;<br>c.switchPhase()<br>c.selectTaskAfterSwitchPhase(msg.response)<br>&#125;<br>log.Printf(<span class="hljs-string">&quot;Coordinator: Heartbeat response: %v\n&quot;</span>, msg.response)<br>msg.ok &lt;- <span class="hljs-keyword">struct</span>&#123;&#125;&#123;&#125;<br><br><span class="hljs-keyword">case</span> msg := &lt;-c.reportCh:<br><span class="hljs-keyword">if</span> msg.request.Phase == c.phase &#123;<br>log.Printf(<span class="hljs-string">&quot;Coordinator: Worker has finished %v-task%d\n&quot;</span>, c.phase, msg.request.Id)<br>c.tasks[msg.request.Id].status = Finished<br>&#125;<br>msg.ok &lt;- <span class="hljs-keyword">struct</span>&#123;&#125;&#123;&#125;<br>&#125;<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>生成一个新任务时需要去遍历查看是否有处于等待状态的任务或者是运行时间过久（说明worker可能已经挂掉了）的任务，然后将其分配出去，主要代码如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Coordinator)</span></span> selectANewTask(response *HeartbeatResponse) <span class="hljs-type">bool</span> &#123;<br>isAllTaskDone, isNewTaskScheduled := <span class="hljs-literal">true</span>, <span class="hljs-literal">false</span><br><br><span class="hljs-keyword">for</span> id, task := <span class="hljs-keyword">range</span> c.tasks &#123;<br><span class="hljs-keyword">switch</span> task.status &#123;<br><span class="hljs-keyword">case</span> Idle:<br>isAllTaskDone, isNewTaskScheduled = <span class="hljs-literal">false</span>, <span class="hljs-literal">true</span><br>c.tasks[id].status, c.tasks[id].startTime = Working, time.Now()<br>c.scheduleTaskToResponse(id, response)<br><br><span class="hljs-keyword">case</span> Working:<br>isAllTaskDone = <span class="hljs-literal">false</span><br><span class="hljs-keyword">if</span> time.Since(task.startTime) &gt; MaxTaskRunInterval &#123;<br>isNewTaskScheduled = <span class="hljs-literal">true</span><br>c.tasks[id].startTime = time.Now()<br>c.scheduleTaskToResponse(id, response)<br>&#125;<br><br><span class="hljs-keyword">case</span> Finished:<br>&#125;<br><br><span class="hljs-keyword">if</span> isNewTaskScheduled &#123;<br><span class="hljs-keyword">break</span><br>&#125;<br>&#125;<br><br><span class="hljs-keyword">if</span> !isNewTaskScheduled &amp;&amp; !isAllTaskDone &#123;<br>response.JobType = WaitJob<br>&#125;<br><br><span class="hljs-keyword">return</span> isAllTaskDone<br>&#125;<br></code></pre></td></tr></table></figure><p>当coordinator进行Complete阶段后其实并不会再去处理其他事情，比如给worker发送运行结束的指令，而是直接给doneCh赋值，然后以此退出运行</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(c *Coordinator)</span></span> Done() <span class="hljs-type">bool</span> &#123;<br>&lt;-c.doneCh<br>log.Printf(<span class="hljs-string">&quot;Coordinator: Done\n&quot;</span>)<br><span class="hljs-keyword">return</span> <span class="hljs-literal">true</span><br>&#125;<br></code></pre></td></tr></table></figure><h1 id="worker-设计"><a href="#worker-设计" class="headerlink" title="worker 设计"></a>worker 设计</h1><h2 id="运行流程"><a href="#运行流程" class="headerlink" title="运行流程"></a>运行流程</h2><p>worker就是不断地发送heartbeat命令然后获取任务进行运行，直到接收到了Complete任务或者发送heartbeat失败，就可以结束运行了。如下</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">Worker</span><span class="hljs-params">(mapf <span class="hljs-keyword">func</span>(<span class="hljs-type">string</span>, <span class="hljs-type">string</span>)</span></span> []KeyValue,<br>reducef <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(<span class="hljs-type">string</span>, []<span class="hljs-type">string</span>)</span></span> <span class="hljs-type">string</span>) &#123;<br><br><span class="hljs-keyword">for</span> &#123;<br>response := doHeartbeat()<br>log.Printf(<span class="hljs-string">&quot;Worker: receive coordinator&#x27;s response, new job is %v \n&quot;</span>, response)<br><br><span class="hljs-keyword">switch</span> response.JobType &#123;<br><span class="hljs-keyword">case</span> MapJob:<br>doMapTask(mapf, response)<br><span class="hljs-keyword">case</span> ReduceJob:<br>doReduceTask(reducef, response)<br><span class="hljs-keyword">case</span> WaitJob:<br>time.Sleep(<span class="hljs-number">1</span> * time.Second)<br><span class="hljs-keyword">case</span> CompleteJob:<br><span class="hljs-keyword">return</span><br><span class="hljs-keyword">default</span>:<br><span class="hljs-built_in">panic</span>(fmt.Sprintf(<span class="hljs-string">&quot;worker get an unexpected jobType %v&quot;</span>, response.JobType))<br>&#125;<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="Map类型task"><a href="#Map类型task" class="headerlink" title="Map类型task"></a>Map类型task</h2><p>Map类型的task的处理如下所示，总体就是调用mapF统计文件中各个单词的数量，并记录到中间文件中，由于将中间结果写入到文件中是可以并行运行的，所以这里启动了多个协程来进行处理</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">doMapTask</span><span class="hljs-params">(mapF <span class="hljs-keyword">func</span>(<span class="hljs-type">string</span>, <span class="hljs-type">string</span>)</span></span> []KeyValue, response *HeartbeatResponse) &#123;<br>wordCountList := getWordCountListOfFile(mapF, response.FilePath)<br><br>intermediate := splitWordCountListToReduceNNum(wordCountList, response.NReduce)<br><br><span class="hljs-keyword">var</span> writeIntermediateFilewg sync.WaitGroup<br><span class="hljs-keyword">for</span> reduceNumber, splitedWordCountList := <span class="hljs-keyword">range</span> intermediate &#123;<br>writeIntermediateFilewg.Add(<span class="hljs-number">1</span>)<br><span class="hljs-keyword">go</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(reduceNumber <span class="hljs-type">int</span>, splitedWordCountList []KeyValue)</span></span> &#123;<br><span class="hljs-keyword">defer</span> writeIntermediateFilewg.Done()<br>writeIntermediateFile(response.Id, reduceNumber, splitedWordCountList)<br>&#125;(reduceNumber, splitedWordCountList)<br>&#125;<br>writeIntermediateFilewg.Wait()<br><br>doReport(response.Id, MapPhase)<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="Reduce类型task"><a href="#Reduce类型task" class="headerlink" title="Reduce类型task"></a>Reduce类型task</h2><p>Reduce类型task的处理如下所示，总体就是把对应的中间文件读出来，将结果通过reduceF进行聚集，输出到最终的文件中</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">doReduceTask</span><span class="hljs-params">(reduceF <span class="hljs-keyword">func</span>(<span class="hljs-type">string</span>, []<span class="hljs-type">string</span>)</span></span> <span class="hljs-type">string</span>, response *HeartbeatResponse) &#123;<br>wordCountList := getWordCountListFromIntermediateFile(response.NMap, response.Id)<br><br>wordCountMap := gatherAndSortIntermediateWordCountList(wordCountList)<br><br><span class="hljs-keyword">var</span> buf bytes.Buffer<br>reducIntermediateWordCount(reduceF, wordCountMap, &amp;buf)<br><br>fileName := generateReduceResultFileName(response.Id)<br>atomicWriteFile(fileName, &amp;buf)<br><br>doReport(response.Id, ReducePhase)<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="原子写入文件"><a href="#原子写入文件" class="headerlink" title="原子写入文件"></a>原子写入文件</h2><p>这里采用了一种原子写入的方式，以防止多个worker都需要写入同一个文件名的文件时可能出现的问题。总体思想就是先写入到一个临时文件中，然后再将其改名为对应的文件名，如果临时文件没有写成功，就用defer命令将其删除。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">atomicWriteFile</span><span class="hljs-params">(filename <span class="hljs-type">string</span>, reader io.Reader)</span></span> (err <span class="hljs-type">error</span>) &#123;<br>tmpFileName, err := writeToTmpFile(filename, reader)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;cannot write to temp file: %v&quot;</span>, err)<br>&#125;<br><br><span class="hljs-keyword">if</span> err := os.Rename(tmpFileName, filename); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> fmt.Errorf(<span class="hljs-string">&quot;cannot rename temp file: %v&quot;</span>, err)<br>&#125;<br><br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">writeToTmpFile</span><span class="hljs-params">(filename <span class="hljs-type">string</span>, reader io.Reader)</span></span> (tmpFileName <span class="hljs-type">string</span>, err <span class="hljs-type">error</span>) &#123;<br>dir, file := filepath.Split(filename)<br><span class="hljs-keyword">if</span> dir == <span class="hljs-string">&quot;&quot;</span> &#123;<br>dir = <span class="hljs-string">&quot;.&quot;</span><br>&#125;<br><br>tmpFile, err := os.CreateTemp(dir, file)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>, fmt.Errorf(<span class="hljs-string">&quot;cannot create temp file: %v&quot;</span>, err)<br>&#125;<br><span class="hljs-keyword">defer</span> tmpFile.Close()<br><span class="hljs-keyword">defer</span> <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">()</span></span> &#123;<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br>os.Remove(tmpFile.Name())<br>&#125;<br>&#125;()<br><br>_, err = io.Copy(tmpFile, reader)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>, fmt.Errorf(<span class="hljs-string">&quot;cannot write to temp file: %v&quot;</span>, err)<br>&#125;<br><span class="hljs-keyword">if</span> err := tmpFile.Close(); err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>, fmt.Errorf(<span class="hljs-string">&quot;cannot close temp file: %v&quot;</span>, err)<br>&#125;<br><br><span class="hljs-keyword">return</span> tmpFile.Name(), <span class="hljs-literal">nil</span><br>&#125;<br></code></pre></td></tr></table></figure><h1 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h1><p>coordinator运行结果:</p><p><img src="/./MIT6-824lab1/coordinator%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C.jpg" alt="coordinator运行结果"></p><p>worker运行结果:</p><p><img src="/./MIT6-824lab1/worker%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C.jpg" alt="worker运行结果"></p><p>中间文件：</p><p><img src="/./MIT6-824lab1/%E4%B8%AD%E9%97%B4%E6%96%87%E4%BB%B6.jpg" alt="中间文件"></p><p>输出结果：</p><p><img src="/./MIT6-824lab1/%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%9C.jpg" alt="输出结果"></p><p>测试脚本结果：</p><p><img src="/./MIT6-824lab1/%E6%B5%8B%E8%AF%95%E8%84%9A%E6%9C%AC%E7%BB%93%E6%9E%9C.jpg" alt="测试脚本结果"></p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>最后进行脚本测试的时候发现early_exit这个点总是通不过，这个脚本会捕捉最早退出运行的进程，然后拷贝所有输出文件，然后再在所有进程都退出的时候拷贝所有输出文件，以此对比两个文件是否相同，来判断是否coordinator和所有的woker都在任务全部完成后再退出。</p><p>后面仔细查看不通过的原因发现是因为其依靠下面的部分来进行捕捉退出的进程,本机器上使用的是<code>wait -n</code>,但是实际查看发现其并没有正确地在相关进程退出时进行触发，而是一开始就触发了，其触发时coordinator和所有worker其实都还在前台运行了，后面讲这部分改成了if里面的测试，就可以正常捕捉退出的进程然后顺利通过了。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs sh"><span class="hljs-built_in">jobs</span> &amp;&gt; /dev/null<br><span class="hljs-keyword">if</span> [[ <span class="hljs-string">&quot;<span class="hljs-variable">$OSTYPE</span>&quot;</span> = <span class="hljs-string">&quot;darwin&quot;</span>* ]]<br><span class="hljs-keyword">then</span><br>  <span class="hljs-comment"># bash on the Mac doesn&#x27;t have wait -n</span><br>  <span class="hljs-keyword">while</span> [ ! -e <span class="hljs-variable">$DF</span> ]<br>  <span class="hljs-keyword">do</span><br>    <span class="hljs-built_in">sleep</span> 0.2<br>  <span class="hljs-keyword">done</span><br><span class="hljs-keyword">else</span><br>  <span class="hljs-comment"># the -n causes wait to wait for just one child process,</span><br>  <span class="hljs-comment"># rather than waiting for all to finish.</span><br>  <span class="hljs-built_in">wait</span> -n<br><span class="hljs-keyword">fi</span><br></code></pre></td></tr></table></figure><p>从看MapReduce论文到学go再到能看懂作业要求，再到能看懂别人写的代码，再到能自己独立完成这部分代码总共断断续续持续了一个月，能够感受到自己在这之中的不断的精进，MapReduce的设计确实也很巧妙，go总体的设计确实很适合分布式，MIT6.824确实不愧是一名深受好评的课，学一下是很有必要的，希望自己后面也能都将其他部分啃下来了。</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol><li><a href="https://github.com/OneSizeFitsQuorum/MIT6.824-2021">https://github.com/OneSizeFitsQuorum/MIT6.824-2021</a></li><li><a href="https://github.com/PKUFlyingPig/MIT6.824">https://github.com/PKUFlyingPig/MIT6.824</a></li><li><a href="https://github.com/szw2021/MIT6.824-2021/tree/practice/src">https://github.com/szw2021/MIT6.824-2021/tree/practice/src</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>MIT6.824</category>
      
      <category>Lab</category>
      
    </categories>
    
    
    <tags>
      
      <tag>go</tag>
      
      <tag>分布式</tag>
      
      <tag>MIT6.824</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RPC介绍</title>
    <link href="/2023/11/14/RPC%E4%BB%8B%E7%BB%8D/"/>
    <url>/2023/11/14/RPC%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<pre><code class="hljs">最近在学MIT6.824的lab1——MapReduce，发现其中使用到了RPC来让work和coordinator之间通信，故而乘机学习一下。</code></pre><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>RPC（Remote Procedure Call，远程过程调用）是一种在分布式系统中进行进程间通信的协议。它允许一个程序（客户端）调用另一个程序（服务器）上的函数或过程，就像调用本地函数一样，而不必关心底层网络细节。</p><p>在 RPC 中，客户端和服务器可以在不同的机器上，甚至在不同的网络上。RPC 提供了一种抽象，使得远程调用看起来就像是本地调用一样。通过 RPC，程序可以通过网络传输数据和调用远程函数，使得分布式系统中的组件可以协同工作。</p><p>其整体流程如下图所示：</p><p><img src="https://pic4.zhimg.com/45366c44f775abfd0ac3b43bccc1abc3_b" alt="RPC流程"></p><p>对于RPC需要关注的主要有三点：</p><ul><li><strong>通信协议：</strong> RPC可以基于TCP或者HTTP协议，一般而言TCP的协议更快</li><li><strong>寻址：</strong> 远程提供服务器需要提供服务所在地址，例如IP和端口</li><li><strong>数据序列化：</strong> 远程调用无法依据内存进行参数和结果传递，所以需要规定序列化的格式，例如Json格式</li></ul><p>常用的RPC框架有如下这些： </p><ul><li>Thrift：thrift是一个软件框架，用来进行可扩展且跨语言的服务的开发。它结合了功能强大的软件堆栈和代码生成引擎，以构建在 C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js, Smalltalk, and OCaml 这些编程语言间无缝结合的、高效的服务。</li><li>gRPC：一开始由 google 开发，是一款语言中立、平台中立、开源的远程过程调用(RPC)系统。</li><li>Dubbo：Dubbo是一个分布式服务框架，以及SOA治理方案。其功能主要包括：高性能NIO通讯及多协议集成，服务动态寻址与路由，软负载均衡与容错，依赖分析与降级等。Dubbo是阿里巴巴内部的SOA服务化治理方案的核心框架，Dubbo自2011年开源后，已被许多非阿里系公司使用。</li><li>Spring Cloud：Spring Cloud由众多子项目组成，如Spring Cloud Config、Spring Cloud Netflix、Spring Cloud Consul 等，提供了搭建分布式系统及微服务常用的工具，如配置管理、服务发现、断路器、智能路由、微代理、控制总线、一次性token、全局锁、选主、分布式会话和集群状态等，满足了构建微服务所需的所有解决方案。Spring Cloud基于Spring Boot, 使得开发部署极其简单。</li></ul><h1 id="GO语言示例"><a href="#GO语言示例" class="headerlink" title="GO语言示例"></a>GO语言示例</h1><p>在 Go 语言中，标准库提供了一个 net&#x2F;rpc 包，用于实现 RPC。基本的使用流程包括注册对象、注册服务、处理请求等。以下是一个简单的 Go RPC 例子：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<br><br><span class="hljs-keyword">import</span> (<br><span class="hljs-string">&quot;net&quot;</span><br><span class="hljs-string">&quot;net/rpc&quot;</span><br>)<br><br><span class="hljs-comment">// 定义一个用于 RPC 的对象</span><br><span class="hljs-keyword">type</span> MyService <span class="hljs-keyword">struct</span>&#123;&#125;<br><br><span class="hljs-comment">// 定义一个 RPC 方法</span><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(s *MyService)</span></span> Multiply(args *Args, reply *<span class="hljs-type">int</span>) <span class="hljs-type">error</span> &#123;<br>*reply = args.A * args.B<br><span class="hljs-keyword">return</span> <span class="hljs-literal">nil</span><br>&#125;<br><br><span class="hljs-comment">// 定义传递给 RPC 方法的参数结构</span><br><span class="hljs-keyword">type</span> Args <span class="hljs-keyword">struct</span> &#123;<br>A, B <span class="hljs-type">int</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<br><span class="hljs-comment">// 注册服务</span><br>rpc.Register(<span class="hljs-built_in">new</span>(MyService))<br><br><span class="hljs-comment">// 创建监听器</span><br>listener, err := net.Listen(<span class="hljs-string">&quot;tcp&quot;</span>, <span class="hljs-string">&quot;:1234&quot;</span>)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-built_in">panic</span>(err)<br>&#125;<br><br><span class="hljs-comment">// 处理连接</span><br><span class="hljs-keyword">for</span> &#123;<br>conn, err := listener.Accept()<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-keyword">continue</span><br>&#125;<br><span class="hljs-keyword">go</span> rpc.ServeConn(conn)<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>在该例子中，MyService 结构体中的 Multiply 方法被注册为 RPC 服务。通过监听端口 1234，该服务可以接收客户端的 RPC 调用请求。客户端可以通过 net&#x2F;rpc 包中的函数来发起远程调用。RPC 在分布式系统中广泛用于实现不同节点之间的通信和协作。</p><p>以下是一个简单的 Go RPC 客户端的调用代码示例,程序需要确保 RPC 客户端的网络协议和端口与服务器端一致，这样它们才能正确地进行通信。在这个例子中，服务器端监听的是 TCP 端口 1234。：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">package</span> main<br><br><span class="hljs-keyword">import</span> (<br><span class="hljs-string">&quot;fmt&quot;</span><br><span class="hljs-string">&quot;net/rpc&quot;</span><br>)<br><br><span class="hljs-comment">// 定义传递给 RPC 方法的参数结构</span><br><span class="hljs-keyword">type</span> Args <span class="hljs-keyword">struct</span> &#123;<br>A, B <span class="hljs-type">int</span><br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> &#123;<br><span class="hljs-comment">// 连接 RPC 服务器</span><br>client, err := rpc.Dial(<span class="hljs-string">&quot;tcp&quot;</span>, <span class="hljs-string">&quot;localhost:1234&quot;</span>)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-built_in">panic</span>(err)<br>&#125;<br><span class="hljs-keyword">defer</span> client.Close()<br><br><span class="hljs-comment">// 准备 RPC 调用的参数</span><br>args := Args&#123;<span class="hljs-number">3</span>, <span class="hljs-number">4</span>&#125;<br><br><span class="hljs-comment">// 调用 Multiply 方法</span><br><span class="hljs-keyword">var</span> result <span class="hljs-type">int</span><br>err = client.Call(<span class="hljs-string">&quot;MyService.Multiply&quot;</span>, args, &amp;result)<br><span class="hljs-keyword">if</span> err != <span class="hljs-literal">nil</span> &#123;<br><span class="hljs-built_in">panic</span>(err)<br>&#125;<br><br><span class="hljs-comment">// 打印结果</span><br>fmt.Printf(<span class="hljs-string">&quot;Result of 3 * 4: %d\n&quot;</span>, result)<br>&#125;<br></code></pre></td></tr></table></figure><p>在这个例子中，rpc.Dial 函数用于连接到服务器，然后通过 client.Call 方法调用远程的 Multiply 方法。在调用过程中，需要传递参数结构 Args 和一个用于接收结果的变量。最后，打印出调用结果:</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Result</span> of <span class="hljs-number">3</span> * <span class="hljs-number">4</span>: <span class="hljs-number">12</span><br></code></pre></td></tr></table></figure><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://jaminzhang.github.io/architecture/RPC-Introduction/">https://jaminzhang.github.io/architecture/RPC-Introduction/</a></li><li><a href="https://zhuanlan.zhihu.com/p/187560185">https://zhuanlan.zhihu.com/p/187560185</a></li><li><a href="https://cloud.tencent.com/developer/article/2021745">https://cloud.tencent.com/developer/article/2021745</a></li><li><a href="https://chat.openai.com/">ChatGPT</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>MIT6.824</category>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>分布式</tag>
      
      <tag>MIT6.824</tag>
      
      <tag>Go</tag>
      
      <tag>RPC</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何写出优雅的代码</title>
    <link href="/2023/11/09/%E5%A6%82%E4%BD%95%E5%86%99%E5%87%BA%E4%BC%98%E9%9B%85%E7%9A%84%E4%BB%A3%E7%A0%81/"/>
    <url>/2023/11/09/%E5%A6%82%E4%BD%95%E5%86%99%E5%87%BA%E4%BC%98%E9%9B%85%E7%9A%84%E4%BB%A3%E7%A0%81/</url>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>最近在写模拟器的代码，现在需要对整体的架构都进行一个大更改，而当我回过头去看时发现好多代码都写得很丑，越写越像屎山代码，需要对整体进行一轮迭代，故而正好趁此机会来学习一下如何写一个优雅的代码。写之前看了很多博客，这里记录一下对自己比较有启发的点。</p><h1 id="什么是优雅的代码"><a href="#什么是优雅的代码" class="headerlink" title="什么是优雅的代码"></a>什么是优雅的代码</h1><pre><code class="hljs">这一个衡量标准很直接：WTF min</code></pre><p><img src="https://camo.githubusercontent.com/ded622d3db4ad28f7b47098ae182c09dc1d629c90f487a885f683527825ecc94/687474703a2f2f692e737461636b2e696d6775722e636f6d2f65545a76572e6a7067" alt="优雅的代码"></p><p>代码最后还是需要给人读的，需要认识到：</p><pre><code class="hljs">任何一个傻瓜都能写出计算机可以理解的代码。唯有写出人类容易理解的代码，才是优秀的程序员。—— Martin Fowler</code></pre><p>好的代码最重要的特点：整洁</p><pre><code class="hljs">整洁的代码如同优美的散文。—— Grady Booch</code></pre><h1 id="如何保证代码整洁"><a href="#如何保证代码整洁" class="headerlink" title="如何保证代码整洁"></a>如何保证代码整洁</h1><h2 id="1-有意义的命名"><a href="#1-有意义的命名" class="headerlink" title="1. 有意义的命名"></a>1. 有意义的命名</h2><p>命名要尽可能地输出多的信息，让人能快速理解这个类、变量或者函数的含义、功能。<strong>花时间来取名是值得的</strong></p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs angelscript"><span class="hljs-comment">/**</span><br><span class="hljs-comment">* 创建后的天数</span><br><span class="hljs-comment">**/</span><br><span class="hljs-built_in">int</span> d;<br></code></pre></td></tr></table></figure><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf">int daysSinceCreation<span class="hljs-comment">;</span><br></code></pre></td></tr></table></figure><p>后一种命名就能够更加凸显变量的含义了。</p><p>命名是越短越好的，但是为了能够清晰表达意思，取长名字也是值得的。</p><p>尽量不要取相似的名字以让人困惑。</p><h2 id="2-优雅的注释"><a href="#2-优雅的注释" class="headerlink" title="2. 优雅的注释"></a>2. 优雅的注释</h2><p>一份优雅的代码本身就应该具有足够的表达力，不需要注释就能看懂。<strong>注释的存在往往是弥补我们无法用代码清晰表达意图的情况</strong>。当发现自己需要对某处的代码写注释时就需要考虑是不是应该用更好的代码对其进行替代了。</p><pre><code class="hljs">好代码&gt;&gt;坏代码+注释</code></pre><p>注释应该只需要注重解释上层的东西，包括设计的意图、功能，而不需要解释执行的细节，这由代码来进行展示。</p><p>注释不要去写能由git记录的信息。</p><h2 id="3-简洁的函数"><a href="#3-简洁的函数" class="headerlink" title="3. 简洁的函数"></a>3. 简洁的函数</h2><p>控制函数的长度，一般不要长于一个屏幕的大小，也就是<strong>控制在30行以内</strong></p><p><strong>if语句、else语句、while语句等，其中的代码应该只有一行</strong>，该行通常是一个调用语句，这样不但能保持短小，还可以给调用方法命名一个有说明性的名字，进一步增加代码的可读性</p><p>函数的功能要单一，具有原子性，以方便函数复用。最简单的规则就是看看该方法是否能在拆出一个方法，且拆出去的方法是不同于该方法的诠释和实现。</p><p>需要保证函数中的抽象层级一致。</p><p>不要返回null或者特殊对象等，不要传入null。</p><p>函数参数不要超过3个，如果超过就需要考虑将其抽象为类。</p><h2 id="4-整齐的代码结构"><a href="#4-整齐的代码结构" class="headerlink" title="4. 整齐的代码结构"></a>4. 整齐的代码结构</h2><p>一个文件的代码数量需要<strong>控制在200行以内</strong>，最多不要超过500行。</p><p>关系紧密的代码放在一起</p><ul><li>变量声明放在其使用的位置</li><li>函数的调用者要放在被调用者上面，以从上到下展示调用依赖顺序</li></ul><p>对象暴露行为，隐藏数据，调用对象时不应该了解该对象的内部情况。</p><p>一组代码代表一个完整的思路，不同组的代码中间要用空行间隔。</p><h2 id="5-重构代码"><a href="#5-重构代码" class="headerlink" title="5. 重构代码"></a>5. 重构代码</h2><p>好代码不是一蹴而就的，重构代码是必要的。</p><p>重复的代码肯定是可以抽象出一个上层函数的。</p><ul><li>都同一个类中就考虑将其提炼出一个函数</li><li>不在同一个类中，就需要考虑创建一个共享的地方，可以是一个工具类，以便多个类都可以使用它</li><li>只是相似的话，可以重拍顺序，将相同的部分提料出来</li></ul><p>过长的函数可以通过提取函数的方式来缩短。</p><p>如果一个类不是单一职责的，则可能会导致一旦其变化就需要修改多个其他类，或者不同类的变化都需要修改这个类，这时就需要考虑对其重构，划定职责。</p><p>有时候会发现三四个相同的字段，在多个类和函数中均出现，这时候说明有必要给这一组字段建立一个类，将其封装起来。</p><p>以查询代替临时变量，也就是对于复杂的赋值表达式，使用函数来进行替代。</p><p>对于复杂过长的函数，可能将其转化为一个类进行重构。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>写一份优雅的代码远比写一份能跑的代码难，但是这是值得的。想如何写代码远比写代码来得更加重要，写代码时的角度需要变化，不要以自己的角度去看自己写的代码，而是要以一个代码阅读者的角度来审视自己的代码。</p><h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><ul><li><a href="https://mp.weixin.qq.com/s?__biz=MzAxNDEwNjk5OQ==&mid=2650403801&idx=1&sn=5dab2af09f753fd089fe0f1f56260b10&chksm=83953bc1b4e2b2d79f78028ed6e79651167d85bf67453af791d9304fa4003eed73d9f2957fc7&scene=21#wechat_redirect">阿里工程师谈，什么是好的代码？</a></li><li><a href="https://developer.aliyun.com/article/1117703">一文详解｜如何写出优雅的代码</a></li><li><a href="https://www.zhihu.com/question/28492982">如何写出优雅的代码？</a></li><li><a href="https://juejin.cn/post/7016992016521232421">如何写出优雅的代码？</a></li><li><a href="https://github.com/CodingDocs/advanced-programmer/blob/master/docs/%E5%85%AB%E7%82%B9%E5%BB%BA%E8%AE%AE%E5%8A%A9%E6%82%A8%E5%86%99%E5%87%BA%E4%BC%98%E9%9B%85%E7%9A%84Java%E4%BB%A3%E7%A0%81.md">八点建议助您写出优雅的Java代码</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>代码风格</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文阅读】MapReduce: Simplified Data Processing on Large Clusters</title>
    <link href="/2023/11/05/MapReducePaperRead/"/>
    <url>/2023/11/05/MapReducePaperRead/</url>
    
    <content type="html"><![CDATA[<p>原博客链接：<a href="https://tanxinyu.work/mapreduce-thesis/">https://tanxinyu.work/mapreduce-thesis/</a></p><h1 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h1><p>在 20 世纪初，包括本文作者在内的 Google 的很多程序员，为了处理海量的原始数据，已经实现了数以百计的、专用的计算方法。这些计算方法用来处理大量的原始数据，比如，文档抓取（类似网络爬虫的程序）、Web 请求日志等等；也为了计算处理各种类型的衍生数据，比如倒排索引、Web 文档的图结构的各种表示形势、每台主机上网络爬虫抓取的页面数量的汇总、每天被请求的最多的查询的集合等等。</p><h1 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h1><p>大多数以上提到的数据处理运算在概念上很容易理解。然而由于输入的数据量巨大，因此要想在可接受的时间内完成运算，只有将这些计算分布在成百上千的主机上。如何处理并行计算、如何分发数据、如何处理错误？所有这些问题综合在一起，需要大量的代码处理，因此也使得原本简单的运算变得难以处理。</p><h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>为了解决上述复杂的问题，本文设计一个新的抽象模型，使用这个抽象模型，用户只要表述想要执行的简单运算即可，而不必关心并行计算、容错、数据分布、负载均衡等复杂的细节，这些问题都被封装在了一个库里面：利用一个输入 key&#x2F;value pair 集合来产生一个输出的 key&#x2F;value pair 集合。</p><p>MapReduce 库的用户可以用两个函数表达这个计算：Map 和 Reduce。</p><ul><li>用户自定义的 Map 函数接受一个输入的 key&#x2F;value pair 值，然后产生一个中间 key&#x2F;value pair 值的集合。MapReduce 库把所有具有相同中间 key 值 I 的中间 value 值集合在一起后按照一定的规律传递给 reduce 函数。</li><li>用户自定义的 Reduce 函数接受一个中间 key 的值 I 和相关的一个 value 值的集合。Reduce 函数合并这些 value 值，形成一个较小的 value 值的集合。一般的，每次 Reduce 函数调用只产生 0 或 1 个输出 value 值。通常 Map 通过一个迭代器把中间 value 值提供给 Reduce 函数，这样 Reduce Worker 就可以处理无法全部放入内存中的大量的 value 值的集合。<br>在概念上，用户定义的 Map 和 Reduce 函数都有相关联的类型：</li></ul><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs livescript"><span class="hljs-keyword">map</span><span class="hljs-function"><span class="hljs-params">(k1,v1)</span> -&gt;</span><span class="hljs-keyword">list</span>(k2,v2)<br>reduce<span class="hljs-function"><span class="hljs-params">(k2,<span class="hljs-keyword">list</span>(v2))</span> -&gt;</span><span class="hljs-keyword">list</span>(v2)<br></code></pre></td></tr></table></figure><p>比如，输入的 key 和 value 值与输出的 key 和 value 值在类型上推导的域不同。此外，中间 key 和 value 值与输出 key 和 value 值在类型上推导的域相同。</p><h1 id="执行流程"><a href="#执行流程" class="headerlink" title="执行流程"></a>执行流程</h1><p>通过将 Map 调用的输入数据自动分割为 M 个数据片段的集合，Map 调用被分布到多台机器上执行。输入的数据片段能够在不同的机器上并行处理。使用分区函数将 Map 调用产生的中间 key 值分成 R 个不同分区（例如，hash(key) mod R），Reduce 调用也被分布到多台机器上执行。分区数量（R）和分区函数由用户来指定。</p><p><img src="https://tanxinyu.work/mapreduce-thesis/mapreduce.png" alt="执行流程"></p><p>上图展示了 MapReduce 实现中操作的全部流程。当用户调用 MapReduce 函数时，将发生下面的一系列动作：</p><ol><li>用户程序首先调用的 MapReduce 库将输入文件分成 M 个数据片度，每个数据片段的大小一般从 16MB 到 64MB（可以通过可选的参数来控制每个数据片段的大小）。然后用户程序在机群中创建大量的程序副本。</li><li>这些程序副本中的有一个特殊的程序–master。副本中其它的程序都是 worker 程序，由 master 分配任务。有 M 个 Map 任务和 R 个 Reduce 任务将被分配，master 将一个 Map 任务或 Reduce 任务分配给一个空闲的 worker。</li><li>被分配了 map 任务的 worker 程序读取相关的输入数据片段，从输入的数据片段中解析出 key&#x2F;value pair，然后把 key&#x2F;value pair 传递给用户自定义的 Map 函数，由 Map 函数生成并输出的中间 key&#x2F;value pair，并缓存在内存中。</li><li>缓存中的 key&#x2F;value pair 通过分区函数分成 R 个区域，之后周期性的写入到本地磁盘上。缓存的 key&#x2F;value pair 在本地磁盘上的存储位置将被回传给 master，由 master 负责把这些存储位置再传送给 Reduce worker</li><li>当 Reduce worker 程序接收到 master 程序发来的数据存储位置信息后，使用 RPC 从 Map worker 所在主机的磁盘上读取这些缓存数据。当 Reduce worker 读取了所有的中间数据后，通过对 key 进行排序后使得具有相同 key 值的数据聚合在一起。由于许多不同的 key 值会映射到相同的 Reduce 任务上，因此必须进行排序。如果中间数据太大无法在内存中完成排序，那么就要在外部进行排序。</li><li>Reduce worker 程序遍历排序后的中间数据，对于每一个唯一的中间 key 值，Reduce worker 程序将这个 key 值和它相关的中间 value 值的集合传递给用户自定义的 Reduce 函数。Reduce 函数的输出被追加到所属分区的输出文件。</li><li>当所有的 Map 和 Reduce 任务都完成之后，master 唤醒用户程序。在这个时候，在用户程序里的对 MapReduce 调用才返回。</li></ol><h1 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h1><h2 id="worker-故障"><a href="#worker-故障" class="headerlink" title="worker 故障"></a>worker 故障</h2><p>master 与 worker 之间同步心跳，对于失效的 worker，根据其类型来做进一步处理：</p><ul><li>Map worker 故障：由于 Map 任务将数据临时存储在本地，所以需要重新执行。</li><li>Reduce worker 故障：由于 Reduce 任务将数据存储在全局文件系统中 ，所以不需要重新执行。</li></ul><h2 id="master-故障"><a href="#master-故障" class="headerlink" title="master 故障"></a>master 故障</h2><p>MapReduce 任务重新执行</p><h2 id="故障语义保证"><a href="#故障语义保证" class="headerlink" title="故障语义保证"></a>故障语义保证</h2><p>当用户提供的 Map 和 Reduce 操作是输入确定性函数（即相同的输入产生相同的输出）时，MapReduce 的分布式实现在任何情况下的输出都和所有程序没有出现任何错误、顺序的执行产生的输出是一样的。</p><ul><li>Map worker 任务的原子提交：每个 Map 任务生成 R 个本地临时文件，当一个 Map 任务完成时，worker 发送一个包含 R 个临时文件名的完成消息给 master。如果 master 从一个已经完成的 Map 任务再次接收到一个完成消息，master 将忽略这个消息；</li><li>Reduce worker 任务的原子提交：当 Reduce 任务完成时，Reduce worker 进程以原子的方式把临时文件重命名为最终的输出文件。如果同一个 Reduce 任务在多台机器上执行，针对同一个最终的输出文件将有多个重命名操作执行。MapReduce 依赖底层文件系统提供的重命名操作的原子性来保证最终的文件系统状态仅仅包含一个 Reduce 任务产生的数据。</li></ul><h1 id="存储位置优化"><a href="#存储位置优化" class="headerlink" title="存储位置优化"></a>存储位置优化</h1><p>核心思想：本地读文件以减少流量消耗</p><p>MapReduce 的 master 在调度 Map 任务时会考虑输入文件的位置信息，尽量将一个 Map 任务调度在包含相关输入数据拷贝的机器上执行；如果上述努力失败了，master 将尝试在保存有输入数据拷贝的机器附近的机器上执行 Map 任务（例如，分配到一个和包含输入数据的机器在一个交换机里的 worker 机器上执行）。</p><h1 id="任务粒度"><a href="#任务粒度" class="headerlink" title="任务粒度"></a>任务粒度</h1><p>理想情况下，M 和 R 应当比集群中 worker 的机器数量要多得多。在每台 worker 机器都执行大量的不同任务能够提高集群的动态的负载均衡能力，并且能够加快故障恢复的速度：失效机器上执行的大量 Map 任务都可以分布到所有其他的 worker 机器上去执行。</p><p>实际使用时建议用户选择合适的 M 值，以使得每一个独立任务都是处理大约 16M 到 64M 的输入数据（这样，上面描写的输入数据本地存储优化策略才最有效），另外，也建议把 R 值设置使用的 worker 机器数量的小倍数。比如：M&#x3D;200000，R&#x3D;5000，使用 2000 台 worker 机器。</p><h1 id="备用任务"><a href="#备用任务" class="headerlink" title="备用任务"></a>备用任务</h1><p>影响一个 MapReduce 的总执行时间最通常的因素是“落伍者”：在运算过程中，如果有一台机器花了很长的时间才完成最后几个 Map 或 Reduce 任务，导致 MapReduce 操作总的执行时间超过预期。</p><p>为了解决落伍者的问题，当一个 MapReduce 操作接近完成的时候，master 调度备用（backup）任务进程来执行剩下的、处于处理中状态（in-progress）的任务。无论是最初的执行进程、还是备用（backup）任务进程完成了任务，MapReduce 都把这个任务标记成为已经完成。此个机制通常只会占用比正常操作多几个百分点的计算资源。但能减少近 50% 的任务完成总时间。</p><h1 id="技巧"><a href="#技巧" class="headerlink" title="技巧"></a>技巧</h1><h2 id="分区函数"><a href="#分区函数" class="headerlink" title="分区函数"></a>分区函数</h2><p>MapReduce 缺省的分区函数是使用 hash 方法(比如，hash(key) mod R) 进行分区。hash 方法能产生非常平衡的分区。然而，有的时候，其它的一些分区函数对 key 值进行的分区将非常有用。比如，输出的 key 值是 URLs，有的用户希望每个主机的所有条目保持在同一个输出文件中。为了支持类似的情况，MapReduce 库的用户需要提供专门的分区函数。例如，使用“hash(Hostname(urlkey))mod R”作为分区函数就可以把所有来自同一个主机的 URLs 保存在同一个输出文件中。</p><h2 id="顺序保证"><a href="#顺序保证" class="headerlink" title="顺序保证"></a>顺序保证</h2><p>MapReduce 确保在给定的分区中，中间 key&#x2F;value pair 数据的处理顺序是按照 key 值增量顺序处理的。这样的顺序保证对每个分成生成一个有序的输出文件，这对于需要对输出文件按 key 值随机存取的应用非常有意义，对在排序输出的数据集也很有帮助。</p><h2 id="Combiner-函数"><a href="#Combiner-函数" class="headerlink" title="Combiner 函数"></a>Combiner 函数</h2><p>在某些情况下，Map 函数产生的中间 key 值的重复数据会占很大的比重，并且，用户自定义的 Reduce 函数满足结合律和交换律。在 2.1 节的词数统计程序是个很好的例子。由于词频率倾向于一个 zipf 分布（齐夫分布），每个 Map 任务将产生成千上万个这样的记录。所有的这些记录将通过网络被发送到一个单独的 Reduce 任务，然后由这个 Reduce 任务把所有这些记录累加起来产生一个数字。MapReduce 允许用户指定一个可选的 combiner 函数，combiner 函数首先在本地将这些记录进行一次合并，然后将合并的结果再通过网络发送出去。</p><p>Combiner 函数在每台执行 Map 任务的机器上都会被执行一次。一般情况下，Combiner 和 Reduce 函数是一样的。Combiner 函数和 Reduce 函数之间唯一的区别是 MapReduce 库怎样控制函数的输出。Reduce 函数的输出被保存在最终的输出文件里，而 Combiner 函数的输出被写到中间文件里，然后被发送给 Reduce 任务。</p><p>部分的合并中间结果可以显著的提高一些 MapReduce 操作的速度。</p><h2 id="输入和输出的类型"><a href="#输入和输出的类型" class="headerlink" title="输入和输出的类型"></a>输入和输出的类型</h2><p>支持常用的类型，可以通过提供一个简单的 Reader 接口实现来支持一个新的输入类型。Reader 并非一定要从文件中读取数据，比如可以很容易的实现一个从数据库里读记录的 Reader，或者从内存中的数据结构读取数据的 Reader。</p><h2 id="副作用"><a href="#副作用" class="headerlink" title="副作用"></a>副作用</h2><p>在某些情况下，MapReduce 的使用者发现，如果在 Map 或 Reduce 操作过程中增加辅助的输出文件会比较省事。MapReduce 依靠程序 writer 把这种“副作用”变成原子的和幂等的。通常应用程序首先把输出结果写到一个临时文件中，在输出全部数据之后，在使用系统级的原子操作 rename 重新命名这个临时文件。</p><h2 id="跳过损坏的记录"><a href="#跳过损坏的记录" class="headerlink" title="跳过损坏的记录"></a>跳过损坏的记录</h2><p>每个 worker 进程都设置了信号处理函数捕获内存段异常（segmentation violation）和总线错误（bus error）。 在执行 Map 或者 Reduce 操作之前，MapReduce 库通过全局变量保存记录序号。如果用户程序触发了一个系统信号，消息处理函数将用“最后一口气”通过 UDP 包向 master 发送处理的最后一条记录的序号。当 master 看到在处理某条特定记录不止失败一次时，master 就标志着条记录需要被跳过，并且在下次重新执行相关的 Map 或者 Reduce 任务的时候跳过这条记录。</p><h2 id="本地执行"><a href="#本地执行" class="headerlink" title="本地执行"></a>本地执行</h2><p>支持本地串行执行以方便调试</p><h2 id="状态信息"><a href="#状态信息" class="headerlink" title="状态信息"></a>状态信息</h2><p>master 支持嵌入 HTTP 服务器以显示一组状态信息页面，用户可以监控各种执行状态。状态信息页面显示了包括计算执行的进度，比如已经完成了多少任务、有多少任务正在处理、输入的字节数、中间数据的字节数、输出的字节数、处理百分比等等</p><h2 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h2><p>MapReduce 库使用计数器统计不同事件发生次数。比如，用户可能想统计已经处理了多少个单词、已经索引的多少篇 German 文档等等。</p><p>这些计数器的值周期性的从各个单独的 worker 机器上传递给 master（附加在 ping 的应答包中传递）。master 把执行成功的 Map 和 Reduce 任务的计数器值进行累计，当 MapReduce 操作完成之后，返回给用户代码。</p><p>计数器当前的值也会显示在 master 的状态页面上，这样用户就可以看到当前计算的进度。当累加计数器的值的时候，master 要检查重复运行的 Map 或者 Reduce 任务，避免重复累加（之前提到的备用任务和失效后重新执行任务这两种情况会导致相同的任务被多次执行）。</p><h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><p>分布式的 Grep：Map 函数输出匹配某个模式的一行，Reduce 函数是一个恒等函数，即把中间数据复制到输出。</p><ul><li>计算 URL 访问频率：Map 函数处理日志中 web 页面请求的记录，然后输出 (URL,1)。Reduce 函数把相同 URL 的 value 值都累加起来，产生 (URL, 记录总数）结果。<br>网络链接倒排：Map 函数在源页面（source）中搜索所有的链接目标（target）并输出为 (target,source)。Reduce 函数把给定链接目标（target）的链接组合成一个列表，输出 (target,list(source))。</li><li>每个主机的检索词向量：检索词向量用一个（词，频率）列表来概述出现在文档或文档集中的最重要的一些词。Map 函数为每一个输入文档输出（主机名，检索词向量），其中主机名来自文档的 URL。Reduce 函数接收给定主机的所有文档的检索词向量，并把这些检索词向量加在一起，丢弃掉低频的检索词，输出一个最终的（主机名，检索词向量）。</li><li>倒排索引：Map 函数分析每个文档输出一个（词，文档号）的列表，Reduce 函数的输入是一个给定词的所有（词，文档号），排序所有的文档号，输出（词，list（文档号）)。所有的输出集合形成一个简单的倒排索引，它以一种简单的算法跟踪词在文档中的位置。<br>分布式排序：Map 函数从每个记录提取 key，输出 (key,record)。Reduce 函数不改变任何的值。这个运算依赖分区机制和排序属性。</li></ul><h1 id="经验分享"><a href="#经验分享" class="headerlink" title="经验分享"></a>经验分享</h1><ul><li>约束编程模式使得并行和分布式计算非常容易，也易于构造容错的计算环境；</li><li>网络带宽是稀有资源。大量的系统优化是针对减少网络传输量为目的的：本地优化策略使大量的数据从本地磁盘读取，中间文件写入本地磁盘、并且只写一份中间文件也节约了网络带宽。</li><li>多次执行相同的任务可以减少硬件配置不平衡带来的负面影响，同时解决了由于机器失效导致的数据丢失问题。</li></ul><h1 id="创新之处"><a href="#创新之处" class="headerlink" title="创新之处"></a>创新之处</h1><ul><li>通过简单的接口实现了自动的并行化和大规模的分布式计算，通过使用 MapReduce 模型接口实现了在大量普通 PC 机上的高性能计算。</li><li>向工业界证明了 MapReduce 模型在分布式计算上的可行性，拉开了分布式计算的序幕并影响了其后所有的计算框架，包括现在流行的批处理框架 Spark 和流处理框架 Flink 都很受其影响。</li></ul><h1 id="不足之处"><a href="#不足之处" class="headerlink" title="不足之处"></a>不足之处</h1><ul><li>基于历史局限性和当时的成本考虑，没有利用内存去更高效的处理数据，不过也为 Spark 提供了思路。</li><li>没有将资料调度和计算调度分离，使得 MapReduce 系统看起来较为冗杂。在开源的 Hadoop 生态中，MapReduce 现只关注于计算，具体的资源调度由 Yarn 管理。</li></ul><h1 id="相关系统"><a href="#相关系统" class="headerlink" title="相关系统"></a>相关系统</h1><ul><li>分布式存储系统：GFS&#x2F;Colossus&#x2F;HDFS</li><li>批处理框架：Spark</li><li>流处理框架：Flink</li><li>高可用机制：Chubby&#x2F;ZooKeeper</li></ul><h1 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h1><ul><li><a href="http://nil.csail.mit.edu/6.824/2020/notes/l01.txt">6.824 讲义</a></li><li><a href="http://nil.csail.mit.edu/6.824/2020/video/1.html">6.824 视频</a></li><li><a href="https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf">论文</a></li><li><a href="https://github.com/Cxka/paper/blob/0a72fe0b354b65bac25e45163163eb2573f1faf2/map-reduce/map-reduce-cn.pdf">中文翻译</a></li><li><a href="https://blog.mrcroxx.com/posts/paper-reading/mapreduce-osdi04/">其他优质博客</a></li></ul><h1 id="补充MapReduce论文示例代码"><a href="#补充MapReduce论文示例代码" class="headerlink" title="补充MapReduce论文示例代码"></a>补充MapReduce论文示例代码</h1><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&quot;mapreduce/mapreduce.h&quot;</span></span><br><br><span class="hljs-comment">// User’s map function</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WordCounter</span> : <span class="hljs-keyword">public</span> Mapper &#123;<br>  <span class="hljs-keyword">public</span>:<br>    <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-type">void</span> <span class="hljs-title">Map</span><span class="hljs-params">(<span class="hljs-type">const</span> MapInput&amp; input)</span> </span>&#123;<br>      <span class="hljs-type">const</span> string&amp; text = input.<span class="hljs-built_in">value</span>();<br>      <span class="hljs-type">const</span> <span class="hljs-type">int</span> n = text.<span class="hljs-built_in">size</span>();<br>      <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i &lt; n; ) &#123;<br>        <span class="hljs-comment">// Skip past leading whitespace</span><br>        <span class="hljs-keyword">while</span> ((i &lt; n) &amp;&amp; <span class="hljs-built_in">isspace</span>(text[i]))<br>          i++;<br><br>        <span class="hljs-comment">// Find word end</span><br>        <span class="hljs-type">int</span> start = i;<br>        <span class="hljs-keyword">while</span> ((i &lt; n) &amp;&amp; !<span class="hljs-built_in">isspace</span>(text[i]))<br>          i++;<br>        <br>        <span class="hljs-keyword">if</span> (start &lt; i)<br>          <span class="hljs-built_in">Emit</span>(text.<span class="hljs-built_in">substr</span>(start,i-start),<span class="hljs-string">&quot;1&quot;</span>);<br>      &#125;<br>  &#125;<br>&#125;;<br><span class="hljs-built_in">REGISTER_MAPPER</span>(WordCounter);<br><br><span class="hljs-comment">// User’s reduce function</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Adder</span> : <span class="hljs-keyword">public</span> Reducer &#123;<br>  <span class="hljs-function"><span class="hljs-keyword">virtual</span> <span class="hljs-type">void</span> <span class="hljs-title">Reduce</span><span class="hljs-params">(ReduceInput* input)</span> </span>&#123;<br>    <span class="hljs-comment">// Iterate over all entries with the</span><br>    <span class="hljs-comment">// same key and add the values</span><br>    int64 value = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">while</span> (!input-&gt;<span class="hljs-built_in">done</span>()) &#123;<br>      value += <span class="hljs-built_in">StringToInt</span>(input-&gt;<span class="hljs-built_in">value</span>());<br>      input-&gt;<span class="hljs-built_in">NextValue</span>();<br>    &#125;<br><br>    <span class="hljs-comment">// Emit sum for input-&gt;key()</span><br>    <span class="hljs-built_in">Emit</span>(<span class="hljs-built_in">IntToString</span>(value));<br>  &#125;<br>&#125;;<br><span class="hljs-built_in">REGISTER_REDUCER</span>(Adder);<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-type">int</span> argc, <span class="hljs-type">char</span>** argv)</span> </span>&#123;<br>  <span class="hljs-built_in">ParseCommandLineFlags</span>(argc, argv);<br><br>  MapReduceSpecification spec;<br><br>  <span class="hljs-comment">// Store list of input files into &quot;spec&quot;</span><br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt; argc; i++) &#123;<br>    MapReduceInput* input = spec.<span class="hljs-built_in">add_input</span>();<br>    input-&gt;<span class="hljs-built_in">set_format</span>(<span class="hljs-string">&quot;text&quot;</span>);<br>    input-&gt;<span class="hljs-built_in">set_filepattern</span>(argv[i]);<br>    input-&gt;<span class="hljs-built_in">set_mapper_class</span>(<span class="hljs-string">&quot;WordCounter&quot;</span>);<br>  &#125;<br><br>  <span class="hljs-comment">// Specify the output files:</span><br>  <span class="hljs-comment">// /gfs/test/freq-00000-of-00100</span><br>  <span class="hljs-comment">// /gfs/test/freq-00001-of-00100</span><br>  <span class="hljs-comment">// ...</span><br>  MapReduceOutput* out = spec.<span class="hljs-built_in">output</span>();<br>  out-&gt;<span class="hljs-built_in">set_filebase</span>(<span class="hljs-string">&quot;/gfs/test/freq&quot;</span>);<br>  out-&gt;<span class="hljs-built_in">set_num_tasks</span>(<span class="hljs-number">100</span>);<br>  out-&gt;<span class="hljs-built_in">set_format</span>(<span class="hljs-string">&quot;text&quot;</span>);<br>  out-&gt;<span class="hljs-built_in">set_reducer_class</span>(<span class="hljs-string">&quot;Adder&quot;</span>);<br><br>  <span class="hljs-comment">// Optional: do partial sums within map</span><br>  <span class="hljs-comment">// tasks to save network bandwidth</span><br>  out-&gt;<span class="hljs-built_in">set_combiner_class</span>(<span class="hljs-string">&quot;Adder&quot;</span>);<br><br>  <span class="hljs-comment">// Tuning parameters: use at most 2000</span><br>  <span class="hljs-comment">// machines and 100 MB of memory per task</span><br>  spec.<span class="hljs-built_in">set_machines</span>(<span class="hljs-number">2000</span>);<br>  spec.<span class="hljs-built_in">set_map_megabytes</span>(<span class="hljs-number">100</span>);<br>  spec.<span class="hljs-built_in">set_reduce_megabytes</span>(<span class="hljs-number">100</span>);<br><br>  <span class="hljs-comment">// Now run it</span><br>  MapReduceResult result;<br>  <span class="hljs-keyword">if</span> (!<span class="hljs-built_in">MapReduce</span>(spec, &amp;result)) <span class="hljs-built_in">abort</span>();<br><br>  <span class="hljs-comment">// Done: ’result’ structure contains info</span><br>  <span class="hljs-comment">// about counters, time taken, number of</span><br>  <span class="hljs-comment">// machines used, etc.</span><br><br>  <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>MIT6.824</category>
      
      <category>论文</category>
      
    </categories>
    
    
    <tags>
      
      <tag>转载</tag>
      
      <tag>分布式</tag>
      
      <tag>MIT6.824</tag>
      
      <tag>论文阅读</tag>
      
      <tag>MapReduce</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>git学习记录</title>
    <link href="/2023/11/04/git%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <url>/2023/11/04/git%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<p>网站学习链接：<a href="https://learngitbranching.js.org/?locale=zh_CN">https://learngitbranching.js.org/?locale=zh_CN</a></p><p>经验贴：<a href="https://zhuanlan.zhihu.com/p/383960650">https://zhuanlan.zhihu.com/p/383960650</a></p><ul><li><code>git brach -b &lt;branchName&gt;</code> 新建一个<code>branchName</code>分支，并切换到那个分支</li><li><code>git checkout branchName^</code> 使得HEAD指向<code>branchName</code>的前一个分支，有几个<code>^</code>就代表前移几个，如果<code>branchName</code> 有多个parent，那么<code>^num</code> 就是指选择跳到第几个parent</li><li><code>git checkout branchName~2</code> 使得HEAD指向<code>branchName</code> 的前2个分支，如果没有后面的数字就是默认1</li><li><code>git checkout HEAD~^2~2</code> ,<code>git branch -f bugWork HEAD~^2^</code>git的操作符还支持链式操作</li><li><code>git revert</code>用于撤销提交，生成一条新的提交来覆盖之前的修改，保留修改历史，语法：<code>git revert &lt;commit&gt;</code> ，其中，<code>&lt;commit&gt;</code>是<strong>要撤销的提交</strong>的哈希值或引用</li><li><code>git reset</code>用于重置分支的指针，可以选择保留或丢弃之前的修改，改变提交历史，语法：<code>git reset &lt;commit&gt;</code> ，其中，<code>&lt;commit&gt;</code>是<strong>要重置到的目标提交</strong>的哈希值或引用<ul><li><code>-soft</code>：重置分支的指针，但不改变工作区和暂存区的内容。之前的修改将被保留在暂存区中，你可以随时重新提交它们</li><li><code>-mixed</code>（默认选项）：重置分支的指针，并清除暂存区的内容。之前的修改将保留在工作区中，但不会自动提交</li><li><code>-hard</code>：重置分支的指针，并清除暂存区和工作区的内容。之前的修改将完全丢失，慎用该选项</li></ul></li><li>使用<code>git reset</code>会改变分支的提交历史。如果你在公共分支上使用<code>git reset</code>，并将其推送到远程仓库，可能会导致其他人的问题。因此，在公共分支上通常更推荐使用<code>git revert</code></li><li><code>git rebase &lt;branch&gt; &lt;based_branch&gt;</code> 可以指定将branch对based_branch进行rebase</li><li><code>git cherry-pick commit1 commit2</code> 可以将两个commit放到HEAD后面,注意commit的顺序就是这里排列的顺序</li><li><code>git rebase -i HEAD~4</code> 是一个 Git 命令，用于以<strong>交互方式</strong>进行历史提交的重新整理（rebase）。该命令允许你修改最近的 4 个提交，可以根据需要修改这些操作命令。常见的操作命令包括 “pick”（保留提交）、”edit”（修改提交）、”reword”（修改提交信息）、”squash”（合并提交）等</li><li><code>git rebase</code></li><li><code>git branch -f main caption</code> 用于将分支<code>main</code>的指针强制移动到指定的提交<code>caption</code>上</li><li><code>git commit --amend</code>是一个用于修改最近一次提交的Git命令</li><li><code>git tag tagName commitName</code> 给c1提交赋上v0标签</li><li><code>git push origin &lt;tagname&gt;</code> 将tag推送到远程</li><li><code>git describe &lt;commit&gt;</code> 如果不指定<code>&lt;commit&gt;</code>，则默认使用当前所在的提交。如果这个<code>&lt;commit&gt;</code> 有标签就输出相应的标签，如果没有就会去找最近的标签，生成一个描述，格式为<code>&lt;tag&gt;-&lt;num&gt;-g&lt;hash&gt;</code>，其中<code>&lt;tag&gt;</code>是最近的标签，<code>&lt;num&gt;</code>是指定提交与最近标签之间的提交数，<code>&lt;hash&gt;</code>是指定提交的简短哈希值</li><li><code>git fetch</code> 会获取远程厂库的所有分支的所有提交记录，并把<code>orgin/xxx</code> 指向对应的远程提交，但是它并不会更改本地的内容还有本地的分支</li><li><code>git pull</code> 实际上是<code>git fetch</code>和<code>git merge</code>的合集，它会获取远程更新并与当前对应分支合并</li><li><code>git pull --rebase</code> 是<code>git fetch</code> 和<code>git rebase</code> 的合集，它会获取远程更新并对其进行rebase</li><li><code>git push</code> 实际上会merge本地拷贝的<code>origin/xxxx</code>分支然后再提交</li><li><code>git checkout -b foo origin/main</code> 可以把<code>foo</code>和远程分支<code>origin/main</code>绑定起来</li><li><code>git branch -u o/main foo</code> 使得foo分支直接绑定远程分支main，如果当前分支就是foo，就可省略foo</li><li><code>git branch -m &lt;old_branch_name&gt; &lt;new_branch_name&gt;</code> 可以修改branch的名字</li><li><code>git push origin main</code> 将本地 <code>main</code> 分支的提交推送到远程仓库 <code>origin</code> 的 <code>main</code> 分支的命令</li><li><code>git push origin &lt;source&gt;:&lt;destination&gt;</code> 将本地 <code>&lt;source&gt;</code> 分支的提交推送到远程仓库 <code>origin</code> 的 <code>&lt;destination&gt;</code> 分支的命令。这样可以将本地的修改共享给其他协作者，并将本地分支映射到远程仓库的不同分支。<code>source</code>既可以是<code>branch</code>的名字也可以是某个指定的提交位置例如<code>HEAD~2</code></li><li><code>git fetch origin &lt;source&gt;:&lt;destination&gt;</code> 将远程的厂库<code>origin</code>的<code>source</code>分支更新到本地<code>destination</code>分支。注意<code>source</code>也是既可以是分支名也可以是某个指定的提交位置。注意这样子的<code>o/xxxx</code>分支不会进行变化</li><li><code>git push origin :side</code> 会删除远程的side分支</li><li><code>git fetch origin :bugFix</code> 会在本地创建一个bugFix分支</li><li><code>git pull origin foo</code> 相当于：<code>git fetch origin foo; git merge o/foo</code></li><li><code>git pull origin bar~1:bugFix</code> 相当于：<code>git fetch origin bar~1:bugFix; git merge bugFix</code></li></ul>]]></content>
    
    
    <categories>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>博客基础使用指南</title>
    <link href="/2023/11/04/%E5%8D%9A%E5%AE%A2%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <url>/2023/11/04/%E5%8D%9A%E5%AE%A2%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    
    <content type="html"><![CDATA[<h1 id="在本地写文章"><a href="#在本地写文章" class="headerlink" title="在本地写文章"></a>在本地写文章</h1><pre><code class="hljs">指令需要在根目录下的控制台中运行</code></pre><ul><li>直接创建新文章：<code>hexo new a</code></li><li>新建草稿：<code>hexo new draft b</code></li><li>将草稿变成发布文章：<code>hexo publish b</code></li><li>文章首部内容示例：  <figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">我就是标题</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2021-09-25 23:32:04</span><br><span class="hljs-attr">comments:</span> <span class="hljs-literal">true</span> <span class="hljs-comment">#是否可评论 </span><br><span class="hljs-attr">layout:</span> <span class="hljs-string">post</span> <span class="hljs-comment"># 公开文章 </span><br><span class="hljs-attr">toc:</span> <span class="hljs-literal">true</span> <span class="hljs-comment">#是否显示文章目录 </span><br><span class="hljs-attr">tags:</span>   <span class="hljs-comment">#标签 </span><br><span class="hljs-bullet">-</span> <span class="hljs-string">我就是新的标签1</span><br><span class="hljs-bullet">-</span> <span class="hljs-string">老子是新的标签2</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure></li></ul><h1 id="部署运行"><a href="#部署运行" class="headerlink" title="部署运行"></a>部署运行</h1><ul><li>清理缓存： <code>hexo clean</code></li><li>生成静态网页并部署到远程github厂库：<code>hexo g -d</code></li><li>启动服务：<code>hexo s</code></li><li>本地访问的链接：<code>http://localhost:4000/</code></li><li>远程访问链接：<code>https://slipegg.github.io/</code></li></ul><h1 id="备忘录"><a href="#备忘录" class="headerlink" title="备忘录"></a>备忘录</h1><p>站点访问统计:<a href="https://console.leancloud.app/apps">国际版LeanCloud</a></p><p>注意下面的部署文章没有提到要在LeadCloud里创建Comment数据库，以及设置权限，修改serverUrl配置，修改完成后评论功能才能正常使用。</p><p>github推送使用的是git，所以依赖的其实是本地的私钥</p><p>部署的参考文档：</p><ul><li><a href="https://blog.csdn.net/yaorongke/article/details/119089190">https://blog.csdn.net/yaorongke/article/details/119089190</a></li><li><a href="https://blog.csdn.net/PaperJack/article/details/120479912">https://blog.csdn.net/PaperJack/article/details/120479912</a></li><li><a href="https://iyichen.xyz/2022/01/hexo-leancloud-valine-access-fail/">https://iyichen.xyz/2022/01/hexo-leancloud-valine-access-fail/</a></li><li><a href="https://zhengyujie.github.io/2019/08/18/valine%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/">https://zhengyujie.github.io/2019/08/18/valine%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>其他</category>
      
    </categories>
    
    
    <tags>
      
      <tag>博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/11/04/hello-world/"/>
    <url>/2023/11/04/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>测试文章</title>
    <link href="/2021/06/10/test/"/>
    <url>/2021/06/10/test/</url>
    
    <content type="html"><![CDATA[<p>这是一篇测试文章</p><img src="/2021/06/10/test/test.jpg" class="" title="图片引用方法一"><p><img src="/2021/06/10/test/test.jpg" alt="图片引用方法二"></p><p><img src="/images/test.jpg" alt="图片引用方法三"></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
