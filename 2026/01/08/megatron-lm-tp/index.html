

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.jpg">
  <link rel="icon" href="/img/favicon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#1C262C">
  <meta name="author" content="滑滑蛋">
  <meta name="keywords" content="">
  
    <meta name="description" content="理论基础 基础的理论分析可以见之前写的内容：https:&#x2F;&#x2F;slipegg.github.io&#x2F;2025&#x2F;06&#x2F;07&#x2F;Picotron-Tutorial%20Tensor%20Parallel&#x2F;，https:&#x2F;&#x2F;slipegg.github.io&#x2F;2025&#x2F;12&#x2F;07&#x2F;Megatron-LM-paper-note&#x2F;  简单来说就是存在行并行与列并行两种Tensor并行方式。  上述的前向传播好理解">
<meta property="og:type" content="article">
<meta property="og:title" content="【Megatron-LM源码分析（五）】-Tensor并行">
<meta property="og:url" content="http://example.com/2026/01/08/megatron-lm-tp/index.html">
<meta property="og:site_name" content="滑滑蛋的个人博客">
<meta property="og:description" content="理论基础 基础的理论分析可以见之前写的内容：https:&#x2F;&#x2F;slipegg.github.io&#x2F;2025&#x2F;06&#x2F;07&#x2F;Picotron-Tutorial%20Tensor%20Parallel&#x2F;，https:&#x2F;&#x2F;slipegg.github.io&#x2F;2025&#x2F;12&#x2F;07&#x2F;Megatron-LM-paper-note&#x2F;  简单来说就是存在行并行与列并行两种Tensor并行方式。  上述的前向传播好理解">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2026/01/08/megatron-lm-tp/image.png">
<meta property="og:image" content="http://example.com/2026/01/08/megatron-lm-tp/image-1.png">
<meta property="og:image" content="http://example.com/2026/01/08/megatron-lm-tp/image-2.png">
<meta property="og:image" content="http://example.com/2026/01/08/megatron-lm-tp/image-3.png">
<meta property="article:published_time" content="2026-01-08T05:50:53.000Z">
<meta property="article:modified_time" content="2026-01-08T05:54:33.779Z">
<meta property="article:author" content="滑滑蛋">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Megatron-LM">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2026/01/08/megatron-lm-tp/image.png">
  
  
  
  <title>【Megatron-LM源码分析（五）】-Tensor并行 - 滑滑蛋的个人博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"d38d21fca521d897798e5bdd940a90d0","google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"WMtHomhQYlrbIodTwoPU3gTY-MdYXbMMI","app_key":"pZeun9WfI1yaQrIoUbvTQrXv","server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?d38d21fca521d897798e5bdd940a90d0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', '');
        });
      }
    </script>
  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>滑滑蛋</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="【Megatron-LM源码分析（五）】-Tensor并行"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2026-01-08 13:50" pubdate>
          2026年1月8日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          176k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          881 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">【Megatron-LM源码分析（五）】-Tensor并行</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h1><ul>
<li><p>基础的理论分析可以见之前写的内容：<a target="_blank" rel="noopener" href="https://slipegg.github.io/2025/06/07/Picotron-Tutorial%20Tensor%20Parallel/%EF%BC%8Chttps://slipegg.github.io/2025/12/07/Megatron-LM-paper-note/">https://slipegg.github.io/2025/06/07/Picotron-Tutorial%20Tensor%20Parallel/，https://slipegg.github.io/2025/12/07/Megatron-LM-paper-note/</a></p>
</li>
<li><p>简单来说就是存在<strong>行并行</strong>与<strong>列并行</strong>两种Tensor并行方式。</p>
</li>
<li><p>上述的前向传播好理解，但是反向传播会稍微复杂一些，这里可以直接访问ChatGPT查看其介绍，介绍的还是很详细的：<a target="_blank" rel="noopener" href="https://chatgpt.com/share/6953c681-7cd0-8011-8dfe-1d2281834b08">https://chatgpt.com/share/6953c681-7cd0-8011-8dfe-1d2281834b08</a></p>
<ul>
<li><p>结论就是在列并行中求 $\frac{\partial L}{\partial X}$时需要All Reduce(Sum)操作，而求$\frac{\partial L}{\partial W}$不需要，在行并行中求偏导的时候都不需要额外通信操作。</p>
</li>
<li><p>由于在MLP层我们采取的是先列并行再行并行的形式，从而减少前向传播过程中的通信量，故在反向传播过程中在反向传播到列并行时也需要进行一次All Reduce(Sum)操作。</p>
</li>
</ul>
</li>
</ul>
<h1 id="训练数据获取"><a href="#训练数据获取" class="headerlink" title="训练数据获取"></a>训练数据获取</h1><p>在<code>pretrain_gpt.py</code>文件中的<code>get_batch</code>函数可以看到有专门的tp数据处理，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_batch</span>(<span class="hljs-params">data_iterator</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Generate a batch.&quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> this is pretty hacky, find a better way</span><br>    <span class="hljs-keyword">if</span> (<span class="hljs-keyword">not</span> parallel_state.is_pipeline_first_stage(ignore_virtual=<span class="hljs-literal">True</span>)) <span class="hljs-keyword">and</span> (<br>        <span class="hljs-keyword">not</span> parallel_state.is_pipeline_last_stage(ignore_virtual=<span class="hljs-literal">True</span>)<br>    ):<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># get batches based on the TP rank you are on</span><br>    batch = get_batch_on_this_tp_rank(data_iterator)<br><br>    <span class="hljs-comment"># slice batch along sequence dimension for context parallelism</span><br>    batch = get_batch_on_this_cp_rank(batch)<br><br>    <span class="hljs-keyword">return</span> batch.values()<br><br></code></pre></td></tr></table></figure>

<p>进一步的，查看<code>get_batch_on_this_tp_rank</code>函数如下所示，tp rank为0的worker会从data loader中获取一份micro_batch的数据，然后组成batch格式，将其broadcast到tp组的其他worker中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_batch_on_this_tp_rank</span>(<span class="hljs-params">data_iterator</span>):<br><br>    args = get_args()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_broadcast</span>(<span class="hljs-params">item</span>):<br>        <span class="hljs-keyword">if</span> item <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            torch.distributed.broadcast(<br>                item,<br>                mpu.get_tensor_model_parallel_src_rank(),<br>                group=mpu.get_tensor_model_parallel_group(),<br>            )<br><br>    <span class="hljs-keyword">if</span> mpu.get_tensor_model_parallel_rank() == <span class="hljs-number">0</span>:<br><br>        <span class="hljs-keyword">if</span> data_iterator <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            data = <span class="hljs-built_in">next</span>(data_iterator)<br>        <span class="hljs-keyword">else</span>:<br>            data = <span class="hljs-literal">None</span><br><br>        batch = &#123;<br>            <span class="hljs-string">&#x27;tokens&#x27;</span>: data[<span class="hljs-string">&quot;tokens&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>            <span class="hljs-string">&#x27;labels&#x27;</span>: data[<span class="hljs-string">&quot;labels&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>            <span class="hljs-string">&#x27;loss_mask&#x27;</span>: data[<span class="hljs-string">&quot;loss_mask&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>            <span class="hljs-string">&#x27;attention_mask&#x27;</span>: (<br>                <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;attention_mask&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> data<br>                <span class="hljs-keyword">else</span> data[<span class="hljs-string">&quot;attention_mask&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>)<br>            ),<br>            <span class="hljs-string">&#x27;position_ids&#x27;</span>: data[<span class="hljs-string">&quot;position_ids&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>        &#125;<br><br>        <span class="hljs-keyword">if</span> args.pipeline_model_parallel_size == <span class="hljs-number">1</span>:<br>            _broadcast(batch[<span class="hljs-string">&#x27;tokens&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;labels&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;loss_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;position_ids&#x27;</span>])<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_first_stage():<br>            _broadcast(batch[<span class="hljs-string">&#x27;tokens&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;position_ids&#x27;</span>])<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_last_stage():<br>            <span class="hljs-comment"># Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.</span><br>            <span class="hljs-comment"># Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need</span><br>            <span class="hljs-comment"># to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.</span><br>            <span class="hljs-keyword">if</span> args.mtp_num_layers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                _broadcast(batch[<span class="hljs-string">&#x27;tokens&#x27;</span>])<br>                _broadcast(batch[<span class="hljs-string">&#x27;position_ids&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;labels&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;loss_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])<br><br>    <span class="hljs-keyword">else</span>:<br><br>        tokens = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.int64,<br>            device=torch.cuda.current_device(),<br>        )<br>        labels = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.int64,<br>            device=torch.cuda.current_device(),<br>        )<br>        loss_mask = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.float32,<br>            device=torch.cuda.current_device(),<br>        )<br>        <span class="hljs-keyword">if</span> args.create_attention_mask_in_dataloader:<br>            attention_mask = torch.empty(<br>                (args.micro_batch_size, <span class="hljs-number">1</span>, args.seq_length, args.seq_length),<br>                dtype=torch.<span class="hljs-built_in">bool</span>,<br>                device=torch.cuda.current_device(),<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            attention_mask = <span class="hljs-literal">None</span><br>        position_ids = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.int64,<br>            device=torch.cuda.current_device(),<br>        )<br><br>        <span class="hljs-keyword">if</span> args.pipeline_model_parallel_size == <span class="hljs-number">1</span>:<br>            _broadcast(tokens)<br>            _broadcast(labels)<br>            _broadcast(loss_mask)<br>            _broadcast(attention_mask)<br>            _broadcast(position_ids)<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_first_stage():<br>            labels = <span class="hljs-literal">None</span><br>            loss_mask = <span class="hljs-literal">None</span><br><br>            _broadcast(tokens)<br>            _broadcast(attention_mask)<br>            _broadcast(position_ids)<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_last_stage():<br>            <span class="hljs-comment"># Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.</span><br>            <span class="hljs-comment"># Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need</span><br>            <span class="hljs-comment"># to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.</span><br>            <span class="hljs-keyword">if</span> args.mtp_num_layers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                _broadcast(tokens)<br>                _broadcast(position_ids)<br>            <span class="hljs-keyword">else</span>:<br>                tokens = <span class="hljs-literal">None</span><br>                position_ids = <span class="hljs-literal">None</span><br><br>            _broadcast(labels)<br>            _broadcast(loss_mask)<br>            _broadcast(attention_mask)<br><br>        batch = &#123;<br>            <span class="hljs-string">&#x27;tokens&#x27;</span>: tokens,<br>            <span class="hljs-string">&#x27;labels&#x27;</span>: labels,<br>            <span class="hljs-string">&#x27;loss_mask&#x27;</span>: loss_mask,<br>            <span class="hljs-string">&#x27;attention_mask&#x27;</span>: attention_mask,<br>            <span class="hljs-string">&#x27;position_ids&#x27;</span>: position_ids,<br>        &#125;<br><br>    <span class="hljs-keyword">return</span> batch<br><br></code></pre></td></tr></table></figure>

<p>上述可以看到其主要broadcast了tokens、labels、loss_mask、attention_mask、position_ids这五分数据，如下图的torch profiler所示，也确实发生了5次的broadcast。</p>
<p><img src="/2026/01/08/megatron-lm-tp/image.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="Tensor并行相关代码"><a href="#Tensor并行相关代码" class="headerlink" title="Tensor并行相关代码"></a>Tensor并行相关代码</h1><h2 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h2><p>model构建的入口函数在<code>pretrain_gpt.py</code>的<code>model_provider</code>函数中，其默认执行路线如下所示（去除了一些不必要的分支）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_provider</span>(<span class="hljs-params"></span><br><span class="hljs-params">    pre_process=<span class="hljs-literal">True</span>, post_process=<span class="hljs-literal">True</span>, vp_stage: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span></span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Union</span>[GPTModel, megatron.legacy.model.GPTModel]:<br>    <span class="hljs-string">&quot;&quot;&quot;Builds the model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    If you set the use_legacy_models to True, it will return the legacy GPT model and if not the mcore GPT model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        pre_process (bool, optional): Set to true if you need to compute embedings. Defaults to True.</span><br><span class="hljs-string">        post_process (bool, optional): Set to true if you need to want to compute output logits/loss. Defaults to True.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        Union[GPTModel, megatron.legacy.model.GPTModel]: The returned model</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    args = get_args()<br><br>    use_te = args.transformer_impl == <span class="hljs-string">&quot;transformer_engine&quot;</span><br><br>    print_rank_0(<span class="hljs-string">&#x27;building GPT model ...&#x27;</span>)<br>    <span class="hljs-comment"># Experimental loading arguments from yaml</span><br>    <span class="hljs-keyword">if</span> args.yaml_cfg <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config = core_transformer_config_from_yaml(args, <span class="hljs-string">&quot;language_model&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        config = core_transformer_config_from_args(args)<br><br>                <span class="hljs-comment"># Define the decoder layer spec</span><br>                transformer_layer_spec = _get_transformer_layer_spec(use_te, config)<br><br>        model = GPTModel(<br>            config=config,<br>            transformer_layer_spec=transformer_layer_spec,<br>            vocab_size=args.padded_vocab_size,<br>            max_sequence_length=args.max_position_embeddings,<br>            pre_process=pre_process,<br>            post_process=post_process,<br>            fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,<br>            parallel_output=<span class="hljs-literal">True</span>,<br>            share_embeddings_and_output_weights=<span class="hljs-keyword">not</span> args.untie_embeddings_and_output_weights,<br>            position_embedding_type=args.position_embedding_type,<br>            rotary_percent=args.rotary_percent,<br>            rotary_base=args.rotary_base,<br>            rope_scaling=args.use_rope_scaling,<br>            mtp_block_spec=mtp_block_spec,<br>            vp_stage=vp_stage,<br>        )<br><br>    <span class="hljs-keyword">return</span> model<br><br></code></pre></td></tr></table></figure>

<p>对于<code>_get_transformer_layer_spec</code>函数，其实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_transformer_layer_spec</span>(<span class="hljs-params">use_te, config</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Get transformer layer specification based on configuration.</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        use_te (bool): Whether to use Transformer Engine</span><br><span class="hljs-string">        args: Training arguments</span><br><span class="hljs-string">        config: Model configuration</span><br><span class="hljs-string">        </span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        transformer_layer_spec: The transformer layer specification</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    args = get_args()<br>    <span class="hljs-keyword">if</span> use_te:<br>        <span class="hljs-keyword">return</span> get_gpt_layer_with_transformer_engine_spec(<br>            args.num_experts,<br>            args.moe_grouped_gemm,<br>            args.qk_layernorm,<br>            args.multi_latent_attention,<br>            moe_use_legacy_grouped_gemm=args.moe_use_legacy_grouped_gemm,<br>            qk_l2_norm=args.qk_l2_norm,<br>            use_kitchen=config.use_kitchen,<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> get_gpt_layer_local_spec(<br>            args.num_experts,<br>            args.moe_grouped_gemm,<br>            args.qk_layernorm,<br>            args.multi_latent_attention,<br>            moe_use_legacy_grouped_gemm=args.moe_use_legacy_grouped_gemm,<br>            normalization=args.normalization,<br>            use_kitchen=config.use_kitchen,<br>        )<br><br></code></pre></td></tr></table></figure>

<p>默认参数中<code>use_te</code>为<code>True</code>，即使用了具有算子融合等优化的<code>transformer_engine</code>，故走到了<code>get_gpt_layer_with_transformer_engine_spec</code>分支，而不是Megatron-LM本地的<code>get_gpt_layer_local_spec</code>分支，<code>get_gpt_layer_with_transformer_engine_spec</code>如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_gpt_layer_with_transformer_engine_spec</span>(<span class="hljs-params"></span><br><span class="hljs-params">    num_experts: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    moe_grouped_gemm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    qk_layernorm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    multi_latent_attention: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    fp8: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># pylint: disable=unused-argument</span></span><br><span class="hljs-params">    moe_use_legacy_grouped_gemm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    qk_l2_norm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    use_te_op_fuser: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    use_kitchen: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params"></span>) -&gt; ModuleSpec:<br>    <span class="hljs-string">&quot;&quot;&quot;Use this spec to use lower-level Transformer Engine modules (required for fp8 training).</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        num_experts (int, optional): Number of experts. Defaults to None.</span><br><span class="hljs-string">        moe_grouped_gemm (bool, optional): To use Grouped GEMM. Defaults to False.</span><br><span class="hljs-string">        qk_layernorm (bool, optional): To use layernorm for queries/keys. Defaults to False.</span><br><span class="hljs-string">        fp8 (str, optional): Deprecated. For temporary Nemo compatibility.</span><br><span class="hljs-string">        moe_use_legacy_grouped_gemm (bool, optional): Force use the legacy GroupedMLP.</span><br><span class="hljs-string">                                                      Defaults to False.</span><br><span class="hljs-string">        qk_l2_norm (bool, optional): To use l2 norm for queries/keys. Defaults to False.</span><br><span class="hljs-string">        use_te_op_fuser (bool, optional): Use Transformer Engine&#x27;s operation-based API, which may</span><br><span class="hljs-string">                                          enable certain operation fusions. Defaults to False.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        ModuleSpec: Module specification with TE modules</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> fp8 <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        warnings.warn(<br>            <span class="hljs-string">&#x27;The fp8 argument in &quot;get_gpt_layer_with_transformer_engine_spec&quot; has been deprecated&#x27;</span><br>            <span class="hljs-string">&quot; and will be removed soon. Please update your code accordingly.&quot;</span><br>        )<br><br>    <span class="hljs-keyword">if</span> use_kitchen:<br>        <span class="hljs-keyword">assert</span> HAVE_KITCHEN<br>        backend: BackendSpecProvider = KitchenSpecProvider(fallback=TESpecProvider())<br>        <span class="hljs-keyword">if</span> use_te_op_fuser:<br>            <span class="hljs-keyword">raise</span> AssertionError(<span class="hljs-string">&quot;use_te_op_fuser not compatible with using kitchen in mlp.&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        backend = TESpecProvider()<br><br>    mlp = get_mlp_module_spec_for_backend(<br>        backend=backend,<br>        num_experts=num_experts,<br>        moe_grouped_gemm=moe_grouped_gemm,<br>        moe_use_legacy_grouped_gemm=moe_use_legacy_grouped_gemm,<br>        use_te_op_fuser=use_te_op_fuser,<br>    )<br><br>    <span class="hljs-keyword">if</span> multi_latent_attention:<br>        <span class="hljs-keyword">assert</span> qk_l2_norm <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>, <span class="hljs-string">&quot;qk_l2_norm is not supported with MLA.&quot;</span><br>        linear_q_up_proj = (<br>            backend.column_parallel_layer_norm_linear()<br>            <span class="hljs-keyword">if</span> qk_layernorm<br>            <span class="hljs-keyword">else</span> backend.column_parallel_linear()<br>        )<br>        linear_kv_up_proj = (<br>            backend.column_parallel_layer_norm_linear()<br>            <span class="hljs-keyword">if</span> qk_layernorm<br>            <span class="hljs-keyword">else</span> backend.column_parallel_linear()<br>        )<br>        <span class="hljs-keyword">return</span> ModuleSpec(<br>            module=TransformerLayer,<br>            submodules=TransformerLayerSubmodules(<br>                input_layernorm=backend.layer_norm(),<br>                self_attention=ModuleSpec(<br>                    module=MLASelfAttention,<br>                    params=&#123;<span class="hljs-string">&quot;attn_mask_type&quot;</span>: AttnMaskType.causal&#125;,<br>                    submodules=MLASelfAttentionSubmodules(<br>                        linear_q_proj=backend.column_parallel_linear(),<br>                        linear_q_down_proj=backend.linear(),<br>                        linear_q_up_proj=linear_q_up_proj,<br>                        linear_kv_down_proj=backend.linear(),<br>                        linear_kv_up_proj=linear_kv_up_proj,<br>                        core_attention=backend.core_attention(),<br>                        linear_proj=backend.row_parallel_linear(),<br>                        q_layernorm=IdentityOp,<br>                        kv_layernorm=IdentityOp,<br>                    ),<br>                ),<br>                self_attn_bda=get_bias_dropout_add,<br>                pre_mlp_layernorm=backend.layer_norm() <span class="hljs-keyword">if</span> num_experts <span class="hljs-keyword">else</span> IdentityOp,<br>                mlp=mlp,<br>                mlp_bda=get_bias_dropout_add,<br>            ),<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        qk_norm = backend.layer_norm(for_qk=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">return</span> ModuleSpec(<br>            module=TransformerLayer,<br>            submodules=TransformerLayerSubmodules(<br>                self_attention=ModuleSpec(<br>                    module=SelfAttention,<br>                    params=&#123;<span class="hljs-string">&quot;attn_mask_type&quot;</span>: AttnMaskType.causal&#125;,<br>                    submodules=SelfAttentionSubmodules(<br>                        linear_qkv=backend.column_parallel_layer_norm_linear(),<br>                        core_attention=backend.core_attention(),<br>                        linear_proj=backend.row_parallel_linear(),<br>                        q_layernorm=(<br>                            L2Norm <span class="hljs-keyword">if</span> qk_l2_norm <span class="hljs-keyword">else</span> (qk_norm <span class="hljs-keyword">if</span> qk_layernorm <span class="hljs-keyword">else</span> IdentityOp)<br>                        ),<br>                        k_layernorm=(<br>                            L2Norm <span class="hljs-keyword">if</span> qk_l2_norm <span class="hljs-keyword">else</span> (qk_norm <span class="hljs-keyword">if</span> qk_layernorm <span class="hljs-keyword">else</span> IdentityOp)<br>                        ),<br>                    ),<br>                ),<br>                self_attn_bda=get_bias_dropout_add,<br>                pre_mlp_layernorm=backend.layer_norm() <span class="hljs-keyword">if</span> num_experts <span class="hljs-keyword">else</span> IdentityOp,<br>                mlp=mlp,<br>                mlp_bda=get_bias_dropout_add,<br>                sharded_state_dict_keys_map=&#123;<br>                    <span class="hljs-string">&quot;mlp.0.weight&quot;</span>: <span class="hljs-string">&quot;mlp.linear_fc1.layer_norm_weight&quot;</span>,<br>                    <span class="hljs-string">&quot;mlp.0.bias&quot;</span>: <span class="hljs-string">&quot;mlp.linear_fc1.layer_norm_bias&quot;</span>,<br>                    <span class="hljs-string">&quot;mlp.1.basic_ops.0.weight&quot;</span>: <span class="hljs-string">&quot;mlp.linear_fc1.weight&quot;</span>,<br>                    <span class="hljs-string">&quot;mlp.1.basic_ops.1.bias&quot;</span>: <span class="hljs-string">&quot;mlp.linear_fc1.bias&quot;</span>,<br>                    <span class="hljs-string">&quot;mlp.3.basic_ops.0.weight&quot;</span>: <span class="hljs-string">&quot;mlp.linear_fc2.weight&quot;</span>,<br>                    <span class="hljs-string">&quot;mlp.3.basic_ops.1.bias&quot;</span>: <span class="hljs-string">&quot;mlp.linear_fc2.bias&quot;</span>,<br>                &#125;,<br>            ),<br>        )<br><br></code></pre></td></tr></table></figure>

<p>注意上述的<code>use_kitchen</code>很重要，而默认情况下其为False，故<code>backend = TESpecProvider()</code>，即使用的是<code>transformer_engine</code>来生成TransformerLayer，而不是用 NVIDIA Kitchen作为后端来提供（部分）Transformer 子模块的实现&#x2F;spec。而Megatron-LM还进一步对<code>transformer_engine</code>的相关模块进行了简单封装以使其可以支持Tensor并行等功能。</p>
<h3 id="Megatron-LM本地实现gpt-layer"><a href="#Megatron-LM本地实现gpt-layer" class="headerlink" title="Megatron-LM本地实现gpt_layer"></a>Megatron-LM本地实现gpt_layer</h3><p>由于<code>transformer_engine</code>是专有封装过于复杂，所以我们转而去查看Megatron-LM的本地实现，<code>get_gpt_layer_with_transformer_engine_spec</code>如下所示，我们查看的backend为<code>LocalSpecProvider</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_gpt_layer_local_spec</span>(<span class="hljs-params"></span><br><span class="hljs-params">    num_experts: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    moe_grouped_gemm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    qk_layernorm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    multi_latent_attention: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    fp8: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># pylint: disable=unused-argument</span></span><br><span class="hljs-params">    moe_use_legacy_grouped_gemm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    normalization: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    qk_l2_norm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    use_kitchen: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params"></span>) -&gt; ModuleSpec:<br>    <span class="hljs-string">&quot;&quot;&quot;Use this spec for an implementation using only modules in Megatron-Core.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        num_experts (int, optional): Number of experts. Defaults to None.</span><br><span class="hljs-string">        moe_grouped_gemm (bool, optional): To use Grouped GEMM. Defaults to False.</span><br><span class="hljs-string">        qk_layernorm (bool, optional): To use layernorm for queries/keys. Defaults to False.</span><br><span class="hljs-string">        fp8 (str, optional): Deprecated. For temporary Nemo compatibility.</span><br><span class="hljs-string">        moe_use_legacy_grouped_gemm (bool, optional): Force use the legacy GroupedMLP.</span><br><span class="hljs-string">                                                      Defaults to False.</span><br><span class="hljs-string">        qk_l2_norm (bool, optional): To use l2 norm for queries/keys. Defaults to False.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        ModuleSpec: Module specification with Megatron-Core modules</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">if</span> use_kitchen:<br>        <span class="hljs-keyword">assert</span> HAVE_KITCHEN<br>        backend = KitchenSpecProvider(fallback=LocalSpecProvider())<br>    <span class="hljs-keyword">else</span>:<br>        backend = LocalSpecProvider()<br>    <span class="hljs-comment"># Adjust for RMS norm.</span><br>    <span class="hljs-keyword">if</span> normalization == <span class="hljs-string">&quot;RMSNorm&quot;</span>:<br>        layer_norm = backend.layer_norm(rms_norm=<span class="hljs-literal">True</span>, for_qk=<span class="hljs-literal">False</span>)<br>        qk_norm = backend.layer_norm(rms_norm=<span class="hljs-literal">True</span>, for_qk=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">else</span>:<br>        layer_norm = backend.layer_norm(rms_norm=<span class="hljs-literal">False</span>, for_qk=<span class="hljs-literal">False</span>)<br>        qk_norm = backend.layer_norm(rms_norm=<span class="hljs-literal">False</span>, for_qk=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">if</span> fp8 <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        warnings.warn(<br>            <span class="hljs-string">&#x27;The fp8 argument in &quot;get_gpt_layer_local_spec&quot; has been deprecated&#x27;</span><br>            <span class="hljs-string">&quot; and will be removed soon. Please update your code accordingly.&quot;</span><br>        )<br><br>    mlp = get_mlp_module_spec_for_backend(<br>        backend=backend,<br>        num_experts=num_experts,<br>        moe_grouped_gemm=moe_grouped_gemm,<br>        moe_use_legacy_grouped_gemm=moe_use_legacy_grouped_gemm,<br>    )<br><br>    <span class="hljs-keyword">if</span> multi_latent_attention:<br>        <span class="hljs-keyword">assert</span> qk_l2_norm <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>, <span class="hljs-string">&quot;qk_l2_norm is not supported with MLA.&quot;</span><br>        <span class="hljs-keyword">return</span> ModuleSpec(<br>            module=TransformerLayer,<br>            submodules=TransformerLayerSubmodules(<br>                input_layernorm=layer_norm,<br>                self_attention=ModuleSpec(<br>                    module=MLASelfAttention,<br>                    params=&#123;<span class="hljs-string">&quot;attn_mask_type&quot;</span>: AttnMaskType.causal&#125;,<br>                    submodules=MLASelfAttentionSubmodules(<br>                        linear_q_proj=backend.column_parallel_linear(),<br>                        linear_q_down_proj=backend.column_parallel_linear(),<br>                        linear_q_up_proj=backend.column_parallel_linear(),<br>                        linear_kv_down_proj=backend.column_parallel_linear(),<br>                        linear_kv_up_proj=backend.column_parallel_linear(),<br>                        core_attention=backend.core_attention(),<br>                        linear_proj=backend.row_parallel_linear(),<br>                        q_layernorm=qk_norm <span class="hljs-keyword">if</span> qk_layernorm <span class="hljs-keyword">else</span> IdentityOp,<br>                        kv_layernorm=qk_norm <span class="hljs-keyword">if</span> qk_layernorm <span class="hljs-keyword">else</span> IdentityOp,<br>                    ),<br>                ),<br>                self_attn_bda=get_bias_dropout_add,<br>                pre_mlp_layernorm=layer_norm,<br>                mlp=mlp,<br>                mlp_bda=get_bias_dropout_add,<br>            ),<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> ModuleSpec(<br>            module=TransformerLayer,<br>            submodules=TransformerLayerSubmodules(<br>                input_layernorm=layer_norm,<br>                self_attention=ModuleSpec(<br>                    module=SelfAttention,<br>                    params=&#123;<span class="hljs-string">&quot;attn_mask_type&quot;</span>: AttnMaskType.causal&#125;,<br>                    submodules=SelfAttentionSubmodules(<br>                        linear_qkv=backend.column_parallel_linear(),<br>                        core_attention=backend.core_attention(),<br>                        linear_proj=backend.row_parallel_linear(),<br>                        q_layernorm=(<br>                            L2Norm <span class="hljs-keyword">if</span> qk_l2_norm <span class="hljs-keyword">else</span> (qk_norm <span class="hljs-keyword">if</span> qk_layernorm <span class="hljs-keyword">else</span> IdentityOp)<br>                        ),<br>                        k_layernorm=(<br>                            L2Norm <span class="hljs-keyword">if</span> qk_l2_norm <span class="hljs-keyword">else</span> (qk_norm <span class="hljs-keyword">if</span> qk_layernorm <span class="hljs-keyword">else</span> IdentityOp)<br>                        ),<br>                    ),<br>                ),<br>                self_attn_bda=get_bias_dropout_add,<br>                pre_mlp_layernorm=layer_norm,<br>                mlp=mlp,<br>                mlp_bda=get_bias_dropout_add,<br>                sharded_state_dict_keys_map=&#123;<br>                    <span class="hljs-string">&quot;input_layernorm.&quot;</span>: <span class="hljs-string">&quot;self_attention.linear_qkv.layer_norm_&quot;</span>,<br>                    <span class="hljs-string">&quot;pre_mlp_layernorm.&quot;</span>: <span class="hljs-string">&quot;mlp.linear_fc1.layer_norm_&quot;</span>,<br>                &#125;,<br>            ),<br>        )<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>其使用的是<code>TransformerLayer</code>来组装，初始化代码如下所示，初始化的模块依次为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    config: TransformerConfig,</span><br><span class="hljs-params">    submodules: TransformerLayerSubmodules,</span><br><span class="hljs-params">    layer_number: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">    hidden_dropout: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">float</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    model_comm_pgs: <span class="hljs-type">Optional</span>[ModelCommProcessGroups] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    vp_stage: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-built_in">super</span>().__init__(config=config)<br><br>    <span class="hljs-comment"># Enable cuda graphs.</span><br>    <span class="hljs-keyword">if</span> (<br>        config.enable_cuda_graph <span class="hljs-keyword">and</span> config.cuda_graph_scope != <span class="hljs-string">&quot;full_iteration&quot;</span><br>    ) <span class="hljs-keyword">or</span> config.external_cuda_graph:<br>        <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> (<br>            config.enable_cuda_graph <span class="hljs-keyword">and</span> config.external_cuda_graph<br>        ), <span class="hljs-string">&quot;Cudagraphs and external cudagraphs cannot be enabled at the same time&quot;</span><br>        <span class="hljs-keyword">if</span> config.enable_cuda_graph <span class="hljs-keyword">and</span> config.cuda_graph_scope != <span class="hljs-string">&quot;full_iteration&quot;</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.training:<br>                <span class="hljs-comment"># Cudagraphs for inference are only enabled with the flash decoding kernel</span><br>                <span class="hljs-keyword">assert</span> (<br>                    self.config.flash_decode<br>                ), <span class="hljs-string">&quot;--flash-decode is required to use CUDA graphs during inference&quot;</span><br>            self.cudagraph_manager = CudaGraphManager(config, vp_stage=vp_stage)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># List to store CUDA graphs. A list of `N` CUDA graphs for this layer where N is</span><br>            <span class="hljs-comment"># the number of microbatches. Multiple CUDA graphs per layer is required to support</span><br>            <span class="hljs-comment"># pipelining which requires running FWD graph of multiple microbatches before BWD</span><br>            <span class="hljs-comment"># graph. To enable CUDA graph, this list should be populated in the model training</span><br>            <span class="hljs-comment"># script with the graphs returned by make_graphed_callables API before the first</span><br>            <span class="hljs-comment"># training step.</span><br>            self.cuda_graphs = []<br>            <span class="hljs-comment"># List to store forward pre-hooks. Forward pre-hooks are not captured into CUDA</span><br>            <span class="hljs-comment"># graphs. Those hooks and args are collected in this list and should be manually</span><br>            <span class="hljs-comment"># triggered before CUDA Graph running. This is required to ensure the correct param</span><br>            <span class="hljs-comment"># all-gather overlap with forward compute.</span><br>            self.cuda_graph_manual_hooks = []<br>            self.current_microbatch = -<span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">if</span> model_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        model_comm_pgs = ModelCommProcessGroups.use_mpu_process_groups()<br><br>    self.submodules_config = submodules<br>    self.layer_number = layer_number + get_transformer_layer_offset(self.config, vp_stage)<br>    self.hidden_dropout = config.hidden_dropout <span class="hljs-keyword">if</span> hidden_dropout <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> hidden_dropout<br><br>    <span class="hljs-comment"># [Module 1: Input Layernorm] Optional Layernorm on the input data</span><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> add pytorch only layernorm</span><br>    self.input_layernorm = build_module(<br>        submodules.input_layernorm,<br>        config=self.config,<br>        hidden_size=self.config.hidden_size,<br>        eps=self.config.layernorm_epsilon,<br>    )<br><br>    attention_optional_kwargs = &#123;&#125;<br>    <span class="hljs-keyword">if</span> config.context_parallel_size &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> config.cp_comm_type <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(config.cp_comm_type, <span class="hljs-built_in">list</span>):<br>            attention_optional_kwargs[<span class="hljs-string">&quot;cp_comm_type&quot;</span>] = config.cp_comm_type[self.layer_number]<br>        <span class="hljs-keyword">else</span>:<br>            attention_optional_kwargs[<span class="hljs-string">&quot;cp_comm_type&quot;</span>] = config.cp_comm_type<br><br>    attention_optional_kwargs[<span class="hljs-string">&quot;model_comm_pgs&quot;</span>] = model_comm_pgs<br><br>    <span class="hljs-comment"># [Module 2: SelfAttention]</span><br>    self.self_attention = build_module(<br>        submodules.self_attention,<br>        config=self.config,<br>        layer_number=self.layer_number,<br>        **attention_optional_kwargs,<br>    )<br><br>    <span class="hljs-comment"># [Module 3: BiasDropoutFusion]</span><br>    self.self_attn_bda = build_module(submodules.self_attn_bda)<br><br>    <span class="hljs-comment"># [Module 4: Post SelfAttention] Optional Layernorm after self-attn</span><br>    self.pre_cross_attn_layernorm = build_module(<br>        submodules.pre_cross_attn_layernorm,<br>        config=self.config,<br>        hidden_size=self.config.hidden_size,<br>        eps=self.config.layernorm_epsilon,<br>    )<br><br>    <span class="hljs-comment"># [Module 5: CrossAttention]</span><br>    self.cross_attention = build_module(<br>        submodules.cross_attention,<br>        config=self.config,<br>        layer_number=self.layer_number,<br>        **attention_optional_kwargs,<br>    )<br><br>    <span class="hljs-comment"># [Module 6: BiasDropoutFusion]</span><br>    self.cross_attn_bda = build_module(submodules.cross_attn_bda, config=self.config)<br><br>    <span class="hljs-comment"># [Module 7: Pre MLP] Optional Layernorm before MLP</span><br>    self.pre_mlp_layernorm = build_module(<br>        submodules.pre_mlp_layernorm,<br>        config=self.config,<br>        hidden_size=self.config.hidden_size,<br>        eps=self.config.layernorm_epsilon,<br>    )<br>    <span class="hljs-comment"># [Module 8: MLP block]</span><br>    additional_mlp_kwargs = &#123;&#125;<br>    <span class="hljs-comment"># import here to avoid circular import</span><br>    <span class="hljs-keyword">from</span> megatron.core.extensions.transformer_engine <span class="hljs-keyword">import</span> TEFusedMLP<br>    <span class="hljs-keyword">from</span> megatron.core.transformer.moe.experts <span class="hljs-keyword">import</span> GroupedMLP, SequentialMLP, TEGroupedMLP<br>    <span class="hljs-keyword">from</span> megatron.core.transformer.moe.moe_layer <span class="hljs-keyword">import</span> MoELayer<br><br>    <span class="hljs-comment"># MLP expects tp_group but MoELayer expects model_comm_pgs to be passed in.</span><br>    <span class="hljs-comment"># We can change MLP to accept model_comm_pgs but it makes the logic implicit</span><br>    <span class="hljs-comment"># The conditional below is to make the logic explicit</span><br>    <span class="hljs-comment"># if submodules.mlp is not a ModuleSpec,we dont have to handle passing additional kwargs</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(submodules.mlp, ModuleSpec):<br>        <span class="hljs-keyword">if</span> submodules.mlp.module <span class="hljs-keyword">in</span> (MoELayer, GroupedMLP, TEGroupedMLP, SequentialMLP):<br>            additional_mlp_kwargs[<span class="hljs-string">&quot;model_comm_pgs&quot;</span>] = model_comm_pgs<br>        <span class="hljs-keyword">elif</span> submodules.mlp.module == MLP:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(<br>                model_comm_pgs, <span class="hljs-string">&#x27;tp&#x27;</span><br>            ), <span class="hljs-string">&#x27;TP process group is required for MLP in TransformerLayer&#x27;</span><br>            additional_mlp_kwargs[<span class="hljs-string">&quot;tp_group&quot;</span>] = model_comm_pgs.tp<br>        <span class="hljs-keyword">elif</span> TEFusedMLP <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> submodules.mlp.module == TEFusedMLP:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(<br>                model_comm_pgs, <span class="hljs-string">&#x27;tp&#x27;</span><br>            ), <span class="hljs-string">&#x27;TP process group is required for TEFusedMLP in TransformerLayer&#x27;</span><br>            additional_mlp_kwargs[<span class="hljs-string">&quot;tp_group&quot;</span>] = model_comm_pgs.tp<br>        <span class="hljs-keyword">else</span>:<br>            log_single_rank(<br>                logger,<br>                logging.WARNING,<br>                <span class="hljs-string">f&quot;Unknown MLP type: <span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(submodules.mlp)&#125;</span>. Using default kwargs.&quot;</span>,<br>            )<br>    self.mlp = build_module(submodules.mlp, config=self.config, **additional_mlp_kwargs)<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(self.mlp, <span class="hljs-string">&#x27;set_layer_number&#x27;</span>):<br>        self.mlp.set_layer_number(self.layer_number)<br><br>    <span class="hljs-comment"># [Module 9: BiasDropoutFusion]</span><br>    self.mlp_bda = build_module(submodules.mlp_bda)<br><br>    self.recompute_input_layernorm = <span class="hljs-literal">False</span><br>    self.recompute_pre_mlp_layernorm = <span class="hljs-literal">False</span><br>    self.recompute_mlp = <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">if</span> self.config.recompute_granularity == <span class="hljs-string">&#x27;selective&#x27;</span>:<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;layernorm&quot;</span> <span class="hljs-keyword">in</span> self.config.recompute_modules:<br>            <span class="hljs-keyword">if</span> (<br>                <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.input_layernorm, IdentityOp)<br>                <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.config.external_cuda_graph<br>            ):<br>                self.recompute_input_layernorm = <span class="hljs-literal">True</span><br>                <span class="hljs-keyword">if</span> self.config.fp8:<br>                    self.self_attention.set_for_recompute_input_layernorm()<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.pre_mlp_layernorm, IdentityOp):<br>                self.recompute_pre_mlp_layernorm = <span class="hljs-literal">True</span><br>                <span class="hljs-keyword">if</span> self.config.fp8:<br>                    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(self.mlp, MoELayer):<br>                        self.mlp.set_for_recompute_pre_mlp_layernorm()<br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-keyword">from</span> megatron.core.extensions.transformer_engine <span class="hljs-keyword">import</span> (<br>                            set_save_original_input,<br>                        )<br><br>                        set_save_original_input(self.mlp.linear_fc1)<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;mlp&quot;</span> <span class="hljs-keyword">in</span> self.config.recompute_modules:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.mlp, MoELayer):<br>                self.recompute_mlp = <span class="hljs-literal">True</span><br><br>    <span class="hljs-comment"># @jcasper how should we handle nvfuser?</span><br>    <span class="hljs-comment"># Set bias+dropout+add fusion grad_enable execution handler.</span><br>    <span class="hljs-comment"># TORCH_MAJOR = int(torch.__version__.split(&#x27;.&#x27;)[0])</span><br>    <span class="hljs-comment"># TORCH_MINOR = int(torch.__version__.split(&#x27;.&#x27;)[1])</span><br>    <span class="hljs-comment"># use_nvfuser = TORCH_MAJOR &gt; 1 or (TORCH_MAJOR == 1 and TORCH_MINOR &gt;= 10)</span><br>    <span class="hljs-comment"># self.bias_dropout_add_exec_handler = nullcontext if use_nvfuser else torch.enable_grad</span><br>    self.bias_dropout_add_exec_handler = torch.enable_grad<br><br></code></pre></td></tr></table></figure>

<ol>
<li><p>Input Layernorm</p>
</li>
<li><p>SelfAttention</p>
</li>
<li><p>BiasDropoutFusion</p>
</li>
<li><p>Post SelfAttention</p>
</li>
<li><p>CrossAttention</p>
</li>
<li><p>BiasDropoutFusion</p>
</li>
<li><p>Pre MLP</p>
</li>
<li><p>MLP block</p>
</li>
<li><p>BiasDropoutFusion</p>
</li>
</ol>
</li>
<li><p>其前向传播也是一些比较标准的实现，代码如下所示</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Perform a forward pass through the transformer layer.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    This method calls the core computation of a transformer layer, including</span><br><span class="hljs-string">    self-attention, cross-attention (if applicable), and feed-forward operations.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    hidden_states, context = self._forward_attention(*args, **kwargs)<br>    output = self._forward_mlp(hidden_states, kwargs.get(<span class="hljs-string">&quot;inference_context&quot;</span>, <span class="hljs-literal">None</span>))<br>    <span class="hljs-keyword">return</span> output, context<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_attention</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    hidden_states: Tensor,</span><br><span class="hljs-params">    attention_mask: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    context: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    context_mask: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    rotary_pos_emb: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    rotary_pos_cos: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    rotary_pos_sin: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    attention_bias: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    inference_context: <span class="hljs-type">Optional</span>[<span class="hljs-type">Any</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    packed_seq_params: <span class="hljs-type">Optional</span>[PackedSeqParams] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    sequence_len_offset: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    *,</span><br><span class="hljs-params">    inference_params: <span class="hljs-type">Optional</span>[<span class="hljs-type">Any</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Perform a forward pass through the attention layer and the layernorms before and after</span><br><span class="hljs-string">    the attention operations.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        hidden_states (Tensor): Input tensor of shape [s, b, h] where s is sequence length,</span><br><span class="hljs-string">            b is batch size, and h is hidden size.</span><br><span class="hljs-string">        attention_mask (Tensor): Mask tensor for self-attention.</span><br><span class="hljs-string">        context (Tensor, optional): Context tensor for cross-attention.</span><br><span class="hljs-string">        context_mask (Tensor, optional): Mask tensor for cross-attention.</span><br><span class="hljs-string">        rotary_pos_emb (Tensor, optional): Rotary positional embeddings.</span><br><span class="hljs-string">        attention_bias (Tensor, optional): Bias tensor for Q * K.T.</span><br><span class="hljs-string">        inference_context (object, optional): Parameters for inference-time optimizations.</span><br><span class="hljs-string">        packed_seq_params (object, optional): Parameters for packed sequence processing.</span><br><span class="hljs-string">        sequence_len_offset (Tensor, optional): Offset along sequence dimension</span><br><span class="hljs-string">            during inference.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        Tuple[Tensor, Tensor]: A tuple containing:</span><br><span class="hljs-string">            hidden_states (Tensor): Transformed hidden states before the MLP layernorm.</span><br><span class="hljs-string">            context (Tensor): Updated context tensor if cross-attention is used,</span><br><span class="hljs-string">            otherwise None.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    inference_context = deprecate_inference_params(inference_context, inference_params)<br><br>    <span class="hljs-comment"># Residual connection.</span><br>    residual = hidden_states<br><br>    <span class="hljs-comment"># Optional Input Layer norm</span><br>    <span class="hljs-keyword">if</span> self.recompute_input_layernorm:<br>        self.input_layernorm_checkpoint = tensor_parallel.CheckpointWithoutOutput()<br>        input_layernorm_output = self.input_layernorm_checkpoint.checkpoint(<br>            self.input_layernorm, hidden_states<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        input_layernorm_output = self.input_layernorm(hidden_states)<br><br>    <span class="hljs-comment"># Self attention.</span><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;self_attention&quot;</span>)<br>    attention_output_with_bias = self.self_attention(<br>        input_layernorm_output,<br>        attention_mask=attention_mask,<br>        inference_context=inference_context,<br>        rotary_pos_emb=rotary_pos_emb,<br>        rotary_pos_cos=rotary_pos_cos,<br>        rotary_pos_sin=rotary_pos_sin,<br>        attention_bias=attention_bias,<br>        packed_seq_params=packed_seq_params,<br>        sequence_len_offset=sequence_len_offset,<br>    )<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;self_attention&quot;</span>)<br><br>    <span class="hljs-keyword">if</span> self.recompute_input_layernorm:<br>        <span class="hljs-comment"># discard the output of the input layernorm and register the recompute</span><br>        <span class="hljs-comment"># as a gradient hook of attention_output_with_bias[0]</span><br>        self.input_layernorm_checkpoint.discard_output_and_register_recompute(<br>            attention_output_with_bias[<span class="hljs-number">0</span>]<br>        )<br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> could we move `bias_dropout_add_exec_handler` itself</span><br>    <span class="hljs-comment"># inside the module provided in the `bias_dropout_add_spec` module?</span><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;self_attn_bda&quot;</span>)<br>    <span class="hljs-keyword">with</span> self.bias_dropout_add_exec_handler():<br>        hidden_states = self.self_attn_bda(self.training, self.config.bias_dropout_fusion)(<br>            attention_output_with_bias, residual, self.hidden_dropout<br>        )<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;self_attn_bda&quot;</span>)<br><br>    <span class="hljs-comment"># Residual connection.</span><br>    residual = hidden_states<br><br>    <span class="hljs-comment"># Optional Layer norm after self-attention</span><br>    pre_cross_attn_layernorm_output = self.pre_cross_attn_layernorm(hidden_states)<br><br>    <span class="hljs-comment"># Cross attention.</span><br>    attention_output_with_bias = self.cross_attention(<br>        pre_cross_attn_layernorm_output,<br>        attention_mask=context_mask,<br>        key_value_states=context,<br>        inference_context=inference_context,<br>    )<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(attention_output_with_bias, <span class="hljs-built_in">dict</span>) <span class="hljs-keyword">and</span> <span class="hljs-string">&quot;context&quot;</span> <span class="hljs-keyword">in</span> attention_output_with_bias:<br>        context = attention_output_with_bias[<span class="hljs-string">&quot;context&quot;</span>]<br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> could we move `bias_dropout_add_exec_handler` itself</span><br>    <span class="hljs-comment"># inside the module provided in the `bias_dropout_add_spec` module?</span><br>    <span class="hljs-keyword">with</span> self.bias_dropout_add_exec_handler():<br>        hidden_states = self.cross_attn_bda(self.training, self.config.bias_dropout_fusion)(<br>            attention_output_with_bias, residual, self.hidden_dropout<br>        )<br><br>    <span class="hljs-keyword">return</span> hidden_states, context<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_mlp</span>(<span class="hljs-params">self, hidden_states, inference_context=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Perform a forward pass through the feed-forward layer.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        hidden_states (Tensor): Transformed hidden states before the MLP layernorm.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        output (Tensor): Transformed hidden states of shape [s, b, h].</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># Residual connection.</span><br>    residual = hidden_states<br><br>    <span class="hljs-comment"># Optional Layer norm post the cross-attention.</span><br>    <span class="hljs-keyword">if</span> self.recompute_pre_mlp_layernorm:<br>        self.pre_mlp_norm_checkpoint = tensor_parallel.CheckpointWithoutOutput()<br>        pre_mlp_layernorm_output = self.pre_mlp_norm_checkpoint.checkpoint(<br>            self.pre_mlp_layernorm, hidden_states<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        pre_mlp_layernorm_output = self.pre_mlp_layernorm(hidden_states)<br><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;mlp&quot;</span>)<br>    <span class="hljs-comment"># Potentially chunk the MLP computation during prefill to minimize the peak activation size</span><br>    should_chunk_mlp_for_prefill = (<br>        self.config.mlp_chunks_for_prefill &gt; <span class="hljs-number">1</span><br>        <span class="hljs-keyword">and</span> inference_context <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> inference_context.is_decode_only()<br>        <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.mlp, IdentityOp)<br>    )<br><br>    <span class="hljs-keyword">if</span> self.recompute_mlp:<br>        <span class="hljs-keyword">if</span> self.config.fp8:<br>            <span class="hljs-comment"># import here to avoid circular import</span><br>            <span class="hljs-keyword">from</span> megatron.core.extensions.transformer_engine <span class="hljs-keyword">import</span> te_checkpoint<br><br>            mlp_output_with_bias = te_checkpoint(<br>                self.mlp,<br>                <span class="hljs-literal">False</span>,<br>                tensor_parallel.random.get_cuda_rng_tracker,<br>                parallel_state.get_tensor_model_parallel_group(),<br>                pre_mlp_layernorm_output,<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            mlp_output_with_bias = tensor_parallel.checkpoint(<br>                self.mlp, <span class="hljs-literal">False</span>, pre_mlp_layernorm_output<br>            )<br>    <span class="hljs-keyword">elif</span> should_chunk_mlp_for_prefill:<br>        <span class="hljs-comment"># Chunk input along sequence dimension</span><br>        num_chunks = <span class="hljs-built_in">min</span>(self.config.mlp_chunks_for_prefill, pre_mlp_layernorm_output.shape[<span class="hljs-number">0</span>])<br>        chunks = pre_mlp_layernorm_output.chunk(num_chunks, dim=<span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># Compute outputs for each chunk</span><br>        outputs = [self.mlp(chunk) <span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> chunks]<br><br>        <span class="hljs-comment"># Aggregate chunk outputs</span><br>        mlp_output = torch.cat([out <span class="hljs-keyword">for</span> out, _ <span class="hljs-keyword">in</span> outputs], dim=<span class="hljs-number">0</span>)<br>        bias_chunks = [bias <span class="hljs-keyword">for</span> _, bias <span class="hljs-keyword">in</span> outputs <span class="hljs-keyword">if</span> bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>]<br>        bias_output = torch.stack(bias_chunks, dim=<span class="hljs-number">0</span>).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">0</span>) <span class="hljs-keyword">if</span> bias_chunks <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>        mlp_output_with_bias = (mlp_output, bias_output)<br><br>    <span class="hljs-keyword">else</span>:<br>        mlp_output_with_bias = self.mlp(pre_mlp_layernorm_output)<br><br>    <span class="hljs-keyword">if</span> self.recompute_pre_mlp_layernorm:<br>        <span class="hljs-comment"># discard the output of the pre-mlp layernorm and register the recompute</span><br>        <span class="hljs-comment"># as a gradient hook of mlp_output_with_bias[0]</span><br>        self.pre_mlp_norm_checkpoint.discard_output_and_register_recompute(<br>            mlp_output_with_bias[<span class="hljs-number">0</span>]<br>        )<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;mlp&quot;</span>)<br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> could we move `bias_dropout_add_exec_handler` itself</span><br>    <span class="hljs-comment"># inside the module provided in the `bias_dropout_add_spec` module?</span><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;mlp_bda&quot;</span>)<br>    <span class="hljs-keyword">with</span> self.bias_dropout_add_exec_handler():<br>        hidden_states = self.mlp_bda(self.training, self.config.bias_dropout_fusion)(<br>            mlp_output_with_bias, residual, self.hidden_dropout<br>        )<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;mlp_bda&quot;</span>)<br><br>    <span class="hljs-comment"># Jit compiled function creates &#x27;view&#x27; tensor. This tensor</span><br>    <span class="hljs-comment"># potentially gets saved in the MPU checkpoint function context,</span><br>    <span class="hljs-comment"># which rejects view tensors. While making a viewless tensor here</span><br>    <span class="hljs-comment"># won&#x27;t result in memory savings (like the data loader, or</span><br>    <span class="hljs-comment"># p2p_communication), it serves to document the origin of this</span><br>    <span class="hljs-comment"># &#x27;view&#x27; tensor.</span><br>    output = make_viewless_tensor(<br>        inp=hidden_states, requires_grad=hidden_states.requires_grad, keep_graph=<span class="hljs-literal">True</span><br>    )<br><br>    <span class="hljs-keyword">return</span> output<br><br></code></pre></td></tr></table></figure>

<h4 id="MLP模块"><a href="#MLP模块" class="headerlink" title="MLP模块"></a>MLP模块</h4><p>MLP模块中往往是先进行一次全连接计算，在使用类似gelu的激活函数，再使用一次全连接计算，在TP并行中往往采用的是对前一次采用列并行对后一次采用行并行的方式。</p>
<p>本地模块获取MLP的相关代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_mlp_module_spec_for_backend</span>(<span class="hljs-params"></span><br><span class="hljs-params">    backend: BackendSpecProvider,</span><br><span class="hljs-params">    num_experts: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    moe_grouped_gemm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    moe_use_legacy_grouped_gemm: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    use_te_op_fuser: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params"></span>) -&gt; ModuleSpec:<br>    <span class="hljs-string">&quot;&quot;&quot;Helper function to get module spec for MLP/MoE&quot;&quot;&quot;</span><br><br>    linear_fc2 = backend.row_parallel_linear()<br><br>    <span class="hljs-keyword">if</span> num_experts <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># Dense MLP w/ or w/o TE modules.</span><br>        <span class="hljs-keyword">if</span> use_te_op_fuser:<br>            <span class="hljs-keyword">return</span> ModuleSpec(module=TEFusedMLP)<br>        <span class="hljs-keyword">elif</span> backend.fuse_layernorm_and_linear():<br>            linear_fc1 = backend.column_parallel_layer_norm_linear()<br>            <span class="hljs-keyword">assert</span> linear_fc1 <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">else</span>:<br>            linear_fc1 = backend.column_parallel_linear()<br>        <span class="hljs-keyword">return</span> ModuleSpec(<br>            module=MLP, submodules=MLPSubmodules(linear_fc1=linear_fc1, linear_fc2=linear_fc2)<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># Mixture of experts with modules in megatron core.</span><br>        <span class="hljs-keyword">return</span> get_moe_module_spec_for_backend(<br>            backend=backend,<br>            num_experts=num_experts,<br>            moe_grouped_gemm=moe_grouped_gemm,<br>            moe_use_legacy_grouped_gemm=moe_use_legacy_grouped_gemm,<br>        )<br><br></code></pre></td></tr></table></figure>

<p>一般情况下两个linear层分别为<code>column_parallel_linear</code>与<code>row_parallel_linear</code>，然后以此为基础构建了<code>MLP</code>模块，<code>MLP</code>模块的相关代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MLP</span>(<span class="hljs-title class_ inherited__">MegatronModule</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    MLP will take the input with h hidden state, project it to 4*h</span><br><span class="hljs-string">    hidden dimension, perform nonlinear transformation, and project the</span><br><span class="hljs-string">    state back into h hidden dimension.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns an output and a bias to be added to the output.</span><br><span class="hljs-string">    If config.add_bias_linear is False, the bias returned is None.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    We use the following notation:</span><br><span class="hljs-string">     h: hidden size</span><br><span class="hljs-string">     p: number of tensor model parallel partitions</span><br><span class="hljs-string">     b: batch size</span><br><span class="hljs-string">     s: sequence length</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        config: TransformerConfig,</span><br><span class="hljs-params">        submodules: MLPSubmodules,</span><br><span class="hljs-params">        is_expert: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        input_size: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        ffn_hidden_size: <span class="hljs-built_in">int</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        tp_group: <span class="hljs-type">Optional</span>[torch.distributed.ProcessGroup] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__(config=config)<br><br>        self.config: TransformerConfig = config<br><br>        self.input_size = input_size <span class="hljs-keyword">if</span> input_size != <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> self.config.hidden_size<br><br>        tp_group = get_tensor_model_parallel_group_if_none(tp_group, is_expert=is_expert)<br>        <span class="hljs-keyword">if</span> ffn_hidden_size <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> is_expert:<br>                <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;MoE MLP requires `ffn_hidden_size`, but it was not provided.&quot;</span>)<br>            warnings.warn(<br>                <span class="hljs-string">&quot;MLP requires ffn_hidden_size, but it was not provided. Using \</span><br><span class="hljs-string">                    config.ffn_hidden_size by default.&quot;</span>,<br>                DeprecationWarning,<br>                stacklevel=<span class="hljs-number">2</span>,<br>            )<br>            ffn_hidden_size = self.config.ffn_hidden_size<br><br>        <span class="hljs-comment"># If this is a gated linear unit we double the output width</span><br>        <span class="hljs-comment"># see https://arxiv.org/pdf/2002.05202.pdf</span><br>        <span class="hljs-keyword">if</span> self.config.gated_linear_unit:<br>            ffn_hidden_size *= <span class="hljs-number">2</span><br><br>        self.linear_fc1 = build_module(<br>            submodules.linear_fc1,<br>            self.input_size,<br>            ffn_hidden_size,<br>            config=self.config,<br>            init_method=self.config.init_method,<br>            gather_output=<span class="hljs-literal">False</span>,<br>            bias=self.config.add_bias_linear,<br>            skip_bias_add=<span class="hljs-literal">True</span>,<br>            is_expert=is_expert,<br>            tp_comm_buffer_name=<span class="hljs-string">&quot;fc1&quot;</span>,<br>            tp_group=tp_group,<br>        )<br><br>        self.activation_func = self.config.activation_func<br><br>        self.linear_fc2 = build_module(<br>            submodules.linear_fc2,<br>            self.config.ffn_hidden_size,<br>            self.config.hidden_size,<br>            config=self.config,<br>            init_method=self.config.output_layer_init_method,<br>            bias=self.config.add_bias_linear,<br>            input_is_parallel=<span class="hljs-literal">True</span>,<br>            skip_bias_add=<span class="hljs-literal">True</span>,<br>            is_expert=is_expert,<br>            tp_comm_buffer_name=<span class="hljs-string">&quot;fc2&quot;</span>,<br>            tp_group=tp_group,<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states, per_token_scale=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Perform the forward pass through the MLP block.&quot;&quot;&quot;</span><br>        <span class="hljs-comment"># [s, b, 4 * h/p]</span><br>        nvtx_range_push(suffix=<span class="hljs-string">&quot;linear_fc1&quot;</span>)<br>        intermediate_parallel, bias_parallel = self.linear_fc1(hidden_states)<br>        nvtx_range_pop(suffix=<span class="hljs-string">&quot;linear_fc1&quot;</span>)<br><br>        nvtx_range_push(suffix=<span class="hljs-string">&quot;activation&quot;</span>)<br>        <span class="hljs-keyword">if</span> self.config.bias_activation_fusion:<br>            <span class="hljs-keyword">if</span> per_token_scale <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">if</span> self.activation_func == F.silu <span class="hljs-keyword">and</span> self.config.gated_linear_unit:<br>                    <span class="hljs-comment"># dtype is handled inside the fused kernel</span><br>                    intermediate_parallel = weighted_bias_swiglu_impl(<br>                        intermediate_parallel,<br>                        bias_parallel,<br>                        per_token_scale.unsqueeze(-<span class="hljs-number">1</span>),<br>                        self.config.activation_func_fp8_input_store,<br>                    )<br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Only support fusion of swiglu with per_token_scale in MLP.&quot;</span>)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">if</span> self.activation_func == F.gelu:<br>                    <span class="hljs-keyword">if</span> self.config.gated_linear_unit:<br>                        intermediate_parallel = bias_geglu_impl(<br>                            intermediate_parallel, bias_parallel<br>                        )<br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-keyword">assert</span> self.config.add_bias_linear <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span><br>                        intermediate_parallel = bias_gelu_impl(intermediate_parallel, bias_parallel)<br>                <span class="hljs-keyword">elif</span> self.activation_func == F.silu <span class="hljs-keyword">and</span> self.config.gated_linear_unit:<br>                    intermediate_parallel = bias_swiglu_impl(<br>                        intermediate_parallel,<br>                        bias_parallel,<br>                        self.config.activation_func_fp8_input_store,<br>                        self.config.cpu_offloading<br>                        <span class="hljs-keyword">and</span> self.config.cpu_offloading_activations<br>                        <span class="hljs-keyword">and</span> HAVE_TE,<br>                    )<br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Only support fusion of gelu and swiglu&quot;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> bias_parallel <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                intermediate_parallel = intermediate_parallel + bias_parallel<br>            <span class="hljs-keyword">if</span> self.config.gated_linear_unit:<br><br>                <span class="hljs-keyword">def</span> <span class="hljs-title function_">glu</span>(<span class="hljs-params">x</span>):<br>                    x = torch.chunk(x, <span class="hljs-number">2</span>, dim=-<span class="hljs-number">1</span>)<br>                    <span class="hljs-keyword">return</span> self.config.activation_func(x[<span class="hljs-number">0</span>]) * x[<span class="hljs-number">1</span>]<br><br>                intermediate_parallel = glu(intermediate_parallel)<br>            <span class="hljs-keyword">else</span>:<br>                intermediate_parallel = self.activation_func(intermediate_parallel)<br><br>            <span class="hljs-keyword">if</span> per_token_scale <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                original_dtype = intermediate_parallel.dtype<br>                intermediate_parallel = intermediate_parallel * per_token_scale.unsqueeze(-<span class="hljs-number">1</span>)<br>                intermediate_parallel = intermediate_parallel.to(original_dtype)<br>        nvtx_range_pop(suffix=<span class="hljs-string">&quot;activation&quot;</span>)<br><br>        <span class="hljs-comment"># [s, b, h]</span><br>        nvtx_range_push(suffix=<span class="hljs-string">&quot;linear_fc2&quot;</span>)<br>        output, output_bias = self.linear_fc2(intermediate_parallel)<br>        nvtx_range_pop(suffix=<span class="hljs-string">&quot;linear_fc2&quot;</span>)<br><br>        <span class="hljs-keyword">if</span> per_token_scale <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">assert</span> output_bias <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;Bias is not supported with per_token_scale&quot;</span><br><br>        <span class="hljs-keyword">return</span> output, output_bias<br><br>    <span class="hljs-comment"># pylint: disable=missing-function-docstring</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sharded_state_dict</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, prefix: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;&quot;</span>, sharded_offsets: <span class="hljs-built_in">tuple</span> = (<span class="hljs-params"></span>), metadata: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">dict</span>] = <span class="hljs-literal">None</span></span><br><span class="hljs-params">    </span>) -&gt; ShardedStateDict:<br>        sharded_state_dict = &#123;&#125;<br>        singleton_local_shards = (metadata <span class="hljs-keyword">or</span> &#123;&#125;).get(<span class="hljs-string">&#x27;singleton_local_shards&#x27;</span>, <span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">for</span> name, module <span class="hljs-keyword">in</span> self._modules.items():<br>            sub_sd = module.sharded_state_dict(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;prefix&#125;</span><span class="hljs-subst">&#123;name&#125;</span>.&quot;</span>, sharded_offsets, metadata)<br>            <span class="hljs-keyword">if</span> self.config.gated_linear_unit <span class="hljs-keyword">and</span> name == <span class="hljs-string">&quot;linear_fc1&quot;</span>:<br>                <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> sub_sd.items():<br>                    <span class="hljs-keyword">if</span> k <span class="hljs-keyword">in</span> (<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;prefix&#125;</span><span class="hljs-subst">&#123;name&#125;</span>.weight&quot;</span>, <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;prefix&#125;</span><span class="hljs-subst">&#123;name&#125;</span>.bias&quot;</span>):<br>                        sub_sd[k] = apply_swiglu_sharded_factory(<br>                            v, sharded_offsets, singleton_local_shards<br>                        )<br>            sharded_state_dict.update(sub_sd)<br>        <span class="hljs-keyword">return</span> sharded_state_dict<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward_dw</span>(<span class="hljs-params">self</span>):<br>        self.linear_fc2.backward_dw()<br>        self.linear_fc1.backward_dw()<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>在初始化时：</p>
<ul>
<li><p>其读取配置得到了<code>ffn_hidden_size</code>以及<code>tp_group</code>等参数</p>
</li>
<li><p>然后构建了<code>column_parallel_linear</code>类型的fc1以及<code>row_parallel_linear</code>类型的fc2，还要按配置所需的<code>activation_func</code></p>
</li>
</ul>
</li>
<li><p>在Forward时：</p>
<ul>
<li><p>其整个流程为了方便Nsys分析使用<code>nvtx_range_push</code>进行了准确的划分</p>
</li>
<li><p>先调用<code>linear_fc1</code>，再调用<code>activation</code>计算，再调用<code>linear_fc2</code>计算</p>
</li>
</ul>
</li>
</ul>
<h5 id="column-parallel-linear"><a href="#column-parallel-linear" class="headerlink" title="column_parallel_linear"></a><code>column_parallel_linear</code></h5><p>Megatron-LM本地写的<code>ColumnParallelLinear</code>如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ColumnParallelLinear</span>(torch.nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Linear layer with column parallelism.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    The linear layer is defined as Y = XA + b. A is parallelized along</span><br><span class="hljs-string">    its second dimension as A = [A_1, ..., A_p].</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        input_size:</span><br><span class="hljs-string">            first dimension of matrix A.</span><br><span class="hljs-string">        output_size:</span><br><span class="hljs-string">            second dimension of matrix A.</span><br><span class="hljs-string">        bias:</span><br><span class="hljs-string">            If true, add bias</span><br><span class="hljs-string">        gather_output:</span><br><span class="hljs-string">            If true, call all-gather on output and make Y available to all GPUs,</span><br><span class="hljs-string">            otherwise, every GPU will have its output which is Y_i = XA_i</span><br><span class="hljs-string">        init_method:</span><br><span class="hljs-string">            method to initialize weights. Note that bias is always set to zero.</span><br><span class="hljs-string">        stride:</span><br><span class="hljs-string">            For the strided linear layers.</span><br><span class="hljs-string">        keep_master_weight_for_test:</span><br><span class="hljs-string">            This was added for testing and should be set to False. It</span><br><span class="hljs-string">            returns the master weights used for initialization.</span><br><span class="hljs-string">        skip_bias_add:</span><br><span class="hljs-string">            If True, do not add the bias term, instead return it to be added by the</span><br><span class="hljs-string">            caller. This enables performance optimations where bias can be fused with other</span><br><span class="hljs-string">            elementwise operations.</span><br><span class="hljs-string">        skip_weight_param_allocation:</span><br><span class="hljs-string">            If True, weight parameter is not allocated and must be passed</span><br><span class="hljs-string">            as a keyword argument `weight` during the forward pass. Note that this does not</span><br><span class="hljs-string">            affect bias, which will be allocated if bias is True. Defaults to False.</span><br><span class="hljs-string">        embedding_activation_buffer:</span><br><span class="hljs-string">            This buffer holds the input activations of the final embedding</span><br><span class="hljs-string">            linear layer on the last pipeline stage when defer_embedding_wgrad_compute is enabled.</span><br><span class="hljs-string">        grad_output_buffer:</span><br><span class="hljs-string">            This buffer holds the gradient outputs of the final embedding linear</span><br><span class="hljs-string">            layer on the last pipeline stage when defer_embedding_wgrad_compute is enabled.</span><br><span class="hljs-string">        is_expert:</span><br><span class="hljs-string">            If True, the layer is treated as an MoE expert layer.</span><br><span class="hljs-string">        config:</span><br><span class="hljs-string">            ModelParallelConfig object</span><br><span class="hljs-string">        tp_comm_buffer_name:</span><br><span class="hljs-string">            Communication buffer name is not used in non-Transformer-Engine modules.</span><br><span class="hljs-string">        disable_grad_reduce:</span><br><span class="hljs-string">            If True, reduction of output gradients across tensor-parallel ranks</span><br><span class="hljs-string">            will be disabled. Defaults to False. This feature is used by Lora Adapter in Nemo to</span><br><span class="hljs-string">            delay and fuse reduction along with other gradients for performance optimization.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        input_size,</span><br><span class="hljs-params">        output_size,</span><br><span class="hljs-params">        *,</span><br><span class="hljs-params">        config: ModelParallelConfig,</span><br><span class="hljs-params">        init_method: <span class="hljs-type">Callable</span>,</span><br><span class="hljs-params">        bias=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        gather_output=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        stride=<span class="hljs-number">1</span>,</span><br><span class="hljs-params">        keep_master_weight_for_test=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        skip_bias_add=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        skip_weight_param_allocation: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        embedding_activation_buffer: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[torch.Tensor]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        grad_output_buffer: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[torch.Tensor]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        is_expert: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        tp_comm_buffer_name: <span class="hljs-built_in">str</span> = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># Not used</span></span><br><span class="hljs-params">        disable_grad_reduce: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        tp_group: <span class="hljs-type">Optional</span>[torch.distributed.ProcessGroup] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>(ColumnParallelLinear, self).__init__()<br><br>        <span class="hljs-comment"># Keep input parameters</span><br>        self.input_size = input_size<br>        self.output_size = output_size<br>        self.gather_output = gather_output<br>        <span class="hljs-comment"># Divide the weight matrix along the last dimension.</span><br>        self.skip_bias_add = skip_bias_add<br>        self.is_expert = is_expert<br>        self.expert_parallel = config.expert_model_parallel_size &gt; <span class="hljs-number">1</span><br>        self.embedding_activation_buffer = embedding_activation_buffer<br>        self.grad_output_buffer = grad_output_buffer<br>        self.config = config<br>        self.disable_grad_reduce = disable_grad_reduce<br>        self.tp_group = tp_group<br><br>        self.tp_group = get_tensor_model_parallel_group_if_none(<br>            self.tp_group, is_expert=self.is_expert<br>        )<br>        world_size = get_pg_size(self.tp_group)<br>        rank = get_pg_rank(self.tp_group)<br>        self.explicit_expert_comm = self.is_expert <span class="hljs-keyword">and</span> (world_size &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> self.expert_parallel)<br>        self.output_size_per_partition = divide(output_size, world_size)<br><br>        <span class="hljs-comment"># Parameters.</span><br>        <span class="hljs-comment"># Note: torch.nn.functional.linear performs XA^T + b and as a result</span><br>        <span class="hljs-comment"># we allocate the transpose.</span><br>        <span class="hljs-comment"># Initialize weight.</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> skip_weight_param_allocation:<br>            <span class="hljs-keyword">if</span> config.use_cpu_initialization:<br>                self.weight = Parameter(<br>                    torch.empty(<br>                        self.output_size_per_partition, self.input_size, dtype=config.params_dtype<br>                    )<br>                )<br>                <span class="hljs-keyword">if</span> config.perform_initialization:<br>                    self.master_weight = _initialize_affine_weight_cpu(<br>                        self.weight,<br>                        self.output_size,<br>                        self.input_size,<br>                        self.output_size_per_partition,<br>                        <span class="hljs-number">0</span>,<br>                        init_method,<br>                        stride=stride,<br>                        return_master_weight=keep_master_weight_for_test,<br>                        rank=rank,<br>                        world_size=world_size,<br>                    )<br>            <span class="hljs-keyword">else</span>:<br>                self.weight = Parameter(<br>                    torch.empty(<br>                        self.output_size_per_partition,<br>                        self.input_size,<br>                        device=torch.cuda.current_device(),<br>                        dtype=config.params_dtype,<br>                    )<br>                )<br>                <span class="hljs-keyword">if</span> config.perform_initialization:<br>                    _initialize_affine_weight_gpu(<br>                        self.weight,<br>                        init_method,<br>                        partition_dim=<span class="hljs-number">0</span>,<br>                        stride=stride,<br>                        is_expert=self.is_expert,<br>                    )<br><br>            <span class="hljs-built_in">setattr</span>(self.weight, <span class="hljs-string">&quot;allreduce&quot;</span>, <span class="hljs-keyword">not</span> (self.is_expert <span class="hljs-keyword">and</span> self.expert_parallel))<br>        <span class="hljs-keyword">else</span>:<br>            self.weight = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> bias:<br>            <span class="hljs-keyword">if</span> config.use_cpu_initialization:<br>                self.bias = Parameter(<br>                    torch.empty(self.output_size_per_partition, dtype=config.params_dtype)<br>                )<br>            <span class="hljs-keyword">else</span>:<br>                self.bias = Parameter(<br>                    torch.empty(<br>                        self.output_size_per_partition,<br>                        device=torch.cuda.current_device(),<br>                        dtype=config.params_dtype,<br>                    )<br>                )<br>            set_tensor_model_parallel_attributes(self.bias, <span class="hljs-literal">True</span>, <span class="hljs-number">0</span>, stride)<br>            <span class="hljs-keyword">if</span> config.perform_initialization:<br>                <span class="hljs-comment"># Always initialize bias to zero.</span><br>                <span class="hljs-keyword">with</span> torch.no_grad():<br>                    self.bias.zero_()<br>            <span class="hljs-built_in">setattr</span>(self.bias, <span class="hljs-string">&quot;allreduce&quot;</span>, <span class="hljs-keyword">not</span> (self.is_expert <span class="hljs-keyword">and</span> self.expert_parallel))<br>        <span class="hljs-keyword">else</span>:<br>            self.register_parameter(<span class="hljs-string">&quot;bias&quot;</span>, <span class="hljs-literal">None</span>)<br><br>        self.sequence_parallel = config.sequence_parallel<br>        <span class="hljs-keyword">if</span> self.sequence_parallel <span class="hljs-keyword">and</span> world_size &lt;= <span class="hljs-number">1</span>:<br>            warnings.warn(<br>                <span class="hljs-string">&quot;`sequence_parallel` is set to `True`, but tensor model parallel size &quot;</span><br>                <span class="hljs-string">f&quot;is <span class="hljs-subst">&#123;world_size&#125;</span>. Disabling sequence parallel.&quot;</span><br>            )<br>            self.sequence_parallel = <span class="hljs-literal">False</span><br><br>        self.allreduce_dgrad = (<br>            world_size &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.sequence_parallel <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.disable_grad_reduce<br>        )<br><br>        <span class="hljs-keyword">if</span> config.gradient_accumulation_fusion <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> _grad_accum_fusion_available:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<br>                <span class="hljs-string">&quot;ColumnParallelLinear was called with gradient_accumulation_fusion set &quot;</span><br>                <span class="hljs-string">&quot;to True but the custom CUDA extension fused_weight_gradient_mlp_cuda &quot;</span><br>                <span class="hljs-string">&quot;module is not found. To use gradient_accumulation_fusion you must &quot;</span><br>                <span class="hljs-string">&quot;install APEX with --cpp_ext and --cuda_ext. For example: &quot;</span><br>                <span class="hljs-string">&#x27;pip install --global-option=&quot;--cpp_ext&quot; --global-option=&quot;--cuda_ext .&quot; &#x27;</span><br>                <span class="hljs-string">&quot;Note that the extension requires CUDA&gt;=11. Otherwise, you must turn off &quot;</span><br>                <span class="hljs-string">&quot;gradient accumulation fusion.&quot;</span><br>            )<br>        self.gradient_accumulation_fusion = config.gradient_accumulation_fusion<br><br>        <span class="hljs-keyword">if</span> self.allreduce_dgrad <span class="hljs-keyword">and</span> self.sequence_parallel:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<br>                <span class="hljs-string">&quot;`allreduce_dgrad` and `sequence_parallel` cannot be enabled at the same time.&quot;</span><br>            )<br><br>        <span class="hljs-comment"># Hook adding a default empty _extra_state for state dict</span><br>        self._register_load_state_dict_pre_hook(<br>            <span class="hljs-keyword">lambda</span> state_dict, prefix, *args, **kwargs: state_dict.setdefault(<br>                <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;prefix&#125;</span>_extra_state&quot;</span><br>            )<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_impl</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span>, weight, *args, **kwargs</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> weight.requires_grad:<br>            <span class="hljs-keyword">return</span> linear_with_frozen_weight(<span class="hljs-built_in">input</span>, weight, *args, **kwargs)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> linear_with_grad_accumulation_and_async_allreduce(<span class="hljs-built_in">input</span>, weight, *args, **kwargs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        input_: torch.Tensor,</span><br><span class="hljs-params">        weight: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        runtime_gather_output: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Forward of ColumnParallelLinear</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            input_:</span><br><span class="hljs-string">                3D tensor whose order of dimension is [sequence, batch, hidden]</span><br><span class="hljs-string">            weight (optional):</span><br><span class="hljs-string">                weight tensor to use, compulsory when skip_weight_param_allocation is True.</span><br><span class="hljs-string">            runtime_gather_output (bool): Gather output at runtime. Default None means</span><br><span class="hljs-string">                `gather_output` arg in the constructor will be used.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            - output</span><br><span class="hljs-string">            - bias</span><br><span class="hljs-string"></span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> weight <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> self.weight <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">raise</span> RuntimeError(<br>                    <span class="hljs-string">&quot;weight was not supplied to ColumnParallelLinear forward pass &quot;</span><br>                    <span class="hljs-string">&quot;and skip_weight_param_allocation is True.&quot;</span><br>                )<br>            weight = self.weight<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># Check the weight passed in is the correct shape</span><br>            expected_shape = (self.output_size_per_partition, self.input_size)<br>            <span class="hljs-keyword">if</span> weight.shape != expected_shape:<br>                <span class="hljs-keyword">raise</span> RuntimeError(<br>                    <span class="hljs-string">f&quot;supplied weight&#x27;s shape is <span class="hljs-subst">&#123;<span class="hljs-built_in">tuple</span>(weight.shape)&#125;</span>, &quot;</span><br>                    <span class="hljs-string">f&quot;not <span class="hljs-subst">&#123;expected_shape&#125;</span> as expected&quot;</span><br>                )<br><br>        bias = self.bias <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.skip_bias_add <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> (<br>            self.allreduce_dgrad<br>            <span class="hljs-keyword">or</span> self.sequence_parallel<br>            <span class="hljs-keyword">or</span> self.explicit_expert_comm<br>            <span class="hljs-keyword">or</span> self.disable_grad_reduce<br>        ):<br>            input_parallel = input_<br>        <span class="hljs-keyword">else</span>:<br>            input_parallel = copy_to_tensor_model_parallel_region(input_, group=self.tp_group)<br><br>        <span class="hljs-keyword">if</span> self.config.defer_embedding_wgrad_compute:<br>            <span class="hljs-keyword">if</span> (<br>                self.config.wgrad_deferral_limit == <span class="hljs-number">0</span><br>                <span class="hljs-keyword">or</span> <span class="hljs-built_in">len</span>(self.embedding_activation_buffer) &lt; self.config.wgrad_deferral_limit<br>            ):<br>                self.embedding_activation_buffer.append(input_parallel)<br><br>        <span class="hljs-comment"># Matrix multiply.</span><br>        allreduce_dgrad = <span class="hljs-literal">False</span> <span class="hljs-keyword">if</span> self.explicit_expert_comm <span class="hljs-keyword">else</span> self.allreduce_dgrad<br><br>        <span class="hljs-keyword">if</span> self.config._cpu_offloading_context <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> self.config._cpu_offloading_context.inside_context <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span>:<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> HAVE_TE:<br>                    <span class="hljs-keyword">assert</span> (<br>                        self.config.cpu_offloading <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span><br>                    ), <span class="hljs-string">&quot;CPU Offloading cannot be enabled while TE is not present&quot;</span><br>                <span class="hljs-keyword">else</span>:<br>                    input_parallel.activation_offloading = self.config.cpu_offloading_activations<br><br>        output_parallel = self._forward_impl(<br>            <span class="hljs-built_in">input</span>=input_parallel,<br>            weight=weight,<br>            bias=bias,<br>            gradient_accumulation_fusion=self.gradient_accumulation_fusion,<br>            allreduce_dgrad=allreduce_dgrad,<br>            sequence_parallel=<span class="hljs-literal">False</span> <span class="hljs-keyword">if</span> self.explicit_expert_comm <span class="hljs-keyword">else</span> self.sequence_parallel,<br>            grad_output_buffer=(<br>                self.grad_output_buffer <span class="hljs-keyword">if</span> self.config.defer_embedding_wgrad_compute <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>            ),<br>            wgrad_deferral_limit=(<br>                self.config.wgrad_deferral_limit<br>                <span class="hljs-keyword">if</span> self.config.defer_embedding_wgrad_compute<br>                <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>            ),<br>            tp_group=self.tp_group,<br>        )<br><br>        gather_output = self.gather_output<br>        <span class="hljs-comment"># Use the runtime gather output if it&#x27;s set explicitly.</span><br>        <span class="hljs-keyword">if</span> runtime_gather_output <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            gather_output = runtime_gather_output<br><br>        <span class="hljs-keyword">if</span> gather_output:<br>            <span class="hljs-comment"># All-gather across the partitions.</span><br>            output = gather_from_tensor_model_parallel_region(output_parallel, group=self.tp_group)<br>        <span class="hljs-keyword">else</span>:<br>            output = output_parallel<br>        output_bias = self.bias <span class="hljs-keyword">if</span> self.skip_bias_add <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">return</span> output, output_bias<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sharded_state_dict</span>(<span class="hljs-params">self, prefix=<span class="hljs-string">&quot;&quot;</span>, sharded_offsets=(<span class="hljs-params"></span>), metadata=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Sharding along axis 0, bias sharded&quot;&quot;&quot;</span><br>        state_dict = self.state_dict(prefix=<span class="hljs-string">&quot;&quot;</span>, keep_vars=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">return</span> make_sharded_tensors_for_checkpoint(<br>            state_dict, prefix, &#123;<span class="hljs-string">&quot;weight&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;bias&quot;</span>: <span class="hljs-number">0</span>&#125;, sharded_offsets<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">set_extra_state</span>(<span class="hljs-params">self, state: <span class="hljs-type">Any</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Extra state is ignored&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_extra_state</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Keep compatibility with TE state dict.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__repr__</span>(<span class="hljs-params">self</span>):<br>        tp = self.output_size // self.output_size_per_partition<br>        use_bias = self.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> self.bias <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">return</span> (<br>            <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(self).__name__&#125;</span>(in_features=<span class="hljs-subst">&#123;self.input_size&#125;</span>, &quot;</span><br>            <span class="hljs-string">f&quot;out_features=<span class="hljs-subst">&#123;self.output_size&#125;</span>, bias=<span class="hljs-subst">&#123;use_bias&#125;</span>, TP=<span class="hljs-subst">&#123;tp&#125;</span>)&quot;</span><br>        )<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>在初始化时：</p>
<ul>
<li><p>其首先计算出在TP列并行下<code>self.output_size_per_partition = divide(output_size, world_size)</code>，并以此为基础初始化权重<code>self.weight = Parameter(torch.empty(self.output_size_per_partition, self.input_size, ...))</code></p>
</li>
<li><p>此外还标记了计算梯度时是否需要<code>allreduce_dgrad</code>，需要的条件是<code>world_size &gt; 1 and not self.sequence_parallel and not self.disable_grad_reduce</code>，因为<code>sequence_parallel </code>与梯度并行有冲突。</p>
</li>
</ul>
</li>
<li><p>在Forward时，流程如下：</p>
<ol>
<li><p>首先如果没有<code>weight</code>参数就使用自身初始化的<code>weight</code>，然后检查形状。</p>
</li>
<li><p>对于列并行而言，典型的实现是输入在所有 TP ranks 上一致（复制一份），每个 rank 用自己的 <code>W_i</code> 计算 <code>Y_i = X @ W_i^T</code>。<code>copy_to_tensor_model_parallel_region</code>在 TP&gt;1 时会涉及通信&#x2F;广播式的“让 input 在 TP ranks 上一致”，但如果启用了某些模式（sequence_parallel &#x2F; allreduce_dgrad &#x2F; expert 显式通信 &#x2F; disable_grad_reduce），这里会选择不走 copy 路径（因为这些模式下输入已经按其它语义准备好了，或者通信由别处负责），直接使用传入的<code>input_</code></p>
</li>
<li><p>然后其调用了<code>_forward_impl</code>计算结果，这里进行了多层包装，主要是为了应对<code>sequence_parallel</code>的情况，因为如果<code>sequence_parallel</code>为True，那么其会使用<code>All gather</code>获取input完整序列再做Gemm。</p>
<ul>
<li><p>注意其这里也定义了在 backward 中：</p>
<ul>
<li><p>如果 ctx.allreduce_dgrad&#x3D;True：会 torch.distributed.all_reduce(grad_input, async_op&#x3D;True)<br>这是 TP 下典型的 dgrad 通信重叠。</p>
</li>
<li><p>如果 ctx.sequence_parallel&#x3D;True：会 <code>reduce_scatter</code> 把 grad_input 分发回 sequence-parallel 格式。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>然后其还需要根据<code>runtime_gather_output</code>参数来判断是否需要执行All Gather来复原所有结果。注意在上述的MLP Forward计算时并没有配置<code>runtime_gather_output</code>，所以没有执行All Gather，这也符合TP并行的需要</p>
</li>
<li><p>最后返回<code>output</code>, <code>output_bias</code></p>
</li>
</ol>
</li>
</ul>
<blockquote>
<p>注意这里并没有直接定义backward的行为，但是正如我们前面所分析的，列并行在反向传播时求 $$\frac{\partial L}{\partial X}$$时需要All Reduce(Sum)操作，这部分backward的行为是Pytorch自动生成的</p>
</blockquote>
<h5 id="row-parallel-linear"><a href="#row-parallel-linear" class="headerlink" title="row_parallel_linear"></a><code>row_parallel_linear</code></h5><p><code>row_parallel_linear</code>代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RowParallelLinear</span>(torch.nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Linear layer with row parallelism.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    The linear layer is defined as Y = XA + b. A is parallelized along its first dimension and X</span><br><span class="hljs-string">    along its second dimension. A = transpose([A_1 .. A_p]) X = [X_1, ..., X_p]</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        input_size:</span><br><span class="hljs-string">            first dimension of matrix A.</span><br><span class="hljs-string">        output_size:</span><br><span class="hljs-string">            second dimension of matrix A.</span><br><span class="hljs-string">        bias:</span><br><span class="hljs-string">            If true, add bias. Note that bias is not parallelized.</span><br><span class="hljs-string">        input_is_parallel:</span><br><span class="hljs-string">            If true, we assume that the input is already split across the GPUs</span><br><span class="hljs-string">            and we do not split again.</span><br><span class="hljs-string">        init_method:</span><br><span class="hljs-string">            method to initialize weights. Note that bias is always set to zero.</span><br><span class="hljs-string">        stride:</span><br><span class="hljs-string">            For the strided linear layers.</span><br><span class="hljs-string">        keep_master_weight_for_test:</span><br><span class="hljs-string">            This was added for testing and should be set to False. It returns the master weights</span><br><span class="hljs-string">            used for initialization.</span><br><span class="hljs-string">        skip_bias_add:</span><br><span class="hljs-string">            If True, do not add the bias term, instead return it to be added by the</span><br><span class="hljs-string">            caller. This enables performance optimations where bias can be fused with other</span><br><span class="hljs-string">            elementwise operations.</span><br><span class="hljs-string">        is_expert:</span><br><span class="hljs-string">            If True, the layer is treated as an MoE expert layer</span><br><span class="hljs-string">        tp_comm_buffer_name:</span><br><span class="hljs-string">            Communication buffer name. Not used in non-Transformer-Engine modules.</span><br><span class="hljs-string">        config:</span><br><span class="hljs-string">            ModelParallelConfig object</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        input_size: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        output_size: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        *,</span><br><span class="hljs-params">        config: ModelParallelConfig,</span><br><span class="hljs-params">        init_method: <span class="hljs-type">Callable</span>,</span><br><span class="hljs-params">        bias: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        input_is_parallel: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        skip_bias_add: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        stride: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">        keep_master_weight_for_test: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        is_expert: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        tp_comm_buffer_name: <span class="hljs-built_in">str</span> = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># Not used</span></span><br><span class="hljs-params">        tp_group: <span class="hljs-type">Optional</span>[torch.distributed.ProcessGroup] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>(RowParallelLinear, self).__init__()<br><br>        <span class="hljs-comment"># Keep input parameters</span><br>        self.input_size = input_size<br>        self.output_size = output_size<br>        self.input_is_parallel = input_is_parallel<br>        self.skip_bias_add = skip_bias_add<br>        self.config = config<br>        self.is_expert = is_expert<br>        self.expert_parallel = config.expert_model_parallel_size &gt; <span class="hljs-number">1</span><br>        self.gradient_accumulation_fusion = config.gradient_accumulation_fusion<br>        self.sequence_parallel = config.sequence_parallel<br>        self.tp_group = tp_group<br><br>        <span class="hljs-keyword">if</span> self.sequence_parallel <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.input_is_parallel:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&quot;To enable `sequence_parallel`, `input_is_parallel` must be `True`&quot;</span>)<br><br>        <span class="hljs-comment"># Divide the weight matrix along the last dimension.</span><br>        self.tp_group = get_tensor_model_parallel_group_if_none(<br>            self.tp_group, is_expert=self.is_expert<br>        )<br><br>        world_size = get_pg_size(self.tp_group)<br>        rank = get_pg_rank(self.tp_group)<br>        self.explicit_expert_comm = self.is_expert <span class="hljs-keyword">and</span> (world_size &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> self.expert_parallel)<br><br>        self.input_size_per_partition = divide(input_size, world_size)<br><br>        <span class="hljs-comment"># Parameters.</span><br>        <span class="hljs-comment"># Note: torch.nn.functional.linear performs XA^T + b and as a result</span><br>        <span class="hljs-comment"># we allocate the transpose.</span><br>        <span class="hljs-comment"># Initialize weight.</span><br>        <span class="hljs-keyword">if</span> config.use_cpu_initialization:<br>            self.weight = Parameter(<br>                torch.empty(<br>                    self.output_size, self.input_size_per_partition, dtype=config.params_dtype<br>                )<br>            )<br>            <span class="hljs-keyword">if</span> config.perform_initialization:<br>                self.master_weight = _initialize_affine_weight_cpu(<br>                    self.weight,<br>                    self.output_size,<br>                    self.input_size,<br>                    self.input_size_per_partition,<br>                    <span class="hljs-number">1</span>,<br>                    init_method,<br>                    stride=stride,<br>                    return_master_weight=keep_master_weight_for_test,<br>                    params_dtype=config.params_dtype,<br>                    rank=rank,<br>                    world_size=world_size,<br>                )<br>        <span class="hljs-keyword">else</span>:<br>            self.weight = Parameter(<br>                torch.empty(<br>                    self.output_size,<br>                    self.input_size_per_partition,<br>                    device=torch.cuda.current_device(),<br>                    dtype=config.params_dtype,<br>                )<br>            )<br>            <span class="hljs-keyword">if</span> config.perform_initialization:<br>                _initialize_affine_weight_gpu(<br>                    self.weight,<br>                    init_method,<br>                    partition_dim=<span class="hljs-number">1</span>,<br>                    stride=stride,<br>                    is_expert=self.is_expert,<br>                )<br>        <span class="hljs-built_in">setattr</span>(self.weight, <span class="hljs-string">&quot;allreduce&quot;</span>, <span class="hljs-keyword">not</span> (self.is_expert <span class="hljs-keyword">and</span> self.expert_parallel))<br><br>        <span class="hljs-keyword">if</span> bias:<br>            <span class="hljs-keyword">if</span> config.use_cpu_initialization:<br>                self.bias = Parameter(torch.empty(self.output_size, dtype=config.params_dtype))<br>            <span class="hljs-keyword">else</span>:<br>                self.bias = Parameter(<br>                    torch.empty(<br>                        self.output_size,<br>                        device=torch.cuda.current_device(),<br>                        dtype=config.params_dtype,<br>                    )<br>                )<br><br>            <span class="hljs-keyword">if</span> config.perform_initialization:<br>                <span class="hljs-comment"># Always initialize bias to zero.</span><br>                <span class="hljs-keyword">with</span> torch.no_grad():<br>                    self.bias.zero_()<br>            <span class="hljs-built_in">setattr</span>(self.bias, <span class="hljs-string">&quot;allreduce&quot;</span>, <span class="hljs-keyword">not</span> (self.is_expert <span class="hljs-keyword">and</span> self.expert_parallel))<br>            <span class="hljs-built_in">setattr</span>(self.bias, <span class="hljs-string">&quot;sequence_parallel&quot;</span>, self.sequence_parallel)<br>        <span class="hljs-keyword">else</span>:<br>            self.register_parameter(<span class="hljs-string">&quot;bias&quot;</span>, <span class="hljs-literal">None</span>)<br><br>        <span class="hljs-comment"># Hook adding a default empty _extra_state for state dict</span><br>        self._register_load_state_dict_pre_hook(<br>            <span class="hljs-keyword">lambda</span> state_dict, prefix, *args, **kwargs: state_dict.setdefault(<br>                <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;prefix&#125;</span>_extra_state&quot;</span><br>            )<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_impl</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span>, weight, *args, **kwargs</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> weight.requires_grad:<br>            <span class="hljs-keyword">return</span> linear_with_frozen_weight(<span class="hljs-built_in">input</span>, weight, *args, **kwargs)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> linear_with_grad_accumulation_and_async_allreduce(<span class="hljs-built_in">input</span>, weight, *args, **kwargs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input_</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Forward of RowParallelLinear</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            input_: 3D tensor whose order of dimension is [sequence, batch, hidden]</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            - output</span><br><span class="hljs-string">            - bias</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># Set up backprop all-reduce.</span><br>        <span class="hljs-keyword">if</span> self.input_is_parallel:<br>            input_parallel = input_<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> self.sequence_parallel<br>            input_parallel = scatter_to_tensor_model_parallel_region(input_, group=self.tp_group)<br>        <span class="hljs-comment"># Matrix multiply.</span><br>        allreduce_dgrad = <span class="hljs-literal">False</span><br><br>        <span class="hljs-keyword">if</span> self.config._cpu_offloading_context <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> self.config._cpu_offloading_context.inside_context <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span>:<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> HAVE_TE:<br>                    <span class="hljs-keyword">assert</span> (<br>                        self.config.cpu_offloading <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span><br>                    ), <span class="hljs-string">&quot;CPU Offloading cannot be enabled while TE is not present&quot;</span><br>                <span class="hljs-keyword">else</span>:<br>                    input_parallel.activation_offloading = self.config.cpu_offloading_activations<br><br>        output_parallel = self._forward_impl(<br>            <span class="hljs-built_in">input</span>=input_parallel,<br>            weight=self.weight,<br>            bias=<span class="hljs-literal">None</span>,<br>            gradient_accumulation_fusion=self.gradient_accumulation_fusion,<br>            allreduce_dgrad=allreduce_dgrad,<br>            sequence_parallel=<span class="hljs-literal">False</span>,<br>            tp_group=<span class="hljs-literal">None</span>,<br>            grad_output_buffer=<span class="hljs-literal">None</span>,<br>        )<br><br>        <span class="hljs-comment"># All-reduce across all the partitions.</span><br>        <span class="hljs-keyword">if</span> self.explicit_expert_comm:<br>            <span class="hljs-keyword">assert</span> self.skip_bias_add<br>            output_ = output_parallel<br>        <span class="hljs-keyword">elif</span> self.sequence_parallel:<br>            output_ = reduce_scatter_to_sequence_parallel_region(<br>                output_parallel, group=self.tp_group<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            output_ = reduce_from_tensor_model_parallel_region(output_parallel, group=self.tp_group)<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.skip_bias_add:<br>            output = (output_ + self.bias) <span class="hljs-keyword">if</span> self.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> output_<br>            output_bias = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">else</span>:<br>            output = output_<br>            output_bias = self.bias<br>        <span class="hljs-keyword">return</span> output, output_bias<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sharded_state_dict</span>(<span class="hljs-params">self, prefix=<span class="hljs-string">&quot;&quot;</span>, sharded_offsets=(<span class="hljs-params"></span>), metadata=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Sharding along axis 1, bias not sharded&quot;&quot;&quot;</span><br>        state_dict = self.state_dict(prefix=<span class="hljs-string">&quot;&quot;</span>, keep_vars=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">return</span> make_sharded_tensors_for_checkpoint(<br>            state_dict, prefix, &#123;<span class="hljs-string">&quot;weight&quot;</span>: <span class="hljs-number">1</span>&#125;, sharded_offsets<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">set_extra_state</span>(<span class="hljs-params">self, state: <span class="hljs-type">Any</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Extra state is ignored&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_extra_state</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Keep compatibility with TE state dict.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__repr__</span>(<span class="hljs-params">self</span>):<br>        tp = self.input_size // self.input_size_per_partition<br>        use_bias = self.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> self.bias <span class="hljs-keyword">is</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">return</span> (<br>            <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(self).__name__&#125;</span>(in_features=<span class="hljs-subst">&#123;self.input_size&#125;</span>, &quot;</span><br>            <span class="hljs-string">f&quot;out_features=<span class="hljs-subst">&#123;self.output_size&#125;</span>, bias=<span class="hljs-subst">&#123;use_bias&#125;</span>, TP=<span class="hljs-subst">&#123;tp&#125;</span>)&quot;</span><br>        )<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>在初始化时：</p>
<ul>
<li><p>参数设置整体与<code>row_parallel_linear</code>类似，不同点在于其包含参数<code>input_is_parallel</code>记录输出是否已经被并行切分，并且存在约束如果设置了<code>self.sequence_parallel</code>，那么<code>self.input_is_parallel</code>必须为True。</p>
</li>
<li><p>其切分权重时也是对输入维度进行切分（input_size_per_partition &#x3D; input_size &#x2F; tp_world_size）</p>
</li>
</ul>
</li>
<li><p>在Forward时，流程如下：</p>
<ol>
<li><p>其计查看参数<code>input_is_parallel</code>，如果没有切分就调用scatter在TP组内进行划分</p>
</li>
<li><p>然后其调用<code>_forward_impl</code>来实现具体计算，与<code>ColumnParallelLinear</code>计算类似，如果使用了<code>sequence_parallel</code>会先All Gather获取对应输入数据</p>
</li>
<li><p>然后对局部输出做对应通信得到<code>output</code>：</p>
<ul>
<li><p><strong>普通情况（非 expert、非 sequence_parallel）</strong>：<br>调用 <code>reduce_from_tensor_model_parallel_region</code><br>=&gt; 本质是 **TP all-reduce(sum)**，把各 rank 的 <code>Y_i</code> 求和得到完整 <code>Y</code>（每个 rank 都得到同样的 <code>Y</code>）。</p>
</li>
<li><p><strong>sequence_parallel&#x3D;True</strong>：<br>调用 <code>reduce_scatter_to_sequence_parallel_region</code><br>=&gt; 把 sum 的结果直接按 sequence parallel 需要的布局做 <strong>reduce-scatter</strong>，避免先 all-reduce 再切分的额外开销。</p>
</li>
<li><p><strong>expert 显式通信</strong>（MoE）： 不在这里做 reduce，直接返回本地 output_parallel，因为 MoE 的 token dispatcher 负责跨 rank 的聚合&#x2F;路由。</p>
</li>
</ul>
</li>
<li><p>最后返回<code>output</code>, <code>output_bias</code></p>
</li>
</ol>
</li>
</ul>
<h4 id="Transformer模块"><a href="#Transformer模块" class="headerlink" title="Transformer模块"></a>Transformer模块</h4><p>在具体实现Transformer模块时，其会依赖<code>multi_latent_attention</code>参数来判断GPT 的每一层 self-attention 子模块用标准SelfAttention还是用MLA（Multi‑Latent Attention）变体。</p>
<p>我们这里直接看最标准的实现，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerLayer</span>(MegatronModule, BaseTransformerLayer):<br>    <span class="hljs-string">&quot;&quot;&quot;A single transformer layer.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Transformer layer takes input with size [s, b, h] and returns an</span><br><span class="hljs-string">    output of the same size.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        config: TransformerConfig,</span><br><span class="hljs-params">        submodules: TransformerLayerSubmodules,</span><br><span class="hljs-params">        layer_number: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">        hidden_dropout: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">float</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        model_comm_pgs: <span class="hljs-type">Optional</span>[ModelCommProcessGroups] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        vp_stage: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__(config=config)<br><br>        <span class="hljs-comment"># Enable cuda graphs.</span><br>        <span class="hljs-keyword">if</span> (<br>            config.enable_cuda_graph <span class="hljs-keyword">and</span> config.cuda_graph_scope != <span class="hljs-string">&quot;full_iteration&quot;</span><br>        ) <span class="hljs-keyword">or</span> config.external_cuda_graph:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> (<br>                config.enable_cuda_graph <span class="hljs-keyword">and</span> config.external_cuda_graph<br>            ), <span class="hljs-string">&quot;Cudagraphs and external cudagraphs cannot be enabled at the same time&quot;</span><br>            <span class="hljs-keyword">if</span> config.enable_cuda_graph <span class="hljs-keyword">and</span> config.cuda_graph_scope != <span class="hljs-string">&quot;full_iteration&quot;</span>:<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.training:<br>                    <span class="hljs-comment"># Cudagraphs for inference are only enabled with the flash decoding kernel</span><br>                    <span class="hljs-keyword">assert</span> (<br>                        self.config.flash_decode<br>                    ), <span class="hljs-string">&quot;--flash-decode is required to use CUDA graphs during inference&quot;</span><br>                self.cudagraph_manager = CudaGraphManager(config, vp_stage=vp_stage)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># List to store CUDA graphs. A list of `N` CUDA graphs for this layer where N is</span><br>                <span class="hljs-comment"># the number of microbatches. Multiple CUDA graphs per layer is required to support</span><br>                <span class="hljs-comment"># pipelining which requires running FWD graph of multiple microbatches before BWD</span><br>                <span class="hljs-comment"># graph. To enable CUDA graph, this list should be populated in the model training</span><br>                <span class="hljs-comment"># script with the graphs returned by make_graphed_callables API before the first</span><br>                <span class="hljs-comment"># training step.</span><br>                self.cuda_graphs = []<br>                <span class="hljs-comment"># List to store forward pre-hooks. Forward pre-hooks are not captured into CUDA</span><br>                <span class="hljs-comment"># graphs. Those hooks and args are collected in this list and should be manually</span><br>                <span class="hljs-comment"># triggered before CUDA Graph running. This is required to ensure the correct param</span><br>                <span class="hljs-comment"># all-gather overlap with forward compute.</span><br>                self.cuda_graph_manual_hooks = []<br>                self.current_microbatch = -<span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">if</span> model_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            model_comm_pgs = ModelCommProcessGroups.use_mpu_process_groups()<br><br>        self.submodules_config = submodules<br>        self.layer_number = layer_number + get_transformer_layer_offset(self.config, vp_stage)<br>        self.hidden_dropout = config.hidden_dropout <span class="hljs-keyword">if</span> hidden_dropout <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> hidden_dropout<br><br>        <span class="hljs-comment"># [Module 1: Input Layernorm] Optional Layernorm on the input data</span><br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> add pytorch only layernorm</span><br>        self.input_layernorm = build_module(<br>            submodules.input_layernorm,<br>            config=self.config,<br>            hidden_size=self.config.hidden_size,<br>            eps=self.config.layernorm_epsilon,<br>        )<br><br>        attention_optional_kwargs = &#123;&#125;<br>        <span class="hljs-keyword">if</span> config.context_parallel_size &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> config.cp_comm_type <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(config.cp_comm_type, <span class="hljs-built_in">list</span>):<br>                attention_optional_kwargs[<span class="hljs-string">&quot;cp_comm_type&quot;</span>] = config.cp_comm_type[self.layer_number]<br>            <span class="hljs-keyword">else</span>:<br>                attention_optional_kwargs[<span class="hljs-string">&quot;cp_comm_type&quot;</span>] = config.cp_comm_type<br><br>        attention_optional_kwargs[<span class="hljs-string">&quot;model_comm_pgs&quot;</span>] = model_comm_pgs<br><br>        <span class="hljs-comment"># [Module 2: SelfAttention]</span><br>        self.self_attention = build_module(<br>            submodules.self_attention,<br>            config=self.config,<br>            layer_number=self.layer_number,<br>            **attention_optional_kwargs,<br>        )<br><br>        <span class="hljs-comment"># [Module 3: BiasDropoutFusion]</span><br>        self.self_attn_bda = build_module(submodules.self_attn_bda)<br><br>        <span class="hljs-comment"># [Module 4: Post SelfAttention] Optional Layernorm after self-attn</span><br>        self.pre_cross_attn_layernorm = build_module(<br>            submodules.pre_cross_attn_layernorm,<br>            config=self.config,<br>            hidden_size=self.config.hidden_size,<br>            eps=self.config.layernorm_epsilon,<br>        )<br><br>        <span class="hljs-comment"># [Module 5: CrossAttention]</span><br>        self.cross_attention = build_module(<br>            submodules.cross_attention,<br>            config=self.config,<br>            layer_number=self.layer_number,<br>            **attention_optional_kwargs,<br>        )<br><br>        <span class="hljs-comment"># [Module 6: BiasDropoutFusion]</span><br>        self.cross_attn_bda = build_module(submodules.cross_attn_bda, config=self.config)<br><br>        <span class="hljs-comment"># [Module 7: Pre MLP] Optional Layernorm before MLP</span><br>        self.pre_mlp_layernorm = build_module(<br>            submodules.pre_mlp_layernorm,<br>            config=self.config,<br>            hidden_size=self.config.hidden_size,<br>            eps=self.config.layernorm_epsilon,<br>        )<br>        <span class="hljs-comment"># [Module 8: MLP block]</span><br>        additional_mlp_kwargs = &#123;&#125;<br>        <span class="hljs-comment"># import here to avoid circular import</span><br>        <span class="hljs-keyword">from</span> megatron.core.extensions.transformer_engine <span class="hljs-keyword">import</span> TEFusedMLP<br>        <span class="hljs-keyword">from</span> megatron.core.transformer.moe.experts <span class="hljs-keyword">import</span> GroupedMLP, SequentialMLP, TEGroupedMLP<br>        <span class="hljs-keyword">from</span> megatron.core.transformer.moe.moe_layer <span class="hljs-keyword">import</span> MoELayer<br><br>        <span class="hljs-comment"># MLP expects tp_group but MoELayer expects model_comm_pgs to be passed in.</span><br>        <span class="hljs-comment"># We can change MLP to accept model_comm_pgs but it makes the logic implicit</span><br>        <span class="hljs-comment"># The conditional below is to make the logic explicit</span><br>        <span class="hljs-comment"># if submodules.mlp is not a ModuleSpec,we dont have to handle passing additional kwargs</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(submodules.mlp, ModuleSpec):<br>            <span class="hljs-keyword">if</span> submodules.mlp.module <span class="hljs-keyword">in</span> (MoELayer, GroupedMLP, TEGroupedMLP, SequentialMLP):<br>                additional_mlp_kwargs[<span class="hljs-string">&quot;model_comm_pgs&quot;</span>] = model_comm_pgs<br>            <span class="hljs-keyword">elif</span> submodules.mlp.module == MLP:<br>                <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(<br>                    model_comm_pgs, <span class="hljs-string">&#x27;tp&#x27;</span><br>                ), <span class="hljs-string">&#x27;TP process group is required for MLP in TransformerLayer&#x27;</span><br>                additional_mlp_kwargs[<span class="hljs-string">&quot;tp_group&quot;</span>] = model_comm_pgs.tp<br>            <span class="hljs-keyword">elif</span> TEFusedMLP <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> submodules.mlp.module == TEFusedMLP:<br>                <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(<br>                    model_comm_pgs, <span class="hljs-string">&#x27;tp&#x27;</span><br>                ), <span class="hljs-string">&#x27;TP process group is required for TEFusedMLP in TransformerLayer&#x27;</span><br>                additional_mlp_kwargs[<span class="hljs-string">&quot;tp_group&quot;</span>] = model_comm_pgs.tp<br>            <span class="hljs-keyword">else</span>:<br>                log_single_rank(<br>                    logger,<br>                    logging.WARNING,<br>                    <span class="hljs-string">f&quot;Unknown MLP type: <span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(submodules.mlp)&#125;</span>. Using default kwargs.&quot;</span>,<br>                )<br>        self.mlp = build_module(submodules.mlp, config=self.config, **additional_mlp_kwargs)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(self.mlp, <span class="hljs-string">&#x27;set_layer_number&#x27;</span>):<br>            self.mlp.set_layer_number(self.layer_number)<br><br>        <span class="hljs-comment"># [Module 9: BiasDropoutFusion]</span><br>        self.mlp_bda = build_module(submodules.mlp_bda)<br><br>        self.recompute_input_layernorm = <span class="hljs-literal">False</span><br>        self.recompute_pre_mlp_layernorm = <span class="hljs-literal">False</span><br>        self.recompute_mlp = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">if</span> self.config.recompute_granularity == <span class="hljs-string">&#x27;selective&#x27;</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;layernorm&quot;</span> <span class="hljs-keyword">in</span> self.config.recompute_modules:<br>                <span class="hljs-keyword">if</span> (<br>                    <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.input_layernorm, IdentityOp)<br>                    <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.config.external_cuda_graph<br>                ):<br>                    self.recompute_input_layernorm = <span class="hljs-literal">True</span><br>                    <span class="hljs-keyword">if</span> self.config.fp8:<br>                        self.self_attention.set_for_recompute_input_layernorm()<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.pre_mlp_layernorm, IdentityOp):<br>                    self.recompute_pre_mlp_layernorm = <span class="hljs-literal">True</span><br>                    <span class="hljs-keyword">if</span> self.config.fp8:<br>                        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(self.mlp, MoELayer):<br>                            self.mlp.set_for_recompute_pre_mlp_layernorm()<br>                        <span class="hljs-keyword">else</span>:<br>                            <span class="hljs-keyword">from</span> megatron.core.extensions.transformer_engine <span class="hljs-keyword">import</span> (<br>                                set_save_original_input,<br>                            )<br><br>                            set_save_original_input(self.mlp.linear_fc1)<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;mlp&quot;</span> <span class="hljs-keyword">in</span> self.config.recompute_modules:<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.mlp, MoELayer):<br>                    self.recompute_mlp = <span class="hljs-literal">True</span><br><br>        <span class="hljs-comment"># @jcasper how should we handle nvfuser?</span><br>        <span class="hljs-comment"># Set bias+dropout+add fusion grad_enable execution handler.</span><br>        <span class="hljs-comment"># TORCH_MAJOR = int(torch.__version__.split(&#x27;.&#x27;)[0])</span><br>        <span class="hljs-comment"># TORCH_MINOR = int(torch.__version__.split(&#x27;.&#x27;)[1])</span><br>        <span class="hljs-comment"># use_nvfuser = TORCH_MAJOR &gt; 1 or (TORCH_MAJOR == 1 and TORCH_MINOR &gt;= 10)</span><br>        <span class="hljs-comment"># self.bias_dropout_add_exec_handler = nullcontext if use_nvfuser else torch.enable_grad</span><br>        self.bias_dropout_add_exec_handler = torch.enable_grad<br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_layer_offset</span>(<span class="hljs-params">config: TransformerConfig</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Get the layer offset for the current pipeline stage.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Deprecated: please use `get_transformer_layer_offset` instead.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        warnings.warn(<br>            <span class="hljs-string">&quot;TransformerLayer._get_layer_offset is deprecated.&quot;</span><br>            <span class="hljs-string">&quot;Please use get_transformer_layer_offset instead.&quot;</span><br>        )<br>        <span class="hljs-keyword">return</span> get_transformer_layer_offset(config)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Perform a forward pass through the transformer layer.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This method calls the core computation of a transformer layer, including</span><br><span class="hljs-string">        self-attention, cross-attention (if applicable), and feed-forward operations.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        hidden_states, context = self._forward_attention(*args, **kwargs)<br>        output = self._forward_mlp(hidden_states, kwargs.get(<span class="hljs-string">&quot;inference_context&quot;</span>, <span class="hljs-literal">None</span>))<br>        <span class="hljs-keyword">return</span> output, context<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_attention</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        hidden_states: Tensor,</span><br><span class="hljs-params">        attention_mask: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        context: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        context_mask: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        rotary_pos_emb: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        rotary_pos_cos: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        rotary_pos_sin: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        attention_bias: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        inference_context: <span class="hljs-type">Optional</span>[<span class="hljs-type">Any</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        packed_seq_params: <span class="hljs-type">Optional</span>[PackedSeqParams] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        sequence_len_offset: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        *,</span><br><span class="hljs-params">        inference_params: <span class="hljs-type">Optional</span>[<span class="hljs-type">Any</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Perform a forward pass through the attention layer and the layernorms before and after</span><br><span class="hljs-string">        the attention operations.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            hidden_states (Tensor): Input tensor of shape [s, b, h] where s is sequence length,</span><br><span class="hljs-string">                b is batch size, and h is hidden size.</span><br><span class="hljs-string">            attention_mask (Tensor): Mask tensor for self-attention.</span><br><span class="hljs-string">            context (Tensor, optional): Context tensor for cross-attention.</span><br><span class="hljs-string">            context_mask (Tensor, optional): Mask tensor for cross-attention.</span><br><span class="hljs-string">            rotary_pos_emb (Tensor, optional): Rotary positional embeddings.</span><br><span class="hljs-string">            attention_bias (Tensor, optional): Bias tensor for Q * K.T.</span><br><span class="hljs-string">            inference_context (object, optional): Parameters for inference-time optimizations.</span><br><span class="hljs-string">            packed_seq_params (object, optional): Parameters for packed sequence processing.</span><br><span class="hljs-string">            sequence_len_offset (Tensor, optional): Offset along sequence dimension</span><br><span class="hljs-string">                during inference.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Tuple[Tensor, Tensor]: A tuple containing:</span><br><span class="hljs-string">                hidden_states (Tensor): Transformed hidden states before the MLP layernorm.</span><br><span class="hljs-string">                context (Tensor): Updated context tensor if cross-attention is used,</span><br><span class="hljs-string">                otherwise None.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        inference_context = deprecate_inference_params(inference_context, inference_params)<br><br>        <span class="hljs-comment"># Residual connection.</span><br>        residual = hidden_states<br><br>        <span class="hljs-comment"># Optional Input Layer norm</span><br>        <span class="hljs-keyword">if</span> self.recompute_input_layernorm:<br>            self.input_layernorm_checkpoint = tensor_parallel.CheckpointWithoutOutput()<br>            input_layernorm_output = self.input_layernorm_checkpoint.checkpoint(<br>                self.input_layernorm, hidden_states<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            input_layernorm_output = self.input_layernorm(hidden_states)<br><br>        <span class="hljs-comment"># Self attention.</span><br>        nvtx_range_push(suffix=<span class="hljs-string">&quot;self_attention&quot;</span>)<br>        attention_output_with_bias = self.self_attention(<br>            input_layernorm_output,<br>            attention_mask=attention_mask,<br>            inference_context=inference_context,<br>            rotary_pos_emb=rotary_pos_emb,<br>            rotary_pos_cos=rotary_pos_cos,<br>            rotary_pos_sin=rotary_pos_sin,<br>            attention_bias=attention_bias,<br>            packed_seq_params=packed_seq_params,<br>            sequence_len_offset=sequence_len_offset,<br>        )<br>        nvtx_range_pop(suffix=<span class="hljs-string">&quot;self_attention&quot;</span>)<br><br>        <span class="hljs-keyword">if</span> self.recompute_input_layernorm:<br>            <span class="hljs-comment"># discard the output of the input layernorm and register the recompute</span><br>            <span class="hljs-comment"># as a gradient hook of attention_output_with_bias[0]</span><br>            self.input_layernorm_checkpoint.discard_output_and_register_recompute(<br>                attention_output_with_bias[<span class="hljs-number">0</span>]<br>            )<br><br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> could we move `bias_dropout_add_exec_handler` itself</span><br>        <span class="hljs-comment"># inside the module provided in the `bias_dropout_add_spec` module?</span><br>        nvtx_range_push(suffix=<span class="hljs-string">&quot;self_attn_bda&quot;</span>)<br>        <span class="hljs-keyword">with</span> self.bias_dropout_add_exec_handler():<br>            hidden_states = self.self_attn_bda(self.training, self.config.bias_dropout_fusion)(<br>                attention_output_with_bias, residual, self.hidden_dropout<br>            )<br>        nvtx_range_pop(suffix=<span class="hljs-string">&quot;self_attn_bda&quot;</span>)<br><br>        <span class="hljs-comment"># Residual connection.</span><br>        residual = hidden_states<br><br>        <span class="hljs-comment"># Optional Layer norm after self-attention</span><br>        pre_cross_attn_layernorm_output = self.pre_cross_attn_layernorm(hidden_states)<br><br>        <span class="hljs-comment"># Cross attention.</span><br>        attention_output_with_bias = self.cross_attention(<br>            pre_cross_attn_layernorm_output,<br>            attention_mask=context_mask,<br>            key_value_states=context,<br>            inference_context=inference_context,<br>        )<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(attention_output_with_bias, <span class="hljs-built_in">dict</span>) <span class="hljs-keyword">and</span> <span class="hljs-string">&quot;context&quot;</span> <span class="hljs-keyword">in</span> attention_output_with_bias:<br>            context = attention_output_with_bias[<span class="hljs-string">&quot;context&quot;</span>]<br><br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> could we move `bias_dropout_add_exec_handler` itself</span><br>        <span class="hljs-comment"># inside the module provided in the `bias_dropout_add_spec` module?</span><br>        <span class="hljs-keyword">with</span> self.bias_dropout_add_exec_handler():<br>            hidden_states = self.cross_attn_bda(self.training, self.config.bias_dropout_fusion)(<br>                attention_output_with_bias, residual, self.hidden_dropout<br>            )<br><br>        <span class="hljs-keyword">return</span> hidden_states, context<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward_mlp</span>(<span class="hljs-params">self, hidden_states, inference_context=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Perform a forward pass through the feed-forward layer.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            hidden_states (Tensor): Transformed hidden states before the MLP layernorm.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            output (Tensor): Transformed hidden states of shape [s, b, h].</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># Residual connection.</span><br>        residual = hidden_states<br><br>        <span class="hljs-comment"># Optional Layer norm post the cross-attention.</span><br>        <span class="hljs-keyword">if</span> self.recompute_pre_mlp_layernorm:<br>            self.pre_mlp_norm_checkpoint = tensor_parallel.CheckpointWithoutOutput()<br>            pre_mlp_layernorm_output = self.pre_mlp_norm_checkpoint.checkpoint(<br>                self.pre_mlp_layernorm, hidden_states<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            pre_mlp_layernorm_output = self.pre_mlp_layernorm(hidden_states)<br><br>        nvtx_range_push(suffix=<span class="hljs-string">&quot;mlp&quot;</span>)<br>        <span class="hljs-comment"># Potentially chunk the MLP computation during prefill to minimize the peak activation size</span><br>        should_chunk_mlp_for_prefill = (<br>            self.config.mlp_chunks_for_prefill &gt; <span class="hljs-number">1</span><br>            <span class="hljs-keyword">and</span> inference_context <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> inference_context.is_decode_only()<br>            <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(self.mlp, IdentityOp)<br>        )<br><br>        <span class="hljs-keyword">if</span> self.recompute_mlp:<br>            <span class="hljs-keyword">if</span> self.config.fp8:<br>                <span class="hljs-comment"># import here to avoid circular import</span><br>                <span class="hljs-keyword">from</span> megatron.core.extensions.transformer_engine <span class="hljs-keyword">import</span> te_checkpoint<br><br>                mlp_output_with_bias = te_checkpoint(<br>                    self.mlp,<br>                    <span class="hljs-literal">False</span>,<br>                    tensor_parallel.random.get_cuda_rng_tracker,<br>                    parallel_state.get_tensor_model_parallel_group(),<br>                    pre_mlp_layernorm_output,<br>                )<br>            <span class="hljs-keyword">else</span>:<br>                mlp_output_with_bias = tensor_parallel.checkpoint(<br>                    self.mlp, <span class="hljs-literal">False</span>, pre_mlp_layernorm_output<br>                )<br>        <span class="hljs-keyword">elif</span> should_chunk_mlp_for_prefill:<br>            <span class="hljs-comment"># Chunk input along sequence dimension</span><br>            num_chunks = <span class="hljs-built_in">min</span>(self.config.mlp_chunks_for_prefill, pre_mlp_layernorm_output.shape[<span class="hljs-number">0</span>])<br>            chunks = pre_mlp_layernorm_output.chunk(num_chunks, dim=<span class="hljs-number">0</span>)<br><br>            <span class="hljs-comment"># Compute outputs for each chunk</span><br>            outputs = [self.mlp(chunk) <span class="hljs-keyword">for</span> chunk <span class="hljs-keyword">in</span> chunks]<br><br>            <span class="hljs-comment"># Aggregate chunk outputs</span><br>            mlp_output = torch.cat([out <span class="hljs-keyword">for</span> out, _ <span class="hljs-keyword">in</span> outputs], dim=<span class="hljs-number">0</span>)<br>            bias_chunks = [bias <span class="hljs-keyword">for</span> _, bias <span class="hljs-keyword">in</span> outputs <span class="hljs-keyword">if</span> bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>]<br>            bias_output = torch.stack(bias_chunks, dim=<span class="hljs-number">0</span>).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">0</span>) <span class="hljs-keyword">if</span> bias_chunks <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>            mlp_output_with_bias = (mlp_output, bias_output)<br><br>        <span class="hljs-keyword">else</span>:<br>            mlp_output_with_bias = self.mlp(pre_mlp_layernorm_output)<br><br>        <span class="hljs-keyword">if</span> self.recompute_pre_mlp_layernorm:<br>            <span class="hljs-comment"># discard the output of the pre-mlp layernorm and register the recompute</span><br>            <span class="hljs-comment"># as a gradient hook of mlp_output_with_bias[0]</span><br>            self.pre_mlp_norm_checkpoint.discard_output_and_register_recompute(<br>                mlp_output_with_bias[<span class="hljs-number">0</span>]<br>            )<br>        nvtx_range_pop(suffix=<span class="hljs-string">&quot;mlp&quot;</span>)<br><br>        <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> could we move `bias_dropout_add_exec_handler` itself</span><br>        <span class="hljs-comment"># inside the module provided in the `bias_dropout_add_spec` module?</span><br>        nvtx_range_push(suffix=<span class="hljs-string">&quot;mlp_bda&quot;</span>)<br>        <span class="hljs-keyword">with</span> self.bias_dropout_add_exec_handler():<br>            hidden_states = self.mlp_bda(self.training, self.config.bias_dropout_fusion)(<br>                mlp_output_with_bias, residual, self.hidden_dropout<br>            )<br>        nvtx_range_pop(suffix=<span class="hljs-string">&quot;mlp_bda&quot;</span>)<br><br>        <span class="hljs-comment"># Jit compiled function creates &#x27;view&#x27; tensor. This tensor</span><br>        <span class="hljs-comment"># potentially gets saved in the MPU checkpoint function context,</span><br>        <span class="hljs-comment"># which rejects view tensors. While making a viewless tensor here</span><br>        <span class="hljs-comment"># won&#x27;t result in memory savings (like the data loader, or</span><br>        <span class="hljs-comment"># p2p_communication), it serves to document the origin of this</span><br>        <span class="hljs-comment"># &#x27;view&#x27; tensor.</span><br>        output = make_viewless_tensor(<br>            inp=hidden_states, requires_grad=hidden_states.requires_grad, keep_graph=<span class="hljs-literal">True</span><br>        )<br><br>        <span class="hljs-keyword">return</span> output<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sharded_state_dict</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, prefix: <span class="hljs-built_in">str</span> = <span class="hljs-string">&#x27;&#x27;</span>, sharded_offsets: <span class="hljs-built_in">tuple</span> = (<span class="hljs-params"></span>), metadata: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">dict</span>] = <span class="hljs-literal">None</span></span><br><span class="hljs-params">    </span>) -&gt; ShardedStateDict:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Generate a sharded state dictionary for the transformer layer.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            prefix (str, optional): Prefix to be added to all keys in the state dict.</span><br><span class="hljs-string">            sharded_offsets (tuple, optional): Tuple of sharding offsets.</span><br><span class="hljs-string">            metadata (Optional[dict], optional): Additional metadata for sharding.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            ShardedStateDict: A dictionary containing the sharded state of the transformer layer.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        sharded_state_dict = <span class="hljs-built_in">super</span>().sharded_state_dict(prefix, sharded_offsets, metadata)<br>        prefixed_map = &#123;<br>            <span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;prefix&#125;</span><span class="hljs-subst">&#123;k&#125;</span>&#x27;</span>: <span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;prefix&#125;</span><span class="hljs-subst">&#123;v&#125;</span>&#x27;</span><br>            <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> self.submodules_config.sharded_state_dict_keys_map.items()<br>        &#125;<br>        <span class="hljs-keyword">if</span> prefixed_map:<br>            apply_prefix_mapping(sharded_state_dict, prefixed_map)<br>        <span class="hljs-keyword">return</span> sharded_state_dict<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_layer_static_inputs</span>(<span class="hljs-params">self, seq_length, micro_batch_size</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Get the static inputs for the transformer layer.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Dict[str, torch.Tensor]: A dictionary containing the static inputs for the layer.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Calculate data shape related values.</span><br>        context_parallel_size = self.config.context_parallel_size<br>        slen_per_cp = seq_length // context_parallel_size<br>        sequence_parallel = self.config.sequence_parallel<br>        tensor_model_parallel_size = self.config.tensor_model_parallel_size<br>        slen_per_cptp = (<br>            slen_per_cp // tensor_model_parallel_size <span class="hljs-keyword">if</span> sequence_parallel <span class="hljs-keyword">else</span> slen_per_cp<br>        )<br><br>        static_inputs = &#123;&#125;<br>        static_inputs[<span class="hljs-string">&quot;hidden_states&quot;</span>] = torch.ones(<br>            (slen_per_cptp, micro_batch_size, self.config.hidden_size),<br>            dtype=torch.bfloat16,<br>            requires_grad=<span class="hljs-literal">True</span>,<br>            device=torch.cuda.current_device(),<br>        )<br>        static_inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>] = (<br>            ~(torch.tril(torch.ones((slen_per_cp, seq_length))).<span class="hljs-built_in">bool</span>())<br>            .to(torch.cuda.current_device())<br>            .reshape(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, slen_per_cp, seq_length)<br>            .tile(micro_batch_size, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        )<br>        <span class="hljs-keyword">return</span> static_inputs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_manual_hooks</span>(<span class="hljs-params">self, make_hook_func</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Set CUDA Graph manual hooks for the modules that contain direct parameters and are</span><br><span class="hljs-string">        covered by cudagraphs.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.cuda_graph_manual_hooks = []<br><br>        <span class="hljs-comment"># Select the modules who contain direct parameters and are covered by cudagraphs.</span><br>        <span class="hljs-comment"># Add these modules to the `cuda_graph_manual_hooks` because their hooks will not</span><br>        <span class="hljs-comment"># be automatically triggered when they go through the CUDA Graph path.</span><br>        <span class="hljs-keyword">if</span> self.config.cuda_graph_scope == <span class="hljs-string">&#x27;full&#x27;</span>:<br>            high_level_modules = [self]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">assert</span> (<br>                self.config.cuda_graph_scope == <span class="hljs-string">&#x27;attn&#x27;</span><br>            ), <span class="hljs-string">&quot;Invalid cuda_graph_scope $&#123;self.config.cuda_graph_scope&#125;&quot;</span><br>            high_level_modules = [<br>                self.input_layernorm,<br>                self.self_attention,<br>                self.pre_cross_attn_layernorm,<br>                self.cross_attention,<br>            ]<br><br>        param_modules = []<br>        <span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> high_level_modules:<br>            <span class="hljs-keyword">for</span> submodule <span class="hljs-keyword">in</span> module.modules():<br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">next</span>(submodule.parameters(recurse=<span class="hljs-literal">False</span>), <span class="hljs-literal">None</span>) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    <span class="hljs-comment"># Module contains direct parameters.</span><br>                    param_modules.append(submodule)<br>                    <span class="hljs-keyword">continue</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(param_modules) &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> param_modules:<br>                self.cuda_graph_manual_hooks.append((make_hook_func(), (module,)))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_cuda_graph_capture</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        CUDA Graph capture for this layer. There are some differences from the normal pass:</span><br><span class="hljs-string">        1. In some conditions CUDA graph cannot cover the entire layer. The `cuda_graph_scope`</span><br><span class="hljs-string">           attribute can be set to control the scope of the CUDA graph.</span><br><span class="hljs-string">        2. If context is None, it cannot be returned as output.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        hidden_states, context = self._forward_attention(*args, **kwargs)<br><br>        <span class="hljs-keyword">if</span> self.config.cuda_graph_scope == <span class="hljs-string">&quot;full&quot;</span>:<br>            hidden_states = self._forward_mlp(hidden_states)<br>        cuda_graph_outputs = [hidden_states]<br><br>        <span class="hljs-keyword">if</span> context <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            cuda_graph_outputs.append(context)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">tuple</span>(cuda_graph_outputs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_cuda_graph_replay</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        CUDA graph replay for this layer and microbatch</span><br><span class="hljs-string">        `self.current_microbatch`. TransformerEngine versions&gt;=1.10</span><br><span class="hljs-string">        allow keyword arguments with CUDA graph. However, CUDA graph</span><br><span class="hljs-string">        acccepts only Tensor inputs and Tensor outputs. Hence,</span><br><span class="hljs-string">        `inference_context` and `packed_seq_params` are excluded from</span><br><span class="hljs-string">        input list while output is limited to `hidden_states`.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">_check_cuda_graph_replay_args</span>(<span class="hljs-params">*args, **kwargs</span>):<br>            <span class="hljs-string">&quot;&quot;&quot;Helper function to get optional tensor arguments for CUDA graph.&quot;&quot;&quot;</span><br><br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(args) &lt;= <span class="hljs-number">1</span>, <span class="hljs-string">&quot;At most one positional argument `hidden_states` is expected.&quot;</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(args) == <span class="hljs-number">1</span>:<br>                hidden_states = args[<span class="hljs-number">0</span>]<br>            <span class="hljs-keyword">else</span>:<br>                hidden_states = kwargs.pop(<span class="hljs-string">&quot;hidden_states&quot;</span>)<br>            cudagraph_args = [hidden_states]<br><br>            optional_inputs = kwargs.copy()<br>            optional_inputs[<span class="hljs-string">&#x27;is_first_microbatch&#x27;</span>] = self.current_microbatch == <span class="hljs-number">0</span><br>            <span class="hljs-keyword">try</span>:<br>                <span class="hljs-keyword">import</span> transformer_engine.pytorch <span class="hljs-keyword">as</span> te  <span class="hljs-comment"># pylint: disable=unused-import</span><br><br>                <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_zero_attention_mask</span>(<span class="hljs-params">slen_per_tpcp, micro_batch_size</span>):<br>                    sequence_parallel = self.config.sequence_parallel<br>                    tensor_model_parallel_size = self.config.tensor_model_parallel_size<br>                    slen_per_cp = (<br>                        slen_per_tpcp * tensor_model_parallel_size<br>                        <span class="hljs-keyword">if</span> sequence_parallel<br>                        <span class="hljs-keyword">else</span> slen_per_tpcp<br>                    )<br>                    slen = slen_per_cp * self.config.context_parallel_size<br>                    <span class="hljs-keyword">return</span> torch.zeros(<br>                        (micro_batch_size, <span class="hljs-number">1</span>, slen_per_cp, slen),<br>                        dtype=torch.<span class="hljs-built_in">bool</span>,<br>                        device=torch.cuda.current_device(),<br>                    )<br><br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> is_te_min_version(<span class="hljs-string">&quot;1.10.0&quot;</span>):<br>                    <span class="hljs-comment"># TE version &lt; 1.10.0 does not support keyword arguments with CUDA graph.</span><br>                    <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> kwargs.items():<br>                        <span class="hljs-keyword">if</span> k == <span class="hljs-string">&quot;attention_mask&quot;</span>:<br>                            <span class="hljs-keyword">if</span> v <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                                cudagraph_args.append(v)<br>                                optional_inputs[k] = <span class="hljs-literal">None</span><br>                            <span class="hljs-keyword">else</span>:<br>                                cudagraph_args.append(<br>                                    get_zero_attention_mask(<br>                                        hidden_states.size(<span class="hljs-number">0</span>), hidden_states.size(<span class="hljs-number">1</span>)<br>                                    )<br>                                )<br>                        <span class="hljs-keyword">else</span>:<br>                            <span class="hljs-keyword">assert</span> v <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;Keyword Arguments not supported with CUDA graph.&quot;</span><br>                <span class="hljs-keyword">elif</span> optional_inputs[<span class="hljs-string">&#x27;attention_mask&#x27;</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                    <span class="hljs-comment"># The attention_mask can be None when there is no padding to the input sequence.</span><br>                    <span class="hljs-comment"># However, an attention_mask Tensor must be passed into cudagraph for replay, so</span><br>                    <span class="hljs-comment"># we create an equivalent zero Tensor as the attention_mask.</span><br>                    optional_inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>] = get_zero_attention_mask(<br>                        hidden_states.size(<span class="hljs-number">0</span>), hidden_states.size(<span class="hljs-number">1</span>)<br>                    )<br>            <span class="hljs-keyword">except</span> ImportError:<br>                <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&quot;CUDAGraph requires TransformerEngine, but not installed&quot;</span>)<br>            <span class="hljs-keyword">return</span> <span class="hljs-built_in">tuple</span>(cudagraph_args), optional_inputs<br><br>        cg_index = self.current_microbatch % <span class="hljs-built_in">len</span>(self.cuda_graphs)<br>        <span class="hljs-keyword">assert</span> (<span class="hljs-string">&#x27;inference_context&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> kwargs <span class="hljs-keyword">or</span> kwargs[<span class="hljs-string">&#x27;inference_context&#x27;</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>) <span class="hljs-keyword">and</span> (<br>            <span class="hljs-string">&#x27;packed_seq_params&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> kwargs <span class="hljs-keyword">or</span> kwargs[<span class="hljs-string">&#x27;packed_seq_params&#x27;</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>        ), <span class="hljs-string">&quot;CUDA graph accepts only Tensor inputs.&quot;</span><br>        cudagraph_args, cudagraph_kwargs = _check_cuda_graph_replay_args(*args, **kwargs)<br><br>        <span class="hljs-keyword">for</span> hook, hook_args <span class="hljs-keyword">in</span> self.cuda_graph_manual_hooks:<br>            hook(*hook_args)<br>        cuda_graph_output = self.cuda_graphs[cg_index](*cudagraph_args, **cudagraph_kwargs)<br><br>        <span class="hljs-keyword">if</span> cudagraph_kwargs.get(<span class="hljs-string">&#x27;context&#x27;</span>) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            context = cuda_graph_output[-<span class="hljs-number">1</span>]<br>            cuda_graph_output = cuda_graph_output[:-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">else</span>:<br>            context = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> self.config.cuda_graph_scope == <span class="hljs-string">&quot;attn&quot;</span>:<br>            <span class="hljs-comment"># CUDA Graph only covers the attention layer. Feed-forward</span><br>            <span class="hljs-comment"># layer still goes through the normal pass.</span><br>            output = self._forward_mlp(*cuda_graph_output)<br>        <span class="hljs-keyword">else</span>:<br>            output = cuda_graph_output[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">return</span> output, context<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, *args, **kwargs</span>):<br>        <span class="hljs-comment"># Training and validation mode CUDA graphs</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(self, <span class="hljs-string">&#x27;cudagraph_manager&#x27;</span>) <span class="hljs-keyword">and</span> kwargs.get(<span class="hljs-string">&#x27;inference_context&#x27;</span>) <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span> self.cudagraph_manager(self, args, kwargs)<br>        <span class="hljs-comment"># Inference mode. CUDA graphs are used in the decode phase only, when attn mask is None</span><br>        <span class="hljs-keyword">elif</span> <span class="hljs-keyword">not</span> self.training <span class="hljs-keyword">and</span> (<br>            <span class="hljs-built_in">hasattr</span>(self, <span class="hljs-string">&#x27;cudagraph_manager&#x27;</span>)<br>            <span class="hljs-keyword">and</span> kwargs[<span class="hljs-string">&#x27;attention_mask&#x27;</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">and</span> (<br>                (<br>                    kwargs.get(<span class="hljs-string">&#x27;inference_context&#x27;</span>) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>                    <span class="hljs-keyword">and</span> kwargs[<span class="hljs-string">&#x27;inference_context&#x27;</span>].is_decode_only()<br>                )<br>                <span class="hljs-keyword">or</span> (<br>                    kwargs.get(<span class="hljs-string">&#x27;inference_params&#x27;</span>) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>                    <span class="hljs-keyword">and</span> kwargs[<span class="hljs-string">&#x27;inference_params&#x27;</span>].is_decode_only()<br>                )<br>            )<br>        ):<br>            <span class="hljs-keyword">assert</span> (<br>                kwargs.get(<span class="hljs-string">&#x27;attention_mask&#x27;</span>) <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>            ), <span class="hljs-string">f&quot;Attention mask must not be set when using CUDA graphs for decode&quot;</span><br>            <span class="hljs-keyword">return</span> self.cudagraph_manager(self, args, kwargs)<br>        <span class="hljs-keyword">elif</span> (<br>            self.config.external_cuda_graph<br>            <span class="hljs-keyword">and</span> self.training<br>            <span class="hljs-keyword">and</span> (is_graph_capturing() <span class="hljs-keyword">or</span> self.cuda_graphs)<br>        ):<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.cuda_graphs:<br>                <span class="hljs-comment"># Do CUDA Graphs capture.</span><br>                cuda_graph_func = self._cuda_graph_capture<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># Do CUDA Graphs replay.</span><br>                cuda_graph_func = self._cuda_graph_replay<br>            <span class="hljs-keyword">return</span> cuda_graph_func(*args, **kwargs)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">super</span>(MegatronModule, self).__call__(*args, **kwargs)<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>其初始化的时候初始化了下面几个模块：</p>
<ol>
<li><p>Input Layernorm：对输入数据进行可选的层归一化</p>
</li>
<li><p>SelfAttention</p>
</li>
<li><p>BiasDropoutFusion</p>
</li>
<li><p>Post SelfAttention：自注意力后的可选层归一化</p>
</li>
<li><p>CrossAttention</p>
</li>
<li><p>BiasDropoutFusion</p>
</li>
<li><p>Pre MLP：MLP 前的可选层归一化</p>
</li>
<li><p>MLP block</p>
</li>
<li><p>BiasDropoutFusion</p>
</li>
</ol>
</li>
</ul>
<p>我们下面再看一下SelfAttention模块是如何设计的，尤其关注其与TP并行相关的内容</p>
<h5 id="SelfAttention"><a href="#SelfAttention" class="headerlink" title="SelfAttention"></a>SelfAttention</h5><p>SelfAttention的相关代码如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SelfAttention</span>(<span class="hljs-title class_ inherited__">Attention</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Self-attention layer class</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Self-attention layer takes input with size [s, b, h]</span><br><span class="hljs-string">    and returns output of the same size.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        config: TransformerConfig,</span><br><span class="hljs-params">        submodules: SelfAttentionSubmodules,</span><br><span class="hljs-params">        layer_number: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        attn_mask_type=AttnMaskType.padding,</span><br><span class="hljs-params">        cp_comm_type: <span class="hljs-built_in">str</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        model_comm_pgs: ModelCommProcessGroups = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__(<br>            config=config,<br>            submodules=submodules,<br>            layer_number=layer_number,<br>            attn_mask_type=attn_mask_type,<br>            attention_type=<span class="hljs-string">&quot;self&quot;</span>,<br>            cp_comm_type=cp_comm_type,<br>            model_comm_pgs=model_comm_pgs,<br>        )<br><br>        self.linear_qkv = build_module(<br>            submodules.linear_qkv,<br>            self.config.hidden_size,<br>            self.query_projection_size + <span class="hljs-number">2</span> * self.kv_projection_size,<br>            config=self.config,<br>            init_method=self.config.init_method,<br>            gather_output=<span class="hljs-literal">False</span>,<br>            bias=self.config.add_bias_linear <span class="hljs-keyword">or</span> self.config.add_qkv_bias,<br>            skip_bias_add=<span class="hljs-literal">False</span>,<br>            is_expert=<span class="hljs-literal">False</span>,<br>            tp_comm_buffer_name=<span class="hljs-string">&#x27;qkv&#x27;</span>,<br>            tp_group=self.model_comm_pgs.tp,<br>        )<br><br>        <span class="hljs-keyword">if</span> submodules.q_layernorm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            self.q_layernorm = build_module(<br>                submodules.q_layernorm,<br>                hidden_size=self.hidden_size_per_attention_head,<br>                config=self.config,<br>                eps=self.config.layernorm_epsilon,<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            self.q_layernorm = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> submodules.k_layernorm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            self.k_layernorm = build_module(<br>                submodules.k_layernorm,<br>                hidden_size=self.hidden_size_per_attention_head,<br>                config=self.config,<br>                eps=self.config.layernorm_epsilon,<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            self.k_layernorm = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run_realtime_tests</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Performs a consistency check.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This function makes sure that tensors across devices are the same during an experiment.</span><br><span class="hljs-string">        This is often not guaranteed to be so because of silent hardware failures (eg, memory</span><br><span class="hljs-string">        corruption loading a checkpoint, network traffic corruption encountered during</span><br><span class="hljs-string">        data transmission).</span><br><span class="hljs-string"></span><br><span class="hljs-string">        (TODO) In the future, more tensors should be checked across the training run and</span><br><span class="hljs-string">        checked every X iterations. This is left for future work. Equality of tensors is probably</span><br><span class="hljs-string">        not required; transmitting hashes is sufficient.&quot;&quot;&quot;</span><br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.config.qk_layernorm:<br>            <span class="hljs-keyword">return</span><br><br>        <span class="hljs-comment"># check that all tensor parallel and data parallel ranks have the same</span><br>        <span class="hljs-comment"># Q &amp; K layernorm parameters.</span><br>        rank = get_data_parallel_rank()<br>        inputs = torch.stack(<br>            [<br>                self.q_layernorm.weight.data,<br>                self.q_layernorm.bias.data,<br>                self.k_layernorm.weight.data,<br>                self.k_layernorm.bias.data,<br>            ]<br>        )<br>        dp_list = [torch.empty_like(inputs) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(get_data_parallel_world_size())]<br>        dp_list[rank] = inputs<br>        torch.distributed.all_gather(dp_list, inputs, group=get_data_parallel_group())<br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">_compare</span>(<span class="hljs-params">srcs, tgts, names, parallelism</span>):<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(srcs) == <span class="hljs-built_in">len</span>(tgts) == <span class="hljs-built_in">len</span>(names)<br>            <span class="hljs-keyword">for</span> src, tgt, name <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(srcs, tgts, names):<br>                <span class="hljs-keyword">assert</span> torch.<span class="hljs-built_in">all</span>(src == tgt), (<br>                    <span class="hljs-string">f&quot;Discrepancy between <span class="hljs-subst">&#123;name&#125;</span> in <span class="hljs-subst">&#123;parallelism&#125;</span> ranks <span class="hljs-subst">&#123;i&#125;</span> and <span class="hljs-subst">&#123;rank&#125;</span>. &quot;</span><br>                    <span class="hljs-string">f&quot;Diff: <span class="hljs-subst">&#123;torch.norm(src - tgt)&#125;</span>&quot;</span><br>                )<br><br>        <span class="hljs-keyword">for</span> i, dp <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dp_list):<br>            q_w, q_b, k_w, k_b = torch.unbind(dp)<br>            _compare(<br>                [q_w, q_b, k_w, k_b],<br>                [<br>                    self.q_layernorm.weight.data,<br>                    self.q_layernorm.bias.data,<br>                    self.k_layernorm.weight.data,<br>                    self.k_layernorm.bias.data,<br>                ],<br>                [<span class="hljs-string">&quot;q_w&quot;</span>, <span class="hljs-string">&quot;q_b&quot;</span>, <span class="hljs-string">&quot;k_w&quot;</span>, <span class="hljs-string">&quot;k_b&quot;</span>],<br>                <span class="hljs-string">&quot;DP&quot;</span>,<br>            )<br><br>        rank = get_tensor_model_parallel_rank()<br>        tp_list = [torch.empty_like(inputs) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(get_tensor_model_parallel_world_size())]<br>        tp_list[rank] = inputs<br>        torch.distributed.all_gather(tp_list, inputs, group=get_tensor_model_parallel_group())<br><br>        <span class="hljs-keyword">for</span> i, tp <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tp_list):<br>            q_w, q_b, k_w, k_b = torch.unbind(tp)<br>            _compare(<br>                [q_w, q_b, k_w, k_b],<br>                [<br>                    self.q_layernorm.weight.data,<br>                    self.q_layernorm.bias.data,<br>                    self.k_layernorm.weight.data,<br>                    self.k_layernorm.bias.data,<br>                ],<br>                [<span class="hljs-string">&quot;q_w&quot;</span>, <span class="hljs-string">&quot;q_b&quot;</span>, <span class="hljs-string">&quot;k_w&quot;</span>, <span class="hljs-string">&quot;k_b&quot;</span>],<br>                <span class="hljs-string">&quot;TP&quot;</span>,<br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_query_key_value_tensors</span>(<span class="hljs-params">self, hidden_states, key_value_states=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Derives `query`, `key` and `value` tensors from `hidden_states`.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Attention heads [sq, b, h] --&gt; [sq, b, ng * (np/ng + 2) * hn)]</span><br>        mixed_qkv, _ = self.linear_qkv(hidden_states)<br><br>        <span class="hljs-comment"># [sq, b, hp] --&gt; [sq, b, ng, (np/ng + 2) * hn]</span><br>        new_tensor_shape = mixed_qkv.size()[:-<span class="hljs-number">1</span>] + (<br>            self.num_query_groups_per_partition,<br>            (<br>                (self.num_attention_heads_per_partition // self.num_query_groups_per_partition + <span class="hljs-number">2</span>)<br>                * self.hidden_size_per_attention_head<br>            ),<br>        )<br>        mixed_qkv = mixed_qkv.view(*new_tensor_shape)<br><br>        split_arg_list = [<br>            (<br>                self.num_attention_heads_per_partition<br>                // self.num_query_groups_per_partition<br>                * self.hidden_size_per_attention_head<br>            ),<br>            self.hidden_size_per_attention_head,<br>            self.hidden_size_per_attention_head,<br>        ]<br><br>        <span class="hljs-keyword">if</span> SplitAlongDim <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br><br>            <span class="hljs-comment"># [sq, b, ng, (np/ng + 2) * hn]</span><br>            <span class="hljs-comment"># --&gt; [sq, b, ng, np/ng * hn], [sq, b, ng, hn], [sq, b, ng, hn]</span><br>            (query, key, value) = SplitAlongDim(mixed_qkv, <span class="hljs-number">3</span>, split_arg_list)<br>        <span class="hljs-keyword">else</span>:<br><br>            <span class="hljs-comment"># [sq, b, ng, (np/ng + 2) * hn]</span><br>            <span class="hljs-comment"># --&gt; [sq, b, ng, np/ng * hn], [sq, b, ng, hn], [sq, b, ng, hn]</span><br>            (query, key, value) = torch.split(mixed_qkv, split_arg_list, dim=<span class="hljs-number">3</span>)<br><br>        <span class="hljs-comment"># [sq, b, ng, np/ng * hn] -&gt; [sq, b, np, hn]</span><br>        query = query.reshape(query.size(<span class="hljs-number">0</span>), query.size(<span class="hljs-number">1</span>), -<span class="hljs-number">1</span>, self.hidden_size_per_attention_head)<br><br>        <span class="hljs-keyword">if</span> self.q_layernorm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            query = self.q_layernorm(query)<br><br>        <span class="hljs-keyword">if</span> self.k_layernorm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            key = self.k_layernorm(key)<br><br>        <span class="hljs-keyword">if</span> self.config.test_mode:<br>            self.run_realtime_tests()<br><br>        <span class="hljs-keyword">return</span> query, key, value<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward_dw</span>(<span class="hljs-params">self</span>) -&gt; NoReturn:<br>        <span class="hljs-string">&quot;&quot;&quot;Execute weight update operations&quot;&quot;&quot;</span><br>        self._backward_qkv_proj()<br>        self._backward_output_proj()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_backward_qkv_proj</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Update weights for QKV projection layer&quot;&quot;&quot;</span><br>        self.linear_qkv.backward_dw()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_backward_output_proj</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Update weights for output projection layer&quot;&quot;&quot;</span><br>        self.linear_proj.backward_dw()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">set_for_recompute_input_layernorm</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Set the attention layer for recompute input_layernorm. Only needed for fp8.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">from</span> megatron.core.extensions.transformer_engine <span class="hljs-keyword">import</span> set_save_original_input<br><br>        set_save_original_input(self.linear_qkv)<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>在初始化时：</p>
<ul>
<li><p>尤其它是拓展了<code>Attention</code>类，所以其首先对Attention进行了初始化，Attention的初始化代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Attention</span>(MegatronModule, ABC):<br>    <span class="hljs-string">&quot;&quot;&quot;Attention layer abstract class.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    This layer only contains common modules required for the &quot;self attn&quot; and</span><br><span class="hljs-string">    &quot;cross attn&quot; specializations.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        config: TransformerConfig,</span><br><span class="hljs-params">        submodules: <span class="hljs-type">Union</span>[SelfAttentionSubmodules, CrossAttentionSubmodules],</span><br><span class="hljs-params">        layer_number: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        attn_mask_type: AttnMaskType,</span><br><span class="hljs-params">        attention_type: <span class="hljs-built_in">str</span>,</span><br><span class="hljs-params">        cp_comm_type: <span class="hljs-built_in">str</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        model_comm_pgs: ModelCommProcessGroups = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__(config=config)<br><br>        self.config = config<br>        self.layer_number = layer_number<br>        self.attn_mask_type = attn_mask_type<br>        self.attention_type = attention_type<br><br>        <span class="hljs-comment"># For normal attention without groups, num_query_groups == num_attention_heads,</span><br>        <span class="hljs-comment"># so these two will be the same</span><br>        self.query_projection_size = self.config.kv_channels * self.config.num_attention_heads<br>        self.kv_projection_size = self.config.kv_channels * self.config.num_query_groups<br><br>        <span class="hljs-keyword">if</span> model_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            model_comm_pgs = ModelCommProcessGroups.use_mpu_process_groups(<br>                required_pgs=[<span class="hljs-string">&#x27;tp&#x27;</span>, <span class="hljs-string">&#x27;cp&#x27;</span>]<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(<br>                model_comm_pgs, <span class="hljs-string">&#x27;tp&#x27;</span><br>            ), <span class="hljs-string">&quot;Attention model_comm_pgs must have tp process group&quot;</span><br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(<br>                model_comm_pgs, <span class="hljs-string">&#x27;cp&#x27;</span><br>            ), <span class="hljs-string">&quot;Attention model_comm_pgs must have cp process group&quot;</span><br>        self.model_comm_pgs = model_comm_pgs<br><br>        <span class="hljs-comment"># Per attention head and per partition values</span><br>        world_size = get_pg_size(self.model_comm_pgs.tp)<br>        self.hidden_size_per_attention_head = divide(<br>            self.query_projection_size, self.config.num_attention_heads<br>        )<br>        self.num_attention_heads_per_partition = divide(self.config.num_attention_heads, world_size)<br>        self.num_query_groups_per_partition = divide(self.config.num_query_groups, world_size)<br><br>        <span class="hljs-comment"># To support both CUDA Graphs and key value with different hidden size</span><br>        self.key_hidden_size = self.hidden_size_per_attention_head<br>        self.val_hidden_size = self.hidden_size_per_attention_head<br><br>        self.core_attention = build_module(<br>            submodules.core_attention,<br>            config=self.config,<br>            layer_number=self.layer_number,<br>            attn_mask_type=self.attn_mask_type,<br>            attention_type=self.attention_type,<br>            cp_comm_type=cp_comm_type,<br>            softmax_scale=self.config.softmax_scale,<br>            model_comm_pgs=self.model_comm_pgs,<br>        )<br><br>        self.checkpoint_core_attention = (<br>            self.config.recompute_granularity == <span class="hljs-string">&#x27;selective&#x27;</span><br>            <span class="hljs-keyword">and</span> <span class="hljs-string">&quot;core_attn&quot;</span> <span class="hljs-keyword">in</span> self.config.recompute_modules<br>        )<br><br>        <span class="hljs-comment"># Output.</span><br>        self.linear_proj = build_module(<br>            submodules.linear_proj,<br>            self.query_projection_size,<br>            self.config.hidden_size,<br>            config=self.config,<br>            init_method=self.config.output_layer_init_method,<br>            bias=self.config.add_bias_linear,<br>            input_is_parallel=<span class="hljs-literal">True</span>,<br>            skip_bias_add=<span class="hljs-literal">True</span>,<br>            is_expert=<span class="hljs-literal">False</span>,<br>            tp_comm_buffer_name=<span class="hljs-string">&#x27;proj&#x27;</span>,<br>            tp_group=self.model_comm_pgs.tp,<br>        )<br><br>        <span class="hljs-keyword">if</span> (<br>            HAVE_TE<br>            <span class="hljs-keyword">and</span> self.config.fp8<br>            <span class="hljs-keyword">and</span> self.config.fp8_recipe != <span class="hljs-string">&#x27;delayed&#x27;</span><br>            <span class="hljs-keyword">and</span> is_te_min_version(<span class="hljs-string">&quot;2.6.0dev0&quot;</span>)<br>            <span class="hljs-keyword">and</span> <span class="hljs-built_in">isinstance</span>(self.linear_proj, TELinear)<br>        ):<br>            <span class="hljs-comment"># For fp8 training, the output of the fused core_attn is saved by itself, and</span><br>            <span class="hljs-comment"># linear_proj also saves the quantized tensor of this output. Here we set the</span><br>            <span class="hljs-comment"># linear_proj to save the original input tensors to avoid the extra memory usage of</span><br>            <span class="hljs-comment"># the quantized tensor.</span><br>            set_save_original_input(self.linear_proj)<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>在计算q、k、v的输出维度时，其单独计算了q的维度（<code>self.query_projection_size = self.config.kv_channels * self.config.num_attention_heads</code>），再计算了k与v的维度（<code>self.kv_projection_size = self.config.kv_channels * self.config.num_query_groups</code>），因为在类似在GQA&#x2F;MQA中<code>self.config.num_attention_heads</code>与<code>self.config.num_query_groups</code>可能不同</p>
</li>
<li><p>然后基于TP并行度切分了q所对应的<code>self.config.num_attention_heads</code>个数，还切分了kv所对应的<code>self.config.num_query_groups</code>，注意这里如果不能整除的话会直接报错，所以运行起来的必然是每个TP rank都有均匀切分的q、k、v</p>
</li>
<li><p>然后其构建了core attention，在本地模式中使用的是<code>DotProductAttention</code>，代码如下所示，其主要是在Forward时负责依据传入的q、k、v、attention_mask等计算attention结果，</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DotProductAttention</span>(<span class="hljs-title class_ inherited__">MegatronModule</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Region where selective activation recomputation is applied.</span><br><span class="hljs-string">    This region is memory intensive but less compute intensive which</span><br><span class="hljs-string">    makes activation checkpointing more efficient for LLMs (20B+).</span><br><span class="hljs-string">    See Reducing Activation Recomputation in Large Transformer Models:</span><br><span class="hljs-string">    https://arxiv.org/abs/2205.05198 for more details.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    We use the following notation:</span><br><span class="hljs-string">     h: hidden size</span><br><span class="hljs-string">     n: number of attention heads</span><br><span class="hljs-string">     p: number of tensor model parallel partitions</span><br><span class="hljs-string">     b: batch size</span><br><span class="hljs-string">     s: sequence length</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        config: TransformerConfig,</span><br><span class="hljs-params">        layer_number: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        attn_mask_type: AttnMaskType,</span><br><span class="hljs-params">        attention_type: <span class="hljs-built_in">str</span>,</span><br><span class="hljs-params">        attention_dropout: <span class="hljs-built_in">float</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        softmax_scale: <span class="hljs-built_in">float</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        cp_comm_type: <span class="hljs-built_in">str</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        model_comm_pgs: ModelCommProcessGroups = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__(config=config)<br><br>        self.config: TransformerConfig = config<br><br>        <span class="hljs-keyword">assert</span> (<br>            self.config.context_parallel_size == <span class="hljs-number">1</span><br>        ), <span class="hljs-string">&quot;Context parallelism is only supported by TEDotProductAttention!&quot;</span><br><br>        <span class="hljs-keyword">assert</span> (<br>            self.config.window_size <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>        ), <span class="hljs-string">&quot;Sliding Window Attention is only supported by TEDotProductAttention!&quot;</span><br><br>        self.layer_number = <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, layer_number)<br>        self.attn_mask_type = attn_mask_type<br>        self.attention_type = attention_type  <span class="hljs-comment"># unused for now</span><br><br>        projection_size = self.config.kv_channels * self.config.num_attention_heads<br><br>        <span class="hljs-comment"># Per attention head and per partition values.</span><br>        <span class="hljs-keyword">if</span> model_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># For backward compatibility, remove in v0.14 and raise error</span><br>            <span class="hljs-comment"># raise ValueError(&quot;DotProductAttention was called without ModelCommProcessGroups&quot;)</span><br>            model_comm_pgs = ModelCommProcessGroups.use_mpu_process_groups(required_pgs=[<span class="hljs-string">&#x27;tp&#x27;</span>])<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(<br>                model_comm_pgs, <span class="hljs-string">&#x27;tp&#x27;</span><br>            ), <span class="hljs-string">&quot;DotProductAttention model_comm_pgs must have tp process group&quot;</span><br><br>        world_size = model_comm_pgs.tp.size()<br>        self.hidden_size_per_partition = divide(projection_size, world_size)<br>        self.hidden_size_per_attention_head = divide(projection_size, config.num_attention_heads)<br>        self.num_attention_heads_per_partition = divide(self.config.num_attention_heads, world_size)<br>        self.num_query_groups_per_partition = divide(self.config.num_query_groups, world_size)<br><br>        coeff = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> softmax_scale <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            self.softmax_scale = <span class="hljs-number">1.0</span> / math.sqrt(self.hidden_size_per_attention_head)<br>        <span class="hljs-keyword">else</span>:<br>            self.softmax_scale = softmax_scale<br><br>        <span class="hljs-keyword">if</span> self.config.apply_query_key_layer_scaling:<br>            coeff = self.layer_number<br>            self.softmax_scale /= coeff<br><br>        self.scale_mask_softmax = FusedScaleMaskSoftmax(<br>            input_in_fp16=self.config.fp16,<br>            input_in_bf16=self.config.bf16,<br>            attn_mask_type=self.attn_mask_type,<br>            scaled_masked_softmax_fusion=self.config.masked_softmax_fusion,<br>            mask_func=attention_mask_func,<br>            softmax_in_fp32=self.config.attention_softmax_in_fp32,<br>            scale=coeff,<br>        )<br><br>        <span class="hljs-comment"># Dropout. Note that for a single iteration, this layer will generate</span><br>        <span class="hljs-comment"># different outputs on different number of parallel partitions but</span><br>        <span class="hljs-comment"># on average it should not be partition dependent.</span><br>        self.attention_dropout = torch.nn.Dropout(<br>            self.config.attention_dropout <span class="hljs-keyword">if</span> attention_dropout <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> attention_dropout<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        query: Tensor,</span><br><span class="hljs-params">        key: Tensor,</span><br><span class="hljs-params">        value: Tensor,</span><br><span class="hljs-params">        attention_mask: Tensor,</span><br><span class="hljs-params">        attn_mask_type: AttnMaskType = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        attention_bias: Tensor = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        packed_seq_params: <span class="hljs-type">Optional</span>[PackedSeqParams] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Forward.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">assert</span> packed_seq_params <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, (<br>            <span class="hljs-string">&quot;Packed sequence is not supported by DotProductAttention.&quot;</span><br>            <span class="hljs-string">&quot;Please use TEDotProductAttention instead.&quot;</span><br>        )<br>        <span class="hljs-keyword">assert</span> attention_bias <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;Attention bias is not supported for DotProductAttention.&quot;</span><br><br>        <span class="hljs-comment"># ===================================</span><br>        <span class="hljs-comment"># Raw attention scores. [b, n/p, s, s]</span><br>        <span class="hljs-comment"># ===================================</span><br><br>        <span class="hljs-comment"># expand the key and value [sk, b, ng, hn] -&gt; [sk, b, np, hn]</span><br>        <span class="hljs-comment"># This is a noop for normal attention where ng == np. When using group query attention this</span><br>        <span class="hljs-comment"># creates a view that has the keys and values virtually repeated along their dimension to</span><br>        <span class="hljs-comment"># match the number of queries.</span><br><br>        <span class="hljs-comment"># attn_mask_type is not used.</span><br>        <span class="hljs-keyword">if</span> self.num_attention_heads_per_partition // self.num_query_groups_per_partition &gt; <span class="hljs-number">1</span>:<br>            key = key.repeat_interleave(<br>                self.num_attention_heads_per_partition // self.num_query_groups_per_partition, dim=<span class="hljs-number">2</span><br>            )<br>            value = value.repeat_interleave(<br>                self.num_attention_heads_per_partition // self.num_query_groups_per_partition, dim=<span class="hljs-number">2</span><br>            )<br><br>        <span class="hljs-comment"># [b, np, sq, sk]</span><br>        output_size = (query.size(<span class="hljs-number">1</span>), query.size(<span class="hljs-number">2</span>), query.size(<span class="hljs-number">0</span>), key.size(<span class="hljs-number">0</span>))<br><br>        <span class="hljs-comment"># [sq, b, np, hn] -&gt; [sq, b * np, hn]</span><br>        <span class="hljs-comment"># This will be a simple view when doing normal attention, but in group query attention</span><br>        <span class="hljs-comment"># the key and value tensors are repeated to match the queries so you can&#x27;t use</span><br>        <span class="hljs-comment"># simple strides to extract the queries.</span><br>        query = query.reshape(output_size[<span class="hljs-number">2</span>], output_size[<span class="hljs-number">0</span>] * output_size[<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># [sk, b, np, hn] -&gt; [sk, b * np, hn]</span><br>        key = key.view(output_size[<span class="hljs-number">3</span>], output_size[<span class="hljs-number">0</span>] * output_size[<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># preallocting input tensor: [b * np, sq, sk]</span><br>        matmul_input_buffer = parallel_state.get_global_memory_buffer().get_tensor(<br>            (output_size[<span class="hljs-number">0</span>] * output_size[<span class="hljs-number">1</span>], output_size[<span class="hljs-number">2</span>], output_size[<span class="hljs-number">3</span>]), query.dtype, <span class="hljs-string">&quot;mpu&quot;</span><br>        )<br><br>        <span class="hljs-comment"># Raw attention scores. [b * np, sq, sk]</span><br>        matmul_result = torch.baddbmm(<br>            matmul_input_buffer,<br>            query.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>),  <span class="hljs-comment"># [b * np, sq, hn]</span><br>            key.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>),  <span class="hljs-comment"># [b * np, hn, sk]</span><br>            beta=<span class="hljs-number">0.0</span>,<br>            alpha=self.softmax_scale,<br>        )<br><br>        <span class="hljs-comment"># change view to [b, np, sq, sk]</span><br>        attention_scores = matmul_result.view(*output_size)<br><br>        <span class="hljs-comment"># ===========================</span><br>        <span class="hljs-comment"># Attention probs and dropout</span><br>        <span class="hljs-comment"># ===========================</span><br><br>        <span class="hljs-comment"># attention scores and attention mask [b, np, sq, sk]</span><br>        attention_probs: Tensor = self.scale_mask_softmax(attention_scores, attention_mask)<br><br>        <span class="hljs-comment"># This is actually dropping out entire tokens to attend to, which might</span><br>        <span class="hljs-comment"># seem a bit unusual, but is taken from the original Transformer paper.</span><br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.config.sequence_parallel:<br>            <span class="hljs-keyword">with</span> tensor_parallel.get_cuda_rng_tracker().fork():<br>                attention_probs = self.attention_dropout(attention_probs)<br>        <span class="hljs-keyword">else</span>:<br>            attention_probs = self.attention_dropout(attention_probs)<br><br>        <span class="hljs-comment"># =========================</span><br>        <span class="hljs-comment"># Context layer. [sq, b, hp]</span><br>        <span class="hljs-comment"># =========================</span><br><br>        <span class="hljs-comment"># value -&gt; context layer.</span><br>        <span class="hljs-comment"># [sk, b, np, hn] --&gt; [b, np, sq, hn]</span><br><br>        <span class="hljs-comment"># context layer shape: [b, np, sq, hn]</span><br>        output_size = (value.size(<span class="hljs-number">1</span>), value.size(<span class="hljs-number">2</span>), query.size(<span class="hljs-number">0</span>), value.size(<span class="hljs-number">3</span>))<br><br>        <span class="hljs-comment"># change view [sk, b * np, hn]</span><br>        value = value.view(value.size(<span class="hljs-number">0</span>), output_size[<span class="hljs-number">0</span>] * output_size[<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># change view [b * np, sq, sk]</span><br>        attention_probs = attention_probs.view(output_size[<span class="hljs-number">0</span>] * output_size[<span class="hljs-number">1</span>], output_size[<span class="hljs-number">2</span>], -<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># matmul: [b * np, sq, hn]</span><br>        context = torch.bmm(attention_probs, value.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br><br>        <span class="hljs-comment"># change view [b, np, sq, hn]</span><br>        context = context.view(*output_size)<br><br>        <span class="hljs-comment"># [b, np, sq, hn] --&gt; [sq, b, np, hn]</span><br>        context = context.permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>).contiguous()<br><br>        <span class="hljs-comment"># [sq, b, np, hn] --&gt; [sq, b, hp]</span><br>        new_context_shape = context.size()[:-<span class="hljs-number">2</span>] + (self.hidden_size_per_partition,)<br>        context = context.view(*new_context_shape)<br><br>        <span class="hljs-keyword">return</span> context<br><br></code></pre></td></tr></table></figure>

<ul>
<li>然后其构建了<code>linear_proj</code>，注意其使用的是<code>row_parallel_linear</code>，并且它也明确在参数中指出了其输入是并行的，符合一贯的先列并行再行并行计算的结果</li>
</ul>
</li>
<li><p>其创建了linear_qkv：</p>
<ul>
<li><p><code>linear_qkv</code>是<code>column_parallel_linear</code>类</p>
</li>
<li><p><code>linear_qkv</code>输入维度是标准的<code>self.config.hidden_size</code>，其输出维度是<code>self.query_projection_size + 2 * self.kv_projection_size</code>，因为<code>linear_qkv</code>需要投影生成q、k、v这3个基础张量</p>
</li>
<li><p>此外值得注意的是它还专门设计了<code>gather_output</code>为False，因为其本身就希望使用列并行来多注意力头计算</p>
</li>
</ul>
</li>
<li><p>然后还构建了<code>submodules.q_layernorm</code>与<code>submodules.k_layernorm</code></p>
</li>
</ul>
</li>
<li><p>在Forward中完全走的是Attention的代码如下所示，依据<code>nvtx_range_push</code>其相关流程可以划分为：</p>
<ol>
<li><p>计算出当前Sequence的q、k、v：</p>
<ul>
<li>其代码如下所示，</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_query_key_value_tensors</span>(<span class="hljs-params">self, hidden_states, key_value_states=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Derives `query`, `key` and `value` tensors from `hidden_states`.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># Attention heads [sq, b, h] --&gt; [sq, b, ng * (np/ng + 2) * hn)]</span><br>    mixed_qkv, _ = self.linear_qkv(hidden_states)<br><br>    <span class="hljs-comment"># [sq, b, hp] --&gt; [sq, b, ng, (np/ng + 2) * hn]</span><br>    new_tensor_shape = mixed_qkv.size()[:-<span class="hljs-number">1</span>] + (<br>        self.num_query_groups_per_partition,<br>        (<br>            (self.num_attention_heads_per_partition // self.num_query_groups_per_partition + <span class="hljs-number">2</span>)<br>            * self.hidden_size_per_attention_head<br>        ),<br>    )<br>    mixed_qkv = mixed_qkv.view(*new_tensor_shape)<br><br>    split_arg_list = [<br>        (<br>            self.num_attention_heads_per_partition<br>            // self.num_query_groups_per_partition<br>            * self.hidden_size_per_attention_head<br>        ),<br>        self.hidden_size_per_attention_head,<br>        self.hidden_size_per_attention_head,<br>    ]<br><br>    <span class="hljs-keyword">if</span> SplitAlongDim <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br><br>        <span class="hljs-comment"># [sq, b, ng, (np/ng + 2) * hn]</span><br>        <span class="hljs-comment"># --&gt; [sq, b, ng, np/ng * hn], [sq, b, ng, hn], [sq, b, ng, hn]</span><br>        (query, key, value) = SplitAlongDim(mixed_qkv, <span class="hljs-number">3</span>, split_arg_list)<br>    <span class="hljs-keyword">else</span>:<br><br>        <span class="hljs-comment"># [sq, b, ng, (np/ng + 2) * hn]</span><br>        <span class="hljs-comment"># --&gt; [sq, b, ng, np/ng * hn], [sq, b, ng, hn], [sq, b, ng, hn]</span><br>        (query, key, value) = torch.split(mixed_qkv, split_arg_list, dim=<span class="hljs-number">3</span>)<br><br>    <span class="hljs-comment"># [sq, b, ng, np/ng * hn] -&gt; [sq, b, np, hn]</span><br>    query = query.reshape(query.size(<span class="hljs-number">0</span>), query.size(<span class="hljs-number">1</span>), -<span class="hljs-number">1</span>, self.hidden_size_per_attention_head)<br><br>    <span class="hljs-keyword">if</span> self.q_layernorm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        query = self.q_layernorm(query)<br><br>    <span class="hljs-keyword">if</span> self.k_layernorm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        key = self.k_layernorm(key)<br><br>    <span class="hljs-keyword">if</span> self.config.test_mode:<br>        self.run_realtime_tests()<br><br>    <span class="hljs-keyword">return</span> query, key, value<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>首先通过<code>mixed_qkv, _ = self.linear_qkv(hidden_states)</code>得到<code>mixed_qkv</code>，因为<code>self.linear_qkv</code>是列并行并且初始化时设置了<code>gather_output=False</code>，所以得到的<code>mixed_qkv</code>是被TP并行划分后的部分结果，由于前面的检查，所以它必然是q、k、v维度的整数倍。故是从[sq,b,h]转化为了[sq,b,per_tp_num_query_groups *(per_tp_num_heads &#x2F; per_tp_num_query_groups + 2) * head_dim]，结果的最后一维是q、k、v的维度和</p>
</li>
<li><p>然后会把形状进行调整，最后得到q的维度为[sq,b,per_tp_num_heads,head_dim]，最后得到k与v的维度都是[sq,b,per_tp_num_query_groups, head_dim]</p>
</li>
</ul>
<blockquote>
<p>为什么要引入 <code>num_query_groups</code> 这一维？因为它在支持 <strong>GQA&#x2F;MQA</strong> 时很关键：</p>
<ul>
<li><p>普通 attention：<code>num_query_groups== num_heads</code>，每个 group 只有 1 个 query head，对应关系很直接。</p>
</li>
<li><p>GQA：<code>num_query_groups &lt; num_heads</code>，多个 query heads 共享同一组 K&#x2F;V（在同一个 group 下）。</p>
</li>
</ul>
</blockquote>
</li>
<li><p>调整key值</p>
</li>
<li><p>调用<code>rotary_pos_emb</code></p>
</li>
<li><p>调用<code>core_attention</code>进行计算</p>
</li>
<li><p>调用<code>linear_proj</code>得到最终结果，因为其是一个<code>row_parallel_linear</code>，所以最后会通过all reduce得到完整的结果</p>
</li>
</ol>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    hidden_states: Tensor,</span><br><span class="hljs-params">    attention_mask: Tensor,</span><br><span class="hljs-params">    key_value_states: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    inference_context: <span class="hljs-type">Optional</span>[BaseInferenceContext] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    rotary_pos_emb: <span class="hljs-type">Optional</span>[<span class="hljs-type">Union</span>[Tensor, <span class="hljs-type">Tuple</span>[Tensor, Tensor]]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    rotary_pos_cos: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    rotary_pos_sin: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    attention_bias: <span class="hljs-type">Optional</span>[Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    packed_seq_params: <span class="hljs-type">Optional</span>[PackedSeqParams] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    sequence_len_offset: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    *,</span><br><span class="hljs-params">    inference_params: <span class="hljs-type">Optional</span>[BaseInferenceContext] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Tuple</span>[Tensor, Tensor]:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Perform a forward pass through the attention module.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        hidden_states (Tensor): Hidden states.</span><br><span class="hljs-string">        attention_mask (Tensor): Attention mask.</span><br><span class="hljs-string">        key_value_states (Optional[Tensor]): Key/value states (for cross attention).</span><br><span class="hljs-string">        inference_context (Optional[BaseInferenceContext]): Inference context that manages</span><br><span class="hljs-string">            KV cache.</span><br><span class="hljs-string">        rotary_pos_emb (Optional[Union[Tensor, Tuple[Tensor, Tensor]]]): Rotary</span><br><span class="hljs-string">            embedding tensor(s).</span><br><span class="hljs-string">        rotary_pos_cos (Optional[Tensor]): Rotary embedding cosine.</span><br><span class="hljs-string">        rotary_pos_sin (Optional[Tensor]): Rotary embedding sine.</span><br><span class="hljs-string">        attention_bias (Optional[Tensor]): Attention bias.</span><br><span class="hljs-string">        packed_seq_params (Optional[PackedSeqparams]): Parameters used for THD format.</span><br><span class="hljs-string">        sequence_len_offset (Optional[int]): Sequence length offset used for</span><br><span class="hljs-string">            inference CUDA graphs.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Return:</span><br><span class="hljs-string">        (Tuple[Tensor, Tensor]) Attention output and bias.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># Check if we need to skip RoPE</span><br>    <span class="hljs-comment"># no_rope is 0-indexed array and self.layer_number is 1-indexed</span><br>    no_rope = (<br>        self.config.no_rope_freq[self.layer_number - <span class="hljs-number">1</span>] <span class="hljs-keyword">if</span> self.config.no_rope_freq <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span><br>    )<br>    <span class="hljs-keyword">if</span> no_rope:<br>        rotary_pos_emb = <span class="hljs-literal">None</span><br><br>    inference_context = deprecate_inference_params(inference_context, inference_params)<br><br>    <span class="hljs-keyword">if</span> inference_context <span class="hljs-keyword">and</span> inference_context.is_dynamic_batching():<br>        <span class="hljs-keyword">assert</span> HAVE_FA3 <span class="hljs-keyword">or</span> is_fa_min_version(<br>            <span class="hljs-string">&quot;2.7.3&quot;</span><br>        ), <span class="hljs-string">&quot;flash attn verion v2.7.3 and above is required for dynamic batching.&quot;</span><br><br>    <span class="hljs-comment"># hidden_states: [sq, b, h]</span><br>    <span class="hljs-keyword">if</span> self.config.flash_decode <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.training <span class="hljs-keyword">and</span> inference_context <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        rotary_pos_emb = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">assert</span> rotary_pos_cos <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> rotary_pos_sin <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># For self attention we just duplicate the rotary_pos_emb if it isn&#x27;t already</span><br>    <span class="hljs-keyword">if</span> rotary_pos_emb <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(rotary_pos_emb, <span class="hljs-built_in">tuple</span>):<br>        rotary_pos_emb = (rotary_pos_emb,) * <span class="hljs-number">2</span><br><br>    <span class="hljs-comment"># =====================</span><br>    <span class="hljs-comment"># Query, Key, and Value</span><br>    <span class="hljs-comment"># =====================</span><br>    <span class="hljs-comment"># Get the query, key and value tensors based on the type of attention -</span><br>    <span class="hljs-comment"># self or cross attn.</span><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;qkv&quot;</span>)<br>    query, key, value = self.get_query_key_value_tensors(hidden_states, key_value_states)<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;qkv&quot;</span>)<br><br>    <span class="hljs-comment"># ===================================================</span><br>    <span class="hljs-comment"># Adjust key, value, and rotary_pos_emb for inference</span><br>    <span class="hljs-comment"># ===================================================</span><br><br>    in_decode_mode = (<br>        inference_context <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">and</span> inference_context.is_decode_only()<br>        <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.training<br>    )<br><br>    <span class="hljs-comment"># This branch only runs in the decode phase of flash decoding and returns after the linear</span><br>    <span class="hljs-comment"># projection. This conditional is not used in the prefill phase or non-flash-decoding cases.</span><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;adjust_key_value&quot;</span>)<br>    <span class="hljs-keyword">if</span> in_decode_mode <span class="hljs-keyword">and</span> self.config.flash_decode:<br>        <span class="hljs-keyword">assert</span> self.layer_number <span class="hljs-keyword">in</span> inference_context.key_value_memory_dict<br>        <span class="hljs-keyword">assert</span> inference_context.sequence_len_offset <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        inference_key_memory, inference_value_memory = inference_context.key_value_memory_dict[<br>            self.layer_number<br>        ]<br>        output = self.flash_decode(<br>            sequence_len_offset=sequence_len_offset,<br>            query_layer=query,<br>            key_layer=key,<br>            value_layer=value,<br>            inference_key_memory=inference_key_memory,<br>            inference_value_memory=inference_value_memory,<br>            rotary_cos=rotary_pos_cos,<br>            rotary_sin=rotary_pos_sin,<br>            rotary_interleaved=self.config.rotary_interleaved,<br>        )<br>        out = output.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>).contiguous()<br>        context_layer = out.view(out.size(<span class="hljs-number">0</span>), out.size(<span class="hljs-number">1</span>), -<span class="hljs-number">1</span>)<br>        output, bias = self.linear_proj(context_layer)<br>        <span class="hljs-keyword">return</span> output, bias<br><br>    <span class="hljs-keyword">if</span> (<br>        in_decode_mode<br>        <span class="hljs-keyword">and</span> self.config.enable_cuda_graph<br>        <span class="hljs-keyword">and</span> self.config.cuda_graph_scope != <span class="hljs-string">&quot;full_iteration&quot;</span><br>        <span class="hljs-keyword">and</span> inference_context.is_static_batching()<br>    ):<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;CUDA graphs must use flash decode with static batching!&quot;</span>)<br><br>    query, key, value, rotary_pos_emb, attn_mask_type, block_table = (<br>        self._adjust_key_value_for_inference(<br>            inference_context,<br>            query,<br>            key,<br>            value,<br>            rotary_pos_emb,<br>            rotary_pos_cos,<br>            rotary_pos_sin,<br>            sequence_len_offset,<br>        )<br>    )<br><br>    <span class="hljs-keyword">if</span> packed_seq_params <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        query = query.squeeze(<span class="hljs-number">1</span>)<br>        key = key.squeeze(<span class="hljs-number">1</span>)<br>        value = value.squeeze(<span class="hljs-number">1</span>)<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;adjust_key_value&quot;</span>)<br><br>    <span class="hljs-comment"># ================================================</span><br>    <span class="hljs-comment"># relative positional embedding (rotary embedding)</span><br>    <span class="hljs-comment"># ================================================</span><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;rotary_pos_emb&quot;</span>)<br>    <span class="hljs-keyword">if</span> rotary_pos_emb <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.config.flash_decode:<br>        q_pos_emb, k_pos_emb = rotary_pos_emb<br><br>        <span class="hljs-keyword">if</span> packed_seq_params <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> packed_seq_params.cu_seqlens_q_padded <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                cu_seqlens_q = packed_seq_params.cu_seqlens_q_padded<br>            <span class="hljs-keyword">else</span>:<br>                cu_seqlens_q = packed_seq_params.cu_seqlens_q<br>            <span class="hljs-keyword">if</span> packed_seq_params.cu_seqlens_kv_padded <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                cu_seqlens_kv = packed_seq_params.cu_seqlens_kv_padded<br>            <span class="hljs-keyword">else</span>:<br>                cu_seqlens_kv = packed_seq_params.cu_seqlens_kv<br>        <span class="hljs-keyword">else</span>:<br>            cu_seqlens_q = cu_seqlens_kv = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> q_pos_emb <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># TODO VIJAY: simplify</span><br>            <span class="hljs-keyword">if</span> inference_context <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> inference_context.is_static_batching():<br>                query = apply_rotary_pos_emb(<br>                    query,<br>                    q_pos_emb,<br>                    config=self.config,<br>                    cu_seqlens=cu_seqlens_q,<br>                    cp_group=self.model_comm_pgs.cp,<br>                )<br>            <span class="hljs-keyword">else</span>:<br>                query = inference_context.apply_rotary_emb_query(<br>                    query, q_pos_emb, self.config, cu_seqlens_q, self.model_comm_pgs.cp<br>                )<br>        <span class="hljs-keyword">if</span> k_pos_emb <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            key = apply_rotary_pos_emb(<br>                key,<br>                k_pos_emb,<br>                config=self.config,<br>                cu_seqlens=cu_seqlens_kv,<br>                cp_group=self.model_comm_pgs.cp,<br>            )<br><br>        <span class="hljs-comment"># TODO, can apply positional embedding to value_layer so it has</span><br>        <span class="hljs-comment"># absolute positional embedding.</span><br>        <span class="hljs-comment"># otherwise, only relative positional embedding takes effect</span><br>        <span class="hljs-comment"># value_layer = apply_rotary_pos_emb(value_layer, k_pos_emb)</span><br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;rotary_pos_emb&quot;</span>)<br><br>    <span class="hljs-comment"># ==================================</span><br>    <span class="hljs-comment"># core attention computation</span><br>    <span class="hljs-comment"># ==================================</span><br><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;core_attention&quot;</span>)<br>    <span class="hljs-keyword">if</span> self.checkpoint_core_attention <span class="hljs-keyword">and</span> self.training:<br>        core_attn_out = self._checkpointed_attention_forward(<br>            query,<br>            key,<br>            value,<br>            attention_mask,<br>            attn_mask_type=attn_mask_type,<br>            attention_bias=attention_bias,<br>            packed_seq_params=packed_seq_params,<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">if</span> inference_context <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> inference_context.is_static_batching():<br>            <span class="hljs-comment"># Static batching attention kernel.</span><br>            core_attn_out = self.core_attention(<br>                query,<br>                key,<br>                value,<br>                attention_mask,<br>                attn_mask_type=attn_mask_type,<br>                attention_bias=attention_bias,<br>                packed_seq_params=packed_seq_params,<br>            )<br><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># Dynamic batching attention kernel.</span><br>            q, k, v = (query, key, value)<br>            cu_query_lengths, max_seqlen_q = inference_context.cu_query_lengths()<br>            cu_kv_lengths, kv_lengths, max_seqlen_k = inference_context.cu_kv_lengths()<br><br>            core_attn_out = self.flash_decode_and_prefill(<br>                q,<br>                k,<br>                v,<br>                max_seqlen_q,<br>                max_seqlen_k,<br>                cu_query_lengths,<br>                cu_kv_lengths,<br>                kv_lengths,<br>                block_table,<br>            )<br>            core_attn_out = rearrange(core_attn_out, <span class="hljs-string">&#x27;s b h d -&gt; s b (h d)&#x27;</span>)<br><br>    <span class="hljs-keyword">if</span> packed_seq_params <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> packed_seq_params.qkv_format == <span class="hljs-string">&#x27;thd&#x27;</span>:<br>        <span class="hljs-comment"># reshape to same output shape as unpacked case</span><br>        <span class="hljs-comment"># (t, np, hn) -&gt; (t, b=1, h=np*hn)</span><br>        <span class="hljs-comment"># t is the pack size = sum (sq_i)</span><br>        <span class="hljs-comment"># note that batch is a dummy dimension in the packed case</span><br>        core_attn_out = core_attn_out.reshape(core_attn_out.size(<span class="hljs-number">0</span>), <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;core_attention&quot;</span>)<br><br>    <span class="hljs-comment"># =================</span><br>    <span class="hljs-comment"># Output. [sq, b, h]</span><br>    <span class="hljs-comment"># =================</span><br><br>    nvtx_range_push(suffix=<span class="hljs-string">&quot;linear_proj&quot;</span>)<br>    output, bias = self.linear_proj(core_attn_out)<br>    nvtx_range_pop(suffix=<span class="hljs-string">&quot;linear_proj&quot;</span>)<br><br>    <span class="hljs-keyword">return</span> output, bias<br><br></code></pre></td></tr></table></figure>

<h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>在构建<code>GPTModel</code>时，在初始化时对<code>embedding</code>层使用的是<code>LanguageModelEmbedding</code>进行初始化，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> self.pre_process <span class="hljs-keyword">or</span> self.mtp_process:<br>    self.embedding = LanguageModelEmbedding(<br>        config=self.config,<br>        vocab_size=self.vocab_size,<br>        max_sequence_length=self.max_sequence_length,<br>        position_embedding_type=position_embedding_type,<br>        scatter_to_sequence_parallel=scatter_embedding_sequence_parallel,<br>        tp_group=self.model_comm_pgs.tp,<br>    )<br></code></pre></td></tr></table></figure>

<p><code>LanguageModelEmbedding</code>也会涉及到TP并行切分，因为词表可能会难以放入一个GPU内，所以就可以进行TP切分，每个GPU只保留一部分词表 embedding，然后在Forward时每个GPU只去获取在自己范围内的token的内容，最后all reduce得到完整的embedding。</p>
<h4 id="LanguageModelEmbedding"><a href="#LanguageModelEmbedding" class="headerlink" title="LanguageModelEmbedding"></a><code>LanguageModelEmbedding</code></h4><p><code>LanguageModelEmbedding</code>的代码如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">LanguageModelEmbedding</span>(<span class="hljs-title class_ inherited__">MegatronModule</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Language model embeddings.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        config (TransformerConfig): config object with all necessary configs for TransformerBlock</span><br><span class="hljs-string">        vocab_size (int): vocabulary size</span><br><span class="hljs-string">        max_sequence_length (int): maximum size of sequence. This</span><br><span class="hljs-string">                             is used for positional embedding</span><br><span class="hljs-string">        add_position_embedding (bool): Add a position embedding.</span><br><span class="hljs-string">        embedding_dropout_prob (float): dropout probability for embeddings</span><br><span class="hljs-string">        num_tokentypes (int): Set to 0 without binary head, and 2 with a binary head. Defaults to 0.</span><br><span class="hljs-string">        scatter_to_sequence_parallel (bool): Set to False to disable scatter of embedding</span><br><span class="hljs-string">            across sequence parallel region. Defaults to True.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        config: TransformerConfig,</span><br><span class="hljs-params">        vocab_size: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        max_sequence_length: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        position_embedding_type: <span class="hljs-type">Literal</span>[<span class="hljs-string">&#x27;learned_absolute&#x27;</span>, <span class="hljs-string">&#x27;rope&#x27;</span>, <span class="hljs-string">&#x27;none&#x27;</span>] = <span class="hljs-string">&#x27;learned_absolute&#x27;</span>,</span><br><span class="hljs-params">        num_tokentypes: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span>,</span><br><span class="hljs-params">        scatter_to_sequence_parallel: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        tp_group: <span class="hljs-type">Optional</span>[torch.distributed.ProcessGroup] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__(config=config)<br><br>        self.config: TransformerConfig = config<br>        self.vocab_size: <span class="hljs-built_in">int</span> = vocab_size<br>        self.max_sequence_length: <span class="hljs-built_in">int</span> = max_sequence_length<br>        self.add_position_embedding: <span class="hljs-built_in">bool</span> = position_embedding_type == <span class="hljs-string">&#x27;learned_absolute&#x27;</span><br>        self.num_tokentypes = num_tokentypes<br>        self.scatter_to_sequence_parallel = scatter_to_sequence_parallel<br>        self.tp_group = get_tensor_model_parallel_group_if_none(tp_group)<br>        self.reduce_scatter_embeddings = (<br>            (<span class="hljs-keyword">not</span> self.add_position_embedding)<br>            <span class="hljs-keyword">and</span> self.num_tokentypes &lt;= <span class="hljs-number">0</span><br>            <span class="hljs-keyword">and</span> self.config.sequence_parallel<br>            <span class="hljs-keyword">and</span> self.scatter_to_sequence_parallel<br>        )<br><br>        <span class="hljs-comment"># Word embeddings (parallel).</span><br>        self.word_embeddings = tensor_parallel.VocabParallelEmbedding(<br>            num_embeddings=self.vocab_size,<br>            embedding_dim=self.config.hidden_size,<br>            init_method=self.config.embedding_init_method,<br>            reduce_scatter_embeddings=self.reduce_scatter_embeddings,<br>            config=self.config,<br>            tp_group=self.tp_group,<br>        )<br><br>        <span class="hljs-comment"># Position embedding (serial).</span><br>        <span class="hljs-keyword">if</span> self.add_position_embedding:<br>            self.position_embeddings = torch.nn.Embedding(<br>                self.max_sequence_length, self.config.hidden_size<br>            )<br><br>            <span class="hljs-comment"># Initialize the position embeddings.</span><br>            <span class="hljs-keyword">if</span> self.config.perform_initialization:<br>                self.config.embedding_init_method(self.position_embeddings.weight)<br><br>        <span class="hljs-keyword">if</span> self.num_tokentypes &gt; <span class="hljs-number">0</span>:<br>            self.tokentype_embeddings = torch.nn.Embedding(<br>                self.num_tokentypes, self.config.hidden_size<br>            )<br>            <span class="hljs-comment"># Initialize the token-type embeddings.</span><br>            <span class="hljs-keyword">if</span> self.config.perform_initialization:<br>                self.config.embedding_init_method(self.tokentype_embeddings.weight)<br>        <span class="hljs-keyword">else</span>:<br>            self.tokentype_embeddings = <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># Embeddings dropout</span><br>        self.embedding_dropout = torch.nn.Dropout(self.config.hidden_dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">zero_parameters</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Zero out all parameters in embedding.&quot;&quot;&quot;</span><br>        self.word_embeddings.weight.data.fill_(<span class="hljs-number">0</span>)<br>        self.word_embeddings.weight.shared = <span class="hljs-literal">True</span><br>        self.position_embeddings.weight.data.fill_(<span class="hljs-number">0</span>)<br>        self.position_embeddings.weight.shared = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">if</span> self.num_tokentypes &gt; <span class="hljs-number">0</span>:<br>            self.tokentype_embeddings.weight.data.fill_(<span class="hljs-number">0</span>)<br>            self.tokentype_embeddings.weight.shared = <span class="hljs-literal">True</span><br><br><span class="hljs-meta">    @nvtx_decorator()</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input_ids: Tensor, position_ids: Tensor, tokentype_ids: <span class="hljs-built_in">int</span> = <span class="hljs-literal">None</span></span>) -&gt; Tensor:<br>        <span class="hljs-string">&quot;&quot;&quot;Forward pass of the embedding module.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            input_ids (Tensor): The input tokens</span><br><span class="hljs-string">            position_ids (Tensor): The position id&#x27;s used to calculate position embeddings</span><br><span class="hljs-string">            tokentype_ids (int): The token type ids. Used when args.bert_binary_head is</span><br><span class="hljs-string">                set to True. Defaults to None</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Tensor: The output embeddings</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        word_embeddings = self.word_embeddings(input_ids)<br>        <span class="hljs-keyword">if</span> self.add_position_embedding:<br>            position_embeddings = self.position_embeddings(position_ids)<br>            embeddings = word_embeddings + position_embeddings<br>        <span class="hljs-keyword">else</span>:<br>            embeddings = word_embeddings<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.reduce_scatter_embeddings:<br>            <span class="hljs-comment"># Data format change to avoid explicit tranposes : [b s h] --&gt; [s b h].</span><br>            embeddings = embeddings.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>).contiguous()<br><br>        <span class="hljs-keyword">if</span> tokentype_ids <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">assert</span> self.tokentype_embeddings <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>            <span class="hljs-comment"># [b s h] -&gt; [s b h] (So that it can be added with embeddings)</span><br>            tokentype_embedding = self.tokentype_embeddings(tokentype_ids).permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br>            embeddings = embeddings + tokentype_embedding<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">assert</span> self.tokentype_embeddings <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># If the input flag for fp32 residual connection is set, convert for float.</span><br>        <span class="hljs-keyword">if</span> self.config.fp32_residual_connection:<br>            embeddings = embeddings.<span class="hljs-built_in">float</span>()<br><br>        <span class="hljs-comment"># Dropout.</span><br>        <span class="hljs-keyword">if</span> self.config.sequence_parallel:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.reduce_scatter_embeddings <span class="hljs-keyword">and</span> self.scatter_to_sequence_parallel:<br>                embeddings = tensor_parallel.scatter_to_sequence_parallel_region(<br>                    embeddings, group=self.tp_group<br>                )<br>            <span class="hljs-comment"># `scatter_to_sequence_parallel_region` returns a view, which prevents</span><br>            <span class="hljs-comment"># the original tensor from being garbage collected. Clone to facilitate GC.</span><br>            <span class="hljs-comment"># Has a small runtime cost (~0.5%).</span><br>            <span class="hljs-keyword">if</span> self.config.clone_scatter_output_in_embedding <span class="hljs-keyword">and</span> self.scatter_to_sequence_parallel:<br>                embeddings = embeddings.clone()<br>            <span class="hljs-keyword">with</span> tensor_parallel.get_cuda_rng_tracker().fork():<br>                embeddings = self.embedding_dropout(embeddings)<br>        <span class="hljs-keyword">else</span>:<br>            embeddings = self.embedding_dropout(embeddings)<br><br>        <span class="hljs-keyword">return</span> embeddings<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>在初始化时，其使用<code>tensor_parallel.VocabParallelEmbedding</code>进行初始化，</p>
<ul>
<li><code>VocabParallelEmbedding</code>的代码如下所示</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">VocabParallelEmbedding</span>(torch.nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Embedding parallelized in the vocabulary dimension.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    This is mainly adapted from torch.nn.Embedding and all the default</span><br><span class="hljs-string">    values are kept.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        num_embeddings: vocabulary size.</span><br><span class="hljs-string">        embedding_dim: size of hidden state.</span><br><span class="hljs-string">        reduce_scatter_embeddings: Decides whether to perform ReduceScatter after embedding lookup</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Keyword Args:</span><br><span class="hljs-string">        config: A megatron.core.ModelParallelConfig object</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        num_embeddings: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        embedding_dim: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">        *,</span><br><span class="hljs-params">        init_method: <span class="hljs-type">Callable</span>,</span><br><span class="hljs-params">        reduce_scatter_embeddings: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        config: ModelParallelConfig,</span><br><span class="hljs-params">        tp_group: <span class="hljs-type">Optional</span>[torch.distributed.ProcessGroup] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>(VocabParallelEmbedding, self).__init__()<br>        <span class="hljs-comment"># Keep the input dimensions.</span><br>        self.num_embeddings = num_embeddings<br>        self.embedding_dim = embedding_dim<br>        self.reduce_scatter_embeddings = reduce_scatter_embeddings<br>        self.tp_group = tp_group<br><br>        self.tp_group = get_tensor_model_parallel_group_if_none(self.tp_group)<br><br>        (self.vocab_start_index, self.vocab_end_index) = (<br>            VocabUtility.vocab_range_from_global_vocab_size(<br>                self.num_embeddings, get_pg_rank(self.tp_group), get_pg_size(self.tp_group)<br>            )<br>        )<br>        self.num_embeddings_per_partition = self.vocab_end_index - self.vocab_start_index<br>        self.deterministic_mode = config.deterministic_mode<br><br>        <span class="hljs-comment"># Allocate weights and initialize.</span><br>        <span class="hljs-keyword">if</span> config.use_cpu_initialization:<br>            self.weight = Parameter(<br>                torch.empty(<br>                    self.num_embeddings_per_partition, self.embedding_dim, dtype=config.params_dtype<br>                )<br>            )<br>            <span class="hljs-keyword">if</span> config.perform_initialization:<br>                _initialize_affine_weight_cpu(<br>                    self.weight,<br>                    self.num_embeddings,<br>                    self.embedding_dim,<br>                    self.num_embeddings_per_partition,<br>                    <span class="hljs-number">0</span>,<br>                    init_method,<br>                    params_dtype=config.params_dtype,<br>                    rank=get_pg_rank(self.tp_group),<br>                    world_size=get_pg_size(self.tp_group),<br>                )<br>        <span class="hljs-keyword">else</span>:<br>            self.weight = Parameter(<br>                torch.empty(<br>                    self.num_embeddings_per_partition,<br>                    self.embedding_dim,<br>                    device=torch.cuda.current_device(),<br>                    dtype=config.params_dtype,<br>                )<br>            )<br>            <span class="hljs-keyword">if</span> config.perform_initialization:<br>                _initialize_affine_weight_gpu(self.weight, init_method, partition_dim=<span class="hljs-number">0</span>, stride=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input_</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Forward.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            input_ (torch.Tensor): Input tensor.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> self.tp_group.size() &gt; <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># Build the mask.</span><br>            input_mask = (input_ &lt; self.vocab_start_index) | (input_ &gt;= self.vocab_end_index)<br>            <span class="hljs-comment"># Mask the input.</span><br>            masked_input = input_.clone() - self.vocab_start_index<br>            masked_input[input_mask] = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">else</span>:<br>            masked_input = input_<br>        <span class="hljs-comment"># Get the embeddings.</span><br>        <span class="hljs-keyword">if</span> self.deterministic_mode:<br>            output_parallel = self.weight[masked_input]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># F.embedding currently has a non-deterministic backward function</span><br>            output_parallel = F.embedding(masked_input, self.weight)<br>        <span class="hljs-comment"># Mask the output embedding.</span><br>        <span class="hljs-keyword">if</span> self.tp_group.size() &gt; <span class="hljs-number">1</span>:<br>            output_parallel[input_mask, :] = <span class="hljs-number">0.0</span><br><br>        <span class="hljs-keyword">if</span> self.reduce_scatter_embeddings:<br>            <span class="hljs-comment"># Data format change to avoid explicit tranposes : [b s h] --&gt; [s b h].</span><br>            output_parallel = output_parallel.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>).contiguous()<br>            output = reduce_scatter_to_sequence_parallel_region(<br>                output_parallel, group=self.tp_group<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># Reduce across all the model parallel GPUs.</span><br>            output = reduce_from_tensor_model_parallel_region(output_parallel, group=self.tp_group)<br>        <span class="hljs-keyword">return</span> output<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sharded_state_dict</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        prefix: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;&quot;</span>,</span><br><span class="hljs-params">        sharded_offsets: <span class="hljs-type">Tuple</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]] = (<span class="hljs-params"></span>),</span><br><span class="hljs-params">        metadata: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">dict</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>) -&gt; ShardedStateDict:<br>        <span class="hljs-string">&quot;&quot;&quot;Non-default implementation for embeddings due to `allow_shape_mismatch` param&quot;&quot;&quot;</span><br>        state_dict = self.state_dict(prefix=<span class="hljs-string">&quot;&quot;</span>, keep_vars=<span class="hljs-literal">True</span>)<br><br>        weight_prefix = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;prefix&#125;</span>weight&quot;</span><br>        <span class="hljs-keyword">return</span> &#123;<br>            weight_prefix: make_tp_sharded_tensor_for_checkpoint(<br>                tensor=state_dict[<span class="hljs-string">&quot;weight&quot;</span>],<br>                key=weight_prefix,<br>                allow_shape_mismatch=<span class="hljs-literal">True</span>,<br>                prepend_offsets=sharded_offsets,<br>            )<br>        &#125;<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p><code>VocabParallelEmbedding</code>在初始化时首先根据TP对embedding进行分组，得到起始位置<code>self.vocab_start_index</code>,与结束<code>self.vocab_end_index</code></p>
</li>
<li><p><code>VocabParallelEmbedding</code>在Forward时：</p>
<ol>
<li><p>首先得到<code>input_mask = (input_ &lt; self.vocab_start_index) | (input_ &gt;= self.vocab_end_index)</code>，然后再得到<code>masked_input = input_.clone() - self.vocab_start_index</code>，再将不在这个范围内的置零<code>masked_input[input_mask] = 0</code></p>
</li>
<li><p><code>masked_input</code>记录了token更新后的id，然后再在<code>weight</code>中依据<code>masked_input</code>id去取对应的内容，得到<code>output_parallel</code>，并将不属于本rank的清零</p>
</li>
<li><p>然后这里依据<code>reduce_scatter_embeddings</code>有两种输出策略进行选择，注意</p>
<p><code>reduce_scatter_embeddings = ((not self.add_position_embedding) and self.num_tokentypes &lt;= 0 and self.config.sequence_parallel  and self.scatter_to_sequence_parallel)</code>：</p>
<ul>
<li><p><code>reduce_scatter_embeddings=True</code>（配合 sequence parallel）</p>
<ul>
<li><p>先把布局从 <code>[b, s, h]</code> 转成 <code>[s, b, h]</code>，因为 Megatron 的 sequence-parallel 通常以 <code>[seq, batch, hidden]</code> 为主（这样更容易沿 seq 维切分&#x2F;拼接）。</p>
</li>
<li><p>然后调用 <code>reduce_scatter_to_sequence_parallel_region</code>：</p>
<ul>
<li><p>语义上等价于：先对 output_parallel 在 TP 组上做 sum-reduce，再按 sequence 维把结果 scatter 给各 rank。</p>
</li>
<li><p>好处：直接产出 sequence-parallel 需要的分片输出，避免 “all-reduce 得到全量，再手动切分” 的额外开销和内存峰值。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><code>reduce_scatter_embeddings=False</code>（默认更直观）</p>
<ul>
<li><p>用 <code>reduce_from_tensor_model_parallel_region</code>：</p>
<ul>
<li><p>语义就是对 output_parallel 在 TP 组上 all-reduce(sum)；</p>
</li>
<li><p>每个 TP rank 都拿到完整的 embedding 输出（与未切分词表时一致）。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<h1 id="Tensor并行实验"><a href="#Tensor并行实验" class="headerlink" title="Tensor并行实验"></a>Tensor并行实验</h1><p>实验依据采用的是GPT3 857m的模型，运行脚本如下所示，值得注意的是在<code>GPT_MODEL_ARGS</code>参数中设置为了<code>local</code>，即不使用<code>transformer_engine</code>而是使用Megatron-LM本地实现的gpt_layer，与上述介绍对应，此外也设置TP切分维度为4</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-meta">#!/bin/bash</span><br><br><span class="hljs-comment"># Runs the &quot;857m&quot; parameter model</span><br><br><span class="hljs-built_in">export</span> CUDA_DEVICE_MAX_CONNECTIONS=1<br><br>GPUS_PER_NODE=4<br><span class="hljs-comment"># Change for multinode config</span><br>MASTER_ADDR=localhost<br>MASTER_PORT=6000<br>NUM_NODES=1<br>NODE_RANK=0<br>WORLD_SIZE=$((<span class="hljs-variable">$GPUS_PER_NODE</span>*<span class="hljs-variable">$NUM_NODES</span>))<br><br>CHECKPOINT_PATH=<span class="hljs-variable">$1</span> <span class="hljs-comment">#&lt;Specify path&gt;</span><br>TENSORBOARD_LOGS_PATH=<span class="hljs-variable">$2</span> <span class="hljs-comment">#&lt;Specify path&gt;</span><br>VOCAB_FILE=<span class="hljs-variable">$3</span> <span class="hljs-comment">#&lt;Specify path to file&gt;/gpt2-vocab.json</span><br>MERGE_FILE=<span class="hljs-variable">$4</span> <span class="hljs-comment">#&lt;Specify path to file&gt;/gpt2-merges.txt</span><br>DATA_PATH=<span class="hljs-variable">$5</span> <span class="hljs-comment">#&lt;Specify path and file prefix&gt;_text_document</span><br>USE_NSYS=0<br><span class="hljs-keyword">if</span> [[ <span class="hljs-variable">$&#123;6:-&#125;</span> == <span class="hljs-string">&quot;--nsys&quot;</span> ]]; <span class="hljs-keyword">then</span><br>  USE_NSYS=1<br><span class="hljs-keyword">fi</span><br><br>DISTRIBUTED_ARGS=(<br>    --nproc_per_node <span class="hljs-variable">$GPUS_PER_NODE</span> <br>    --nnodes <span class="hljs-variable">$NUM_NODES</span> <br>    --master_addr <span class="hljs-variable">$MASTER_ADDR</span> <br>    --master_port <span class="hljs-variable">$MASTER_PORT</span><br>)<br><br>GPT_MODEL_ARGS=(<br>    --num-layers 24 <br>    --hidden-size 1024 <br>    --num-attention-heads 16 <br>    --seq-length 2048 <br>    --max-position-embeddings 2048 <br>    --transformer-impl <span class="hljs-built_in">local</span><br>)<br><br>TRAINING_ARGS=(<br>    --micro-batch-size 4 <br>    --global-batch-size 16 <br>    <span class="hljs-comment"># --rampup-batch-size 16 16 5859375 </span><br>    --train-iters 20000 <br>    --weight-decay 0.1 <br>    --adam-beta1 0.9 <br>    --adam-beta2 0.95 <br>    --init-method-std 0.006 <br>    --clip-grad 1.0 <br>    --fp16<br>    --lr 6.0e-5 <br>    --lr-decay-style cosine <br>    --min-lr 6.0e-6<br>    --lr-warmup-fraction .001 <br>    --lr-decay-iters 20000 <br>)<br><br>MODEL_PARALLEL_ARGS=(<br>  --tensor-model-parallel-size 4 <br>  --pipeline-model-parallel-size 1 <br>)<br><br>DATA_ARGS=(<br>    --data-path <span class="hljs-variable">$DATA_PATH</span> <br>    --vocab-file <span class="hljs-variable">$VOCAB_FILE</span> <br>    --merge-file <span class="hljs-variable">$MERGE_FILE</span> <br>    --<span class="hljs-built_in">split</span> 949,50,1<br>)<br><br>EVAL_AND_LOGGING_ARGS=(<br>    --log-interval 200<br>    --save-interval 10000 <br>    --eval-interval 1000 <br>    --save <span class="hljs-variable">$CHECKPOINT_PATH</span> <br>    --load <span class="hljs-variable">$CHECKPOINT_PATH</span> <br>    --eval-iters 10<br>    --tensorboard-dir <span class="hljs-variable">$TENSORBOARD_LOGS_PATH</span> <br>)<br><br>PROFILER_ARGS=(<br>    --profile<br>    --use-pytorch-profiler<br>    --profile-step-start 110<br>    --profile-step-end 112<br>    --profile-ranks 0<br>)<br><br><span class="hljs-comment"># Build command as an array (no string concatenation)</span><br>CMD=(<br>  torchrun<br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;DISTRIBUTED_ARGS[@]&#125;</span>&quot;</span><br>  pretrain_gpt.py<br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;GPT_MODEL_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;TRAINING_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;MODEL_PARALLEL_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;DATA_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;EVAL_AND_LOGGING_ARGS[@]&#125;</span>&quot;</span><br>  <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;PROFILER_ARGS[@]&#125;</span>&quot;</span><br>)<br><br><span class="hljs-keyword">if</span> [[ <span class="hljs-string">&quot;<span class="hljs-variable">$USE_NSYS</span>&quot;</span> -eq 1 ]]; <span class="hljs-keyword">then</span><br>  NSIGHT_PREFIX=<span class="hljs-string">&quot;./nsight_profile/gpt3_857m&quot;</span><br>  <span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;Running with Nsight profiling, output prefix: <span class="hljs-variable">$&#123;NSIGHT_PREFIX&#125;</span>&quot;</span><br>  <span class="hljs-built_in">exec</span> nsys profile \<br>    -s none -t nvtx,cuda \<br>    --cudabacktrace=all \<br>    --cuda-graph-trace=node \<br>    --python-backtrace=cuda \<br>    --<span class="hljs-built_in">wait</span> all \<br>    -o <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;NSIGHT_PREFIX&#125;</span>&quot;</span> \<br>    --force-overwrite <span class="hljs-literal">true</span> \<br>    --capture-range=cudaProfilerApi \<br>    --capture-range-end=stop \<br>    <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;CMD[@]&#125;</span>&quot;</span><br><span class="hljs-keyword">else</span><br>  <span class="hljs-built_in">exec</span> <span class="hljs-string">&quot;<span class="hljs-variable">$&#123;CMD[@]&#125;</span>&quot;</span><br><span class="hljs-keyword">fi</span><br></code></pre></td></tr></table></figure>

<p>运行的命令为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">bash examples/gpt3/train_gpt3_857m_distributed.sh     /workspace/megatron-lm/model_ckpt/gpt3_857m_tp4     /workspace/megatron-lm/tb_logs/gpt3_857m_profiler_tp4     /workspace/megatron-lm/data/tokenizer/gpt2-vocab.json     /workspace/megatron-lm/data/tokenizer/gpt2-merges.txt     /workspace/megatron-lm/data/TinyStoriesV2-GPT4-train_text_document      &gt; gpt3_857m_tp4.log 2&gt;&amp;1 &amp;<br></code></pre></td></tr></table></figure>

<p>运行日志如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br><span class="line">821</span><br><span class="line">822</span><br><span class="line">823</span><br><span class="line">824</span><br><span class="line">825</span><br><span class="line">826</span><br><span class="line">827</span><br><span class="line">828</span><br><span class="line">829</span><br><span class="line">830</span><br><span class="line">831</span><br><span class="line">832</span><br><span class="line">833</span><br><span class="line">834</span><br><span class="line">835</span><br><span class="line">836</span><br><span class="line">837</span><br><span class="line">838</span><br><span class="line">839</span><br><span class="line">840</span><br><span class="line">841</span><br><span class="line">842</span><br><span class="line">843</span><br><span class="line">844</span><br><span class="line">845</span><br><span class="line">846</span><br><span class="line">847</span><br><span class="line">848</span><br><span class="line">849</span><br><span class="line">850</span><br><span class="line">851</span><br><span class="line">852</span><br><span class="line">853</span><br><span class="line">854</span><br><span class="line">855</span><br><span class="line">856</span><br><span class="line">857</span><br><span class="line">858</span><br><span class="line">859</span><br><span class="line">860</span><br><span class="line">861</span><br><span class="line">862</span><br><span class="line">863</span><br><span class="line">864</span><br><span class="line">865</span><br><span class="line">866</span><br><span class="line">867</span><br><span class="line">868</span><br><span class="line">869</span><br><span class="line">870</span><br><span class="line">871</span><br><span class="line">872</span><br><span class="line">873</span><br><span class="line">874</span><br><span class="line">875</span><br><span class="line">876</span><br><span class="line">877</span><br><span class="line">878</span><br><span class="line">879</span><br><span class="line">880</span><br><span class="line">881</span><br><span class="line">882</span><br><span class="line">883</span><br><span class="line">884</span><br><span class="line">885</span><br><span class="line">886</span><br><span class="line">887</span><br><span class="line">888</span><br><span class="line">889</span><br><span class="line">890</span><br><span class="line">891</span><br><span class="line">892</span><br><span class="line">893</span><br><span class="line">894</span><br><span class="line">895</span><br><span class="line">896</span><br><span class="line">897</span><br><span class="line">898</span><br><span class="line">899</span><br><span class="line">900</span><br><span class="line">901</span><br><span class="line">902</span><br><span class="line">903</span><br><span class="line">904</span><br><span class="line">905</span><br><span class="line">906</span><br><span class="line">907</span><br><span class="line">908</span><br><span class="line">909</span><br><span class="line">910</span><br><span class="line">911</span><br><span class="line">912</span><br><span class="line">913</span><br><span class="line">914</span><br><span class="line">915</span><br><span class="line">916</span><br><span class="line">917</span><br><span class="line">918</span><br><span class="line">919</span><br><span class="line">920</span><br><span class="line">921</span><br><span class="line">922</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">W0103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:27.318000</span> <span class="hljs-number">1772056</span> <span class="hljs-string">torch/distributed/run.py:766]</span> <br><span class="hljs-string">W0103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:27.318000</span> <span class="hljs-number">1772056</span> <span class="hljs-string">torch/distributed/run.py:766]</span> <span class="hljs-string">*****************************************</span><br><span class="hljs-string">W0103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:27.318000</span> <span class="hljs-number">1772056</span> <span class="hljs-string">torch/distributed/run.py:766]</span> <span class="hljs-string">Setting</span> <span class="hljs-string">OMP_NUM_THREADS</span> <span class="hljs-string">environment</span> <span class="hljs-string">variable</span> <span class="hljs-string">for</span> <span class="hljs-string">each</span> <span class="hljs-string">process</span> <span class="hljs-string">to</span> <span class="hljs-string">be</span> <span class="hljs-number">1</span> <span class="hljs-string">in</span> <span class="hljs-string">default,</span> <span class="hljs-string">to</span> <span class="hljs-string">avoid</span> <span class="hljs-string">your</span> <span class="hljs-string">system</span> <span class="hljs-string">being</span> <span class="hljs-string">overloaded,</span> <span class="hljs-string">please</span> <span class="hljs-string">further</span> <span class="hljs-string">tune</span> <span class="hljs-string">the</span> <span class="hljs-string">variable</span> <span class="hljs-string">for</span> <span class="hljs-string">optimal</span> <span class="hljs-string">performance</span> <span class="hljs-string">in</span> <span class="hljs-string">your</span> <span class="hljs-string">application</span> <span class="hljs-string">as</span> <span class="hljs-string">needed.</span> <br><span class="hljs-string">W0103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:27.318000</span> <span class="hljs-number">1772056</span> <span class="hljs-string">torch/distributed/run.py:766]</span> <span class="hljs-string">*****************************************</span><br><span class="hljs-attr">using world size:</span> <span class="hljs-number">4</span><span class="hljs-string">,</span> <span class="hljs-attr">data-parallel size:</span> <span class="hljs-number">1</span><span class="hljs-string">,</span> <span class="hljs-attr">context-parallel size:</span> <span class="hljs-number">1</span><span class="hljs-string">,</span> <span class="hljs-attr">hierarchical context-parallel sizes:</span> <span class="hljs-string">None,</span> <span class="hljs-attr">tensor-model-parallel size:</span> <span class="hljs-number">4</span><span class="hljs-string">,</span> <span class="hljs-attr">pipeline-model-parallel size:</span> <span class="hljs-number">1</span><br><span class="hljs-attr">Number of virtual stages per pipeline stage:</span> <span class="hljs-string">None</span><br><span class="hljs-attr">WARNING:</span> <span class="hljs-string">Setting</span> <span class="hljs-string">args.check_for_nan_in_loss_and_grad</span> <span class="hljs-string">to</span> <span class="hljs-literal">False</span> <span class="hljs-string">since</span> <span class="hljs-string">dynamic</span> <span class="hljs-string">loss</span> <span class="hljs-string">scaling</span> <span class="hljs-string">is</span> <span class="hljs-string">being</span> <span class="hljs-string">used</span><br><span class="hljs-string">using</span> <span class="hljs-string">torch.float16</span> <span class="hljs-string">for</span> <span class="hljs-string">parameters</span> <span class="hljs-string">...</span><br><span class="hljs-string">------------------------</span> <span class="hljs-string">arguments</span> <span class="hljs-string">------------------------</span><br>  <span class="hljs-string">account_for_embedding_in_pipeline_split</span> <span class="hljs-string">.........</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">account_for_loss_in_pipeline_split</span> <span class="hljs-string">..............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">accumulate_allreduce_grads_in_fp32</span> <span class="hljs-string">..............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">adam_beta1</span> <span class="hljs-string">......................................</span> <span class="hljs-number">0.9</span><br>  <span class="hljs-string">adam_beta2</span> <span class="hljs-string">......................................</span> <span class="hljs-number">0.95</span><br>  <span class="hljs-string">adam_eps</span> <span class="hljs-string">........................................</span> <span class="hljs-number">1e-08</span><br>  <span class="hljs-string">add_bias_linear</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">add_position_embedding</span> <span class="hljs-string">..........................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">add_qkv_bias</span> <span class="hljs-string">....................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">adlr_autoresume</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">adlr_autoresume_interval</span> <span class="hljs-string">........................</span> <span class="hljs-number">1000</span><br>  <span class="hljs-string">align_grad_reduce</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">align_param_gather</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">app_tag_run_name</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">app_tag_run_version</span> <span class="hljs-string">.............................</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><br>  <span class="hljs-string">apply_layernorm_1p</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">apply_query_key_layer_scaling</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">apply_residual_connection_post_layernorm</span> <span class="hljs-string">........</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">apply_rope_fusion</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">async_save</span> <span class="hljs-string">......................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">async_tensor_model_parallel_allreduce</span> <span class="hljs-string">...........</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">attention_backend</span> <span class="hljs-string">...............................</span> <span class="hljs-string">AttnBackend.auto</span><br>  <span class="hljs-string">attention_dropout</span> <span class="hljs-string">...............................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">attention_softmax_in_fp32</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">auto_detect_ckpt_format</span> <span class="hljs-string">.........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">barrier_with_L1_time</span> <span class="hljs-string">............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">bert_binary_head</span> <span class="hljs-string">................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">bert_embedder_type</span> <span class="hljs-string">..............................</span> <span class="hljs-string">megatron</span><br>  <span class="hljs-string">bert_load</span> <span class="hljs-string">.......................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">bf16</span> <span class="hljs-string">............................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">bias_dropout_fusion</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">bias_gelu_fusion</span> <span class="hljs-string">................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">bias_swiglu_fusion</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">biencoder_projection_dim</span> <span class="hljs-string">........................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">biencoder_shared_query_context_model</span> <span class="hljs-string">............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">block_data_path</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">cache_mla_latents</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">calc_ft_timeouts</span> <span class="hljs-string">................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">calculate_per_token_loss</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">check_for_large_grads</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">check_for_nan_in_loss_and_grad</span> <span class="hljs-string">..................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">check_for_spiky_loss</span> <span class="hljs-string">............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">check_weight_hash_across_dp_replicas_interval</span> <span class="hljs-string">...</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">ckpt_assume_constant_structure</span> <span class="hljs-string">..................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">ckpt_convert_format</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">ckpt_convert_save</span> <span class="hljs-string">...............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">ckpt_convert_update_legacy_dist_opt_format</span> <span class="hljs-string">......</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">ckpt_format</span> <span class="hljs-string">.....................................</span> <span class="hljs-string">torch_dist</span><br>  <span class="hljs-string">ckpt_fully_parallel_load</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">ckpt_fully_parallel_save</span> <span class="hljs-string">........................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">ckpt_fully_parallel_save_deprecated</span> <span class="hljs-string">.............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">ckpt_step</span> <span class="hljs-string">.......................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">classes_fraction</span> <span class="hljs-string">................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">clip_grad</span> <span class="hljs-string">.......................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">clone_scatter_output_in_embedding</span> <span class="hljs-string">...............</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">config_logger_dir</span> <span class="hljs-string">...............................</span> <br>  <span class="hljs-string">consumed_train_samples</span> <span class="hljs-string">..........................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">consumed_valid_samples</span> <span class="hljs-string">..........................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">context_parallel_size</span> <span class="hljs-string">...........................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">cp_comm_type</span> <span class="hljs-string">....................................</span> [<span class="hljs-string">&#x27;p2p&#x27;</span>]<br>  <span class="hljs-string">create_attention_mask_in_dataloader</span> <span class="hljs-string">.............</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">cross_entropy_fusion_impl</span> <span class="hljs-string">.......................</span> <span class="hljs-string">native</span><br>  <span class="hljs-string">cross_entropy_loss_fusion</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">cuda_graph_scope</span> <span class="hljs-string">................................</span> <span class="hljs-string">full</span><br>  <span class="hljs-string">cuda_graph_warmup_steps</span> <span class="hljs-string">.........................</span> <span class="hljs-number">3</span><br>  <span class="hljs-string">data_args_path</span> <span class="hljs-string">..................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">data_cache_path</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">data_parallel_random_init</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">data_parallel_sharding_strategy</span> <span class="hljs-string">.................</span> <span class="hljs-string">no_shard</span><br>  <span class="hljs-string">data_parallel_size</span> <span class="hljs-string">..............................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">data_path</span> <span class="hljs-string">.......................................</span> [<span class="hljs-string">&#x27;/workspace/megatron-lm/data/TinyStoriesV2-GPT4-train_text_document&#x27;</span>]<br>  <span class="hljs-string">data_per_class_fraction</span> <span class="hljs-string">.........................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">data_sharding</span> <span class="hljs-string">...................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">dataloader_type</span> <span class="hljs-string">.................................</span> <span class="hljs-string">single</span><br>  <span class="hljs-string">ddp_average_in_collective</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">ddp_bucket_size</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">ddp_num_buckets</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">ddp_pad_buckets_for_high_nccl_busbw</span> <span class="hljs-string">.............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">decoder_first_pipeline_num_layers</span> <span class="hljs-string">...............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">decoder_last_pipeline_num_layers</span> <span class="hljs-string">................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">decoder_num_layers</span> <span class="hljs-string">..............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">decoder_seq_length</span> <span class="hljs-string">..............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">decoupled_lr</span> <span class="hljs-string">....................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">decoupled_min_lr</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">decrease_batch_size_if_needed</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">defer_embedding_wgrad_compute</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">delay_wgrad_compute</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">deprecated_use_mcore_models</span> <span class="hljs-string">.....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">deterministic_mode</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">dino_bottleneck_size</span> <span class="hljs-string">............................</span> <span class="hljs-number">256</span><br>  <span class="hljs-string">dino_freeze_last_layer</span> <span class="hljs-string">..........................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">dino_head_hidden_size</span> <span class="hljs-string">...........................</span> <span class="hljs-number">2048</span><br>  <span class="hljs-string">dino_local_crops_number</span> <span class="hljs-string">.........................</span> <span class="hljs-number">10</span><br>  <span class="hljs-string">dino_local_img_size</span> <span class="hljs-string">.............................</span> <span class="hljs-number">96</span><br>  <span class="hljs-string">dino_norm_last_layer</span> <span class="hljs-string">............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">dino_teacher_temp</span> <span class="hljs-string">...............................</span> <span class="hljs-number">0.07</span><br>  <span class="hljs-string">dino_warmup_teacher_temp</span> <span class="hljs-string">........................</span> <span class="hljs-number">0.04</span><br>  <span class="hljs-string">dino_warmup_teacher_temp_epochs</span> <span class="hljs-string">.................</span> <span class="hljs-number">30</span><br>  <span class="hljs-string">disable_bf16_reduced_precision_matmul</span> <span class="hljs-string">...........</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">disable_mamba_mem_eff_path</span> <span class="hljs-string">......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">disable_straggler_on_startup</span> <span class="hljs-string">....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">dist_ckpt_format_deprecated</span> <span class="hljs-string">.....................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">dist_ckpt_strictness</span> <span class="hljs-string">............................</span> <span class="hljs-string">assume_ok_unexpected</span><br>  <span class="hljs-string">distribute_saved_activations</span> <span class="hljs-string">....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">distributed_backend</span> <span class="hljs-string">.............................</span> <span class="hljs-string">nccl</span><br>  <span class="hljs-string">distributed_timeout_minutes</span> <span class="hljs-string">.....................</span> <span class="hljs-number">10</span><br>  <span class="hljs-string">embedding_init_method_std</span> <span class="hljs-string">.......................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">embedding_path</span> <span class="hljs-string">..................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">empty_unused_memory_level</span> <span class="hljs-string">.......................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">enable_cuda_graph</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">enable_experimental</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">enable_ft_package</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">enable_full_sharding_in_hsdp</span> <span class="hljs-string">....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">enable_gloo_process_groups</span> <span class="hljs-string">......................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">enable_msc</span> <span class="hljs-string">......................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">enable_one_logger</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">encoder_num_layers</span> <span class="hljs-string">..............................</span> <span class="hljs-number">24</span><br>  <span class="hljs-string">encoder_seq_length</span> <span class="hljs-string">..............................</span> <span class="hljs-number">2048</span><br>  <span class="hljs-string">end_weight_decay</span> <span class="hljs-string">................................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">eod_mask_loss</span> <span class="hljs-string">...................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">error_injection_rate</span> <span class="hljs-string">............................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">error_injection_type</span> <span class="hljs-string">............................</span> <span class="hljs-string">transient_error</span><br>  <span class="hljs-string">eval_interval</span> <span class="hljs-string">...................................</span> <span class="hljs-number">1000</span><br>  <span class="hljs-string">eval_iters</span> <span class="hljs-string">......................................</span> <span class="hljs-number">10</span><br>  <span class="hljs-string">evidence_data_path</span> <span class="hljs-string">..............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">exit_duration_in_mins</span> <span class="hljs-string">...........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">exit_interval</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">exit_on_missing_checkpoint</span> <span class="hljs-string">......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">exit_signal_handler</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">exp_avg_dtype</span> <span class="hljs-string">...................................</span> <span class="hljs-string">torch.float32</span><br>  <span class="hljs-string">exp_avg_sq_dtype</span> <span class="hljs-string">................................</span> <span class="hljs-string">torch.float32</span><br>  <span class="hljs-string">expert_model_parallel_size</span> <span class="hljs-string">......................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">expert_tensor_parallel_size</span> <span class="hljs-string">.....................</span> <span class="hljs-number">4</span><br>  <span class="hljs-string">export_force_local_attention</span> <span class="hljs-string">....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">export_kd_cfg</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">export_kd_teacher_ckpt_format</span> <span class="hljs-string">...................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">export_kd_teacher_load</span> <span class="hljs-string">..........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">export_kv_cache_quant</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">export_legacy_megatron</span> <span class="hljs-string">..........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">export_model_type</span> <span class="hljs-string">...............................</span> <span class="hljs-string">GPTModel</span><br>  <span class="hljs-string">export_moe_apply_probs_on_input</span> <span class="hljs-string">.................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">export_qk_l2_norm</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">export_quant_cfg</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">export_real_quant_cfg</span> <span class="hljs-string">...........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">export_te_mcore_model</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">external_cuda_graph</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">ffn_hidden_size</span> <span class="hljs-string">.................................</span> <span class="hljs-number">4096</span><br>  <span class="hljs-string">finetune</span> <span class="hljs-string">........................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">finetune_data_split</span> <span class="hljs-string">.............................</span> <span class="hljs-string">train</span><br>  <span class="hljs-string">finetune_hf_dataset</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">first_last_layers_bf16</span> <span class="hljs-string">..........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">flash_decode</span> <span class="hljs-string">....................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">fp16</span> <span class="hljs-string">............................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">fp16_lm_cross_entropy</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">fp32_residual_connection</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">fp8</span> <span class="hljs-string">.............................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">fp8_amax_compute_algo</span> <span class="hljs-string">...........................</span> <span class="hljs-string">most_recent</span><br>  <span class="hljs-string">fp8_amax_history_len</span> <span class="hljs-string">............................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">fp8_interval</span> <span class="hljs-string">....................................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">fp8_margin</span> <span class="hljs-string">......................................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">fp8_param_gather</span> <span class="hljs-string">................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">fp8_recipe</span> <span class="hljs-string">......................................</span> <span class="hljs-string">delayed</span><br>  <span class="hljs-string">fp8_wgrad</span> <span class="hljs-string">.......................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">fsdp_double_buffer</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">full_validation</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">global_batch_size</span> <span class="hljs-string">...............................</span> <span class="hljs-number">16</span><br>  <span class="hljs-string">grad_reduce_in_bf16</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">gradient_accumulation_fusion</span> <span class="hljs-string">....................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">gradient_reduce_div_fusion</span> <span class="hljs-string">......................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">group_query_attention</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">head_lr_mult</span> <span class="hljs-string">....................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">heterogeneous_layers_config_encoded_json</span> <span class="hljs-string">........</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">heterogeneous_layers_config_path</span> <span class="hljs-string">................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">hidden_dropout</span> <span class="hljs-string">..................................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">hidden_size</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">1024</span><br>  <span class="hljs-string">hierarchical_context_parallel_sizes</span> <span class="hljs-string">.............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">high_priority_stream_groups</span> <span class="hljs-string">.....................</span> []<br>  <span class="hljs-string">hybrid_attention_ratio</span> <span class="hljs-string">..........................</span> <span class="hljs-number">0.0</span><br>  <span class="hljs-string">hybrid_mlp_ratio</span> <span class="hljs-string">................................</span> <span class="hljs-number">0.0</span><br>  <span class="hljs-string">hybrid_override_pattern</span> <span class="hljs-string">.........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">hysteresis</span> <span class="hljs-string">......................................</span> <span class="hljs-number">2</span><br>  <span class="hljs-string">ict_head_size</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">ict_load</span> <span class="hljs-string">........................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">img_h</span> <span class="hljs-string">...........................................</span> <span class="hljs-number">224</span><br>  <span class="hljs-string">img_w</span> <span class="hljs-string">...........................................</span> <span class="hljs-number">224</span><br>  <span class="hljs-string">indexer_batch_size</span> <span class="hljs-string">..............................</span> <span class="hljs-number">128</span><br>  <span class="hljs-string">indexer_log_interval</span> <span class="hljs-string">............................</span> <span class="hljs-number">1000</span><br>  <span class="hljs-string">inference_batch_times_seqlen_threshold</span> <span class="hljs-string">..........</span> <span class="hljs-number">-1</span><br>  <span class="hljs-string">inference_dynamic_batching</span> <span class="hljs-string">......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">inference_dynamic_batching_buffer_guaranteed_fraction</span>  <span class="hljs-number">0.2</span><br>  <span class="hljs-string">inference_dynamic_batching_buffer_overflow_factor</span>  <span class="hljs-string">None</span><br>  <span class="hljs-string">inference_dynamic_batching_buffer_size_gb</span> <span class="hljs-string">.......</span> <span class="hljs-number">40.0</span><br>  <span class="hljs-string">inference_dynamic_batching_chunk_size</span> <span class="hljs-string">...........</span> <span class="hljs-number">256</span><br>  <span class="hljs-string">inference_dynamic_batching_max_requests_override</span>  <span class="hljs-string">None</span><br>  <span class="hljs-string">inference_dynamic_batching_max_tokens_override</span> <span class="hljs-string">..</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">inference_dynamic_batching_num_cuda_graphs</span> <span class="hljs-string">......</span> <span class="hljs-number">16</span><br>  <span class="hljs-string">inference_max_batch_size</span> <span class="hljs-string">........................</span> <span class="hljs-number">8</span><br>  <span class="hljs-string">inference_max_seq_length</span> <span class="hljs-string">........................</span> <span class="hljs-number">2560</span><br>  <span class="hljs-string">inference_rng_tracker</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">init_method_std</span> <span class="hljs-string">.................................</span> <span class="hljs-number">0.006</span><br>  <span class="hljs-string">init_method_xavier_uniform</span> <span class="hljs-string">......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">init_model_with_meta_device</span> <span class="hljs-string">.....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">initial_loss_scale</span> <span class="hljs-string">..............................</span> <span class="hljs-number">4294967296</span><br>  <span class="hljs-string">inprocess_active_world_size</span> <span class="hljs-string">.....................</span> <span class="hljs-number">4</span><br>  <span class="hljs-string">inprocess_barrier_timeout</span> <span class="hljs-string">.......................</span> <span class="hljs-number">120</span><br>  <span class="hljs-string">inprocess_completion_timeout</span> <span class="hljs-string">....................</span> <span class="hljs-number">120</span><br>  <span class="hljs-string">inprocess_empty_cuda_cache</span> <span class="hljs-string">......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">inprocess_granularity</span> <span class="hljs-string">...........................</span> <span class="hljs-string">node</span><br>  <span class="hljs-string">inprocess_hard_timeout</span> <span class="hljs-string">..........................</span> <span class="hljs-number">90</span><br>  <span class="hljs-string">inprocess_heartbeat_interval</span> <span class="hljs-string">....................</span> <span class="hljs-number">30</span><br>  <span class="hljs-string">inprocess_heartbeat_timeout</span> <span class="hljs-string">.....................</span> <span class="hljs-number">60</span><br>  <span class="hljs-string">inprocess_last_call_wait</span> <span class="hljs-string">........................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">inprocess_max_iterations</span> <span class="hljs-string">........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">inprocess_monitor_process_interval</span> <span class="hljs-string">..............</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">inprocess_monitor_thread_interval</span> <span class="hljs-string">...............</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">inprocess_progress_watchdog_interval</span> <span class="hljs-string">............</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">inprocess_restart</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">inprocess_soft_timeout</span> <span class="hljs-string">..........................</span> <span class="hljs-number">60</span><br>  <span class="hljs-string">inprocess_termination_grace_time</span> <span class="hljs-string">................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">is_hybrid_model</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">iter_per_epoch</span> <span class="hljs-string">..................................</span> <span class="hljs-number">1250</span><br>  <span class="hljs-string">iterations_to_skip</span> <span class="hljs-string">..............................</span> []<br>  <span class="hljs-string">keep_fp8_transpose_cache</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">kitchen_config_file</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">kitchen_recipe_number</span> <span class="hljs-string">...........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">kv_channels</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">64</span><br>  <span class="hljs-string">kv_lora_rank</span> <span class="hljs-string">....................................</span> <span class="hljs-number">32</span><br>  <span class="hljs-string">lazy_mpu_init</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">load</span> <span class="hljs-string">............................................</span> <span class="hljs-string">/workspace/megatron-lm/model_ckpt/gpt3_857m_tp4</span><br>  <span class="hljs-string">load_main_params_from_ckpt</span> <span class="hljs-string">......................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">load_model_opt_format</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">local_rank</span> <span class="hljs-string">......................................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">log_energy</span> <span class="hljs-string">......................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_interval</span> <span class="hljs-string">....................................</span> <span class="hljs-number">200</span><br>  <span class="hljs-string">log_loss_scale_to_tensorboard</span> <span class="hljs-string">...................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">log_memory_to_tensorboard</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_num_zeros_in_grad</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_params_norm</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_progress</span> <span class="hljs-string">....................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_straggler</span> <span class="hljs-string">...................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_throughput</span> <span class="hljs-string">..................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_timers_to_tensorboard</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_validation_ppl_to_tensorboard</span> <span class="hljs-string">...............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">log_world_size_to_tensorboard</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">logging_level</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">loss_scale</span> <span class="hljs-string">......................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">loss_scale_window</span> <span class="hljs-string">...............................</span> <span class="hljs-number">1000</span><br>  <span class="hljs-string">lr</span> <span class="hljs-string">..............................................</span> <span class="hljs-number">6e-05</span><br>  <span class="hljs-string">lr_decay_iters</span> <span class="hljs-string">..................................</span> <span class="hljs-number">20000</span><br>  <span class="hljs-string">lr_decay_samples</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">lr_decay_style</span> <span class="hljs-string">..................................</span> <span class="hljs-string">cosine</span><br>  <span class="hljs-string">lr_warmup_fraction</span> <span class="hljs-string">..............................</span> <span class="hljs-number">0.001</span><br>  <span class="hljs-string">lr_warmup_init</span> <span class="hljs-string">..................................</span> <span class="hljs-number">0.0</span><br>  <span class="hljs-string">lr_warmup_iters</span> <span class="hljs-string">.................................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">lr_warmup_samples</span> <span class="hljs-string">...............................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">lr_wsd_decay_iters</span> <span class="hljs-string">..............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">lr_wsd_decay_samples</span> <span class="hljs-string">............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">lr_wsd_decay_style</span> <span class="hljs-string">..............................</span> <span class="hljs-string">exponential</span><br>  <span class="hljs-string">main_grads_dtype</span> <span class="hljs-string">................................</span> <span class="hljs-string">torch.float32</span><br>  <span class="hljs-string">main_params_dtype</span> <span class="hljs-string">...............................</span> <span class="hljs-string">torch.float32</span><br>  <span class="hljs-string">make_vocab_size_divisible_by</span> <span class="hljs-string">....................</span> <span class="hljs-number">128</span><br>  <span class="hljs-string">mamba_head_dim</span> <span class="hljs-string">..................................</span> <span class="hljs-number">64</span><br>  <span class="hljs-string">mamba_num_groups</span> <span class="hljs-string">................................</span> <span class="hljs-number">8</span><br>  <span class="hljs-string">mamba_num_heads</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">mamba_state_dim</span> <span class="hljs-string">.................................</span> <span class="hljs-number">128</span><br>  <span class="hljs-string">manual_gc</span> <span class="hljs-string">.......................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">manual_gc_eval</span> <span class="hljs-string">..................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">manual_gc_interval</span> <span class="hljs-string">..............................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">mask_factor</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">mask_prob</span> <span class="hljs-string">.......................................</span> <span class="hljs-number">0.15</span><br>  <span class="hljs-string">mask_type</span> <span class="hljs-string">.......................................</span> <span class="hljs-string">random</span><br>  <span class="hljs-string">masked_softmax_fusion</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">max_position_embeddings</span> <span class="hljs-string">.........................</span> <span class="hljs-number">2048</span><br>  <span class="hljs-string">max_tokens_to_oom</span> <span class="hljs-string">...............................</span> <span class="hljs-number">12000</span><br>  <span class="hljs-string">memory_snapshot_path</span> <span class="hljs-string">............................</span> <span class="hljs-string">snapshot.pickle</span><br>  <span class="hljs-string">merge_file</span> <span class="hljs-string">......................................</span> <span class="hljs-string">/workspace/megatron-lm/data/tokenizer/gpt2-merges.txt</span><br>  <span class="hljs-string">micro_batch_size</span> <span class="hljs-string">................................</span> <span class="hljs-number">4</span><br>  <span class="hljs-string">microbatch_group_size_per_vp_stage</span> <span class="hljs-string">..............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">mid_level_dataset_surplus</span> <span class="hljs-string">.......................</span> <span class="hljs-number">0.005</span><br>  <span class="hljs-string">min_loss_scale</span> <span class="hljs-string">..................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">min_lr</span> <span class="hljs-string">..........................................</span> <span class="hljs-number">6e-06</span><br>  <span class="hljs-string">mlp_chunks_for_prefill</span> <span class="hljs-string">..........................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">mmap_bin_files</span> <span class="hljs-string">..................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">mock_data</span> <span class="hljs-string">.......................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_apply_probs_on_input</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_aux_loss_coeff</span> <span class="hljs-string">..............................</span> <span class="hljs-number">0.0</span><br>  <span class="hljs-string">moe_deepep_num_sms</span> <span class="hljs-string">..............................</span> <span class="hljs-number">20</span><br>  <span class="hljs-string">moe_enable_deepep</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_expert_capacity_factor</span> <span class="hljs-string">......................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_extended_tp</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_ffn_hidden_size</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_grouped_gemm</span> <span class="hljs-string">................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_input_jitter_eps</span> <span class="hljs-string">............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_layer_freq</span> <span class="hljs-string">..................................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">moe_layer_recompute</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_pad_expert_input_to_capacity</span> <span class="hljs-string">................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_per_layer_logging</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_permute_fusion</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_router_bias_update_rate</span> <span class="hljs-string">.....................</span> <span class="hljs-number">0.001</span><br>  <span class="hljs-string">moe_router_dtype</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_router_enable_expert_bias</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_router_force_load_balancing</span> <span class="hljs-string">.................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_router_fusion</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_router_group_topk</span> <span class="hljs-string">...........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_router_load_balancing_type</span> <span class="hljs-string">..................</span> <span class="hljs-string">aux_loss</span><br>  <span class="hljs-string">moe_router_num_groups</span> <span class="hljs-string">...........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_router_padding_for_fp8</span> <span class="hljs-string">......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_router_pre_softmax</span> <span class="hljs-string">..........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_router_score_function</span> <span class="hljs-string">.......................</span> <span class="hljs-string">softmax</span><br>  <span class="hljs-string">moe_router_topk</span> <span class="hljs-string">.................................</span> <span class="hljs-number">2</span><br>  <span class="hljs-string">moe_router_topk_scaling_factor</span> <span class="hljs-string">..................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_shared_expert_intermediate_size</span> <span class="hljs-string">.............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">moe_shared_expert_overlap</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_token_dispatcher_type</span> <span class="hljs-string">.......................</span> <span class="hljs-string">allgather</span><br>  <span class="hljs-string">moe_token_drop_policy</span> <span class="hljs-string">...........................</span> <span class="hljs-string">probs</span><br>  <span class="hljs-string">moe_upcycling_granularity</span> <span class="hljs-string">.......................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">moe_use_legacy_grouped_gemm</span> <span class="hljs-string">.....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_use_upcycling</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">moe_z_loss_coeff</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">mrope_section</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">mscale</span> <span class="hljs-string">..........................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">mscale_all_dim</span> <span class="hljs-string">..................................</span> <span class="hljs-number">0.0</span><br>  <span class="hljs-string">mtp_loss_scaling_factor</span> <span class="hljs-string">.........................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">mtp_num_layers</span> <span class="hljs-string">..................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">multi_latent_attention</span> <span class="hljs-string">..........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">multiple_validation_sets</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">nccl_all_reduce_for_prefill</span> <span class="hljs-string">.....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">nccl_communicator_config_path</span> <span class="hljs-string">...................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">nccl_ub</span> <span class="hljs-string">.........................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">no_load_optim</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">no_load_rng</span> <span class="hljs-string">.....................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">no_persist_layer_norm</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">no_rope_freq</span> <span class="hljs-string">....................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">no_save_optim</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">no_save_rng</span> <span class="hljs-string">.....................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">non_persistent_ckpt_type</span> <span class="hljs-string">........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">non_persistent_global_ckpt_dir</span> <span class="hljs-string">..................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">non_persistent_local_ckpt_algo</span> <span class="hljs-string">..................</span> <span class="hljs-string">fully_parallel</span><br>  <span class="hljs-string">non_persistent_local_ckpt_dir</span> <span class="hljs-string">...................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">non_persistent_save_interval</span> <span class="hljs-string">....................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">norm_epsilon</span> <span class="hljs-string">....................................</span> <span class="hljs-number">1e-05</span><br>  <span class="hljs-string">normalization</span> <span class="hljs-string">...................................</span> <span class="hljs-string">LayerNorm</span><br>  <span class="hljs-string">num_attention_heads</span> <span class="hljs-string">.............................</span> <span class="hljs-number">16</span><br>  <span class="hljs-string">num_channels</span> <span class="hljs-string">....................................</span> <span class="hljs-number">3</span><br>  <span class="hljs-string">num_classes</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">1000</span><br>  <span class="hljs-string">num_dataset_builder_threads</span> <span class="hljs-string">.....................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">num_distributed_optimizer_instances</span> <span class="hljs-string">.............</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">num_experts</span> <span class="hljs-string">.....................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">num_layers</span> <span class="hljs-string">......................................</span> <span class="hljs-number">24</span><br>  <span class="hljs-string">num_layers_at_end_in_bf16</span> <span class="hljs-string">.......................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">num_layers_at_start_in_bf16</span> <span class="hljs-string">.....................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">num_layers_per_virtual_pipeline_stage</span> <span class="hljs-string">...........</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">num_query_groups</span> <span class="hljs-string">................................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">num_virtual_stages_per_pipeline_rank</span> <span class="hljs-string">............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">num_workers</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">2</span><br>  <span class="hljs-string">object_storage_cache_path</span> <span class="hljs-string">.......................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">one_logger_async</span> <span class="hljs-string">................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">one_logger_project</span> <span class="hljs-string">..............................</span> <span class="hljs-string">megatron-lm</span><br>  <span class="hljs-string">one_logger_run_name</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">onnx_safe</span> <span class="hljs-string">.......................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">openai_gelu</span> <span class="hljs-string">.....................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">optimizer</span> <span class="hljs-string">.......................................</span> <span class="hljs-string">adam</span><br>  <span class="hljs-string">optimizer_cpu_offload</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">optimizer_offload_fraction</span> <span class="hljs-string">......................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">output_bert_embeddings</span> <span class="hljs-string">..........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">overlap_cpu_optimizer_d2h_h2d</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">overlap_grad_reduce</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">overlap_moe_expert_parallel_comm</span> <span class="hljs-string">................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">overlap_p2p_comm</span> <span class="hljs-string">................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">overlap_p2p_comm_warmup_flush</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">overlap_param_gather</span> <span class="hljs-string">............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">overlap_param_gather_with_optimizer_step</span> <span class="hljs-string">........</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">override_opt_param_scheduler</span> <span class="hljs-string">....................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">padded_vocab_size</span> <span class="hljs-string">...............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">params_dtype</span> <span class="hljs-string">....................................</span> <span class="hljs-string">torch.float16</span><br>  <span class="hljs-string">patch_dim</span> <span class="hljs-string">.......................................</span> <span class="hljs-number">16</span><br>  <span class="hljs-string">per_split_data_args_path</span> <span class="hljs-string">........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">perform_initialization</span> <span class="hljs-string">..........................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">pin_cpu_grads</span> <span class="hljs-string">...................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">pin_cpu_params</span> <span class="hljs-string">..................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">pipeline_model_parallel_comm_backend</span> <span class="hljs-string">............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">pipeline_model_parallel_layout</span> <span class="hljs-string">..................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">pipeline_model_parallel_size</span> <span class="hljs-string">....................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">position_embedding_type</span> <span class="hljs-string">.........................</span> <span class="hljs-string">learned_absolute</span><br>  <span class="hljs-string">pretrained_checkpoint</span> <span class="hljs-string">...........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">profile</span> <span class="hljs-string">.........................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">profile_ranks</span> <span class="hljs-string">...................................</span> [<span class="hljs-number">0</span>]<br>  <span class="hljs-string">profile_step_end</span> <span class="hljs-string">................................</span> <span class="hljs-number">112</span><br>  <span class="hljs-string">profile_step_start</span> <span class="hljs-string">..............................</span> <span class="hljs-number">110</span><br>  <span class="hljs-string">q_lora_rank</span> <span class="hljs-string">.....................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">qk_head_dim</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">128</span><br>  <span class="hljs-string">qk_l2_norm</span> <span class="hljs-string">......................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">qk_layernorm</span> <span class="hljs-string">....................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">qk_pos_emb_head_dim</span> <span class="hljs-string">.............................</span> <span class="hljs-number">64</span><br>  <span class="hljs-string">query_in_block_prob</span> <span class="hljs-string">.............................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">rampup_batch_size</span> <span class="hljs-string">...............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">rank</span> <span class="hljs-string">............................................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">recompute_granularity</span> <span class="hljs-string">...........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">recompute_method</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">recompute_modules</span> <span class="hljs-string">...............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">recompute_num_layers</span> <span class="hljs-string">............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">record_memory_history</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">relative_attention_max_distance</span> <span class="hljs-string">.................</span> <span class="hljs-number">128</span><br>  <span class="hljs-string">relative_attention_num_buckets</span> <span class="hljs-string">..................</span> <span class="hljs-number">32</span><br>  <span class="hljs-string">replication</span> <span class="hljs-string">.....................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">replication_factor</span> <span class="hljs-string">..............................</span> <span class="hljs-number">2</span><br>  <span class="hljs-string">replication_jump</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">rerun_mode</span> <span class="hljs-string">......................................</span> <span class="hljs-string">validate_results</span><br>  <span class="hljs-string">reset_attention_mask</span> <span class="hljs-string">............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">reset_position_ids</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">result_rejected_tracker_filename</span> <span class="hljs-string">................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">retriever_report_topk_accuracies</span> <span class="hljs-string">................</span> []<br>  <span class="hljs-string">retriever_score_scaling</span> <span class="hljs-string">.........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">retriever_seq_length</span> <span class="hljs-string">............................</span> <span class="hljs-number">256</span><br>  <span class="hljs-string">retro_add_retriever</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">retro_attention_gate</span> <span class="hljs-string">............................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">retro_cyclic_train_iters</span> <span class="hljs-string">........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">retro_encoder_attention_dropout</span> <span class="hljs-string">.................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">retro_encoder_hidden_dropout</span> <span class="hljs-string">....................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">retro_encoder_layers</span> <span class="hljs-string">............................</span> <span class="hljs-number">2</span><br>  <span class="hljs-string">retro_num_neighbors</span> <span class="hljs-string">.............................</span> <span class="hljs-number">2</span><br>  <span class="hljs-string">retro_num_retrieved_chunks</span> <span class="hljs-string">......................</span> <span class="hljs-number">2</span><br>  <span class="hljs-string">retro_project_dir</span> <span class="hljs-string">...............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">retro_verify_neighbor_count</span> <span class="hljs-string">.....................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">reuse_grad_buf_for_mxfp8_param_ag</span> <span class="hljs-string">...............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">rope_scaling_factor</span> <span class="hljs-string">.............................</span> <span class="hljs-number">8.0</span><br>  <span class="hljs-string">rope_type</span> <span class="hljs-string">.......................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">rotary_base</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">10000</span><br>  <span class="hljs-string">rotary_interleaved</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">rotary_percent</span> <span class="hljs-string">..................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">rotary_scaling_factor</span> <span class="hljs-string">...........................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">rotary_seq_len_interpolation_factor</span> <span class="hljs-string">.............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">run_workload_inspector_server</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">sample_rate</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">1.0</span><br>  <span class="hljs-string">save</span> <span class="hljs-string">............................................</span> <span class="hljs-string">/workspace/megatron-lm/model_ckpt/gpt3_857m_tp4</span><br>  <span class="hljs-string">save_interval</span> <span class="hljs-string">...................................</span> <span class="hljs-number">10000</span><br>  <span class="hljs-string">save_retain_interval</span> <span class="hljs-string">............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">scatter_gather_tensors_in_pipeline</span> <span class="hljs-string">..............</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">seed</span> <span class="hljs-string">............................................</span> <span class="hljs-number">1234</span><br>  <span class="hljs-string">seq_length</span> <span class="hljs-string">......................................</span> <span class="hljs-number">2048</span><br>  <span class="hljs-string">sequence_parallel</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">sft</span> <span class="hljs-string">.............................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">sft_tokenizer_prompt_format</span> <span class="hljs-string">.....................</span> <span class="hljs-string">nemotron-h-aligned</span><br>  <span class="hljs-string">sgd_momentum</span> <span class="hljs-string">....................................</span> <span class="hljs-number">0.9</span><br>  <span class="hljs-string">sharp_enabled_group</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">short_seq_prob</span> <span class="hljs-string">..................................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">skip_train</span> <span class="hljs-string">......................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">skipped_train_samples</span> <span class="hljs-string">...........................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">spec</span> <span class="hljs-string">............................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">split</span> <span class="hljs-string">...........................................</span> <span class="hljs-number">949</span><span class="hljs-string">,50,1</span><br>  <span class="hljs-string">squared_relu</span> <span class="hljs-string">....................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">start_weight_decay</span> <span class="hljs-string">..............................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">straggler_ctrlr_port</span> <span class="hljs-string">............................</span> <span class="hljs-number">65535</span><br>  <span class="hljs-string">straggler_minmax_count</span> <span class="hljs-string">..........................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">strict_fsdp_dtensor_load</span> <span class="hljs-string">........................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">suggested_communication_unit_size</span> <span class="hljs-string">...............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">swiglu</span> <span class="hljs-string">..........................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">swin_backbone_type</span> <span class="hljs-string">..............................</span> <span class="hljs-string">tiny</span><br>  <span class="hljs-string">symmetric_ar_type</span> <span class="hljs-string">...............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">te_rng_tracker</span> <span class="hljs-string">..................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">tensor_model_parallel_size</span> <span class="hljs-string">......................</span> <span class="hljs-number">4</span><br>  <span class="hljs-string">tensorboard_dir</span> <span class="hljs-string">.................................</span> <span class="hljs-string">/workspace/megatron-lm/tb_logs/gpt3_857m_profiler_tp4</span><br>  <span class="hljs-string">tensorboard_log_interval</span> <span class="hljs-string">........................</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">tensorboard_queue_size</span> <span class="hljs-string">..........................</span> <span class="hljs-number">1000</span><br>  <span class="hljs-string">test_data_path</span> <span class="hljs-string">..................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">test_mode</span> <span class="hljs-string">.......................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">tiktoken_num_special_tokens</span> <span class="hljs-string">.....................</span> <span class="hljs-number">1000</span><br>  <span class="hljs-string">tiktoken_pattern</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">tiktoken_special_tokens</span> <span class="hljs-string">.........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">timing_log_level</span> <span class="hljs-string">................................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">timing_log_option</span> <span class="hljs-string">...............................</span> <span class="hljs-string">minmax</span><br>  <span class="hljs-string">titles_data_path</span> <span class="hljs-string">................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">tokenizer_model</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">tokenizer_type</span> <span class="hljs-string">..................................</span> <span class="hljs-string">GPT2BPETokenizer</span><br>  <span class="hljs-string">torch_fsdp2_reshard_after_forward</span> <span class="hljs-string">...............</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">tp_comm_bootstrap_backend</span> <span class="hljs-string">.......................</span> <span class="hljs-string">nccl</span><br>  <span class="hljs-string">tp_comm_bulk_dgrad</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">tp_comm_bulk_wgrad</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">tp_comm_overlap</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">tp_comm_overlap_ag</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">tp_comm_overlap_cfg</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">tp_comm_overlap_rs</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">tp_comm_overlap_rs_dgrad</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">tp_comm_split_ag</span> <span class="hljs-string">................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">tp_comm_split_rs</span> <span class="hljs-string">................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">train_data_path</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">train_iters</span> <span class="hljs-string">.....................................</span> <span class="hljs-number">20000</span><br>  <span class="hljs-string">train_samples</span> <span class="hljs-string">...................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">train_sync_interval</span> <span class="hljs-string">.............................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">transformer_impl</span> <span class="hljs-string">................................</span> <span class="hljs-string">local</span><br>  <span class="hljs-string">transformer_pipeline_model_parallel_size</span> <span class="hljs-string">........</span> <span class="hljs-number">1</span><br>  <span class="hljs-string">untie_embeddings_and_output_weights</span> <span class="hljs-string">.............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_checkpoint_args</span> <span class="hljs-string">.............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_checkpoint_opt_param_scheduler</span> <span class="hljs-string">..............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_cpu_initialization</span> <span class="hljs-string">..........................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">use_dist_ckpt</span> <span class="hljs-string">...................................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">use_dist_ckpt_deprecated</span> <span class="hljs-string">........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_distributed_optimizer</span> <span class="hljs-string">.......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_flash_attn</span> <span class="hljs-string">..................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_fused_weighted_squared_relu</span> <span class="hljs-string">.................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_legacy_models</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_megatron_fsdp</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_mp_args_from_checkpoint_args</span> <span class="hljs-string">................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_one_sent_docs</span> <span class="hljs-string">...............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_persistent_ckpt_worker</span> <span class="hljs-string">......................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_precision_aware_optimizer</span> <span class="hljs-string">...................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_pytorch_profiler</span> <span class="hljs-string">............................</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">use_ring_exchange_p2p</span> <span class="hljs-string">...........................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_rope_scaling</span> <span class="hljs-string">................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_rotary_position_embeddings</span> <span class="hljs-string">..................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_sharp</span> <span class="hljs-string">.......................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_tokenizer_model_from_checkpoint_args</span> <span class="hljs-string">........</span> <span class="hljs-literal">True</span><br>  <span class="hljs-string">use_torch_fsdp2</span> <span class="hljs-string">.................................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_torch_optimizer_for_cpu_offload</span> <span class="hljs-string">.............</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">use_tp_pp_dp_mapping</span> <span class="hljs-string">............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">v_head_dim</span> <span class="hljs-string">......................................</span> <span class="hljs-number">128</span><br>  <span class="hljs-string">valid_data_path</span> <span class="hljs-string">.................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">variable_seq_lengths</span> <span class="hljs-string">............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">virtual_pipeline_model_parallel_size</span> <span class="hljs-string">............</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">vision_backbone_type</span> <span class="hljs-string">............................</span> <span class="hljs-string">vit</span><br>  <span class="hljs-string">vision_pretraining</span> <span class="hljs-string">..............................</span> <span class="hljs-literal">False</span><br>  <span class="hljs-string">vision_pretraining_type</span> <span class="hljs-string">.........................</span> <span class="hljs-string">classify</span><br>  <span class="hljs-string">vocab_extra_ids</span> <span class="hljs-string">.................................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">vocab_file</span> <span class="hljs-string">......................................</span> <span class="hljs-string">/workspace/megatron-lm/data/tokenizer/gpt2-vocab.json</span><br>  <span class="hljs-string">vocab_size</span> <span class="hljs-string">......................................</span> <span class="hljs-string">None</span><br>  <span class="hljs-string">wandb_exp_name</span> <span class="hljs-string">..................................</span> <br>  <span class="hljs-string">wandb_project</span> <span class="hljs-string">...................................</span> <br>  <span class="hljs-string">wandb_save_dir</span> <span class="hljs-string">..................................</span> <br>  <span class="hljs-string">weight_decay</span> <span class="hljs-string">....................................</span> <span class="hljs-number">0.1</span><br>  <span class="hljs-string">weight_decay_incr_style</span> <span class="hljs-string">.........................</span> <span class="hljs-string">constant</span><br>  <span class="hljs-string">wgrad_deferral_limit</span> <span class="hljs-string">............................</span> <span class="hljs-number">0</span><br>  <span class="hljs-string">world_size</span> <span class="hljs-string">......................................</span> <span class="hljs-number">4</span><br>  <span class="hljs-string">yaml_cfg</span> <span class="hljs-string">........................................</span> <span class="hljs-string">None</span><br><span class="hljs-string">--------------------</span> <span class="hljs-string">end</span> <span class="hljs-string">of</span> <span class="hljs-string">arguments</span> <span class="hljs-string">---------------------</span><br><span class="hljs-string">INFO:megatron.core.num_microbatches_calculator:setting</span> <span class="hljs-string">number</span> <span class="hljs-string">of</span> <span class="hljs-string">microbatches</span> <span class="hljs-string">to</span> <span class="hljs-string">constant</span> <span class="hljs-number">4</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">building</span> <span class="hljs-string">GPT2BPETokenizer</span> <span class="hljs-string">tokenizer</span> <span class="hljs-string">...</span><br> <span class="hljs-string">&gt;</span> <span class="hljs-string">padded</span> <span class="hljs-string">vocab</span> <span class="hljs-string">(size:</span> <span class="hljs-number">50257</span><span class="hljs-string">)</span> <span class="hljs-string">with</span> <span class="hljs-number">431</span> <span class="hljs-string">dummy</span> <span class="hljs-string">tokens</span> <span class="hljs-string">(new</span> <span class="hljs-attr">size:</span> <span class="hljs-number">50688</span><span class="hljs-string">)</span><br><span class="hljs-string">WARNING:megatron.core.rerun_state_machine:RerunStateMachine</span> <span class="hljs-string">initialized</span> <span class="hljs-string">in</span> <span class="hljs-string">mode</span> <span class="hljs-string">RerunMode.VALIDATE_RESULTS</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">initializing</span> <span class="hljs-string">torch</span> <span class="hljs-string">distributed</span> <span class="hljs-string">...</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">initialized</span> <span class="hljs-string">tensor</span> <span class="hljs-string">model</span> <span class="hljs-string">parallel</span> <span class="hljs-string">with</span> <span class="hljs-string">size</span> <span class="hljs-number">4</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">initialized</span> <span class="hljs-string">pipeline</span> <span class="hljs-string">model</span> <span class="hljs-string">parallel</span> <span class="hljs-string">with</span> <span class="hljs-string">size</span> <span class="hljs-number">1</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">setting</span> <span class="hljs-string">random</span> <span class="hljs-string">seeds</span> <span class="hljs-string">to</span> <span class="hljs-number">1234</span> <span class="hljs-string">...</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">compiling</span> <span class="hljs-string">dataset</span> <span class="hljs-string">index</span> <span class="hljs-string">builder</span> <span class="hljs-string">...</span><br><span class="hljs-attr">make:</span> <span class="hljs-string">Entering</span> <span class="hljs-string">directory</span> <span class="hljs-string">&#x27;/workspace/megatron-lm/megatron/core/datasets&#x27;</span><br>[<span class="hljs-string">rank2</span>]<span class="hljs-string">:[W103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:34.420203100</span> <span class="hljs-string">ProcessGroupNCCL.cpp:4751]</span> [<span class="hljs-string">PG</span> <span class="hljs-string">ID</span> <span class="hljs-number">0</span> <span class="hljs-string">PG</span> <span class="hljs-string">GUID</span> <span class="hljs-number">0</span> <span class="hljs-string">Rank</span> <span class="hljs-number">2</span>]  <span class="hljs-string">using</span> <span class="hljs-string">GPU</span> <span class="hljs-number">2</span> <span class="hljs-string">as</span> <span class="hljs-string">device</span> <span class="hljs-string">used</span> <span class="hljs-string">by</span> <span class="hljs-string">this</span> <span class="hljs-string">process</span> <span class="hljs-string">is</span> <span class="hljs-string">currently</span> <span class="hljs-string">unknown.</span> <span class="hljs-string">This</span> <span class="hljs-string">can</span> <span class="hljs-string">potentially</span> <span class="hljs-string">cause</span> <span class="hljs-string">a</span> <span class="hljs-string">hang</span> <span class="hljs-string">if</span> <span class="hljs-string">this</span> <span class="hljs-string">rank</span> <span class="hljs-string">to</span> <span class="hljs-string">GPU</span> <span class="hljs-string">mapping</span> <span class="hljs-string">is</span> <span class="hljs-string">incorrect.</span> <span class="hljs-string">You</span> <span class="hljs-string">can</span> <span class="hljs-string">pecify</span> <span class="hljs-string">device_id</span> <span class="hljs-string">in</span> <span class="hljs-string">init_process_group()</span> <span class="hljs-string">to</span> <span class="hljs-string">force</span> <span class="hljs-string">use</span> <span class="hljs-string">of</span> <span class="hljs-string">a</span> <span class="hljs-string">particular</span> <span class="hljs-string">device.</span><br>[<span class="hljs-string">rank1</span>]<span class="hljs-string">:[W103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:34.605196852</span> <span class="hljs-string">ProcessGroupNCCL.cpp:4751]</span> [<span class="hljs-string">PG</span> <span class="hljs-string">ID</span> <span class="hljs-number">0</span> <span class="hljs-string">PG</span> <span class="hljs-string">GUID</span> <span class="hljs-number">0</span> <span class="hljs-string">Rank</span> <span class="hljs-number">1</span>]  <span class="hljs-string">using</span> <span class="hljs-string">GPU</span> <span class="hljs-number">1</span> <span class="hljs-string">as</span> <span class="hljs-string">device</span> <span class="hljs-string">used</span> <span class="hljs-string">by</span> <span class="hljs-string">this</span> <span class="hljs-string">process</span> <span class="hljs-string">is</span> <span class="hljs-string">currently</span> <span class="hljs-string">unknown.</span> <span class="hljs-string">This</span> <span class="hljs-string">can</span> <span class="hljs-string">potentially</span> <span class="hljs-string">cause</span> <span class="hljs-string">a</span> <span class="hljs-string">hang</span> <span class="hljs-string">if</span> <span class="hljs-string">this</span> <span class="hljs-string">rank</span> <span class="hljs-string">to</span> <span class="hljs-string">GPU</span> <span class="hljs-string">mapping</span> <span class="hljs-string">is</span> <span class="hljs-string">incorrect.</span> <span class="hljs-string">You</span> <span class="hljs-string">can</span> <span class="hljs-string">pecify</span> <span class="hljs-string">device_id</span> <span class="hljs-string">in</span> <span class="hljs-string">init_process_group()</span> <span class="hljs-string">to</span> <span class="hljs-string">force</span> <span class="hljs-string">use</span> <span class="hljs-string">of</span> <span class="hljs-string">a</span> <span class="hljs-string">particular</span> <span class="hljs-string">device.</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">setting</span> <span class="hljs-string">tensorboard</span> <span class="hljs-string">...</span><br><span class="hljs-attr">WARNING:</span> <span class="hljs-string">one_logger</span> <span class="hljs-string">package</span> <span class="hljs-string">is</span> <span class="hljs-string">required</span> <span class="hljs-string">to</span> <span class="hljs-string">enable</span> <span class="hljs-string">e2e</span> <span class="hljs-string">metrics</span> <span class="hljs-string">tracking.</span> <span class="hljs-string">please</span> <span class="hljs-string">go</span> <span class="hljs-string">to</span> <span class="hljs-string">https://confluence.nvidia.com/display/MLWFO/Package+Repositories</span> <span class="hljs-string">for</span> <span class="hljs-string">details</span> <span class="hljs-string">to</span> <span class="hljs-string">install</span> <span class="hljs-string">it</span><br><span class="hljs-attr">make:</span> <span class="hljs-string">Nothing</span> <span class="hljs-string">to</span> <span class="hljs-string">be</span> <span class="hljs-string">done</span> <span class="hljs-string">for</span> <span class="hljs-string">&#x27;default&#x27;</span><span class="hljs-string">.</span><br><span class="hljs-attr">make:</span> <span class="hljs-string">Leaving</span> <span class="hljs-string">directory</span> <span class="hljs-string">&#x27;/workspace/megatron-lm/megatron/core/datasets&#x27;</span><br><span class="hljs-string">&gt;&gt;&gt;</span> <span class="hljs-attr">done with dataset index builder. Compilation time:</span> <span class="hljs-number">0.207</span> <span class="hljs-string">seconds</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">compiling</span> <span class="hljs-string">and</span> <span class="hljs-string">loading</span> <span class="hljs-string">fused</span> <span class="hljs-string">kernels</span> <span class="hljs-string">...</span><br>[<span class="hljs-string">rank3</span>]<span class="hljs-string">:[W103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:34.713259577</span> <span class="hljs-string">ProcessGroupNCCL.cpp:4751]</span> [<span class="hljs-string">PG</span> <span class="hljs-string">ID</span> <span class="hljs-number">0</span> <span class="hljs-string">PG</span> <span class="hljs-string">GUID</span> <span class="hljs-number">0</span> <span class="hljs-string">Rank</span> <span class="hljs-number">3</span>]  <span class="hljs-string">using</span> <span class="hljs-string">GPU</span> <span class="hljs-number">3</span> <span class="hljs-string">as</span> <span class="hljs-string">device</span> <span class="hljs-string">used</span> <span class="hljs-string">by</span> <span class="hljs-string">this</span> <span class="hljs-string">process</span> <span class="hljs-string">is</span> <span class="hljs-string">currently</span> <span class="hljs-string">unknown.</span> <span class="hljs-string">This</span> <span class="hljs-string">can</span> <span class="hljs-string">potentially</span> <span class="hljs-string">cause</span> <span class="hljs-string">a</span> <span class="hljs-string">hang</span> <span class="hljs-string">if</span> <span class="hljs-string">this</span> <span class="hljs-string">rank</span> <span class="hljs-string">to</span> <span class="hljs-string">GPU</span> <span class="hljs-string">mapping</span> <span class="hljs-string">is</span> <span class="hljs-string">incorrect.</span> <span class="hljs-string">You</span> <span class="hljs-string">can</span> <span class="hljs-string">pecify</span> <span class="hljs-string">device_id</span> <span class="hljs-string">in</span> <span class="hljs-string">init_process_group()</span> <span class="hljs-string">to</span> <span class="hljs-string">force</span> <span class="hljs-string">use</span> <span class="hljs-string">of</span> <span class="hljs-string">a</span> <span class="hljs-string">particular</span> <span class="hljs-string">device.</span><br>[<span class="hljs-string">rank0</span>]<span class="hljs-string">:[W103</span> <span class="hljs-number">04</span><span class="hljs-string">:22:34.732739062</span> <span class="hljs-string">ProcessGroupNCCL.cpp:4751]</span> [<span class="hljs-string">PG</span> <span class="hljs-string">ID</span> <span class="hljs-number">0</span> <span class="hljs-string">PG</span> <span class="hljs-string">GUID</span> <span class="hljs-number">0</span> <span class="hljs-string">Rank</span> <span class="hljs-number">0</span>]  <span class="hljs-string">using</span> <span class="hljs-string">GPU</span> <span class="hljs-number">0</span> <span class="hljs-string">as</span> <span class="hljs-string">device</span> <span class="hljs-string">used</span> <span class="hljs-string">by</span> <span class="hljs-string">this</span> <span class="hljs-string">process</span> <span class="hljs-string">is</span> <span class="hljs-string">currently</span> <span class="hljs-string">unknown.</span> <span class="hljs-string">This</span> <span class="hljs-string">can</span> <span class="hljs-string">potentially</span> <span class="hljs-string">cause</span> <span class="hljs-string">a</span> <span class="hljs-string">hang</span> <span class="hljs-string">if</span> <span class="hljs-string">this</span> <span class="hljs-string">rank</span> <span class="hljs-string">to</span> <span class="hljs-string">GPU</span> <span class="hljs-string">mapping</span> <span class="hljs-string">is</span> <span class="hljs-string">incorrect.</span> <span class="hljs-string">You</span> <span class="hljs-string">can</span> <span class="hljs-string">pecify</span> <span class="hljs-string">device_id</span> <span class="hljs-string">in</span> <span class="hljs-string">init_process_group()</span> <span class="hljs-string">to</span> <span class="hljs-string">force</span> <span class="hljs-string">use</span> <span class="hljs-string">of</span> <span class="hljs-string">a</span> <span class="hljs-string">particular</span> <span class="hljs-string">device.</span><br><span class="hljs-string">&gt;&gt;&gt;</span> <span class="hljs-attr">done with compiling and loading fused kernels. Compilation time:</span> <span class="hljs-number">0.324</span> <span class="hljs-string">seconds</span><br><span class="hljs-string">time</span> <span class="hljs-string">to</span> <span class="hljs-string">initialize</span> <span class="hljs-string">megatron</span> <span class="hljs-string">(seconds):</span> <span class="hljs-number">2.393</span><br>[<span class="hljs-string">after</span> <span class="hljs-string">megatron</span> <span class="hljs-string">is</span> <span class="hljs-string">initialized</span>] <span class="hljs-attr">datetime:</span> <span class="hljs-number">2026-01-03 04:22:36</span> <br><span class="hljs-string">building</span> <span class="hljs-string">GPT</span> <span class="hljs-string">model</span> <span class="hljs-string">...</span><br> <span class="hljs-string">&gt;</span> <span class="hljs-string">number</span> <span class="hljs-string">of</span> <span class="hljs-string">parameters</span> <span class="hljs-string">on</span> <span class="hljs-string">(tensor,</span> <span class="hljs-string">pipeline)</span> <span class="hljs-string">model</span> <span class="hljs-string">parallel</span> <span class="hljs-string">rank</span> <span class="hljs-string">(3,</span> <span class="hljs-number">0</span><span class="hljs-string">):</span> <span class="hljs-number">90763264</span><br> <span class="hljs-string">&gt;</span> <span class="hljs-string">number</span> <span class="hljs-string">of</span> <span class="hljs-string">parameters</span> <span class="hljs-string">on</span> <span class="hljs-string">(tensor,</span> <span class="hljs-string">pipeline)</span> <span class="hljs-string">model</span> <span class="hljs-string">parallel</span> <span class="hljs-string">rank</span> <span class="hljs-string">(0,</span> <span class="hljs-number">0</span><span class="hljs-string">):</span> <span class="hljs-number">90763264</span><br> <span class="hljs-string">&gt;</span> <span class="hljs-string">number</span> <span class="hljs-string">of</span> <span class="hljs-string">parameters</span> <span class="hljs-string">on</span> <span class="hljs-string">(tensor,</span> <span class="hljs-string">pipeline)</span> <span class="hljs-string">model</span> <span class="hljs-string">parallel</span> <span class="hljs-string">rank</span> <span class="hljs-string">(2,</span> <span class="hljs-number">0</span><span class="hljs-string">):</span> <span class="hljs-number">90763264</span><br> <span class="hljs-string">&gt;</span> <span class="hljs-string">number</span> <span class="hljs-string">of</span> <span class="hljs-string">parameters</span> <span class="hljs-string">on</span> <span class="hljs-string">(tensor,</span> <span class="hljs-string">pipeline)</span> <span class="hljs-string">model</span> <span class="hljs-string">parallel</span> <span class="hljs-string">rank</span> <span class="hljs-string">(1,</span> <span class="hljs-number">0</span><span class="hljs-string">):</span> <span class="hljs-number">90763264</span><br><span class="hljs-string">INFO:megatron.core.distributed.distributed_data_parallel:Setting</span> <span class="hljs-string">up</span> <span class="hljs-string">DistributedDataParallel</span> <span class="hljs-string">with</span> <span class="hljs-string">config</span> <span class="hljs-string">DistributedDataParallelConfig(grad_reduce_in_fp32=False,</span> <span class="hljs-string">overlap_grad_reduce=False,</span> <span class="hljs-string">overlap_param_gather=False,</span> <span class="hljs-string">align_param_gather=False,</span> <span class="hljs-string">use_distributed_optimizer=False,</span> <span class="hljs-string">num_distributed_optimizer_instances=1,</span> <span class="hljs-string">check_for_nan_in_grad=False,</span> <span class="hljs-string">check_for_large_grads=False,</span> <span class="hljs-string">bucket_size=None,</span> <span class="hljs-string">pad_buckets_for_high_nccl_busbw=False,</span> <span class="hljs-string">average_in_collective=False,</span> <span class="hljs-string">fp8_param_gather=False,</span> <span class="hljs-string">reuse_grad_buf_for_mxfp8_param_ag=False,</span> <span class="hljs-string">use_megatron_fsdp=False,</span> <span class="hljs-string">use_custom_fsdp=False,</span> <span class="hljs-string">data_parallel_sharding_strategy=&#x27;no_shard&#x27;,</span> <span class="hljs-string">gradient_reduce_div_fusion=True,</span> <span class="hljs-string">suggested_communication_unit_size=None,</span> <span class="hljs-string">preserve_fp32_weights=True,</span> <span class="hljs-string">keep_fp8_transpose_cache=False,</span> <span class="hljs-string">nccl_ub=False,</span> <span class="hljs-string">fsdp_double_buffer=False,</span> <span class="hljs-string">outer_dp_sharding_strategy=&#x27;no_shard&#x27;,</span> <span class="hljs-string">disable_symmetric_registration=False,</span> <span class="hljs-string">delay_wgrad_compute=False)</span><br><span class="hljs-attr">INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter:</span> <span class="hljs-number">1</span><br><span class="hljs-string">Params</span> <span class="hljs-string">for</span> <span class="hljs-string">bucket</span> <span class="hljs-number">1</span> <span class="hljs-string">(90763264</span> <span class="hljs-string">elements,</span> <span class="hljs-number">90763264</span> <span class="hljs-string">padded</span> <span class="hljs-string">size):</span><br>    <span class="hljs-string">module.decoder.layers.22.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.20.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.18.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.16.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.8.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.7.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.3.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.1.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.21.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.14.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.11.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.6.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.5.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.2.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.21.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.20.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.18.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.15.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.11.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.6.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.23.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.22.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.14.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.13.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.10.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.9.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.1.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.23.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.22.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.20.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.17.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.16.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.15.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.13.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.10.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.5.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.4.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.22.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.15.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.11.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.9.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.6.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.embedding.word_embeddings.weight</span><br>    <span class="hljs-string">module.decoder.layers.22.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.19.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.17.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.11.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.9.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.5.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.4.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.17.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.13.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.11.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.7.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.final_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.22.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.20.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.19.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.16.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.15.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.13.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.11.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.9.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.0.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.19.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.16.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.15.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.13.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.6.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.6.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.5.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.4.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.3.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.1.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.22.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.17.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.15.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.13.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.13.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.11.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.17.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.15.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.8.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.4.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.4.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.2.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.1.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.embedding.position_embeddings.weight</span><br>    <span class="hljs-string">module.decoder.layers.1.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.final_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.21.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.20.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.19.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.17.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.13.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.11.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.6.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.17.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.20.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.19.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.15.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.23.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.22.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.21.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.19.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.13.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.10.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.9.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.5.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.1.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.21.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.19.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.17.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.15.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.13.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.4.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.2.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.23.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.11.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.10.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.9.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.8.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.5.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.5.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.2.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.23.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.19.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.17.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.14.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.10.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.6.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.4.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.0.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.22.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.21.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.12.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.6.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.4.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.22.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.21.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.19.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.16.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.15.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.12.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.9.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.5.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.4.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.23.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.21.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.11.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.10.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.9.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.5.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.5.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.17.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.23.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.21.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.18.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.16.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.11.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.10.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.9.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.5.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.23.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.16.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.13.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.12.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.10.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.9.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.6.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.23.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.21.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.19.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.15.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.13.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.10.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.1.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.15.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.9.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.7.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.5.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.4.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.1.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.23.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.21.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.20.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.17.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.16.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.14.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.10.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.3.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.1.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.20.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.17.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.16.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.4.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.1.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.23.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.19.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.16.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.12.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.10.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.1.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.18.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.20.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.19.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.16.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.7.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.6.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.3.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.2.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.22.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.21.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.20.self_attention.linear_proj.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.12.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.9.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.6.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.0.mlp.linear_fc1.weight</span><br>    <span class="hljs-string">module.decoder.layers.20.pre_mlp_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.18.mlp.linear_fc2.weight</span><br>    <span class="hljs-string">module.decoder.layers.16.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.14.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.8.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.6.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.4.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.0.self_attention.linear_qkv.weight</span><br>    <span class="hljs-string">module.decoder.layers.23.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.22.pre_mlp_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.20.input_layernorm.weight</span><br>    <span class="hljs-string">module.decoder.layers.11.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.10.mlp.linear_fc1.bias</span><br>    <span class="hljs-string">module.decoder.layers.8.self_attention.linear_proj.weight</span><br>    <span class="hljs-string">module.decoder.layers.7.input_layernorm.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.mlp.linear_fc2.bias</span><br>    <span class="hljs-string">module.decoder.layers.3.self_attention.linear_qkv.bias</span><br>    <span class="hljs-string">module.decoder.layers.1.input_layernorm.bias</span><br><span class="hljs-string">INFO:megatron.core.optimizer:Setting</span> <span class="hljs-string">up</span> <span class="hljs-string">optimizer</span> <span class="hljs-string">with</span> <span class="hljs-string">config</span> <span class="hljs-string">OptimizerConfig(optimizer=&#x27;adam&#x27;,</span> <span class="hljs-string">lr=6e-05,</span> <span class="hljs-string">min_lr=6e-06,</span> <span class="hljs-string">decoupled_lr=None,</span> <span class="hljs-string">decoupled_min_lr=None,</span> <span class="hljs-string">weight_decay=0.1,</span> <span class="hljs-string">fp8_recipe=&#x27;delayed&#x27;,</span> <span class="hljs-string">fp16=True,</span> <span class="hljs-string">bf16=False,</span> <span class="hljs-string">reuse_grad_buf_for_mxfp8_param_ag=False,</span> <span class="hljs-string">params_dtype=torch.float16,</span> <span class="hljs-string">use_precision_aware_optimizer=False,</span> <span class="hljs-string">store_param_remainders=True,</span> <span class="hljs-string">main_grads_dtype=torch.float32,</span> <span class="hljs-string">main_params_dtype=torch.float32,</span> <span class="hljs-string">exp_avg_dtype=torch.float32,</span> <span class="hljs-string">exp_avg_sq_dtype=torch.float32,</span> <span class="hljs-string">loss_scale=None,</span> <span class="hljs-string">initial_loss_scale=4294967296,</span> <span class="hljs-string">min_loss_scale=1.0,</span> <span class="hljs-string">loss_scale_window=1000,</span> <span class="hljs-string">hysteresis=2,</span> <span class="hljs-string">adam_beta1=0.9,</span> <span class="hljs-string">adam_beta2=0.95,</span> <span class="hljs-string">adam_eps=1e-08,</span> <span class="hljs-string">sgd_momentum=0.9,</span> <span class="hljs-string">use_distributed_optimizer=False,</span> <span class="hljs-string">overlap_param_gather=False,</span> <span class="hljs-string">overlap_param_gather_with_optimizer_step=False,</span> <span class="hljs-string">use_megatron_fsdp=False,</span> <span class="hljs-string">optimizer_cpu_offload=False,</span> <span class="hljs-string">optimizer_offload_fraction=1.0,</span> <span class="hljs-string">use_torch_optimizer_for_cpu_offload=False,</span> <span class="hljs-string">overlap_cpu_optimizer_d2h_h2d=False,</span> <span class="hljs-string">pin_cpu_grads=True,</span> <span class="hljs-string">pin_cpu_params=True,</span> <span class="hljs-string">clip_grad=1.0,</span> <span class="hljs-string">log_num_zeros_in_grad=False,</span> <span class="hljs-string">barrier_with_L1_time=True,</span> <span class="hljs-string">timers=&lt;megatron.core.timers.Timers</span> <span class="hljs-string">object</span> <span class="hljs-string">at</span> <span class="hljs-number">0x7f59b73c1430</span><span class="hljs-string">&gt;,</span> <span class="hljs-string">config_logger_dir=&#x27;&#x27;)</span><br><span class="hljs-string">INFO:megatron.core.optimizer_param_scheduler:&gt;</span> <span class="hljs-attr">learning rate decay style:</span> <span class="hljs-string">cosine</span><br><span class="hljs-attr">WARNING:</span> <span class="hljs-string">could</span> <span class="hljs-string">not</span> <span class="hljs-string">find</span> <span class="hljs-string">the</span> <span class="hljs-string">metadata</span> <span class="hljs-string">file</span> <span class="hljs-string">/workspace/megatron-lm/model_ckpt/gpt3_857m_tp4/latest_checkpointed_iteration.txt</span><br>    <span class="hljs-string">will</span> <span class="hljs-string">not</span> <span class="hljs-string">load</span> <span class="hljs-string">any</span> <span class="hljs-string">checkpoints</span> <span class="hljs-string">and</span> <span class="hljs-string">will</span> <span class="hljs-string">start</span> <span class="hljs-string">from</span> <span class="hljs-string">random</span><br><span class="hljs-string">(min,</span> <span class="hljs-string">max)</span> <span class="hljs-string">time</span> <span class="hljs-string">across</span> <span class="hljs-string">ranks</span> <span class="hljs-string">(ms):</span><br>    <span class="hljs-attr">load-checkpoint ................................:</span> <span class="hljs-string">(0.29,</span> <span class="hljs-number">0.30</span><span class="hljs-string">)</span><br>[<span class="hljs-string">after</span> <span class="hljs-string">model</span>, <span class="hljs-string">optimizer</span>, <span class="hljs-string">and</span> <span class="hljs-string">learning</span> <span class="hljs-string">rate</span> <span class="hljs-string">scheduler</span> <span class="hljs-string">are</span> <span class="hljs-string">built</span>] <span class="hljs-attr">datetime:</span> <span class="hljs-number">2026-01-03 04:22:36</span> <br><span class="hljs-string">&gt;</span> <span class="hljs-string">building</span> <span class="hljs-string">train,</span> <span class="hljs-string">validation,</span> <span class="hljs-string">and</span> <span class="hljs-string">test</span> <span class="hljs-string">datasets</span> <span class="hljs-string">...</span><br> <span class="hljs-string">&gt;</span> <span class="hljs-string">datasets</span> <span class="hljs-string">target</span> <span class="hljs-string">sizes</span> <span class="hljs-string">(minimum</span> <span class="hljs-string">size):</span><br>    <span class="hljs-attr">train:</span>      <span class="hljs-number">320000</span><br>    <span class="hljs-attr">validation:</span> <span class="hljs-number">3360</span><br>    <span class="hljs-attr">test:</span>       <span class="hljs-number">160</span><br><span class="hljs-string">INFO:megatron.core.datasets.blended_megatron_dataset_config:Let</span> <span class="hljs-string">split_matrix</span> <span class="hljs-string">=</span> [<span class="hljs-string">(0</span>, <span class="hljs-number">0.949</span><span class="hljs-string">)</span>, <span class="hljs-string">(0.949</span>, <span class="hljs-number">0.999</span><span class="hljs-string">)</span>, <span class="hljs-string">(0.999</span>, <span class="hljs-number">1.0</span><span class="hljs-string">)</span>]<br><span class="hljs-string">&gt;</span> <span class="hljs-string">building</span> <span class="hljs-string">train,</span> <span class="hljs-string">validation,</span> <span class="hljs-string">and</span> <span class="hljs-string">test</span> <span class="hljs-string">datasets</span> <span class="hljs-string">for</span> <span class="hljs-string">GPT</span> <span class="hljs-string">...</span><br><span class="hljs-string">INFO:megatron.core.datasets.blended_megatron_dataset_builder:Building</span> <span class="hljs-string">GPTDataset</span> <span class="hljs-string">splits</span> <span class="hljs-string">with</span> <span class="hljs-string">sizes=(320000,</span> <span class="hljs-number">3360</span><span class="hljs-string">,</span> <span class="hljs-number">160</span><span class="hljs-string">)</span> <span class="hljs-string">and</span> <span class="hljs-string">config=GPTDatasetConfig(random_seed=1234,</span> <span class="hljs-string">sequence_length=2048,</span> <span class="hljs-string">blend=([&#x27;/workspace/megatron-lm/data/TinyStoriesV2-GPT4-train_text_document&#x27;],</span> <span class="hljs-string">None),</span> <span class="hljs-string">blend_per_split=None,</span> <span class="hljs-string">multiple_validation_sets=False,</span> <span class="hljs-string">full_validation=False,</span> <span class="hljs-string">split=&#x27;949,50,1&#x27;,</span> <span class="hljs-string">split_matrix=[(0,</span> <span class="hljs-number">0.949</span><span class="hljs-string">),</span> <span class="hljs-string">(0.949,</span> <span class="hljs-number">0.999</span><span class="hljs-string">),</span> <span class="hljs-string">(0.999,</span> <span class="hljs-number">1.0</span><span class="hljs-string">)],</span> <span class="hljs-string">num_dataset_builder_threads=1,</span> <span class="hljs-string">path_to_cache=None,</span> <span class="hljs-string">mmap_bin_files=True,</span> <span class="hljs-string">mock=False,</span> <span class="hljs-string">tokenizer=&lt;megatron.training.tokenizer.tokenizer._GPT2BPETokenizer</span> <span class="hljs-string">object</span> <span class="hljs-string">at</span> <span class="hljs-number">0x7f59b702e240</span><span class="hljs-string">&gt;,</span> <span class="hljs-string">mid_level_dataset_surplus=0.005,</span> <span class="hljs-string">reset_position_ids=False,</span> <span class="hljs-string">reset_attention_mask=False,</span> <span class="hljs-string">eod_mask_loss=False,</span> <span class="hljs-string">create_attention_mask=True,</span> <span class="hljs-string">drop_last_partial_validation_sequence=True,</span> <span class="hljs-string">add_extra_token_to_sequence=True,</span> <span class="hljs-string">object_storage_cache_path=None)</span><br><span class="hljs-string">INFO:megatron.core.datasets.indexed_dataset:Load</span> <span class="hljs-string">the</span> <span class="hljs-string">_IndexReader</span> <span class="hljs-string">from</span> <span class="hljs-string">/workspace/megatron-lm/data/TinyStoriesV2-GPT4-train_text_document.idx</span><br><span class="hljs-attr">INFO:megatron.core.datasets.indexed_dataset:</span>    <span class="hljs-string">Extract</span> <span class="hljs-string">the</span> <span class="hljs-string">sequence</span> <span class="hljs-string">lengths</span><br><span class="hljs-attr">INFO:megatron.core.datasets.indexed_dataset:</span>    <span class="hljs-string">Extract</span> <span class="hljs-string">the</span> <span class="hljs-string">sequence</span> <span class="hljs-string">pointers</span><br><span class="hljs-attr">INFO:megatron.core.datasets.indexed_dataset:</span>    <span class="hljs-string">Extract</span> <span class="hljs-string">the</span> <span class="hljs-string">document</span> <span class="hljs-string">indices</span><br><span class="hljs-string">INFO:megatron.core.datasets.indexed_dataset:&gt;</span> <span class="hljs-attr">total number of sequences:</span> <span class="hljs-number">14548094</span><br><span class="hljs-string">INFO:megatron.core.datasets.indexed_dataset:&gt;</span> <span class="hljs-attr">total number of documents:</span> <span class="hljs-number">14548094</span><br><span class="hljs-string">INFO:megatron.core.datasets.gpt_dataset:Load</span> <span class="hljs-string">the</span> <span class="hljs-string">GPTDataset</span> <span class="hljs-string">train</span> <span class="hljs-string">indices</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">document</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">c376e20e5de541283d4ccc974c960cb8-GPTDataset-train-document_index.npy</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">sample</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">c376e20e5de541283d4ccc974c960cb8-GPTDataset-train-sample_index.npy</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">shuffle</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">c376e20e5de541283d4ccc974c960cb8-GPTDataset-train-shuffle_index.npy</span><br><span class="hljs-string">INFO:megatron.core.datasets.gpt_dataset:&gt;</span> <span class="hljs-attr">total number of samples:</span> <span class="hljs-number">521301</span><br><span class="hljs-string">INFO:megatron.core.datasets.gpt_dataset:Load</span> <span class="hljs-string">the</span> <span class="hljs-string">GPTDataset</span> <span class="hljs-string">valid</span> <span class="hljs-string">indices</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">document</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">f975a8258f34477c465b869135b1a202-GPTDataset-valid-document_index.npy</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">sample</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">f975a8258f34477c465b869135b1a202-GPTDataset-valid-sample_index.npy</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">shuffle</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">f975a8258f34477c465b869135b1a202-GPTDataset-valid-shuffle_index.npy</span><br><span class="hljs-string">INFO:megatron.core.datasets.gpt_dataset:&gt;</span> <span class="hljs-attr">total number of samples:</span> <span class="hljs-number">13728</span><br><span class="hljs-string">INFO:megatron.core.datasets.gpt_dataset:Load</span> <span class="hljs-string">the</span> <span class="hljs-string">GPTDataset</span> <span class="hljs-string">test</span> <span class="hljs-string">indices</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">document</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">b12f62104fc19a6d3c6c6402fedd7e04-GPTDataset-test-document_index.npy</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">sample</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">b12f62104fc19a6d3c6c6402fedd7e04-GPTDataset-test-sample_index.npy</span><br><span class="hljs-attr">INFO:megatron.core.datasets.gpt_dataset:</span>    <span class="hljs-string">Load</span> <span class="hljs-string">the</span> <span class="hljs-string">shuffle</span> <span class="hljs-string">index</span> <span class="hljs-string">from</span> <span class="hljs-string">b12f62104fc19a6d3c6c6402fedd7e04-GPTDataset-test-shuffle_index.npy</span><br><span class="hljs-string">INFO:megatron.core.datasets.gpt_dataset:&gt;</span> <span class="hljs-attr">total number of samples:</span> <span class="hljs-number">273</span><br><span class="hljs-string">&gt;</span> <span class="hljs-string">finished</span> <span class="hljs-string">creating</span> <span class="hljs-string">GPT</span> <span class="hljs-string">datasets</span> <span class="hljs-string">...</span><br>[<span class="hljs-string">after</span> <span class="hljs-string">dataloaders</span> <span class="hljs-string">are</span> <span class="hljs-string">built</span>] <span class="hljs-attr">datetime:</span> <span class="hljs-number">2026-01-03 04:22:36</span> <br><span class="hljs-string">done</span> <span class="hljs-string">with</span> <span class="hljs-string">setup</span> <span class="hljs-string">...</span><br><span class="hljs-string">(min,</span> <span class="hljs-string">max)</span> <span class="hljs-string">time</span> <span class="hljs-string">across</span> <span class="hljs-string">ranks</span> <span class="hljs-string">(ms):</span><br>    <span class="hljs-attr">model-and-optimizer-setup ......................:</span> <span class="hljs-string">(162.43,</span> <span class="hljs-number">174.52</span><span class="hljs-string">)</span><br>    <span class="hljs-attr">train/valid/test-data-iterators-setup ..........:</span> <span class="hljs-string">(26.90,</span> <span class="hljs-number">152.19</span><span class="hljs-string">)</span><br><span class="hljs-string">training</span> <span class="hljs-string">...</span><br><span class="hljs-string">Overwriting</span> <span class="hljs-string">rerun_state_machine.current_iteration</span> <span class="hljs-string">from</span> <span class="hljs-number">-1</span> <span class="hljs-string">to</span> <span class="hljs-number">0</span><span class="hljs-string">...</span><br>[<span class="hljs-string">before</span> <span class="hljs-string">the</span> <span class="hljs-string">start</span> <span class="hljs-string">of</span> <span class="hljs-string">training</span> <span class="hljs-string">step</span>] <span class="hljs-attr">datetime:</span> <span class="hljs-number">2026-01-03 04:22:36</span> <br><span class="hljs-attr">Number of parameters in transformer block in billions:</span>  <span class="hljs-number">0.30</span><br><span class="hljs-attr">Number of parameters in embedding layers in billions:</span> <span class="hljs-number">0.05</span><br><span class="hljs-attr">Total number of parameters in billions:</span> <span class="hljs-number">0.35</span><br><span class="hljs-attr">Number of parameters in most loaded shard in billions:</span> <span class="hljs-number">0.0885</span><br><span class="hljs-attr">Theoretical memory footprints:</span> <span class="hljs-string">weight</span> <span class="hljs-string">and</span> <span class="hljs-string">optimizer=1519.18</span> <span class="hljs-string">MB</span><br> [<span class="hljs-number">2026-01-03 04:29:07</span>] <span class="hljs-string">iteration</span>      <span class="hljs-number">200</span><span class="hljs-string">/</span>   <span class="hljs-number">20000</span> <span class="hljs-string">|</span> <span class="hljs-attr">consumed samples:</span>         <span class="hljs-number">3200</span> <span class="hljs-string">|</span> <span class="hljs-string">elapsed</span> <span class="hljs-string">time</span> <span class="hljs-string">per</span> <span class="hljs-string">iteration</span> <span class="hljs-string">(ms):</span> <span class="hljs-number">1952.1</span> <span class="hljs-string">|</span> <span class="hljs-attr">learning rate:</span> <span class="hljs-number">5.999146E-05</span> <span class="hljs-string">|</span> <span class="hljs-attr">global batch size:</span>    <span class="hljs-number">16</span> <span class="hljs-string">|</span> <span class="hljs-attr">lm loss:</span> <span class="hljs-number">5.665651E+00</span> <span class="hljs-string">|</span> <span class="hljs-attr">loss scale:</span> <span class="hljs-number">8192.0</span> <span class="hljs-string">|</span> <span class="hljs-attr">grad norm:</span> <span class="hljs-number">0.833</span> <span class="hljs-string">|</span> <span class="hljs-attr">number of skipped iterations:</span>  <span class="hljs-number">20</span> <span class="hljs-string">|</span> <span class="hljs-attr">number of nan iterations:</span>   <span class="hljs-number">0</span> <span class="hljs-string">|</span><br>[<span class="hljs-string">Rank</span> <span class="hljs-number">1</span>] <span class="hljs-string">(after</span> <span class="hljs-number">200</span> <span class="hljs-string">iterations)</span> <span class="hljs-string">memory</span> <span class="hljs-string">(MB)</span> <span class="hljs-string">|</span> <span class="hljs-attr">allocated:</span> <span class="hljs-number">1935.56005859375</span> <span class="hljs-string">|</span> <span class="hljs-attr">max allocated:</span> <span class="hljs-number">12972.46923828125</span> <span class="hljs-string">|</span> <span class="hljs-attr">reserved:</span> <span class="hljs-number">14548.0</span> <span class="hljs-string">|</span> <span class="hljs-attr">max reserved:</span> <span class="hljs-number">14548.0</span><br>[<span class="hljs-string">Rank</span> <span class="hljs-number">2</span>] <span class="hljs-string">(after</span> <span class="hljs-number">200</span> <span class="hljs-string">iterations)</span> <span class="hljs-string">memory</span> <span class="hljs-string">(MB)</span> <span class="hljs-string">|</span> <span class="hljs-attr">allocated:</span> <span class="hljs-number">1911.18505859375</span> <span class="hljs-string">|</span> <span class="hljs-attr">max allocated:</span> <span class="hljs-number">12948.21923828125</span> <span class="hljs-string">|</span> <span class="hljs-attr">reserved:</span> <span class="hljs-number">14516.0</span> <span class="hljs-string">|</span> <span class="hljs-attr">max reserved:</span> <span class="hljs-number">14516.0</span><br>[<span class="hljs-string">Rank</span> <span class="hljs-number">0</span>] <span class="hljs-string">(after</span> <span class="hljs-number">200</span> <span class="hljs-string">iterations)</span> <span class="hljs-string">memory</span> <span class="hljs-string">(MB)</span> <span class="hljs-string">|</span> <span class="hljs-attr">allocated:</span> <span class="hljs-number">1923.56005859375</span> <span class="hljs-string">|</span> <span class="hljs-attr">max allocated:</span> <span class="hljs-number">12960.71923828125</span> <span class="hljs-string">|</span> <span class="hljs-attr">reserved:</span> <span class="hljs-number">14812.0</span> <span class="hljs-string">|</span> <span class="hljs-attr">max reserved:</span> <span class="hljs-number">14812.0</span><br>[<span class="hljs-string">Rank</span> <span class="hljs-number">3</span>] <span class="hljs-string">(after</span> <span class="hljs-number">200</span> <span class="hljs-string">iterations)</span> <span class="hljs-string">memory</span> <span class="hljs-string">(MB)</span> <span class="hljs-string">|</span> <span class="hljs-attr">allocated:</span> <span class="hljs-number">1911.18505859375</span> <span class="hljs-string">|</span> <span class="hljs-attr">max allocated:</span> <span class="hljs-number">12947.96923828125</span> <span class="hljs-string">|</span> <span class="hljs-attr">reserved:</span> <span class="hljs-number">14532.0</span> <span class="hljs-string">|</span> <span class="hljs-attr">max reserved:</span> <span class="hljs-number">14532.0</span><br><br></code></pre></td></tr></table></figure>

<h2 id="profiler文件"><a href="#profiler文件" class="headerlink" title="profiler文件"></a>profiler文件</h2><p>下图就是初始的<code>LanguageModelEmbedding</code>因为TP维度是4，并且没有Sequence并行，所以后续采用<code>reduce_from_tensor_model_parallel_region</code>来进行all reduce获得token转化结果</p>
<p><img src="/2026/01/08/megatron-lm-tp/image-1.png" srcset="/img/loading.gif" lazyload></p>
<p>下图是MHA计算时最后一步通过与<code>linear_proj</code>的计算将维度转换回去的计算，这里<code>linear_proj</code>是行并行最后会调用all reduce得到结果</p>
<p><img src="/2026/01/08/megatron-lm-tp/image-2.png" srcset="/img/loading.gif" lazyload></p>
<p>下图是MLP模块中最后行并行后调用all reduce的地方</p>
<p><img src="/2026/01/08/megatron-lm-tp/image-3.png" srcset="/img/loading.gif" lazyload></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/LLM/" class="category-chain-item">LLM</a>
  
  
    <span>></span>
    
  <a href="/categories/LLM/Megatron-LM/" class="category-chain-item">Megatron-LM</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/LLM/" class="print-no-link">#LLM</a>
      
        <a href="/tags/Megatron-LM/" class="print-no-link">#Megatron-LM</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>【Megatron-LM源码分析（五）】-Tensor并行</div>
      <div>http://example.com/2026/01/08/megatron-lm-tp/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>滑滑蛋</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2026年1月8日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/12/28/ByteScale-paper-note/" title="【论文阅读】ByteScale:Efficient Scaling of LLM Training with a  2048K Context Length on More Than 12,000">
                        <span class="hidden-mobile">【论文阅读】ByteScale:Efficient Scaling of LLM Training with a  2048K Context Length on More Than 12,000</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"WMtHomhQYlrbIodTwoPU3gTY-MdYXbMMI","appKey":"pZeun9WfI1yaQrIoUbvTQrXv","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"https://wmthomhq.api.lncldglobal.com","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         次
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
