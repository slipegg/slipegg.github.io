

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.jpg">
  <link rel="icon" href="/img/favicon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#1C262C">
  <meta name="author" content="滑滑蛋">
  <meta name="keywords" content="">
  
    <meta name="description" content="概览构建大模型的全景图如下，本文介绍了最开始的数据处理。  数据处理的全景图如下所示，大致流程为：  将原文本，一般为一个string，进行分割。  对分割后的词转化为id。  生成一个embeddings层，然后id作为序号去embeddings层中取对应的行作为自己的表征。    如此转化后就将原本深度学习模型不能处理的原始数据转化为了可以处理的矩阵，同时我们希望这最后表示原始数据的矩阵中也能">
<meta property="og:type" content="article">
<meta property="og:title" content="【从零构建大模型】一、文本数据处理">
<meta property="og:url" content="http://example.com/2025/05/01/LLMFromScratch1/index.html">
<meta property="og:site_name" content="滑滑蛋的个人博客">
<meta property="og:description" content="概览构建大模型的全景图如下，本文介绍了最开始的数据处理。  数据处理的全景图如下所示，大致流程为：  将原文本，一般为一个string，进行分割。  对分割后的词转化为id。  生成一个embeddings层，然后id作为序号去embeddings层中取对应的行作为自己的表征。    如此转化后就将原本深度学习模型不能处理的原始数据转化为了可以处理的矩阵，同时我们希望这最后表示原始数据的矩阵中也能">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2025/05/01/LLMFromScratch1/image-9.png">
<meta property="og:image" content="http://example.com/2025/05/01/LLMFromScratch1/image-8.png">
<meta property="og:image" content="http://example.com/2025/05/01/LLMFromScratch1/image-4.png">
<meta property="og:image" content="http://example.com/2025/05/01/LLMFromScratch1/image-3.png">
<meta property="og:image" content="http://example.com/2025/05/01/LLMFromScratch1/image-6.png">
<meta property="og:image" content="http://example.com/2025/05/01/LLMFromScratch1/image-7.png">
<meta property="og:image" content="http://example.com/2025/05/01/LLMFromScratch1/image-5.png">
<meta property="og:image" content="http://example.com/2025/05/01/LLMFromScratch1/image-1.png">
<meta property="og:image" content="http://example.com/2025/05/01/LLMFromScratch1/image-2.png">
<meta property="og:image" content="http://example.com/2025/05/01/LLMFromScratch1/image.png">
<meta property="article:published_time" content="2025-05-01T12:53:03.000Z">
<meta property="article:modified_time" content="2025-10-17T05:23:31.783Z">
<meta property="article:author" content="滑滑蛋">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2025/05/01/LLMFromScratch1/image-9.png">
  
  
  
  <title>【从零构建大模型】一、文本数据处理 - 滑滑蛋的个人博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"d38d21fca521d897798e5bdd940a90d0","google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"WMtHomhQYlrbIodTwoPU3gTY-MdYXbMMI","app_key":"pZeun9WfI1yaQrIoUbvTQrXv","server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?d38d21fca521d897798e5bdd940a90d0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', '');
        });
      }
    </script>
  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>滑滑蛋</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="【从零构建大模型】一、文本数据处理"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-05-01 20:53" pubdate>
          2025年5月1日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          5.8k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          29 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">【从零构建大模型】一、文本数据处理</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h1><p>构建大模型的全景图如下，本文介绍了最开始的数据处理。</p>
<p><img src="/2025/05/01/LLMFromScratch1/image-9.png" srcset="/img/loading.gif" lazyload></p>
<p>数据处理的全景图如下所示，大致流程为：</p>
<ol>
<li><p>将原文本，一般为一个string，进行分割。</p>
</li>
<li><p>对分割后的词转化为id。</p>
</li>
<li><p>生成一个embeddings层，然后id作为序号去embeddings层中取对应的行作为自己的表征。</p>
</li>
</ol>
<p><img src="/2025/05/01/LLMFromScratch1/image-8.png" srcset="/img/loading.gif" lazyload></p>
<p>如此转化后就将原本深度学习模型不能处理的原始数据转化为了可以处理的矩阵，同时我们希望这最后表示原始数据的矩阵中也能表示词之间的关系，例如词意相似的单词在可以空间上较为接近。</p>
<p><img src="/2025/05/01/LLMFromScratch1/image-4.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><h2 id="Tokenizing-text"><a href="#Tokenizing-text" class="headerlink" title="Tokenizing text"></a>Tokenizing text</h2><p><img src="/2025/05/01/LLMFromScratch1/image-3.png" srcset="/img/loading.gif" lazyload></p>
<p>拆分的规则可以有多种，最简单的就是直接按单词粒度进行拆分，例如直接以空格作为分隔符，但是需要注意拆分过程中需要注意标点符合与单词之间没有空格。</p>
<p>简单的代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">text = <span class="hljs-string">&quot;Hello, world. Is this-- a test?&quot;</span><br><br>result = re.split(<span class="hljs-string">r&#x27;([,.:;?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, text)<br>result = [item.strip() <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> result <span class="hljs-keyword">if</span> item.strip()]<br><span class="hljs-built_in">print</span>(result)<br></code></pre></td></tr></table></figure>

<h2 id="Converting-tokens-into-token-IDs"><a href="#Converting-tokens-into-token-IDs" class="headerlink" title="Converting tokens into token IDs"></a>Converting tokens into token IDs</h2><p>在进行拆分后需要对所有的单词、符合进行去重，建立一张可以单词与id互相映射的词汇表，最简单的方法就是直接按照字母顺序对其进行排序。然后这张词汇表在代码中的显示就是一个支持encode和decode的类，如下所示。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">all_words = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">set</span>(preprocessed))<br>vocab = &#123;token:integer <span class="hljs-keyword">for</span> integer,token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(all_words)&#125;<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleTokenizerV1</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab</span>):<br>        self.str_to_int = vocab<br>        self.int_to_str = &#123;i:s <span class="hljs-keyword">for</span> s,i <span class="hljs-keyword">in</span> vocab.items()&#125;<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, text</span>):<br>        preprocessed = re.split(<span class="hljs-string">r&#x27;([,.:;?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, text)<br>                                <br>        preprocessed = [<br>            item.strip() <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> preprocessed <span class="hljs-keyword">if</span> item.strip()<br>        ]<br>        ids = [self.str_to_int[s] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> preprocessed]<br>        <span class="hljs-keyword">return</span> ids<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">self, ids</span>):<br>        text = <span class="hljs-string">&quot; &quot;</span>.join([self.int_to_str[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ids])<br>        <span class="hljs-comment"># Replace spaces before the specified punctuations</span><br>        text = re.sub(<span class="hljs-string">r&#x27;\s+([,.?!&quot;()\&#x27;])&#x27;</span>, <span class="hljs-string">r&#x27;\1&#x27;</span>, text)<br>        <span class="hljs-keyword">return</span> text<br></code></pre></td></tr></table></figure>

<h2 id="Adding-special-context-tokens"><a href="#Adding-special-context-tokens" class="headerlink" title="Adding special context tokens"></a>Adding special context tokens</h2><p>常见的特殊标识符有：</p>
<ul>
<li><p><code>[BOS]</code> 一个文本序列的起始标识</p>
</li>
<li><p><code>[EOS]</code> 一个文本序列的结束标识</p>
</li>
<li><p><code>[PAD]</code> 如果batch size大于1，那么就需要用它来填充那些较短的文本，以做到长度统一</p>
</li>
<li><p><code>[UNK]</code> 用于表示在词汇表之外的单词</p>
</li>
</ul>
<blockquote>
<p>GPT、GPT-2 只使用了<code>&lt;|endoftext|&gt;</code> ，当做结束的标识符号，也当做填充的标识符。GPT-2不需要<code>[UNK]</code> ，因为它使用 byte-pair encoding (BPE)来编码</p>
</blockquote>
<p>加入<code>[UNK]</code> 后的词汇表如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleTokenizerV2</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab</span>):<br>        self.str_to_int = vocab<br>        self.int_to_str = &#123; i:s <span class="hljs-keyword">for</span> s,i <span class="hljs-keyword">in</span> vocab.items()&#125;<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, text</span>):<br>        preprocessed = re.split(<span class="hljs-string">r&#x27;([,.:;?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, text)<br>        preprocessed = [item.strip() <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> preprocessed <span class="hljs-keyword">if</span> item.strip()]<br>        preprocessed = [<br>            item <span class="hljs-keyword">if</span> item <span class="hljs-keyword">in</span> self.str_to_int <br>            <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;&lt;|unk|&gt;&quot;</span> <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> preprocessed<br>        ]<br><br>        ids = [self.str_to_int[s] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> preprocessed]<br>        <span class="hljs-keyword">return</span> ids<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">self, ids</span>):<br>        text = <span class="hljs-string">&quot; &quot;</span>.join([self.int_to_str[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ids])<br>        <span class="hljs-comment"># Replace spaces before the specified punctuations</span><br>        text = re.sub(<span class="hljs-string">r&#x27;\s+([,.:;?!&quot;()\&#x27;])&#x27;</span>, <span class="hljs-string">r&#x27;\1&#x27;</span>, text)<br>        <span class="hljs-keyword">return</span> text<br></code></pre></td></tr></table></figure>

<h2 id="BytePair-encoding-BPE"><a href="#BytePair-encoding-BPE" class="headerlink" title="BytePair encoding(BPE)"></a>BytePair encoding(BPE)</h2><p>BPE分词器可以将句子分解并转化为id，对于未知词，可以将其分解为子词和单个字符，这样它就可以解析任何单词。</p>
<ul>
<li>OpenAI 开源 [tiktoken]( <a target="_blank" rel="noopener" href="https://github.com/openai/tiktoken">https://github.com/openai/tiktoken</a> ) 库中的 BPE 标记器，其核心算法以 Rust 实现，以提高计算性能</li>
</ul>
<p><img src="/2025/05/01/LLMFromScratch1/image-6.png" srcset="/img/loading.gif" lazyload></p>
<p>简单使用如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> importlib<br><span class="hljs-keyword">import</span> tiktoken<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;tiktoken version:&quot;</span>, importlib.metadata.version(<span class="hljs-string">&quot;tiktoken&quot;</span>))<br>tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br><br>text = (<br>    <span class="hljs-string">&quot;Hello, do you like tea? &lt;|endoftext|&gt; In the sunlit terraces&quot;</span><br>     <span class="hljs-string">&quot;of someunknownPlace.&quot;</span><br>)<br><br>integers = tokenizer.encode(text, allowed_special=&#123;<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>&#125;)<br><br><span class="hljs-built_in">print</span>(integers)<br></code></pre></td></tr></table></figure>

<h2 id="Data-sampling-with-a-sliding-window"><a href="#Data-sampling-with-a-sliding-window" class="headerlink" title="Data sampling with a sliding window"></a>Data sampling with a sliding window</h2><p>在训练过程中往往会将文本划分为多个块，一个模型进行训练的预测时只会基于这个块中前面的部分词来预测后一个词，简单的表示如下：</p>
<p><img src="/2025/05/01/LLMFromScratch1/image-7.png" srcset="/img/loading.gif" lazyload></p>
<p>因为是基于已有的输入预测后一个，所以对于正确的output实际上就是input往后移一位，如下：</p>
<p><img src="/2025/05/01/LLMFromScratch1/image-5.png" srcset="/img/loading.gif" lazyload></p>
<p>上面这个是一个batch，为了产生多组batch可以使用滑动窗口的思想，通过控制步长来生成多个batch：</p>
<p><img src="/2025/05/01/LLMFromScratch1/image-1.png" srcset="/img/loading.gif" lazyload></p>
<p>一个简单的生成数据集的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GPTDatasetV1</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, txt, tokenizer, max_length, stride</span>):<br>        self.input_ids = []<br>        self.target_ids = []<br><br>        <span class="hljs-comment"># Tokenize the entire text</span><br>        token_ids = tokenizer.encode(txt, allowed_special=&#123;<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>&#125;)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(token_ids) &gt; max_length, <span class="hljs-string">&quot;Number of tokenized inputs must at least be equal to max_length+1&quot;</span><br><br>        <span class="hljs-comment"># Use a sliding window to chunk the book into overlapping sequences of max_length</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(token_ids) - max_length, stride):<br>            input_chunk = token_ids[i:i + max_length]<br>            target_chunk = token_ids[i + <span class="hljs-number">1</span>: i + max_length + <span class="hljs-number">1</span>]<br>            self.input_ids.append(torch.tensor(input_chunk))<br>            self.target_ids.append(torch.tensor(target_chunk))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.input_ids)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> self.input_ids[idx], self.target_ids[idx]<br>        <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_dataloader_v1</span>(<span class="hljs-params">txt, batch_size=<span class="hljs-number">4</span>, max_length=<span class="hljs-number">256</span>, </span><br><span class="hljs-params">                         stride=<span class="hljs-number">128</span>, shuffle=<span class="hljs-literal">True</span>, drop_last=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">                         num_workers=<span class="hljs-number">0</span></span>):<br><br>    <span class="hljs-comment"># Initialize the tokenizer</span><br>    tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br><br>    <span class="hljs-comment"># Create dataset</span><br>    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)<br><br>    <span class="hljs-comment"># Create dataloader</span><br>    dataloader = DataLoader(<br>        dataset,<br>        batch_size=batch_size,<br>        shuffle=shuffle,<br>        drop_last=drop_last,<br>        num_workers=num_workers<br>    )<br><br>    <span class="hljs-keyword">return</span> dataloader<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;the-verdict.txt&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    raw_text = f.read()<br>dataloader = create_dataloader_v1(<br>    raw_text, batch_size=<span class="hljs-number">1</span>, max_length=<span class="hljs-number">4</span>, stride=<span class="hljs-number">1</span>, shuffle=<span class="hljs-literal">False</span><br>)<br><br>data_iter = <span class="hljs-built_in">iter</span>(dataloader)<br>first_batch = <span class="hljs-built_in">next</span>(data_iter)<br><span class="hljs-built_in">print</span>(first_batch)<br><span class="hljs-comment"># [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]</span><br></code></pre></td></tr></table></figure>

<h2 id="Creating-token-embeddings"><a href="#Creating-token-embeddings" class="headerlink" title="Creating token embeddings"></a>Creating token embeddings</h2><p>得到标记 ID 后需要将其转换为连续的向量表示，即所谓的token embeddings。</p>
<p>其首先需要初始化一个embeddings层，其行数为id的数量，列数为用来表示一个token的信息量，也就是输出的维度。</p>
<p>然后根据当前token的id就直接去对应的行取出那一行作为其token的表示，如下：</p>
<p><img src="/2025/05/01/LLMFromScratch1/image-2.png" srcset="/img/loading.gif" lazyload></p>
<p>简单的代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">vocab_size = <span class="hljs-number">6</span><br>output_dim = <span class="hljs-number">3</span><br><br>torch.manual_seed(<span class="hljs-number">123</span>)<br>embedding_layer = torch.nn.Embedding(vocab_size, output_dim)<br><br><span class="hljs-built_in">print</span>(embedding_layer.weight)<br><span class="hljs-built_in">print</span>(embedding_layer(torch.tensor([<span class="hljs-number">3</span>])))<br><span class="hljs-comment"># tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=&lt;EmbeddingBackward0&gt;)</span><br></code></pre></td></tr></table></figure>

<h2 id="Encoding-word-positions"><a href="#Encoding-word-positions" class="headerlink" title="Encoding word positions"></a>Encoding word positions</h2><p>一个词在不同的位置其表示的含义可能会有所不同，所以还需要有位置编码的信息。position-aware embeddings有两类：</p>
<ul>
<li><p>相对位置嵌入的重点并非关注标记的绝对位置，而是标记之间的相对位置或距离。这意味着模型学习的是“相距多远”而不是“具体在哪个位置”的关系。这样做的好处是，即使在训练过程中没有遇到过这种长度的序列，模型也能更好地泛化到不同长度的序列。</p>
</li>
<li><p>OpenAI 的 GPT 模型使用绝对位置嵌入，这些嵌入在训练过程中进行优化，而不是像原始 Transformer 模型中的位置编码那样固定或预定义。此优化过程是模型训练本身的一部分。</p>
</li>
</ul>
<p>简单的位置嵌入编码就是直接生成一个行数为最大位置数的embeddings层，然后各个词按照序列id去取对应的embddings来得到位置嵌入编码，之后需要将Token embeddings和Positional embeddings进行相加最后得到一个完整的Iput embeddings。</p>
<p><img src="/2025/05/01/LLMFromScratch1/image.png" srcset="/img/loading.gif" lazyload></p>
<p>简单的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">context_length = max_length<br>pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)<br><br>pos_embeddings = pos_embedding_layer(torch.arange(max_length))<br><br>input_embeddings = token_embeddings + pos_embeddings<br><span class="hljs-built_in">print</span>(input_embeddings.shape)<br></code></pre></td></tr></table></figure>





<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a target="_blank" rel="noopener" href="https://knowledge.zhaoweiguo.com/build/html/x-learning/books/ais/2024/build/_llm/_from/_scratch#understanding-llm">https://knowledge.zhaoweiguo.com/build/html/x-learning/books/ais/2024/build\_llm\_from\_scratch#understanding-llm</a></li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/LLM/" class="category-chain-item">LLM</a>
  
  
    <span>></span>
    
  <a href="/categories/LLM/Build-a-Large-Language-Model-From-Scratch/" class="category-chain-item">Build a Large Language Model (From Scratch)</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/LLM/" class="print-no-link">#LLM</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>【从零构建大模型】一、文本数据处理</div>
      <div>http://example.com/2025/05/01/LLMFromScratch1/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>滑滑蛋</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年5月1日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/05/02/LLMFromScratch2/" title="【从零构建大模型】二、编码Attention机制">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">【从零构建大模型】二、编码Attention机制</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/04/30/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8/" title="大模型显存占用浅析">
                        <span class="hidden-mobile">大模型显存占用浅析</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"WMtHomhQYlrbIodTwoPU3gTY-MdYXbMMI","appKey":"pZeun9WfI1yaQrIoUbvTQrXv","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"https://wmthomhq.api.lncldglobal.com","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         次
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
