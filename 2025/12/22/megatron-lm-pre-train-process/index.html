

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.jpg">
  <link rel="icon" href="/img/favicon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#1C262C">
  <meta name="author" content="滑滑蛋">
  <meta name="keywords" content="">
  
    <meta name="description" content="本次查看Megatron-LM的版本是core_r0.14.0，查看的GPT训练文件是pretrain_gpt.py 入口函数main入口函数代码如下： 123456789101112131415161718if __name__ &#x3D;&#x3D; &quot;__main__&quot;:    # Temporary for transition to core datasets    train_val">
<meta property="og:type" content="article">
<meta property="og:title" content="【Megatron-LM源码分析（二）】-GPT模型pretrain流程">
<meta property="og:url" content="http://example.com/2025/12/22/megatron-lm-pre-train-process/index.html">
<meta property="og:site_name" content="滑滑蛋的个人博客">
<meta property="og:description" content="本次查看Megatron-LM的版本是core_r0.14.0，查看的GPT训练文件是pretrain_gpt.py 入口函数main入口函数代码如下： 123456789101112131415161718if __name__ &#x3D;&#x3D; &quot;__main__&quot;:    # Temporary for transition to core datasets    train_val">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2025/12/22/megatron-lm-pre-train-process/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E4%BB%B6.jpg">
<meta property="article:published_time" content="2025-12-22T07:41:59.000Z">
<meta property="article:modified_time" content="2025-12-30T07:46:11.447Z">
<meta property="article:author" content="滑滑蛋">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Megatron-LM">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2025/12/22/megatron-lm-pre-train-process/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E4%BB%B6.jpg">
  
  
  
  <title>【Megatron-LM源码分析（二）】-GPT模型pretrain流程 - 滑滑蛋的个人博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"d38d21fca521d897798e5bdd940a90d0","google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"WMtHomhQYlrbIodTwoPU3gTY-MdYXbMMI","app_key":"pZeun9WfI1yaQrIoUbvTQrXv","server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?d38d21fca521d897798e5bdd940a90d0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', '');
        });
      }
    </script>
  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>滑滑蛋</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="【Megatron-LM源码分析（二）】-GPT模型pretrain流程"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-12-22 15:41" pubdate>
          2025年12月22日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          134k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          669 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">【Megatron-LM源码分析（二）】-GPT模型pretrain流程</h1>
            
            
              <div class="markdown-body">
                
                <p>本次查看Megatron-LM的版本是<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/Megatron-LM/tree/core_r0.14.0"><code>core_r0.14.0</code></a>，查看的GPT训练文件是<code>pretrain_gpt.py</code></p>
<h1 id="入口函数"><a href="#入口函数" class="headerlink" title="入口函数"></a>入口函数</h1><p>main入口函数代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br><br>    <span class="hljs-comment"># Temporary for transition to core datasets</span><br>    train_valid_test_datasets_provider.is_distributed = <span class="hljs-literal">True</span><br><br>    <span class="hljs-comment"># Optionally enable inprocess restart on pretrain</span><br>    pretrain, store = inprocess_restart.maybe_wrap_for_inprocess_restart(pretrain)<br><br>    pretrain(<br>        train_valid_test_datasets_provider,<br>        model_provider,<br>        ModelType.encoder_or_decoder,<br>        forward_step,<br>        args_defaults=&#123;<span class="hljs-string">&#x27;tokenizer_type&#x27;</span>: <span class="hljs-string">&#x27;GPT2BPETokenizer&#x27;</span>&#125;,<br>        extra_args_provider=add_modelopt_args <span class="hljs-keyword">if</span> has_nvidia_modelopt <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>        store=store,<br>    )<br><br></code></pre></td></tr></table></figure>

<p>其功能主要为：</p>
<ol>
<li><p>临时函数，告诉数据集构建器这是一个分布式训练环境，需要在多个进程间协调数据集构建</p>
</li>
<li><p>可选地启用进程内重启功能，为训练函数添加故障恢复能力，允许在 GPU 故障时自动重启而不中断整个作业</p>
</li>
<li><p>调用核心pretrain函数并传入自定义相关函数作为参数进行训练</p>
</li>
</ol>
<h1 id="进程重启功能"><a href="#进程重启功能" class="headerlink" title="进程重启功能"></a>进程重启功能</h1><p>其调用的是<code>maybe_wrap_for_inprocess_restart</code>函数，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">maybe_wrap_for_inprocess_restart</span>(<span class="hljs-params">pretrain</span>):<br><br>    args = arguments.parse_args(ignore_unknown_args=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">if</span> args.inprocess_restart:<br>        pretrain = inprocess_restart(pretrain, args)<br><br>        torch.distributed.TCPStore(<br>            host_name=os.environ[<span class="hljs-string">&#x27;MASTER_ADDR&#x27;</span>],      <span class="hljs-comment"># 主节点 IP 地址</span><br>            port=<span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&#x27;MASTER_PORT&#x27;</span>])+<span class="hljs-number">1</span>,    <span class="hljs-comment"># 端口 (避免与主通信冲突)</span><br>            world_size=<span class="hljs-built_in">int</span>(os.getenv(<span class="hljs-string">&#x27;WORLD_SIZE&#x27;</span>, <span class="hljs-string">&#x27;1&#x27;</span>)),  <span class="hljs-comment"># 总进程数</span><br>            is_master=(<span class="hljs-built_in">int</span>(os.getenv(<span class="hljs-string">&#x27;RANK&#x27;</span>, <span class="hljs-string">&#x27;0&#x27;</span>)) == <span class="hljs-number">0</span>),  <span class="hljs-comment"># 是否为主进程</span><br>            timeout=timedelta(seconds=<span class="hljs-number">300</span>),           <span class="hljs-comment"># 连接超时 (5分钟)</span><br>            wait_for_workers=<span class="hljs-literal">True</span>,                    <span class="hljs-comment"># 等待所有 worker 连接</span><br>            use_libuv=<span class="hljs-literal">True</span>,                          <span class="hljs-comment"># 使用 libuv 提高性能</span><br>        )<br>    <span class="hljs-keyword">else</span>:<br>        store = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">return</span> pretrain, store<br></code></pre></td></tr></table></figure>

<p>其主要功能是查看是否带有<code>inprocess_restart</code>启动参数，如果没有就不操作，如果有就继续操作，包括：</p>
<ul>
<li><p>调用inprocess_restart对pretrain关键函数进行包装</p>
</li>
<li><p>创建TCPStore，TCPStore类似于是一个分布式KV存储系统，充当控制面。它作用有：</p>
<ul>
<li><p>底层采用TCP协议，所以如果NCCL或训练的通信组出错也不会受影响。注意使用的是<code>int(os.environ[&#39;MASTER_PORT&#39;]) + 1</code>端口，以避免端口冲突</p>
</li>
<li><p><code>wait_for_workers=True</code>参数确保等待所有worker都正常运行</p>
</li>
<li><p>用以控制保证所有的worker都进入了新一轮的训练</p>
</li>
<li><p>其容错的运行流程类似如下</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">1. 训练开始 → 所有进程连接到 TCPStore<br>2. 进程 A 检测到 GPU 故障 → 向 TCPStore 报告<br>3. TCPStore 广播故障状态 → 所有进程暂停<br>4. 协调重启决策 → 隔离故障进程<br>5. 启动新进程 → 通过 TCPStore 同步状态<br>6. 所有进程确认 → 恢复训练<br></code></pre></td></tr></table></figure>

<blockquote>
<p>不过注意的是如果一个节点确实损坏了，它无法找到新的节点来替代，只能不断地重启了，除非有足够的热备节点</p>
</blockquote>
<p>调用<code>inprocess_restart</code>对<code>pretrain</code>关键函数进行包装的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">inprocess_restart</span>(<span class="hljs-params">train, args</span>):<br>    <span class="hljs-keyword">if</span> inprocess <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        warnings.warn(<span class="hljs-string">&#x27;In-process restart is not available&#x27;</span>)<br>        <span class="hljs-keyword">return</span> train<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;TORCH_CPP_LOG_LEVEL&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> os.environ <span class="hljs-keyword">or</span> os.environ[<span class="hljs-string">&#x27;TORCH_CPP_LOG_LEVEL&#x27;</span>] <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> (<br>        <span class="hljs-string">&#x27;error&#x27;</span>,<br>        <span class="hljs-string">&#x27;fatal&#x27;</span>,<br>    ):<br>        warnings.warn(<br>            <span class="hljs-string">&#x27;Set TORCH_CPP_LOG_LEVEL=error to suppress c10d waitForInput timeout warning messages&#x27;</span><br>        )<br><br>    <span class="hljs-comment"># Layers represents a configuration for a layer of branches at a certain</span><br>    <span class="hljs-comment"># depth in a topology tree constructed by inprocess.rank_assignment.Tree.</span><br>    <span class="hljs-comment"># First layer contains all ranks and it&#x27;s the root of the topology tree,</span><br>    <span class="hljs-comment"># the second optional layer groups ranks by nodes.</span><br>    layers = [<br>        inprocess.rank_assignment.Layer(<br>            min_ranks=args.inprocess_active_world_size,<br>            max_ranks=args.inprocess_active_world_size,<br>            flag=inprocess.rank_assignment.LayerFlag.RESERVE,<br>        )<br>    ]<br>    <span class="hljs-keyword">if</span> args.inprocess_granularity == <span class="hljs-string">&#x27;node&#x27;</span>:<br>        device_count = torch.cuda.device_count()<br><br>        layers.append(<br>            inprocess.rank_assignment.Layer(<br>                min_ranks=device_count,<br>                max_ranks=device_count,<br>                key_or_fn=<span class="hljs-keyword">lambda</span> _: socket.gethostname(),<br>                flag=inprocess.rank_assignment.LayerFlag.RESERVE,<br>            )<br>        )<br><br>    finalize = [<br>        inprocess.finalize.ThreadedFinalize(timeout=timedelta(seconds=<span class="hljs-number">10</span>), fn=destroy_state)<br>    ]<br><br>    <span class="hljs-keyword">if</span> args.inprocess_empty_cuda_cache:<br>        finalize.append(<br>            inprocess.finalize.ThreadedFinalize(<br>                timeout=timedelta(seconds=<span class="hljs-number">10</span>), fn=torch.cuda.empty_cache<br>            )<br>        )<br><br>    initialize = inprocess.Compose(<br>        inprocess.initialize.RetryController(min_world_size=args.inprocess_active_world_size),<br>        inprocess.nested_restarter.NestedRestarterHandlingCompleted(),<br>    )<br>    abort = inprocess.Compose(<br>        inprocess.abort.AbortTransformerEngine(),<br>        inprocess.abort.AbortTorchDistributed(),<br>        inprocess.nested_restarter.NestedRestarterHandlingStarting(),<br>    )<br>    completion = inprocess.nested_restarter.NestedRestarterFinalized()<br>    terminate = inprocess.nested_restarter.NestedRestarterAborted()<br><br>    train = inprocess.Wrapper(<br>        store_kwargs=&#123;<br>            <span class="hljs-string">&#x27;timeout&#x27;</span>: timedelta(seconds=<span class="hljs-number">300</span>),<br>            <span class="hljs-string">&#x27;port&#x27;</span>: <span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&#x27;MASTER_PORT&#x27;</span>]) + <span class="hljs-number">2</span>,<br>        &#125;,<br>        initialize=initialize,<br>        abort=abort,<br>        completion=completion,<br>        terminate=terminate,<br>        health_check=inprocess.health_check.CudaHealthCheck(timeout=timedelta(seconds=<span class="hljs-number">10</span>)),<br>        rank_assignment=inprocess.rank_assignment.Tree(layers=layers),<br>        finalize=inprocess.Compose(*finalize),<br>        heartbeat_interval=timedelta(seconds=args.inprocess_heartbeat_interval),<br>        heartbeat_timeout=timedelta(seconds=args.inprocess_heartbeat_timeout),<br>        barrier_timeout=timedelta(seconds=args.inprocess_barrier_timeout),<br>        completion_timeout=timedelta(seconds=args.inprocess_completion_timeout),<br>        monitor_process_interval=timedelta(seconds=args.inprocess_monitor_process_interval),<br>        monitor_thread_interval=timedelta(seconds=args.inprocess_monitor_thread_interval),<br>        last_call_wait=timedelta(seconds=args.inprocess_last_call_wait),<br>        soft_timeout=timedelta(seconds=args.inprocess_soft_timeout),<br>        hard_timeout=timedelta(seconds=args.inprocess_hard_timeout),<br>        termination_grace_time=timedelta(seconds=args.inprocess_termination_grace_time),<br>        enabled=<span class="hljs-literal">True</span>,<br>    )(train)<br><br>    <span class="hljs-keyword">return</span> train<br></code></pre></td></tr></table></figure>

<p>其主要功能为：</p>
<ol>
<li><p>查看是否成功从<code>import nvidia_resiliency_ext.inprocess as inprocess</code>引入<code>inprocess</code>，如果没有就直接返回</p>
</li>
<li><p>提醒设置日志级别</p>
</li>
<li><p>构建Layers：（这里的Layers有啥作用没咋看懂）</p>
<ol>
<li><p>设置最小 &#x2F; 最大存活 rank以及是否采用RESERVE模式</p>
</li>
<li><p>如果是node粒度还需要再构建node层级的layers，以做到node级别的移除</p>
</li>
</ol>
</li>
<li><p>构建 abort 之后 &#x2F; restart 之前 执行的清理逻辑finalize，包含的处理逻辑有</p>
<ol>
<li><p><code>destroy_state</code>：</p>
<ul>
<li><p>destroy process group</p>
</li>
<li><p>释放 NCCL communicator</p>
</li>
<li><p>清理 Megatron 内部全局状态</p>
</li>
</ul>
</li>
<li><p><code>empty_cache</code>（可选）：</p>
<ul>
<li><p>清除从cache</p>
</li>
<li><p>在OOM场景下很有用</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p>再就是构建状态机中Initialize &#x2F; Abort &#x2F; Completion &#x2F; Terminate这四个状态：</p>
<ol>
<li><p>initialize：等待至少 <code>min_world_size</code> 个 rank 可用</p>
</li>
<li><p>abort（失败时触发）：负责停 Transformer Engine，abort torch.distributed，通知 nested restarter开始重启</p>
</li>
<li><p>completion（正常结束）：标记这一轮执行完成，不触发 restart</p>
</li>
<li><p>terminate（彻底失败）： 直接终止，不再尝试恢复</p>
</li>
</ol>
</li>
<li><p>包装训练函数：</p>
<ol>
<li><p>设置了上述的状态机</p>
</li>
<li><p>设置了很多timeout</p>
</li>
<li><p>将端口设置为<code>int(os.environ[&#39;MASTER_PORT&#39;]) + 2</code>以避免端口冲突</p>
</li>
</ol>
</li>
</ol>
<h1 id="pretrain参数"><a href="#pretrain参数" class="headerlink" title="pretrain参数"></a>pretrain参数</h1><p>pretrain是训练的核心入口，它更加类似于一个训练流程的驱动，用户负责通过参数提供数据、模型、loss计算方法等，它负责对其进行组装然后将分布式训练策略、checkpoint、log等方法进行执行。</p>
<p>其函数定义如下，下面对其进行分组介绍：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pretrain</span>(<span class="hljs-params"></span><br><span class="hljs-params">    train_valid_test_dataset_provider,</span><br><span class="hljs-params">    model_provider,</span><br><span class="hljs-params">    model_type,</span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    process_non_loss_data_func=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    extra_args_provider=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    args_defaults=&#123;&#125;,</span><br><span class="hljs-params">    get_embedding_ranks=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    get_position_embedding_ranks=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    non_loss_data_func=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    store=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    inprocess_call_wrapper: <span class="hljs-type">Optional</span>[CallWrapper] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br></code></pre></td></tr></table></figure>

<h2 id="数据相关参数"><a href="#数据相关参数" class="headerlink" title="数据相关参数"></a>数据相关参数</h2><ul>
<li><p><code>train_valid_test_dataset_provider</code>负责告诉Megatron如何划分出train_ds、valid_ds、test_ds这3类数据集。</p>
<ul>
<li>本示例中传入的函数如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_valid_test_datasets_provider</span>(<span class="hljs-params">train_val_test_num_samples</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build the train test and validation datasets.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        train_val_test_num_samples : A list containing the number of samples in train test and validation.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    args = get_args()<br><br>    config = core_gpt_dataset_config_from_args(args)<br><br>    <span class="hljs-keyword">if</span> args.sft:<br>        dataset_type = SFTDataset<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">if</span> args.mock_data:<br>            dataset_type = MockGPTDataset<br>        <span class="hljs-keyword">else</span>:<br>            dataset_type = GPTDataset<br><br>    print_rank_0(<span class="hljs-string">&quot;&gt; building train, validation, and test datasets for GPT ...&quot;</span>)<br><br>    train_ds, valid_ds, test_ds = BlendedMegatronDatasetBuilder(<br>        dataset_type, train_val_test_num_samples, is_dataset_built_on_rank, config<br>    ).build()<br><br>    print_rank_0(<span class="hljs-string">&quot;&gt; finished creating GPT datasets ...&quot;</span>)<br><br>    <span class="hljs-keyword">return</span> train_ds, valid_ds, test_ds<br></code></pre></td></tr></table></figure>

<ul>
<li><p>其输入参数为<code>train_val_test_num_samples</code>，如注释所言，其代表的train、val、test对应的sample的数量</p>
</li>
<li><p>在函数里其根据参数得到<code>dataset_type</code></p>
</li>
<li><p>然后还有函数<code>is_dataset_built_on_rank</code>作为参数，该函数用于决定是否在当前进程构建数据集，其函数如下，实际效果是只在PP并行组的第一个和最后一个中的TP组的第一个rank构建数据集。</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">def is_dataset_built_on_rank():<br>    <span class="hljs-built_in">return</span> (<br>        parallel_state.is_pipeline_first_stage(ignore_virtual=True)<br>        or parallel_state.is_pipeline_last_stage(ignore_virtual=True)<br>    ) and parallel_state.get_tensor_model_parallel_rank() == 0<br></code></pre></td></tr></table></figure>

<ul>
<li><p>然后其构建了<code>BlendedMegatronDatasetBuilder</code>这个数据集处理类，并调用其build()函数得到了train_ds, valid_ds, test_ds。<code>BlendedMegatronDatasetBuilder</code>的功能主要如下，后面会再找机会详细介绍</p>
<ul>
<li><p>负责从多个数据源构建混合数据集</p>
</li>
<li><p>支持分布式训练环境下的数据集协调</p>
</li>
<li><p>提供高效的数据集缓存和并行构建机制</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="模型相关参数"><a href="#模型相关参数" class="headerlink" title="模型相关参数"></a>模型相关参数</h2><ul>
<li><p><code>model_provider</code>参数负责提供一个原始模型，即提供一个在CPU上的没有进行fp16转换没有ddp切割的原始模型。</p>
<ul>
<li>本示例中传入的函数如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_provider</span>(<span class="hljs-params"></span><br><span class="hljs-params">    pre_process=<span class="hljs-literal">True</span>, post_process=<span class="hljs-literal">True</span>, vp_stage: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span></span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Union</span>[GPTModel, megatron.legacy.model.GPTModel]:<br>    <span class="hljs-string">&quot;&quot;&quot;Builds the model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    If you set the use_legacy_models to True, it will return the legacy GPT model and if not the mcore GPT model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        pre_process (bool, optional): Set to true if you need to compute embedings. Defaults to True.</span><br><span class="hljs-string">        post_process (bool, optional): Set to true if you need to want to compute output logits/loss. Defaults to True.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        Union[GPTModel, megatron.legacy.model.GPTModel]: The returned model</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    args = get_args()<br><br>    <span class="hljs-keyword">if</span> has_nvidia_modelopt <span class="hljs-keyword">and</span> modelopt_args_enabled(args):  <span class="hljs-comment"># [ModelOpt]</span><br>        <span class="hljs-keyword">return</span> model_provider_modelopt(pre_process, post_process)<br><br>    use_te = args.transformer_impl == <span class="hljs-string">&quot;transformer_engine&quot;</span><br><br>    <span class="hljs-keyword">if</span> args.record_memory_history:<br>        torch.cuda.memory._record_memory_history(<br>            <span class="hljs-literal">True</span>,<br>            <span class="hljs-comment"># keep 100,000 alloc/free events from before the snapshot</span><br>            trace_alloc_max_entries=<span class="hljs-number">100000</span>,<br>            <span class="hljs-comment"># record stack information for the trace events</span><br>            trace_alloc_record_context=<span class="hljs-literal">True</span>,<br>        )<br><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">oom_observer</span>(<span class="hljs-params">device, alloc, device_alloc, device_free</span>):<br>            <span class="hljs-comment"># snapshot right after an OOM happened</span><br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;saving allocated state during OOM&#x27;</span>)<br>            snapshot = torch.cuda.memory._snapshot()<br>            <span class="hljs-keyword">from</span> pickle <span class="hljs-keyword">import</span> dump<br><br>            dump(<br>                snapshot,<br>                <span class="hljs-built_in">open</span>(<span class="hljs-string">f&quot;oom_rank-<span class="hljs-subst">&#123;torch.distributed.get_rank()&#125;</span>_<span class="hljs-subst">&#123;args.memory_snapshot_path&#125;</span>&quot;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>),<br>            )<br><br>        torch._C._cuda_attach_out_of_memory_observer(oom_observer)<br><br>    print_rank_0(<span class="hljs-string">&#x27;building GPT model ...&#x27;</span>)<br>    <span class="hljs-comment"># Experimental loading arguments from yaml</span><br>    <span class="hljs-keyword">if</span> args.yaml_cfg <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config = core_transformer_config_from_yaml(args, <span class="hljs-string">&quot;language_model&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        config = core_transformer_config_from_args(args)<br><br>    <span class="hljs-keyword">if</span> args.use_legacy_models:<br>        model = megatron.legacy.model.GPTModel(<br>            config,<br>            num_tokentypes=<span class="hljs-number">0</span>,<br>            parallel_output=<span class="hljs-literal">True</span>,<br>            pre_process=pre_process,<br>            post_process=post_process,<br>        )<br>    <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># using core models</span><br>        <span class="hljs-keyword">if</span> args.spec <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            transformer_layer_spec = import_module(args.spec)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> args.num_experts:<br>                <span class="hljs-comment"># Define the decoder block spec</span><br>                transformer_layer_spec = get_gpt_decoder_block_spec(<br>                    config, use_transformer_engine=use_te, normalization=args.normalization, qk_l2_norm=args.qk_l2_norm, vp_stage=vp_stage<br>                )<br>            <span class="hljs-keyword">elif</span> args.heterogeneous_layers_config_path <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                transformer_layer_spec = get_gpt_heterogeneous_layer_spec(config, use_te)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># Define the decoder layer spec</span><br>                transformer_layer_spec = _get_transformer_layer_spec(use_te, config)<br>        mtp_block_spec = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> args.mtp_num_layers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(transformer_layer_spec, <span class="hljs-string">&#x27;layer_specs&#x27;</span>) <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(transformer_layer_spec.layer_specs) == <span class="hljs-number">0</span>:<br>                <span class="hljs-comment"># Get the decoder layer spec explicitly if no decoder layer in the last stage,</span><br>                <span class="hljs-comment"># Only happens with block spec (TransformerBlockSubmodules) when using MoE.</span><br>                transformer_layer_spec_for_mtp = _get_transformer_layer_spec(use_te, config)<br>            <span class="hljs-keyword">else</span>:<br>                transformer_layer_spec_for_mtp = transformer_layer_spec<br>            mtp_block_spec = get_gpt_mtp_block_spec(<br>                config, transformer_layer_spec_for_mtp, use_transformer_engine=use_te, vp_stage=vp_stage<br>            )<br><br>        model = GPTModel(<br>            config=config,<br>            transformer_layer_spec=transformer_layer_spec,<br>            vocab_size=args.padded_vocab_size,<br>            max_sequence_length=args.max_position_embeddings,<br>            pre_process=pre_process,<br>            post_process=post_process,<br>            fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,<br>            parallel_output=<span class="hljs-literal">True</span>,<br>            share_embeddings_and_output_weights=<span class="hljs-keyword">not</span> args.untie_embeddings_and_output_weights,<br>            position_embedding_type=args.position_embedding_type,<br>            rotary_percent=args.rotary_percent,<br>            rotary_base=args.rotary_base,<br>            rope_scaling=args.use_rope_scaling,<br>            mtp_block_spec=mtp_block_spec,<br>            vp_stage=vp_stage,<br>        )<br><br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure>

<ul>
<li><p>该函数的参数为：</p>
<ul>
<li><p><code>pre_process</code>: 是否计算嵌入层（输入处理）</p>
</li>
<li><p><code>post_process</code>: 是否计算输出 logits&#x2F;损失</p>
</li>
<li><p><code>vp_stage</code>: 虚拟 pipeline stage（用于梯度累积优化）</p>
</li>
</ul>
</li>
<li><p>获取训练参数，如果启用 NVIDIA ModelOpt，委托给专门的 model provider</p>
</li>
<li><p>依据参数决定是否使用 Transformer Engine（NVIDIA 的高性能 transformer 实现）</p>
</li>
<li><p>依据参数决定是否启用内存历史记录用于调试 OOM 问题，以及是否自动保存内存快照到文件</p>
</li>
<li><p>依据yaml文件或者是输入参数args获取config</p>
</li>
<li><p>然后进入核心模型分支，依据参数获取transformer_layer_spec和mtp_block_spec</p>
</li>
<li><p>最后根据各参数构建出GPTModel</p>
</li>
</ul>
</li>
<li><p><code>model_type</code>：用于告诉 Megatron 模型的“拓扑语义”，其有这三类：</p>
<ul>
<li><p>encoder_or_decoder &#x3D; 1：传统的编码器-解码器模型，或仅包含解码器的自回归模型</p>
</li>
<li><p>retro_encoder &#x3D; 2：Retrieval-Enhanced Transformer (RETRO) 模型中的编码器组件，RETRO 是一种特殊的 transformer 架构，使用外部知识库进行检索增强</p>
</li>
<li><p>retro_decoder &#x3D; 3：RETRO 模型中的解码器组件，负责最终的文本生成</p>
</li>
</ul>
</li>
<li><p><code>get_embedding_ranks</code>：指定哪些 rank 持有 word embedding</p>
</li>
<li><p><code>get_position_embedding_ranks</code>：指定哪些 rank 持有 position embedding</p>
</li>
</ul>
<h2 id="Forward执行参数"><a href="#Forward执行参数" class="headerlink" title="Forward执行参数"></a>Forward执行参数</h2><ul>
<li><p>forward_step_func：最核心的训练函数，其定义了一次 iteration 的“前向 + loss 计算”逻辑，</p>
<ul>
<li><p>其主要负责：</p>
<ol>
<li><p>从 <code>data_iterator</code> 里取 batch</p>
</li>
<li><p>调用 <code>model(...)</code></p>
</li>
<li><p>计算 loss</p>
</li>
<li><p>返回：</p>
<ul>
<li><p>loss（标量 tensor）</p>
</li>
<li><p>dict（用于 logging 的指标）</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p>示例中的代码如下所示，基本与上述功能一样，但是返回不一样了，合理怀疑是现在代码改了但是注释没改，返回的是计算结果以及计算loss的组合，此外还多了一些指标采集和专门的分支处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_step</span>(<span class="hljs-params">data_iterator, model: GPTModel, return_schedule_plan: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Forward training step.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        data_iterator : Input data iterator</span><br><span class="hljs-string">        model (GPTModel): The GPT Model</span><br><span class="hljs-string">        return_schedule_plan (bool): Whether to return the schedule plan instead of the output tensor</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    args = get_args()<br>    timers = get_timers()<br><br>    <span class="hljs-comment"># Get the batch.</span><br>    timers(<span class="hljs-string">&#x27;batch-generator&#x27;</span>, log_level=<span class="hljs-number">2</span>).start()<br>    <span class="hljs-keyword">global</span> stimer<br>    <span class="hljs-keyword">with</span> stimer(bdata=<span class="hljs-literal">True</span>):<br>        tokens, labels, loss_mask, attention_mask, position_ids = get_batch(data_iterator)<br>    timers(<span class="hljs-string">&#x27;batch-generator&#x27;</span>).stop()<br><br>    <span class="hljs-keyword">with</span> stimer:<br>        <span class="hljs-keyword">if</span> args.use_legacy_models:<br>            output_tensor = model(tokens, position_ids, attention_mask, labels=labels)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> return_schedule_plan:<br>                <span class="hljs-comment"># MoE 专家并行重叠通信模式</span><br>                <span class="hljs-keyword">assert</span> args.overlap_moe_expert_parallel_comm, \<br>                    <span class="hljs-string">&quot;overlap_moe_expert_parallel_comm must be enabled to return the schedule plan&quot;</span><br>                schedule_plan = model.build_schedule_plan(<br>                    tokens, position_ids, attention_mask, labels=labels, loss_mask=loss_mask<br>                )<br>                <span class="hljs-keyword">return</span> schedule_plan, partial(loss_func, loss_mask, model=model)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># 标准前向传播</span><br>                output_tensor = model(<br>                    tokens, position_ids, attention_mask, labels=labels, loss_mask=loss_mask<br>                )<br><br>    <span class="hljs-comment"># [ModelOpt]: model is needed to access ModelOpt distillation losses</span><br>    <span class="hljs-keyword">return</span> output_tensor, partial(loss_func, loss_mask, model=model)<br><br></code></pre></td></tr></table></figure>

<ul>
<li>其中的get_bach也是自定义的，如下，依据rank所属的并行组来获取数据。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_batch</span>(<span class="hljs-params">data_iterator</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Generate a batch.&quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> this is pretty hacky, find a better way</span><br>    <span class="hljs-keyword">if</span> (<span class="hljs-keyword">not</span> parallel_state.is_pipeline_first_stage(ignore_virtual=<span class="hljs-literal">True</span>)) <span class="hljs-keyword">and</span> (<br>        <span class="hljs-keyword">not</span> parallel_state.is_pipeline_last_stage(ignore_virtual=<span class="hljs-literal">True</span>)<br>    ):<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># get batches based on the TP rank you are on</span><br>    batch = get_batch_on_this_tp_rank(data_iterator)<br><br>    <span class="hljs-comment"># slice batch along sequence dimension for context parallelism</span><br>    batch = get_batch_on_this_cp_rank(batch)<br><br>    <span class="hljs-keyword">return</span> batch.values()<br></code></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><code>process_non_loss_data_func</code>：可选参数，用来处理<strong>不参与反向传播的数据</strong>，例如专门把一些数据dump到TensorBoard</p>
</li>
<li><p><code>non_loss_data_func</code>：在 evaluation 阶段执行自定义逻辑</p>
</li>
</ul>
<h2 id="参数与配置扩展"><a href="#参数与配置扩展" class="headerlink" title="参数与配置扩展"></a>参数与配置扩展</h2><ul>
<li><p><code>extra_args_provider</code>：允许“业务代码”向 Megatron 的 argparse 注入自定义参数</p>
</li>
<li><p><code>args_defaults</code>：覆盖 &#x2F; 预设 Megatron 参数默认值</p>
</li>
</ul>
<h2 id="分布式-容错相关参数"><a href="#分布式-容错相关参数" class="headerlink" title="分布式 &#x2F; 容错相关参数"></a>分布式 &#x2F; 容错相关参数</h2><ul>
<li><p><code>store</code>：提供一个外部的控制接口，如前述进程重启功能所示，在实例中就将控制面TCPStore传递给了该参数</p>
</li>
<li><p><code>inprocess_call_wrapper</code>： in-process restart 自动注入的“调用包装器”，负责捕获：Python exception、CUDA error，然后上报给 inprocess controller，决定是retry还是abort或是terminate等，普通用户不用传，开启 inprocess_restart 时自动生效</p>
</li>
</ul>
<h1 id="pretrain流程概览"><a href="#pretrain流程概览" class="headerlink" title="pretrain流程概览"></a>pretrain流程概览</h1><ul>
<li>pretrain的代码如下所示，上面已经对pretrain的传入参数进行了解析，下面先对其整体流程做进一步梳理</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pretrain</span>(<span class="hljs-params"></span><br><span class="hljs-params">    train_valid_test_dataset_provider,</span><br><span class="hljs-params">    model_provider,</span><br><span class="hljs-params">    model_type,</span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    process_non_loss_data_func=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    extra_args_provider=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    args_defaults=&#123;&#125;,</span><br><span class="hljs-params">    get_embedding_ranks=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    get_position_embedding_ranks=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    non_loss_data_func=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    store=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    inprocess_call_wrapper: <span class="hljs-type">Optional</span>[CallWrapper] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Main training program.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    This function will run the followings in the order provided:</span><br><span class="hljs-string">        1) initialize Megatron.</span><br><span class="hljs-string">        2) setup model, optimizer and lr schedule using the model_provider.</span><br><span class="hljs-string">        3) call train_val_test_data_provider to get train/val/test datasets.</span><br><span class="hljs-string">        4) train the model using the forward_step_func.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        train_valid_test_dataset_provider: a function that takes the size of</span><br><span class="hljs-string">            train/valid/test dataset and returns `train, valid, test` datasets.</span><br><span class="hljs-string">        model_provider: a function that returns a vanilla version of the</span><br><span class="hljs-string">            model. By vanilla we mean a simple model on cpu with no fp16 or ddp.</span><br><span class="hljs-string">        model_type: an enum that specifies the type of model being trained.</span><br><span class="hljs-string">        forward_step_func: a function that takes a `data iterator` and `model`,</span><br><span class="hljs-string">            and returns a `loss` scalar with a dictionary with key:values being</span><br><span class="hljs-string">            the info we would like to monitor during training, for example</span><br><span class="hljs-string">            `lm-loss: value`. We also require that this function add</span><br><span class="hljs-string">            `batch generator` to the timers class.</span><br><span class="hljs-string">        process_non_loss_data_func: a function to post process outputs of the</span><br><span class="hljs-string">            network. It can be used for dumping output tensors (e.g images) to</span><br><span class="hljs-string">            tensorboard. It takes `collected data`(list of tensors),</span><br><span class="hljs-string">            `current iteration index` and `tensorboard writer` as arguments.</span><br><span class="hljs-string">        extra_args_provider: a function that takes a parser and adds arguments</span><br><span class="hljs-string">            to it. It is used for programs to add their own arguments.</span><br><span class="hljs-string">        args_defaults: a dictionary from argument-name to argument-value. It</span><br><span class="hljs-string">            to set already parse arguments.</span><br><span class="hljs-string">        get_embedding_ranks (TODO):</span><br><span class="hljs-string">        get_position_embedding_ranks (TODO):</span><br><span class="hljs-string">        non_loss_data_func (callable): A custom function to call during evaluation.</span><br><span class="hljs-string">            It can run e.g. benchmarks.</span><br><span class="hljs-string">        store: an optional instance of torch.distributed.Store, to be used by</span><br><span class="hljs-string">            torch.distributed.init_process_group</span><br><span class="hljs-string">        inprocess_call_wrapper: an optional instance of inprocess.CallWrapper,</span><br><span class="hljs-string">            it is automatically injected when in-process restart is in use</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">if</span> inprocess_call_wrapper <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        iteration = inprocess_call_wrapper.iteration<br>        store = torch.distributed.PrefixStore(<span class="hljs-built_in">str</span>(iteration), store)<br><br>    <span class="hljs-comment"># Initalize and get arguments, timers, and Tensorboard writer.</span><br>    initialize_megatron(<br>        extra_args_provider=extra_args_provider,<br>        args_defaults=args_defaults,<br>        get_embedding_ranks=get_embedding_ranks,<br>        get_position_embedding_ranks=get_position_embedding_ranks,<br>        store=store,<br>    )<br><br>    args = get_args()<br>    timers = get_timers()<br><br>    <span class="hljs-keyword">if</span> args.log_progress:<br>        append_to_progress_log(<span class="hljs-string">&quot;Starting job&quot;</span>)<br><br>    <span class="hljs-comment"># Initialize fault tolerance</span><br>    <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> ft_integration functions other than `setup` are no-op if the FT is not initialized</span><br>    <span class="hljs-keyword">if</span> args.enable_ft_package:<br>        ft_integration.setup(args)<br>        ft_integration.maybe_setup_simulated_fault()<br><br>    <span class="hljs-comment"># Set pytorch JIT layer fusion options and warmup JIT functions.</span><br>    set_jit_fusion_options()<br><br>    <span class="hljs-comment"># Adjust the startup time so it reflects the largest value.</span><br>    <span class="hljs-comment"># This will be closer to what scheduler will see (outside of</span><br>    <span class="hljs-comment"># image ... launches.</span><br>    <span class="hljs-keyword">global</span> _TRAIN_START_TIME<br>    start_time_tensor = torch.tensor([_TRAIN_START_TIME], dtype=torch.double, device=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>    torch.distributed.all_reduce(start_time_tensor, op=torch.distributed.ReduceOp.MIN)<br>    _TRAIN_START_TIME = start_time_tensor.item()<br><br>    app_metrics = &#123;&#125;<br>    app_metrics[<span class="hljs-string">&#x27;app_start_time&#x27;</span>] = <span class="hljs-built_in">round</span>(_TRAIN_START_TIME * <span class="hljs-number">1000.0</span>)<br>    app_metrics[<span class="hljs-string">&#x27;app_model_init_start_time&#x27;</span>] = <span class="hljs-built_in">round</span>(_TRAIN_START_TIME * <span class="hljs-number">1000.0</span>)<br><br>    print_rank_0(<br>        <span class="hljs-string">&#x27;time to initialize megatron (seconds): &#123;:.3f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(time.time() - _TRAIN_START_TIME)<br>    )<br>    print_datetime(<span class="hljs-string">&#x27;after megatron is initialized&#x27;</span>)<br>    app_metrics[<span class="hljs-string">&#x27;app_model_init_finish_time&#x27;</span>] = one_logger_utils.get_timestamp_in_ms()<br><br>    <span class="hljs-comment"># Track E2E metrics on pretrain start</span><br>    one_logger_utils.on_pretrain_start()<br><br>    <span class="hljs-comment"># Context used for persisting some state between checkpoint saves.</span><br>    <span class="hljs-keyword">if</span> args.non_persistent_ckpt_type == <span class="hljs-string">&#x27;local&#x27;</span>:<br>        <span class="hljs-keyword">try</span>:<br>            <span class="hljs-keyword">from</span> nvidia_resiliency_ext.checkpointing.local.ckpt_managers.local_manager <span class="hljs-keyword">import</span> (<br>                LocalCheckpointManager,<br>            )<br>            <span class="hljs-keyword">from</span> nvidia_resiliency_ext.checkpointing.local.replication.group_utils <span class="hljs-keyword">import</span> (<br>                parse_group_sequence,<br>                GroupWrapper,<br>            )<br>            <span class="hljs-keyword">from</span> nvidia_resiliency_ext.checkpointing.local.replication.strategies <span class="hljs-keyword">import</span> (<br>                CliqueReplicationStrategy,<br>            )<br>        <span class="hljs-keyword">except</span> ModuleNotFoundError:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<br>                <span class="hljs-string">&quot;The &#x27;nvidia_resiliency_ext&#x27; module is required for local &quot;</span><br>                <span class="hljs-string">&quot;checkpointing but was not found. Please ensure it is installed.&quot;</span><br>            )<br><br>        <span class="hljs-keyword">if</span> args.replication:<br>            repl_strategy = CliqueReplicationStrategy.from_replication_params(<br>                args.replication_jump, args.replication_factor<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            repl_strategy = <span class="hljs-literal">None</span><br><br>        checkpointing_context = &#123;<br>            <span class="hljs-string">&#x27;local_checkpoint_manager&#x27;</span>: LocalCheckpointManager(<br>                args.non_persistent_local_ckpt_dir, repl_strategy=repl_strategy<br>            )<br>        &#125;<br>    <span class="hljs-keyword">else</span>:<br>        checkpointing_context = &#123;&#125;<br><br>    <span class="hljs-comment"># Model, optimizer, and learning rate.</span><br>    timers(<span class="hljs-string">&#x27;model-and-optimizer-setup&#x27;</span>, log_level=<span class="hljs-number">0</span>).start(barrier=<span class="hljs-literal">True</span>)<br>    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(<br>        model_provider, model_type, checkpointing_context=checkpointing_context<br>    )<br><br>    timers(<span class="hljs-string">&#x27;model-and-optimizer-setup&#x27;</span>).stop()<br>    print_datetime(<span class="hljs-string">&#x27;after model, optimizer, and learning rate &#x27;</span> <span class="hljs-string">&#x27;scheduler are built&#x27;</span>)<br>    config = get_model_config(model[<span class="hljs-number">0</span>])<br><br>    <span class="hljs-comment"># Data stuff.</span><br>    app_metrics[<span class="hljs-string">&#x27;app_build_dataiters_start_time&#x27;</span>] = one_logger_utils.get_timestamp_in_ms()<br>    timers(<span class="hljs-string">&#x27;train/valid/test-data-iterators-setup&#x27;</span>, log_level=<span class="hljs-number">0</span>).start(barrier=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">if</span> args.virtual_pipeline_model_parallel_size <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        train_data_iterator = []<br>        valid_data_iterator = []<br>        test_data_iterator = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(model)):<br>            iterators = build_train_valid_test_data_iterators(train_valid_test_dataset_provider)<br>            train_data_iterator.append(iterators[<span class="hljs-number">0</span>])<br>            valid_data_iterator.append(iterators[<span class="hljs-number">1</span>])<br>            test_data_iterator.append(iterators[<span class="hljs-number">2</span>])<br>    <span class="hljs-keyword">else</span>:<br>        train_data_iterator, valid_data_iterator, test_data_iterator = (<br>            build_train_valid_test_data_iterators(train_valid_test_dataset_provider)<br>        )<br>    timers(<span class="hljs-string">&#x27;train/valid/test-data-iterators-setup&#x27;</span>).stop()<br>    print_datetime(<span class="hljs-string">&#x27;after dataloaders are built&#x27;</span>)<br>    app_metrics[<span class="hljs-string">&#x27;app_build_dataiters_finish_time&#x27;</span>] = one_logger_utils.get_timestamp_in_ms()<br><br>    <span class="hljs-comment"># Track if training is enabled. Can only be done once args.do_train is assigned after dataloader is built.</span><br>    one_logger_utils.track_config_flags(<br>        args.train_iters,<br>        args.skip_train,<br>        args.do_train,<br>        args.do_valid,<br>        args.do_test,<br>        args.dataloader_type,<br>        args.retro_project_dir,<br>        args.retro_cyclic_train_iters,<br>    )<br><br>    <span class="hljs-comment"># Print setup timing.</span><br>    print_rank_0(<span class="hljs-string">&#x27;done with setup ...&#x27;</span>)<br>    timers.log([<span class="hljs-string">&#x27;model-and-optimizer-setup&#x27;</span>, <span class="hljs-string">&#x27;train/valid/test-data-iterators-setup&#x27;</span>], barrier=<span class="hljs-literal">True</span>)<br><br>    one_logger = get_one_logger()<br>    one_logger <span class="hljs-keyword">and</span> one_logger.log_metrics(app_metrics)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> args.skip_train:<br>        print_rank_0(<span class="hljs-string">&#x27;training ...&#x27;</span>)<br><br>        <span class="hljs-keyword">if</span> args.dataloader_type == <span class="hljs-string">&#x27;cyclic&#x27;</span> <span class="hljs-keyword">and</span> args.retro_project_dir:<br>            <span class="hljs-keyword">assert</span> args.retro_cyclic_train_iters <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>            args.train_iters = args.retro_cyclic_train_iters<br>            print_rank_0(<span class="hljs-string">&quot;retro cyclic train iters : %d&quot;</span> % args.train_iters)<br><br>        iteration = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> args.do_train <span class="hljs-keyword">and</span> args.train_iters &gt; <span class="hljs-number">0</span>:<br>            iteration, num_floating_point_operations_so_far = train(<br>                forward_step_func,<br>                model,<br>                optimizer,<br>                opt_param_scheduler,<br>                train_data_iterator,<br>                valid_data_iterator,<br>                process_non_loss_data_func,<br>                config,<br>                checkpointing_context,<br>                non_loss_data_func,<br>            )<br><br>        print_datetime(<span class="hljs-string">&#x27;after training is done&#x27;</span>)<br><br>        <span class="hljs-keyword">if</span> args.save <span class="hljs-keyword">and</span> iteration != <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> iteration % args.save_interval != <span class="hljs-number">0</span>:<br>            save_checkpoint(<br>                iteration,<br>                model,<br>                optimizer,<br>                opt_param_scheduler,<br>                num_floating_point_operations_so_far,<br>                checkpointing_context,<br>                train_data_iterator=train_data_iterator,<br>                preprocess_common_state_dict_fn=preprocess_common_state_dict,<br>            )<br><br>        one_logger <span class="hljs-keyword">and</span> one_logger.log_metrics(<br>            &#123;<span class="hljs-string">&#x27;app_train_loop_finish_time&#x27;</span>: one_logger_utils.get_timestamp_in_ms()&#125;<br>        )<br><br>    <span class="hljs-keyword">else</span>:<br>        print_rank_0(<span class="hljs-string">&#x27;skipping training (--skip-train is on) ...&#x27;</span>)<br><br>        iteration = args.iteration<br><br>    <span class="hljs-keyword">if</span> args.do_valid:<br>        prefix = <span class="hljs-string">f&#x27;iteration <span class="hljs-subst">&#123;iteration&#125;</span> on validation set&#x27;</span><br>        evaluate_and_print_results(<br>            prefix,<br>            forward_step_func,<br>            valid_data_iterator,<br>            model,<br>            iteration,<br>            process_non_loss_data_func,<br>            config,<br>            verbose=<span class="hljs-literal">True</span>,<br>            write_to_tensorboard=<span class="hljs-keyword">not</span> args.skip_train,<br>            non_loss_data_func=non_loss_data_func,<br>        )<br><br>    <span class="hljs-keyword">if</span> args.do_test:<br>        prefix = <span class="hljs-string">f&#x27;iteration <span class="hljs-subst">&#123;iteration&#125;</span> on test set&#x27;</span><br>        evaluate_and_print_results(<br>            prefix,<br>            forward_step_func,<br>            test_data_iterator,<br>            model,<br>            iteration,<br>            process_non_loss_data_func,<br>            config,<br>            verbose=<span class="hljs-literal">True</span>,<br>            write_to_tensorboard=<span class="hljs-keyword">not</span> args.skip_train,<br>            non_loss_data_func=non_loss_data_func,<br>        )<br><br>    wandb_writer = get_wandb_writer()<br>    <span class="hljs-keyword">if</span> wandb_writer:<br>        wandb_writer.finish()<br><br>    ft_integration.on_checkpointing_start()<br>    maybe_finalize_async_save(blocking=<span class="hljs-literal">True</span>, terminate=<span class="hljs-literal">True</span>)<br>    ft_integration.on_checkpointing_end(is_async_finalization=<span class="hljs-literal">True</span>)<br><br>    one_logger <span class="hljs-keyword">and</span> one_logger.log_metrics(<br>        &#123;<span class="hljs-string">&#x27;app_finish_time&#x27;</span>: one_logger_utils.get_timestamp_in_ms()&#125;<br>    )<br><br>    ft_integration.shutdown()<br>    one_logger_utils.finish()<br><br></code></pre></td></tr></table></figure>

<h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><ol>
<li><p>如果参数<code>inprocess_call_wrapper</code>不为空，说明需要容错，那么再次进入pretrain的时候，为了避免还接入到原本的控制面，需要调用<code>inprocess_call_wrapper.iteration</code>进行命名空间更新，来接入新的store。</p>
</li>
<li><p>初始化megatron-lm的通信组、并行设置、关键参数等等，下面会具体介绍</p>
</li>
<li><p>获取全局参数，megatron-lm是单例设计，获取参数都是通过<code>get_*</code>来通过获取全局变量获得</p>
</li>
<li><p>&#x20;FT（Fault Tolerance）初始化，FT更偏向于是利用checkpoint进行容错，inprocess是对进程运行时的容错</p>
</li>
<li><p>设置PyTorch JIT fusion进行算子融合，如果有必要还会对其进行预热</p>
</li>
<li><p>通过min操作的all reduce来获取最小的训练开始时间，已记录相关日志</p>
</li>
<li><p>如果参数控制需要不落盘的内存级的checkpoint，就引入相关的包并设置对应的上下文</p>
</li>
<li><p>根据并行化策略等得到model、optimizer、opt_param_scheduler，下面会具体介绍</p>
</li>
<li><p>构建数据迭代器，如果采用了Virtual Pipeline并行，那么每个 pipeline stage都会有自己专门的 data iterator</p>
</li>
</ol>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><ul>
<li>如果参数指示跳过train，那么就不执行train，继续执行后面的，如果没有，那么就执行<code>iteration, num_floating_point_operations_so_far = train(...)</code>进行训练，下面会具体介绍。并且如果迭代次数不是保存checkpoint的倍数那么就会专门对最后的模型进行保存</li>
</ul>
<h2 id="收尾"><a href="#收尾" class="headerlink" title="收尾"></a>收尾</h2><ul>
<li><p>如果参数指示需要valid，那么就调用<code>evaluate_and_print_results</code>使用<code>valid_data_iterator</code>获取数据执行一次valid，下面会具体介绍</p>
</li>
<li><p>如果参数指示需要test，那么就调用<code>evaluate_and_print_results</code>使用<code>test_data_iterator</code>获取数据执行一次test</p>
</li>
<li><p>得到wandb的句柄并关闭</p>
</li>
<li><p>确保async checkpoint 完成，并将所有 IO 收尾</p>
</li>
<li><p>关闭FT和Logger</p>
</li>
</ul>
<h1 id="pretrain核心流程解析"><a href="#pretrain核心流程解析" class="headerlink" title="pretrain核心流程解析"></a>pretrain核心流程解析</h1><h2 id="initialize-megatron"><a href="#initialize-megatron" class="headerlink" title="initialize_megatron"></a>initialize_megatron</h2><p><code>initialize_megatron</code>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_megatron</span>(<span class="hljs-params"></span><br><span class="hljs-params">    extra_args_provider=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    args_defaults=&#123;&#125;,</span><br><span class="hljs-params">    ignore_unknown_args=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    allow_no_cuda=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    skip_mpu_initialization=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    get_embedding_ranks=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    get_position_embedding_ranks=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    parsed_args=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    store=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Set global variables, initialize distributed, and</span><br><span class="hljs-string">    set autoresume and random seeds.</span><br><span class="hljs-string">    `allow_no_cuda` should not be set unless using megatron for cpu only</span><br><span class="hljs-string">    data processing. In general this arg should not be set unless you know</span><br><span class="hljs-string">    what you are doing.</span><br><span class="hljs-string">    Returns a function to finalize distributed env initialization</span><br><span class="hljs-string">    (optionally, only when args.lazy_mpu_init == True)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> allow_no_cuda:<br>        <span class="hljs-comment"># Make sure cuda is available.</span><br>        <span class="hljs-keyword">assert</span> torch.cuda.is_available(), <span class="hljs-string">&quot;Megatron requires CUDA.&quot;</span><br><br>    <span class="hljs-comment"># Parse arguments</span><br>    <span class="hljs-keyword">if</span> parsed_args <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        args = parse_args(extra_args_provider, ignore_unknown_args)<br>    <span class="hljs-keyword">else</span>:<br>        args = parsed_args<br><br>    <span class="hljs-comment"># Prep for checkpoint conversion.</span><br>    <span class="hljs-keyword">if</span> args.ckpt_convert_format <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">assert</span> args.ckpt_convert_save <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">assert</span> args.load <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        args.exit_on_missing_checkpoint = <span class="hljs-literal">True</span><br><br>    <span class="hljs-keyword">if</span> args.use_checkpoint_args <span class="hljs-keyword">or</span> args_defaults.get(<span class="hljs-string">&quot;use_checkpoint_args&quot;</span>, <span class="hljs-literal">False</span>):<br>        <span class="hljs-keyword">assert</span> args.load <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;--use-checkpoint-args requires --load argument&quot;</span><br>        <span class="hljs-keyword">assert</span> args.non_persistent_ckpt_type != <span class="hljs-string">&quot;local&quot;</span>, (<br>            <span class="hljs-string">&quot;--use-checkpoint-args is not supported with --non_persistent_ckpt_type=local. &quot;</span><br>            <span class="hljs-string">&quot;Two-stage checkpoint loading is not implemented, and all arguments must be defined &quot;</span><br>            <span class="hljs-string">&quot;before initializing LocalCheckpointManager.&quot;</span><br>        )<br>        load_args_from_checkpoint(args)<br><br>    <span class="hljs-keyword">if</span> args.async_save <span class="hljs-keyword">and</span> args.use_persistent_ckpt_worker:<br>        init_persistent_async_worker()<br><br>    <span class="hljs-keyword">if</span> args.yaml_cfg <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        args = validate_yaml(args, args_defaults)<br>    <span class="hljs-keyword">else</span>:<br>        validate_args(args, args_defaults)<br><br>    <span class="hljs-comment"># set global args, build tokenizer, and set adlr-autoresume,</span><br>    <span class="hljs-comment"># tensorboard-writer, and timers.</span><br>    set_global_variables(args)<br><br>    <span class="hljs-comment"># set logging level</span><br>    setup_logging()<br><br>    <span class="hljs-comment"># init rerun state</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">state_save_func</span>():<br>        <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&#x27;rng_tracker_states&#x27;</span>: tensor_parallel.get_cuda_rng_tracker().get_states()&#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">state_restore_func</span>(<span class="hljs-params">state_dict</span>):<br>        <span class="hljs-keyword">if</span> state_dict[<span class="hljs-string">&#x27;rng_tracker_states&#x27;</span>]:<br>            tensor_parallel.get_cuda_rng_tracker().set_states(state_dict[<span class="hljs-string">&#x27;rng_tracker_states&#x27;</span>])<br><br>    args = get_args()<br>    initialize_rerun_state_machine(<br>        state_save_func=state_save_func,<br>        state_restore_func=state_restore_func,<br>        mode=RerunMode(args.rerun_mode),<br>        error_injector=RerunErrorInjector(<br>            error_injection_rate=args.error_injection_rate,<br>            error_injection_type=RerunDiagnostic(args.error_injection_type),<br>        ),<br>        result_rejected_tracker_filename=args.result_rejected_tracker_filename,<br>    )<br><br>    <span class="hljs-comment"># torch.distributed initialization</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">finish_mpu_init</span>():<br>        args = get_args()<br>        <span class="hljs-comment"># Pytorch distributed.</span><br>        _initialize_distributed(get_embedding_ranks, get_position_embedding_ranks, store)<br><br>        <span class="hljs-comment"># Random seeds for reproducibility.</span><br>        <span class="hljs-keyword">if</span> args.rank == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; setting random seeds to &#123;&#125; ...&quot;</span>.<span class="hljs-built_in">format</span>(args.seed))<br>        _set_random_seed(<br>            args.seed,<br>            args.data_parallel_random_init,<br>            args.te_rng_tracker,<br>            args.inference_rng_tracker,<br>            use_cudagraphable_rng=args.enable_cuda_graph <span class="hljs-keyword">or</span> args.external_cuda_graph,<br>        )<br><br>        <span class="hljs-comment"># Setup MoE aux loss scale value.</span><br>        <span class="hljs-keyword">if</span> args.num_experts <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">from</span> megatron.core.transformer.moe.router <span class="hljs-keyword">import</span> MoEAuxLossAutoScaler<br><br>            MoEAuxLossAutoScaler.set_loss_scale(torch.ones(<span class="hljs-number">1</span>, device=torch.cuda.current_device()))<br><br>    <span class="hljs-keyword">if</span> skip_mpu_initialization:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    args = get_args()<br>    <span class="hljs-keyword">if</span> args.lazy_mpu_init:<br>        <span class="hljs-comment"># TODO is this still a necessary option?</span><br>        args.use_cpu_initialization = <span class="hljs-literal">True</span><br>        <span class="hljs-comment"># delayed initialization of DDP-related stuff</span><br>        <span class="hljs-comment"># We only set basic DDP globals</span><br>        mpu.set_tensor_model_parallel_world_size(args.tensor_model_parallel_size)<br>        <span class="hljs-comment"># and return function for external DDP manager</span><br>        <span class="hljs-comment"># to call when it has DDP initialized</span><br>        mpu.set_tensor_model_parallel_rank(args.rank)<br>        <span class="hljs-keyword">return</span> finish_mpu_init<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># Megatron&#x27;s MPU is the master. Complete initialization right away.</span><br>        finish_mpu_init()<br><br>        <span class="hljs-comment"># Autoresume.</span><br>        _init_autoresume()<br><br>        <span class="hljs-comment"># Compile dependencies.</span><br>        _compile_dependencies()<br><br>        <span class="hljs-keyword">if</span> args.tp_comm_overlap:<br>            <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> Should this be activated with just decoder-tp-comm-overlap too?</span><br>            _initialize_tp_communicators()<br><br>        <span class="hljs-comment"># No continuation function</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br></code></pre></td></tr></table></figure>

<p><code>initialize_megatron</code>流程如下：</p>
<ol>
<li><p>检查是否包含cuda</p>
</li>
<li><p>解析参数，注意这里还使用了<code>pretrain</code>函数传递进来的<code>extra_args_provider</code></p>
</li>
<li><p>对checkpoint做格式转换并考虑从checkpoint中获取训练参数，如果使用异步checkpoint还负责启动保存checkpoint的IO worker</p>
</li>
<li><p>校验参数，设置全局参数</p>
</li>
<li><p>初始化日志</p>
</li>
<li><p>初始化容错的rerun状态机</p>
</li>
<li><p>如果使用<code>lazy_mpu_init</code>，就先设置一些模型并行参数，返回<code>finish_mpu_init</code>，等待外部调用其初始化</p>
</li>
<li><p>如果不使用<code>lazy_mpu_init</code>，就先调用<code>finish_mpu_init</code>初始化，再自动从 checkpoint 恢复，再提前编译依赖，再做 TP 通信重叠初始化。</p>
</li>
</ol>
<h3 id="finish-mpu-init"><a href="#finish-mpu-init" class="headerlink" title="finish_mpu_init"></a><code>finish_mpu_init</code></h3><p><code>finish_mpu_init</code>是初始化的核心模块，其代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># torch.distributed initialization</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">finish_mpu_init</span>():<br>    args = get_args()<br>    <span class="hljs-comment"># Pytorch distributed.</span><br>    _initialize_distributed(get_embedding_ranks, get_position_embedding_ranks, store)<br><br>    <span class="hljs-comment"># Random seeds for reproducibility.</span><br>    <span class="hljs-keyword">if</span> args.rank == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; setting random seeds to &#123;&#125; ...&quot;</span>.<span class="hljs-built_in">format</span>(args.seed))<br>    _set_random_seed(<br>        args.seed,<br>        args.data_parallel_random_init,<br>        args.te_rng_tracker,<br>        args.inference_rng_tracker,<br>        use_cudagraphable_rng=args.enable_cuda_graph <span class="hljs-keyword">or</span> args.external_cuda_graph,<br>    )<br><br>    <span class="hljs-comment"># Setup MoE aux loss scale value.</span><br>    <span class="hljs-keyword">if</span> args.num_experts <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">from</span> megatron.core.transformer.moe.router <span class="hljs-keyword">import</span> MoEAuxLossAutoScaler<br><br>        MoEAuxLossAutoScaler.set_loss_scale(torch.ones(<span class="hljs-number">1</span>, device=torch.cuda.current_device()))<br><br></code></pre></td></tr></table></figure>

<p><code>finish_mpu_init</code>流程如下：</p>
<ol>
<li><p>调用<code>_initialize_distributed</code>初始化通信组</p>
</li>
<li><p>设置随机随机种子，Megatron 的 RNG 体系是：DP 可以不同，TP &#x2F; PP 必须一致</p>
</li>
<li><p>如果是专家并行，还需要设置MoE 辅助损失缩放</p>
</li>
</ol>
<h3 id="initialize-distributed"><a href="#initialize-distributed" class="headerlink" title="_initialize_distributed"></a><code>_initialize_distributed</code></h3><p>对于关键的<code>_initialize_distributed</code>，其代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_initialize_distributed</span>(<span class="hljs-params">get_embedding_ranks, get_position_embedding_ranks, store</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Initialize torch.distributed and core model parallel.&quot;&quot;&quot;</span><br>    args = get_args()<br><br>    device_count = torch.cuda.device_count()<br>    <span class="hljs-keyword">if</span> torch.distributed.is_initialized():<br><br>        <span class="hljs-keyword">if</span> args.rank == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<br>                <span class="hljs-string">&quot;torch distributed is already initialized, &quot;</span> <span class="hljs-string">&quot;skipping initialization ...&quot;</span>,<br>                flush=<span class="hljs-literal">True</span>,<br>            )<br>        args.rank = torch.distributed.get_rank()<br>        args.world_size = torch.distributed.get_world_size()<br><br>    <span class="hljs-keyword">else</span>:<br><br>        <span class="hljs-keyword">if</span> args.rank == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&gt; initializing torch distributed ...&quot;</span>, flush=<span class="hljs-literal">True</span>)<br>        <span class="hljs-comment"># Manually set the device ids.</span><br>        <span class="hljs-keyword">if</span> device_count &gt; <span class="hljs-number">0</span>:<br>            torch.cuda.set_device(args.local_rank)<br>            device_id = torch.device(<span class="hljs-string">f&#x27;cuda:<span class="hljs-subst">&#123;args.local_rank&#125;</span>&#x27;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            device_id = <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># Set to non-default stream for cudagraph capturing.</span><br>        <span class="hljs-keyword">if</span> args.external_cuda_graph:<br>            torch.cuda.set_stream(torch.cuda.Stream())<br><br>        <span class="hljs-comment"># Call the init process</span><br>        init_process_group_kwargs = &#123;<br>            <span class="hljs-string">&#x27;backend&#x27;</span>: args.distributed_backend,<br>            <span class="hljs-string">&#x27;store&#x27;</span>: store,<br>            <span class="hljs-string">&#x27;world_size&#x27;</span>: args.world_size,<br>            <span class="hljs-string">&#x27;rank&#x27;</span>: args.rank,<br>            <span class="hljs-string">&#x27;timeout&#x27;</span>: timedelta(minutes=args.distributed_timeout_minutes),<br>        &#125;<br><br>        torch.distributed.init_process_group(**init_process_group_kwargs)<br>        inprocess_restart.maybe_force_nccl_backend_init(device_id)<br><br>    <span class="hljs-comment"># Set the tensor model-parallel, pipeline model-parallel, and</span><br>    <span class="hljs-comment"># data-parallel communicators.</span><br>    <span class="hljs-keyword">if</span> device_count &gt; <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">if</span> mpu.model_parallel_is_initialized():<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;model parallel is already initialized&quot;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            mpu.initialize_model_parallel(<br>                args.tensor_model_parallel_size,<br>                args.pipeline_model_parallel_size,<br>                args.virtual_pipeline_model_parallel_size,<br>                pipeline_model_parallel_comm_backend=args.pipeline_model_parallel_comm_backend,<br>                use_sharp=args.use_sharp,<br>                context_parallel_size=args.context_parallel_size,<br>                hierarchical_context_parallel_sizes=args.hierarchical_context_parallel_sizes,<br>                expert_model_parallel_size=args.expert_model_parallel_size,<br>                num_distributed_optimizer_instances=args.num_distributed_optimizer_instances,<br>                expert_tensor_parallel_size=args.expert_tensor_parallel_size,<br>                distributed_timeout_minutes=args.distributed_timeout_minutes,<br>                nccl_communicator_config_path=args.nccl_communicator_config_path,<br>                order=<span class="hljs-string">&#x27;tp-cp-ep-dp-pp&#x27;</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> args.use_tp_pp_dp_mapping <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;tp-cp-ep-pp-dp&#x27;</span>,<br>                get_embedding_ranks=get_embedding_ranks,<br>                get_position_embedding_ranks=get_position_embedding_ranks,<br>                create_gloo_process_groups=args.enable_gloo_process_groups,<br>                high_priority_stream_groups=args.high_priority_stream_groups,<br>                sharp_enabled_group=args.sharp_enabled_group,<br>            )<br>            <span class="hljs-keyword">if</span> args.rank == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<br>                    <span class="hljs-string">f&quot;&gt; initialized tensor model parallel with size &quot;</span><br>                    <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;mpu.get_tensor_model_parallel_world_size()&#125;</span>&quot;</span><br>                )<br>                <span class="hljs-built_in">print</span>(<br>                    <span class="hljs-string">f&quot;&gt; initialized pipeline model parallel with size &quot;</span><br>                    <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;mpu.get_pipeline_model_parallel_world_size()&#125;</span>&quot;</span><br>                )<br></code></pre></td></tr></table></figure>

<p>流程如下：</p>
<ol>
<li><p>通过<code>torch.distributed.is_initialized()</code>检查是否初始化<code>torch.distributed</code>，如果没有就调用<code>torch.distributed.init_process_group(**init_process_group_kwargs)</code>初始化。注意这里使用了<code>pretrain</code>传入的TCPStore。然后为了防止 NCCL communicator 因进程重启而失效，还强制触发一次 NCCL 初始化。</p>
</li>
<li><p>检查设备数是否大于0，如果是就检查是否已经进行模型并行初始化，如果没有就调用<code>mpu.initialize_model_parallel</code>进行初始化。</p>
</li>
</ol>
<p><code>mpu.initialize_model_parallel</code>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># pylint: disable=C0301</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_model_parallel</span>(<span class="hljs-params"></span><br><span class="hljs-params">    tensor_model_parallel_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">    pipeline_model_parallel_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">    virtual_pipeline_model_parallel_size: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    pipeline_model_parallel_comm_backend: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    use_sharp: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    context_parallel_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">    hierarchical_context_parallel_sizes: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    expert_model_parallel_size: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">    num_distributed_optimizer_instances: <span class="hljs-built_in">int</span> = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">    expert_tensor_parallel_size: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    nccl_communicator_config_path: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    distributed_timeout_minutes: <span class="hljs-built_in">int</span> = <span class="hljs-number">30</span>,</span><br><span class="hljs-params">    order: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;tp-cp-ep-dp-pp&quot;</span>,</span><br><span class="hljs-params">    get_embedding_ranks: <span class="hljs-type">Optional</span>[<span class="hljs-type">Callable</span>[[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>]], <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    get_position_embedding_ranks: <span class="hljs-type">Optional</span>[<span class="hljs-type">Callable</span>[[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>]], <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    create_gloo_process_groups: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params">    high_priority_stream_groups: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    sharp_enabled_group: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Initialize model data parallel groups.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        tensor_model_parallel_size (int, default = 1):</span><br><span class="hljs-string">            The number of GPUs to split individual tensors across.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        pipeline_model_parallel_size (int, default = 1):</span><br><span class="hljs-string">            The number of tensor parallel GPU groups to split the</span><br><span class="hljs-string">            Transformer layers across. For example, if</span><br><span class="hljs-string">            tensor_model_parallel_size is 4 and</span><br><span class="hljs-string">            pipeline_model_parallel_size is 2, the model will be split</span><br><span class="hljs-string">            into 2 groups of 4 GPUs.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        virtual_pipeline_model_parallel_size (int, optional):</span><br><span class="hljs-string">            The number of stages that each pipeline group will have,</span><br><span class="hljs-string">            interleaving as necessary. If None, no interleaving is</span><br><span class="hljs-string">            performed. For example, if tensor_model_parallel_size is 1,</span><br><span class="hljs-string">            pipeline_model_parallel_size is 4,</span><br><span class="hljs-string">            virtual_pipeline_model_parallel_size is 2, and there are</span><br><span class="hljs-string">            16 transformer layers in the model, the model will be</span><br><span class="hljs-string">            split into 8 stages with two layers each and each GPU</span><br><span class="hljs-string">            would get 2 stages as such (layer number starting with 1):</span><br><span class="hljs-string"></span><br><span class="hljs-string">            GPU 0: [1, 2] [9, 10]</span><br><span class="hljs-string">            GPU 1: [3, 4] [11, 12]</span><br><span class="hljs-string">            GPU 2: [5, 6] [13, 14]</span><br><span class="hljs-string">            GPU 3: [7, 8] [15, 16]</span><br><span class="hljs-string"></span><br><span class="hljs-string">        pipeline_model_parallel_comm_backend (str, optional):</span><br><span class="hljs-string">            The backend to use for pipeline parallel communication.</span><br><span class="hljs-string">            If None, the default backend will be used.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        use_sharp (bool, default = False):</span><br><span class="hljs-string">            Set the use of SHARP for the collective communications of</span><br><span class="hljs-string">            data-parallel process groups. When `True`, run barrier</span><br><span class="hljs-string">            within each data-parallel process group, which specifies</span><br><span class="hljs-string">            the SHARP application target groups.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        context_parallel_size (int, default = 1):</span><br><span class="hljs-string">            The number of tensor parallel GPU groups to split the</span><br><span class="hljs-string">            network input sequence length across. Compute of attention</span><br><span class="hljs-string">            module requires tokens of full sequence length, so GPUs</span><br><span class="hljs-string">            in a context parallel group need to communicate with each</span><br><span class="hljs-string">            other to exchange information of other sequence chunks.</span><br><span class="hljs-string">            Each GPU and its counterparts in other tensor parallel</span><br><span class="hljs-string">            groups compose a context parallel group.</span><br><span class="hljs-string"></span><br><span class="hljs-string">            For example, assume we have 8 GPUs, if tensor model parallel</span><br><span class="hljs-string">            size is 4 and context parallel size is 2, the network input</span><br><span class="hljs-string">            will be split into two sequence chunks, which are processed</span><br><span class="hljs-string">            by 2 different groups of 4 GPUs. One chunk is processed by</span><br><span class="hljs-string">            GPU0-3, the other chunk is processed by GPU4-7. Four groups</span><br><span class="hljs-string">            are build to do context parallel communications: [GPU0, GPU4],</span><br><span class="hljs-string">            [GPU1, GPU5], [GPU2, GPU6], and [GPU3, GPU7].</span><br><span class="hljs-string"></span><br><span class="hljs-string">            Context parallelism partitions sequence length, so it has no</span><br><span class="hljs-string">            impact on weights, which means weights are duplicated among</span><br><span class="hljs-string">            GPUs in a context parallel group. Hence, weight gradients</span><br><span class="hljs-string">            all-reduce is required in backward. For simplicity, we piggyback</span><br><span class="hljs-string">            GPUs of context parallelism on data parallel group for</span><br><span class="hljs-string">            weight gradient all-reduce.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        expert_model_parallel_size (int, default = 1):</span><br><span class="hljs-string">            The number of Mixture of Experts parallel GPUs in each expert</span><br><span class="hljs-string">            parallel group.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        num_distributed_optimizer_instances (int, default = 1):</span><br><span class="hljs-string">            The number of distributed optimizer replicas across the data-</span><br><span class="hljs-string">            parallel domain.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        expert_tensor_parallel_size (int, default = tp_size):</span><br><span class="hljs-string">            The number of GPUs to split individual tensors of expert.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        nccl_communicator_config_path (str, default = None):</span><br><span class="hljs-string">            Path to the yaml file of NCCL communicator configurations.</span><br><span class="hljs-string">            `min_ctas`, `max_ctas`, and `cga_cluster_size` can be set</span><br><span class="hljs-string">            for each communicator.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        distributed_timeout_minutes (int, default = 30): Timeout, in</span><br><span class="hljs-string">            minutes,for operations executed against distributed</span><br><span class="hljs-string">            process groups. See PyTorch documentation at</span><br><span class="hljs-string">            https://pytorch.org/docs/stable/distributed.html for</span><br><span class="hljs-string">            caveats.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        order (str, default=tp-dp-pp):</span><br><span class="hljs-string">            The rank initialization order of parallelism. Now we support</span><br><span class="hljs-string">            tp-dp-pp and tp-pp-dp orders.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        get_embedding_ranks (Callable[[List[int], Optional[int]], List[int]], optional, default=None):</span><br><span class="hljs-string">            A function that takes in a list of ranks for a pipeline group and returns</span><br><span class="hljs-string">            those ranks that should have embeddings.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        get_position_embedding_ranks (Callable[[List[int], Optional[int]], List[int]], optional, default=None):</span><br><span class="hljs-string">            A function that takes in a list of ranks for a pipeline group, and returns</span><br><span class="hljs-string">            those ranks that should have position embeddings.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        create_gloo_process_groups (bool, default = True):</span><br><span class="hljs-string">            Create Gloo process groups if set to True. If set to False, Gloo process groups are</span><br><span class="hljs-string">            not created and calls to get Gloo process groups will result in assertion errors.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        high_priority_stream_groups (List[str], default = None):</span><br><span class="hljs-string">            Specify which communicator groups should use high priority streams during creation.</span><br><span class="hljs-string">            Assigning high priority to communication streams ensures that communication kernels</span><br><span class="hljs-string">            are scheduled with higher priority, minimizing the exposed communication when it is</span><br><span class="hljs-string">            overlapped with other computation kernels.</span><br><span class="hljs-string">            Example: initialize_parallel_groups(..., high_priority_stream_groups=[&#x27;dp_cp&#x27;,&#x27;ep_dp&#x27;])</span><br><span class="hljs-string"></span><br><span class="hljs-string">        sharp_enabled_group (str, default = None):</span><br><span class="hljs-string">            Specify which communicator group should use SHARP communication.</span><br><span class="hljs-string">            This option is only valid when use_sharp is True.</span><br><span class="hljs-string">            By default (None), it is enabled from dp group.</span><br><span class="hljs-string">            Available options (choose one): [dp, dp_replica]</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Let&#x27;s say we have a total of 16 GPUs denoted by g0 ... g15 and we</span><br><span class="hljs-string">    use 2 GPUs to parallelize the model tensor, and 4 GPUs to parallelize</span><br><span class="hljs-string">    the model pipeline. The present function will</span><br><span class="hljs-string">    create 8 tensor model-parallel groups, 4 pipeline model-parallel groups</span><br><span class="hljs-string">    and 8 data-parallel groups as:</span><br><span class="hljs-string">        8 data_parallel groups:</span><br><span class="hljs-string">            [g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]</span><br><span class="hljs-string">        8 tensor model-parallel groups:</span><br><span class="hljs-string">            [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]</span><br><span class="hljs-string">        4 pipeline model-parallel groups:</span><br><span class="hljs-string">            [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]</span><br><span class="hljs-string">    Note that for efficiency, the caller should make sure adjacent ranks</span><br><span class="hljs-string">    are on the same DGX box. For example if we are using 2 DGX-1 boxes</span><br><span class="hljs-string">    with a total of 16 GPUs, rank 0 to 7 belong to the first box and</span><br><span class="hljs-string">    ranks 8 to 15 belong to the second box.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># NCCL restricts IB SHARP usage to a single communicator group—the first one created</span><br>    <span class="hljs-comment"># with NCCL_COLLNET_ENABLE=1. After this group is created, NCCL_COLLNET_ENABLE must be</span><br>    <span class="hljs-comment"># set to 0 for subsequent groups.</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span> <span class="hljs-keyword">in</span> os.environ:<br>        <span class="hljs-keyword">del</span> os.environ[<span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span>]<br><br>    <span class="hljs-keyword">if</span> use_sharp:<br>        <span class="hljs-keyword">if</span> sharp_enabled_group <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># By default, SHARP is enabled from dp group.</span><br>            sharp_enabled_group = <span class="hljs-string">&quot;dp&quot;</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># Currently, only dp and dp_replica groups are supported for SHARP.</span><br>            <span class="hljs-keyword">assert</span> sharp_enabled_group <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;dp&quot;</span>, <span class="hljs-string">&quot;dp_replica&quot;</span>], <span class="hljs-string">&quot;Invalid sharp_enabled_group&quot;</span><br>            <span class="hljs-keyword">if</span> sharp_enabled_group == <span class="hljs-string">&quot;dp_replica&quot;</span>:<br>                <span class="hljs-keyword">assert</span> (<br>                    num_distributed_optimizer_instances &gt; <span class="hljs-number">1</span><br>                ), <span class="hljs-string">&quot;dp_replica group requires num_distributed_optimizer_instances &gt; 1&quot;</span><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">assert</span> (<br>            sharp_enabled_group <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>        ), <span class="hljs-string">&quot;sharp_enabled_group is only valid when use_sharp is True&quot;</span><br><br>    <span class="hljs-keyword">if</span> get_embedding_ranks <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        get_embedding_ranks = default_embedding_ranks<br><br>    <span class="hljs-keyword">if</span> get_position_embedding_ranks <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        get_position_embedding_ranks = default_position_embedding_ranks<br><br>    <span class="hljs-comment"># Get world size and rank. Ensure some consistencies.</span><br>    <span class="hljs-keyword">assert</span> torch.distributed.is_initialized()<br>    world_size: <span class="hljs-built_in">int</span> = torch.distributed.get_world_size()<br><br>    model_size = tensor_model_parallel_size * pipeline_model_parallel_size * context_parallel_size<br><br>    <span class="hljs-keyword">if</span> world_size % model_size != <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">f&quot;world_size (<span class="hljs-subst">&#123;world_size&#125;</span>) is not divisible by <span class="hljs-subst">&#123;model_size&#125;</span>&quot;</span>)<br><br>    data_parallel_size: <span class="hljs-built_in">int</span> = world_size // model_size<br><br>    <span class="hljs-keyword">if</span> virtual_pipeline_model_parallel_size <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> pipeline_model_parallel_size &gt; <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<br>                <span class="hljs-string">&quot;pipeline-model-parallel size should be greater than 1 with interleaved schedule&quot;</span><br>            )<br>        <span class="hljs-keyword">global</span> _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK<br>        <span class="hljs-keyword">global</span> _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE<br>        _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK = <span class="hljs-number">0</span><br>        _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = virtual_pipeline_model_parallel_size<br><br>    rank = torch.distributed.get_rank()<br><br>    nccl_comm_cfgs = &#123;&#125;<br>    <span class="hljs-keyword">if</span> nccl_communicator_config_path <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">try</span>:<br>            <span class="hljs-keyword">import</span> yaml<br>        <span class="hljs-keyword">except</span> ImportError:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<br>                <span class="hljs-string">&quot;Cannot import `yaml`. Setting custom nccl communicator configs &quot;</span><br>                <span class="hljs-string">&quot;requires the yaml package.&quot;</span><br>            )<br><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(nccl_communicator_config_path, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> stream:<br>            nccl_comm_cfgs = yaml.safe_load(stream)<br><br>    <span class="hljs-comment"># Set is_high_priority_stream flag to the nccl_comm_cfgs if it is in high_priority_stream_groups</span><br>    high_priority_stream_groups = high_priority_stream_groups <span class="hljs-keyword">or</span> []<br>    <span class="hljs-keyword">for</span> pg_name <span class="hljs-keyword">in</span> high_priority_stream_groups:<br>        overwrite_nccl_comm_cfgs(nccl_comm_cfgs, pg_name, (<span class="hljs-string">&quot;is_high_priority_stream&quot;</span>, <span class="hljs-literal">True</span>))<br><br>    decoder_rank_generator = RankGenerator(<br>        tp=tensor_model_parallel_size,<br>        ep=<span class="hljs-number">1</span>,<br>        dp=data_parallel_size,<br>        pp=pipeline_model_parallel_size,<br>        cp=context_parallel_size,<br>        order=order,<br>        rank_offset=<span class="hljs-number">0</span>,<br>    )<br><br>    <span class="hljs-comment"># Build expert rank generator</span><br>    <span class="hljs-keyword">if</span> expert_tensor_parallel_size <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        expert_tensor_parallel_size = tensor_model_parallel_size<br>    expert_tensor_model_pipeline_parallel_size = (<br>        expert_tensor_parallel_size * expert_model_parallel_size * pipeline_model_parallel_size<br>    )<br>    expert_data_parallel_size = world_size // expert_tensor_model_pipeline_parallel_size<br>    <span class="hljs-keyword">if</span> world_size % expert_tensor_model_pipeline_parallel_size != <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">raise</span> RuntimeError(<br>            <span class="hljs-string">f&quot;world_size (<span class="hljs-subst">&#123;world_size&#125;</span>) is not divisible by expert_tensor_model_pipeline_parallel size (<span class="hljs-subst">&#123;expert_tensor_model_pipeline_parallel_size&#125;</span>)&quot;</span><br>        )<br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> support expert specific ordering</span><br>    expert_decoder_rank_generator = RankGenerator(<br>        tp=expert_tensor_parallel_size,<br>        ep=expert_model_parallel_size,<br>        dp=expert_data_parallel_size,<br>        pp=pipeline_model_parallel_size,<br>        cp=<span class="hljs-number">1</span>,<br>        order=order,<br>        rank_offset=<span class="hljs-number">0</span>,<br>    )<br><br>    <span class="hljs-keyword">assert</span> (<br>        order.endswith(<span class="hljs-string">&quot;pp&quot;</span>)<br>        <span class="hljs-keyword">or</span> pipeline_model_parallel_size == <span class="hljs-number">1</span><br>        <span class="hljs-keyword">or</span> expert_data_parallel_size == data_parallel_size<br>    ), <span class="hljs-string">&quot;When not using pp-last rank ordering, the data parallel size of the attention and moe layers must be the same&quot;</span><br><br>    <span class="hljs-keyword">assert</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&quot;pp&quot;</span>) == expert_decoder_rank_generator.get_ranks(<br>        <span class="hljs-string">&quot;pp&quot;</span><br>    ), <span class="hljs-string">f&quot;Pipeline parallel groups are expected to be the same for Non-Expert and Expert part, \</span><br><span class="hljs-string">    but got <span class="hljs-subst">&#123;decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;pp&#x27;</span>)&#125;</span> and <span class="hljs-subst">&#123;expert_decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;pp&#x27;</span>)&#125;</span>&quot;</span><br><br>    timeout = timedelta(minutes=distributed_timeout_minutes)<br><br>    <span class="hljs-comment"># Build the data-parallel groups.</span><br>    <span class="hljs-keyword">global</span> _DATA_PARALLEL_GROUP<br>    <span class="hljs-keyword">global</span> _DATA_PARALLEL_GROUP_GLOO<br>    <span class="hljs-keyword">global</span> _DATA_PARALLEL_GLOBAL_RANKS<br>    <span class="hljs-keyword">global</span> _DATA_PARALLEL_GROUP_WITH_CP<br>    <span class="hljs-keyword">global</span> _DATA_PARALLEL_GROUP_WITH_CP_GLOO<br>    <span class="hljs-keyword">global</span> _DATA_PARALLEL_GLOBAL_RANKS_WITH_CP<br>    <span class="hljs-keyword">global</span> _INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP<br>    <span class="hljs-keyword">global</span> _INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP_GLOO<br>    <span class="hljs-keyword">assert</span> _DATA_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;data parallel group is already initialized&quot;</span><br><br>    <span class="hljs-keyword">assert</span> (<br>        data_parallel_size * context_parallel_size<br>    ) % num_distributed_optimizer_instances == <span class="hljs-number">0</span>, (<br>        <span class="hljs-string">&quot;Data parallel size should be divisible by partial DistOpt shard factor&quot;</span><br>    )<br>    intra_partial_data_parallel_size = (<br>        data_parallel_size * context_parallel_size<br>    ) // num_distributed_optimizer_instances<br><br>    <span class="hljs-comment"># Set NCCL_COLLNET_ENABLE to 1 to enable SHARP for the dp group.</span><br>    <span class="hljs-keyword">if</span> sharp_enabled_group == <span class="hljs-string">&quot;dp&quot;</span>:<br>        os.environ[<span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span>] = <span class="hljs-string">&quot;1&quot;</span><br><br>    <span class="hljs-comment"># In case of using SHARP, the dp-cp group requires to use NCCL COLLNET feature.</span><br>    <span class="hljs-comment"># Due to the hardware limitation, only the initially created communication group</span><br>    <span class="hljs-comment"># is eligible for using the NCCL COLLNET feature.</span><br>    <span class="hljs-comment"># Therefore, dp-cp group, which potentially requires SHARP-enablement,</span><br>    <span class="hljs-comment"># need to be created before all the other groups</span><br>    <span class="hljs-keyword">for</span> ranks_with_cp <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;dp-cp&#x27;</span>):<br>        group_with_cp = create_group(<br>            ranks_with_cp,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;dp_cp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;DATA_PARALLEL_GROUP_WITH_CP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> create_gloo_process_groups:<br>            group_with_cp_gloo = create_group(<br>                ranks_with_cp,<br>                timeout=timeout,<br>                backend=<span class="hljs-string">&quot;gloo&quot;</span>,<br>                group_desc=<span class="hljs-string">&quot;DATA_PARALLEL_GROUP_WITH_CP_GLOO&quot;</span>,<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            group_with_cp_gloo = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks_with_cp:<br>            _DATA_PARALLEL_GROUP_WITH_CP = group_with_cp<br>            _DATA_PARALLEL_GROUP_WITH_CP_GLOO = group_with_cp_gloo<br>            _DATA_PARALLEL_GLOBAL_RANKS_WITH_CP = ranks_with_cp<br><br>        <span class="hljs-keyword">if</span> num_distributed_optimizer_instances &gt; <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># Create groups for intra-partial DP domain</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_distributed_optimizer_instances):<br>                intra_partial_dp_ranks_with_cp = ranks_with_cp[<br>                    (i * intra_partial_data_parallel_size) : (<br>                        (i + <span class="hljs-number">1</span>) * intra_partial_data_parallel_size<br>                    )<br>                ]<br>                intra_partial_dp_group_with_cp = create_group(<br>                    intra_partial_dp_ranks_with_cp,<br>                    timeout=timeout,<br>                    pg_options=get_nccl_options(<span class="hljs-string">&quot;intra_dp_cp&quot;</span>, nccl_comm_cfgs),<br>                    group_desc=<span class="hljs-string">&quot;INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP&quot;</span>,<br>                )<br>                <span class="hljs-keyword">if</span> create_gloo_process_groups:<br>                    intra_partial_dp_group_with_cp_gloo = create_group(<br>                        intra_partial_dp_ranks_with_cp,<br>                        timeout=timeout,<br>                        backend=<span class="hljs-string">&quot;gloo&quot;</span>,<br>                        group_desc=<span class="hljs-string">&quot;INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP_GLOO&quot;</span>,<br>                    )<br>                <span class="hljs-keyword">else</span>:<br>                    intra_partial_dp_group_with_cp_gloo = <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> intra_partial_dp_ranks_with_cp:<br>                    _INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP = intra_partial_dp_group_with_cp<br>                    _INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP_GLOO = (<br>                        intra_partial_dp_group_with_cp_gloo<br>                    )<br>        <span class="hljs-keyword">else</span>:<br>            _INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP = _DATA_PARALLEL_GROUP_WITH_CP<br>            _INTRA_PARTIAL_DATA_PARALLEL_GROUP_WITH_CP_GLOO = _DATA_PARALLEL_GROUP_WITH_CP_GLOO<br><br>    <span class="hljs-comment"># Apply SHARP to the dp group.</span><br>    <span class="hljs-keyword">if</span> sharp_enabled_group == <span class="hljs-string">&quot;dp&quot;</span>:<br>        <span class="hljs-keyword">if</span> rank == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<br>                <span class="hljs-string">&quot;The number of process groups to use SHARP with depends on the type &quot;</span><br>                <span class="hljs-string">&quot;of the network switch. Nvidia QM1 switch supports SAHRP up to 8 &quot;</span><br>                <span class="hljs-string">&quot;process groups and QM2 supports up to 256 process groups. We apply &quot;</span><br>                <span class="hljs-string">&quot;SHARP to the communications of the data-parallel domain. If the &quot;</span><br>                <span class="hljs-string">&quot;number of data-parallel process groups is larger than the max &quot;</span><br>                <span class="hljs-string">&quot;process groups that the network switch supports, the communication &quot;</span><br>                <span class="hljs-string">&quot;will fall back to non-SHARP operators. To enable SHARP, &quot;</span><br>                <span class="hljs-string">&quot;`#SBATCH_NETWORK=sharp` should be set in the sbatch script.&quot;</span><br>            )<br>        <span class="hljs-comment"># PyTorch is performing lazy initialization of the communicator group.</span><br>        <span class="hljs-comment"># Therefore, we need to perform a nccl call to ensure that the communicator group is created.</span><br>        torch.distributed.barrier(<br>            group=get_data_parallel_group(with_context_parallel=<span class="hljs-literal">True</span>),<br>            device_ids=[torch.cuda.current_device()],<br>        )<br>        torch.cuda.synchronize()<br>        <span class="hljs-comment"># Set `NCCL_COLLNET_ENABLE=0` to restrict SHARP application to the dp group.</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span> <span class="hljs-keyword">in</span> os.environ:<br>            <span class="hljs-keyword">del</span> os.environ[<span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span>]<br><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;dp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;dp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;DATA_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> create_gloo_process_groups:<br>            group_gloo = create_group(<br>                ranks, timeout=timeout, backend=<span class="hljs-string">&quot;gloo&quot;</span>, group_desc=<span class="hljs-string">&quot;DATA_PARALLEL_GROUP_GLOO&quot;</span><br>            )<br>        <span class="hljs-keyword">else</span>:<br>            group_gloo = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _DATA_PARALLEL_GROUP = group<br>            _DATA_PARALLEL_GROUP_GLOO = group_gloo<br>            _DATA_PARALLEL_GLOBAL_RANKS = ranks<br><br>    <span class="hljs-comment"># Build the context-parallel groups.</span><br>    <span class="hljs-keyword">global</span> _CONTEXT_PARALLEL_GROUP<br>    <span class="hljs-keyword">global</span> _CONTEXT_PARALLEL_GLOBAL_RANKS<br>    <span class="hljs-keyword">assert</span> _CONTEXT_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&#x27;context parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;cp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;cp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;CONTEXT_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _CONTEXT_PARALLEL_GROUP = group<br>            _CONTEXT_PARALLEL_GLOBAL_RANKS = ranks<br>        <span class="hljs-keyword">if</span> hierarchical_context_parallel_sizes:<br>            <span class="hljs-keyword">assert</span> np.prod(hierarchical_context_parallel_sizes) == context_parallel_size<br>            <span class="hljs-keyword">global</span> _HIERARCHICAL_CONTEXT_PARALLEL_GROUPS<br>            hierarchical_groups, _ = create_hierarchical_groups(<br>                rank,<br>                ranks,<br>                hierarchical_context_parallel_sizes,<br>                create_gloo_process_groups=<span class="hljs-literal">False</span>,<br>                pg_options=get_nccl_options(<span class="hljs-string">&quot;hcp&quot;</span>, nccl_comm_cfgs),<br>                timeout=timeout,<br>                group_desc=<span class="hljs-string">&quot;CONTEXT_PARALLEL_GROUP&quot;</span>,<br>            )<br>            <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>                _HIERARCHICAL_CONTEXT_PARALLEL_GROUPS = hierarchical_groups<br><br>    <span class="hljs-comment"># Build the model-parallel groups.</span><br>    <span class="hljs-keyword">global</span> _MODEL_PARALLEL_GROUP<br>    <span class="hljs-keyword">global</span> _MODEL_PARALLEL_GLOBAL_RANKS<br>    <span class="hljs-keyword">assert</span> _MODEL_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&#x27;model parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp-pp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;mp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;MODEL_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _MODEL_PARALLEL_GROUP = group<br>            _MODEL_PARALLEL_GLOBAL_RANKS = ranks<br><br>    <span class="hljs-comment"># Build the tensor model-parallel groups.</span><br>    <span class="hljs-keyword">global</span> _TENSOR_MODEL_PARALLEL_GROUP<br>    <span class="hljs-keyword">global</span> _TENSOR_MODEL_PARALLEL_GLOBAL_RANKS<br>    <span class="hljs-keyword">assert</span> (<br>        _TENSOR_MODEL_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&#x27;tensor model parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;tp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;TENSOR_MODEL_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _TENSOR_MODEL_PARALLEL_GROUP = group<br>            _TENSOR_MODEL_PARALLEL_GLOBAL_RANKS = ranks<br><br>    <span class="hljs-comment"># Build the pipeline model-parallel groups and embedding groups</span><br>    <span class="hljs-comment"># (first and last rank in each pipeline model-parallel group).</span><br>    <span class="hljs-keyword">global</span> _PIPELINE_MODEL_PARALLEL_GROUP<br>    <span class="hljs-keyword">global</span> _PIPELINE_GLOBAL_RANKS<br>    <span class="hljs-keyword">assert</span> (<br>        _PIPELINE_MODEL_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;pipeline model parallel group is already initialized&quot;</span><br>    <span class="hljs-keyword">global</span> _EMBEDDING_GROUP<br>    <span class="hljs-keyword">global</span> _EMBEDDING_GLOBAL_RANKS<br>    <span class="hljs-keyword">assert</span> _EMBEDDING_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;embedding group is already initialized&quot;</span><br>    <span class="hljs-keyword">global</span> _POSITION_EMBEDDING_GROUP<br>    <span class="hljs-keyword">global</span> _POSITION_EMBEDDING_GLOBAL_RANKS<br>    <span class="hljs-keyword">assert</span> _POSITION_EMBEDDING_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;position embedding group is already initialized&quot;</span><br>    <span class="hljs-keyword">if</span> pipeline_model_parallel_comm_backend == <span class="hljs-string">&quot;ucc&quot;</span>:<br>        <span class="hljs-comment"># The UCC backend provides two key benefits:</span><br>        <span class="hljs-comment"># 1) Achieves better bandwidth utilization than NCCL when using InfiniBand links.</span><br>        <span class="hljs-comment"># 2) Does not use GPU SM resources (Zero-SM), mitigating performance interference</span><br>        <span class="hljs-comment">#    with overlapping compute kernels.</span><br><br>        <span class="hljs-comment"># The UCC backend is recommended in the following cases:</span><br>        <span class="hljs-comment"># 1) When the exposed pipeline-parallel (PP) communications are significant.</span><br>        <span class="hljs-comment">#    - E.g., Pipeline parallelism with very less gradient accumulation steps.</span><br>        <span class="hljs-comment">#    - It may provide better performance due to improved bandwidth utilization.</span><br>        <span class="hljs-comment"># 2) When the critical-path pipeline stage has substantial PP-communication overlap.</span><br>        <span class="hljs-comment">#    - E.g., Uneven pipeline parallelism.</span><br>        <span class="hljs-comment">#    - It may provide better performance due to zero SM resource usage.</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;CUDA_DEVICE_MAX_CONNECTIONS&quot;</span> <span class="hljs-keyword">in</span> os.environ:<br>            <span class="hljs-comment"># UCC backend requires CUDA_DEVICE_MAX_CONNECTIONS variable to be larger than 1,</span><br>            <span class="hljs-comment"># to gurantee the overlapped UCC communications. If this environment variable is set to 1,</span><br>            <span class="hljs-comment"># all the UCC communication will be serialized.</span><br>            <span class="hljs-keyword">assert</span> (<br>                os.environ[<span class="hljs-string">&quot;CUDA_DEVICE_MAX_CONNECTIONS&quot;</span>] != <span class="hljs-string">&quot;1&quot;</span><br>            ), <span class="hljs-string">&quot;UCC-backend requires CUDA_DEVICE_MAX_CONNECTIONS &gt; 1&quot;</span><br><br>        <span class="hljs-comment"># Setting up required environment variables for ucc backend</span><br>        <span class="hljs-comment">#</span><br>        <span class="hljs-comment"># &quot;TORCH_UCC_BLOCKING_WAIT=none&quot; allows non-blocking waits of the communiction handle</span><br>        <span class="hljs-comment"># &quot;UCC_EC_CUDA_STREAM_TASK_MODE&quot; controls how CUDA execution engines (EC)</span><br>        <span class="hljs-comment"># schedule tasks on CUDA streams.</span><br>        <span class="hljs-comment"># &quot;UCX_TLS&quot; controls transport layer selection</span><br>        <span class="hljs-comment"># &quot;NSYS_UCP_COMM_PARAMS=1&quot; enables capturing ucx tracing in nsys profiling</span><br>        <span class="hljs-comment"># &quot;UCX_RNDV_THRESH&quot; controls threshold threshold for switching between</span><br>        <span class="hljs-comment"># eager and rendezvous (RNDV) communication protocols.</span><br>        <span class="hljs-comment"># &quot;UCX_NET_DEVICES&quot; select which network interfaces UCX should use.</span><br>        <span class="hljs-comment"># &quot;UCC_CL_BASIC_TLS&quot; controls which Transport Layers are used by</span><br>        <span class="hljs-comment"># the Basic Collective libraray</span><br><br>        os.environ[<span class="hljs-string">&quot;TORCH_UCC_BLOCKING_WAIT&quot;</span>] = (<br>            os.environ[<span class="hljs-string">&quot;TORCH_UCC_BLOCKING_WAIT&quot;</span>]<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;TORCH_UCC_BLOCKING_WAIT&quot;</span> <span class="hljs-keyword">in</span> os.environ<br>            <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;none&quot;</span><br>        )<br>        os.environ[<span class="hljs-string">&quot;UCC_EC_CUDA_STREAM_TASK_MODE&quot;</span>] = (<br>            os.environ[<span class="hljs-string">&quot;UCC_EC_CUDA_STREAM_TASK_MODE&quot;</span>]<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;UCC_EC_CUDA_STREAM_TASK_MODE&quot;</span> <span class="hljs-keyword">in</span> os.environ<br>            <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;driver&quot;</span><br>        )<br>        os.environ[<span class="hljs-string">&quot;UCX_TLS&quot;</span>] = (<br>            os.environ[<span class="hljs-string">&quot;UCX_TLS&quot;</span>] <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;UCX_TLS&quot;</span> <span class="hljs-keyword">in</span> os.environ <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;ib,cuda_copy&quot;</span><br>        )  <span class="hljs-comment"># cuda_ipc (i.e., NVLink-enablement) will be later supported</span><br>        os.environ[<span class="hljs-string">&quot;NSYS_UCP_COMM_PARAMS&quot;</span>] = <span class="hljs-string">&quot;1&quot;</span><br>        os.environ[<span class="hljs-string">&quot;UCX_RNDV_THRESH&quot;</span>] = <span class="hljs-string">&quot;0&quot;</span><br>        os.environ[<span class="hljs-string">&quot;UCX_NET_DEVICES&quot;</span>] = <span class="hljs-string">&quot;all&quot;</span><br>        os.environ[<span class="hljs-string">&quot;UCC_CL_BASIC_TLS&quot;</span>] = <span class="hljs-string">&quot;^sharp,nccl&quot;</span><br><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;pp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            backend=pipeline_model_parallel_comm_backend,<br>            pg_options=(<br>                <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">if</span> pipeline_model_parallel_comm_backend == <span class="hljs-string">&quot;ucc&quot;</span><br>                <span class="hljs-keyword">else</span> get_nccl_options(<span class="hljs-string">&quot;pp&quot;</span>, nccl_comm_cfgs)<br>            ),<br>            group_desc=<span class="hljs-string">&quot;PIPELINE_MODEL_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">assert</span> (<br>            pipeline_model_parallel_comm_backend == <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">or</span> pipeline_model_parallel_comm_backend == <span class="hljs-string">&quot;nccl&quot;</span><br>            <span class="hljs-keyword">or</span> pipeline_model_parallel_comm_backend == <span class="hljs-string">&quot;ucc&quot;</span><br>        ), <span class="hljs-string">f&#x27;&quot;<span class="hljs-subst">&#123;pipeline_model_parallel_comm_backend&#125;</span>&quot; backend for PP communication is currently not supported&#x27;</span><br><br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            <span class="hljs-keyword">if</span> _PIPELINE_MODEL_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                _PIPELINE_MODEL_PARALLEL_GROUP = group<br>                _PIPELINE_GLOBAL_RANKS = ranks<br>            <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(_PIPELINE_GLOBAL_RANKS[<span class="hljs-number">0</span>], <span class="hljs-built_in">list</span>):<br>                _PIPELINE_MODEL_PARALLEL_GROUP.append(group)<br>                _PIPELINE_GLOBAL_RANKS.append(ranks)<br>            <span class="hljs-keyword">else</span>:<br>                _PIPELINE_MODEL_PARALLEL_GROUP = [_PIPELINE_MODEL_PARALLEL_GROUP, group]<br>                _PIPELINE_GLOBAL_RANKS = [_PIPELINE_GLOBAL_RANKS, ranks]<br><br>        embedding_ranks = get_embedding_ranks(ranks)<br>        group = create_group(<br>            embedding_ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;embd&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;EMBEDDING_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> embedding_ranks:<br>            _EMBEDDING_GROUP = group<br>            _EMBEDDING_GLOBAL_RANKS = embedding_ranks<br><br>        position_embedding_ranks = get_position_embedding_ranks(ranks)<br>        group = create_group(<br>            position_embedding_ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;pos_embd&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;POSITION_EMBEDDING_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> position_embedding_ranks:<br>            _POSITION_EMBEDDING_GROUP = group<br>            _POSITION_EMBEDDING_GLOBAL_RANKS = position_embedding_ranks<br><br>    <span class="hljs-comment"># Build the tensor + data parallel groups.</span><br>    <span class="hljs-keyword">global</span> _TENSOR_AND_DATA_PARALLEL_GROUP<br>    <span class="hljs-keyword">global</span> _TENSOR_AND_DATA_PARALLEL_GROUP_WITH_CP<br>    <span class="hljs-keyword">assert</span> (<br>        _TENSOR_AND_DATA_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&#x27;Tensor + data parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp-dp-cp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;tp_dp_cp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;TENSOR_AND_DATA_PARALLEL_GROUP_WITH_CP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _TENSOR_AND_DATA_PARALLEL_GROUP_WITH_CP = group<br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp-dp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;tp_dp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;TENSOR_AND_DATA_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _TENSOR_AND_DATA_PARALLEL_GROUP = group<br><br>    <span class="hljs-keyword">global</span> _TENSOR_AND_CONTEXT_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> (<br>        _TENSOR_AND_CONTEXT_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&#x27;Tensor + context parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp-cp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;tp_cp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;TENSOR_AND_CONTEXT_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _TENSOR_AND_CONTEXT_PARALLEL_GROUP = group<br><br>    <span class="hljs-comment">### Expert-related parallel groups initialization</span><br>    <span class="hljs-comment"># Build the expert model parallel group</span><br>    <span class="hljs-keyword">global</span> _EXPERT_MODEL_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> _EXPERT_MODEL_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&#x27;Expert parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> expert_decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;ep&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;ep&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;EXPERT_MODEL_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _EXPERT_MODEL_PARALLEL_GROUP = group<br><br>    <span class="hljs-comment"># Build the expert tensor parallel group</span><br>    <span class="hljs-keyword">global</span> _EXPERT_TENSOR_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> (<br>        _EXPERT_TENSOR_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&#x27;Expert tensor model parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> expert_decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;ep_tp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;EXPERT_TENSOR_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _EXPERT_TENSOR_PARALLEL_GROUP = group<br><br>    <span class="hljs-comment"># Build the tensor + expert parallel groups</span><br>    <span class="hljs-keyword">global</span> _EXPERT_TENSOR_AND_MODEL_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> (<br>        _EXPERT_TENSOR_AND_MODEL_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&#x27;Expert tensor + model parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> expert_decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp-ep&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;tp_ep_mp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;EXPERT_TENSOR_AND_MODEL_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _EXPERT_TENSOR_AND_MODEL_PARALLEL_GROUP = group<br><br>    <span class="hljs-comment"># Build the expert+tensor+pipeline parallel groups</span><br>    <span class="hljs-keyword">global</span> _EXPERT_TENSOR_MODEL_PIPELINE_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> (<br>        _EXPERT_TENSOR_MODEL_PIPELINE_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&#x27;The expert_tensor_model_pipeline parallel group is already initialized&#x27;</span><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> expert_decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp-ep-pp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;tp_ep_pp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;EXPERT_TENSOR_MODEL_PIPELINE_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _EXPERT_TENSOR_MODEL_PIPELINE_PARALLEL_GROUP = group<br><br>    <span class="hljs-comment"># Build the expert data parallel group</span><br>    <span class="hljs-keyword">global</span> _EXPERT_DATA_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> _EXPERT_DATA_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;Expert data group is already initialized&quot;</span><br>    <span class="hljs-keyword">global</span> _EXPERT_DATA_PARALLEL_GROUP_GLOO<br>    <span class="hljs-keyword">assert</span> _EXPERT_DATA_PARALLEL_GROUP_GLOO <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;Expert data group-gloo is already initialized&quot;</span><br>    <span class="hljs-keyword">global</span> _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> (<br>        _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;Intra partial expert data group is already initialized&quot;</span><br>    <span class="hljs-keyword">global</span> _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP_GLOO<br>    <span class="hljs-keyword">assert</span> (<br>        _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP_GLOO <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;Intra partial expert data group-gloo is already initialized&quot;</span><br>    <span class="hljs-keyword">global</span> _INTER_PARTIAL_EXPERT_DATA_PARALLEL_GROUP<br>    <span class="hljs-keyword">assert</span> (<br>        _INTER_PARTIAL_EXPERT_DATA_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;Inter partial expert data group is already initialized&quot;</span><br><br>    <span class="hljs-keyword">assert</span> (<br>        expert_data_parallel_size % num_distributed_optimizer_instances == <span class="hljs-number">0</span><br>    ), <span class="hljs-string">&quot;Expert data parallel size should be divisible by partial DistOpt shard factor&quot;</span><br>    intra_partial_expert_data_parallel_size = (<br>        expert_data_parallel_size // num_distributed_optimizer_instances<br>    )<br><br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> expert_decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;dp&#x27;</span>):<br>        group = create_group(<br>            ranks,<br>            timeout=timeout,<br>            pg_options=get_nccl_options(<span class="hljs-string">&quot;ep_dp&quot;</span>, nccl_comm_cfgs),<br>            group_desc=<span class="hljs-string">&quot;EXPERT_DATA_PARALLEL_GROUP&quot;</span>,<br>        )<br>        <span class="hljs-keyword">if</span> create_gloo_process_groups:<br>            group_gloo = create_group(<br>                ranks, backend=<span class="hljs-string">&quot;gloo&quot;</span>, group_desc=<span class="hljs-string">&quot;EXPERT_DATA_PARALLEL_GROUP_GLOO&quot;</span><br>            )<br>        <span class="hljs-keyword">else</span>:<br>            group_gloo = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>            _EXPERT_DATA_PARALLEL_GROUP = group<br>            _EXPERT_DATA_PARALLEL_GROUP_GLOO = group_gloo<br><br>        <span class="hljs-keyword">if</span> num_distributed_optimizer_instances &gt; <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># Create groups for Partial DistOpt, one for intra-partial DP domain</span><br>            <span class="hljs-comment"># Another for inter-partial DP domain</span><br><br>            <span class="hljs-comment"># Set NCCL_COLLNET_ENABLE to 1 to enable SHARP for the dp_replica group.</span><br>            <span class="hljs-keyword">if</span> sharp_enabled_group == <span class="hljs-string">&quot;dp_replica&quot;</span>:<br>                os.environ[<span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span>] = <span class="hljs-string">&quot;1&quot;</span><br>            hierarchical_groups, hierarchical_groups_gloo = create_hierarchical_groups(<br>                rank,<br>                ranks,<br>                [intra_partial_expert_data_parallel_size, num_distributed_optimizer_instances],<br>                create_gloo_process_groups=create_gloo_process_groups,<br>                pg_options=[<br>                    get_nccl_options(<span class="hljs-string">&quot;intra_ep_dp&quot;</span>, nccl_comm_cfgs),<br>                    get_nccl_options(<span class="hljs-string">&quot;inter_ep_dp&quot;</span>, nccl_comm_cfgs),<br>                ],<br>                timeout=timeout,<br>                group_desc=<span class="hljs-string">&quot;EXPERT_DATA_PARALLEL_GROUP&quot;</span>,<br>            )<br>            <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>                _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP = hierarchical_groups[<span class="hljs-number">0</span>]<br>                _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP_GLOO = hierarchical_groups_gloo[<span class="hljs-number">0</span>]<br>                _INTER_PARTIAL_EXPERT_DATA_PARALLEL_GROUP = hierarchical_groups[<span class="hljs-number">1</span>]<br><br>            <span class="hljs-keyword">if</span> sharp_enabled_group == <span class="hljs-string">&quot;dp_replica&quot;</span>:<br>                <span class="hljs-comment"># PyTorch is performing lazy initialization of the communicator group.</span><br>                <span class="hljs-comment"># Therefore, we need to perform a nccl call to ensure that the communicator group is created.</span><br>                <span class="hljs-keyword">if</span> _INTER_PARTIAL_EXPERT_DATA_PARALLEL_GROUP <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    torch.distributed.barrier(<br>                        group=_INTER_PARTIAL_EXPERT_DATA_PARALLEL_GROUP,<br>                        device_ids=[torch.cuda.current_device()],<br>                    )<br>                    torch.cuda.synchronize()<br>                <span class="hljs-comment"># Set NCCL_COLLNET_ENABLE to 0 to restrict SHARP application to the dp_replica group.</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span> <span class="hljs-keyword">in</span> os.environ:<br>                    <span class="hljs-keyword">del</span> os.environ[<span class="hljs-string">&quot;NCCL_COLLNET_ENABLE&quot;</span>]<br>        <span class="hljs-keyword">else</span>:<br>            _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP = _EXPERT_DATA_PARALLEL_GROUP<br>            _INTRA_PARTIAL_EXPERT_DATA_PARALLEL_GROUP_GLOO = _EXPERT_DATA_PARALLEL_GROUP_GLOO<br>    <span class="hljs-comment">### End of expert related parallel groups initialization</span><br><br>    <span class="hljs-comment"># build the intra distributed optimizer instance group</span><br>    <span class="hljs-keyword">global</span> _INTRA_DISTRIBUTED_OPTIMIZER_INSTANCE_GROUP<br>    <span class="hljs-keyword">assert</span> (<br>        _INTRA_DISTRIBUTED_OPTIMIZER_INSTANCE_GROUP <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;Intra distributed optimizer instance group is already initialized&quot;</span><br><br>    model_parallel_group_id = <span class="hljs-number">0</span><br>    intra_dist_opt_ranks = []<br>    <span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> expert_decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;tp-ep-pp&#x27;</span>):<br>        model_parallel_group_id += <span class="hljs-number">1</span><br>        intra_dist_opt_ranks.extend(ranks)<br>        <span class="hljs-keyword">if</span> model_parallel_group_id % intra_partial_expert_data_parallel_size == <span class="hljs-number">0</span>:<br>            intra_dist_opt_instance_group = create_group(<br>                intra_dist_opt_ranks,<br>                timeout=timeout,<br>                pg_options=get_nccl_options(<span class="hljs-string">&quot;intra_dist_opt_instance&quot;</span>, nccl_comm_cfgs),<br>                group_desc=<span class="hljs-string">&quot;INTRA_DISTRIBUTED_OPTIMIZER_INSTANCE_GROUP&quot;</span>,<br>            )<br>            <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> intra_dist_opt_ranks:<br>                _INTRA_DISTRIBUTED_OPTIMIZER_INSTANCE_GROUP = intra_dist_opt_instance_group<br>            intra_dist_opt_ranks = []<br><br>    <span class="hljs-comment"># Initialize global memory buffer</span><br>    <span class="hljs-comment"># This isn&#x27;t really &quot;parallel state&quot; but there isn&#x27;t another good place to</span><br>    <span class="hljs-comment"># put this. If we end up with a more generic initialization of megatron-core</span><br>    <span class="hljs-comment"># we could stick it there</span><br>    _set_global_memory_buffer()<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p><code>mpu.initialize_model_parallel</code>的核心目的是依据并行策略设置创建一堆并行通信组，实现各worker的rank与并行组的映射。这包括了TP、PP、DP、Context Parallel（CP）、Expert Parallel（EP）。</p>
</li>
<li><p>其首先构建了一个<code>RankGenerator</code>，这是rank与并行组匹配的核心</p>
<ul>
<li><code>RankGenerator</code>的相关代码如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RankGenerator</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;A class for generating rank groups for different modes of parallelism.&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, tp: <span class="hljs-built_in">int</span>, ep: <span class="hljs-built_in">int</span>, dp: <span class="hljs-built_in">int</span>, pp: <span class="hljs-built_in">int</span>, cp: <span class="hljs-built_in">int</span>, order: <span class="hljs-built_in">str</span>, rank_offset: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span></span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">assert</span> (<br>            ep == <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> cp == <span class="hljs-number">1</span><br>        ), <span class="hljs-string">&quot;Both EP and CP &gt; 1 in not allow in one rank generator. \</span><br><span class="hljs-string">            CP is only included in default RankGenerator, and EP only in expert RankGenerator.&quot;</span><br><br>        self.tp = tp<br>        self.ep = ep<br>        self.dp = dp<br>        self.pp = pp<br>        self.cp = cp<br>        self.rank_offset = rank_offset<br>        self.world_size = tp * dp * pp * cp * ep<br><br>        self.name_to_size = &#123;<br>            <span class="hljs-string">&quot;tp&quot;</span>: self.tp,<br>            <span class="hljs-string">&quot;pp&quot;</span>: self.pp,<br>            <span class="hljs-string">&quot;dp&quot;</span>: self.dp,<br>            <span class="hljs-string">&quot;ep&quot;</span>: self.ep,<br>            <span class="hljs-string">&quot;cp&quot;</span>: self.cp,<br>        &#125;<br>        self.order = order<br>        order = order.lower()<br><br>        <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> self.name_to_size.keys():<br>            <span class="hljs-keyword">if</span> name <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> order <span class="hljs-keyword">and</span> self.name_to_size[name] != <span class="hljs-number">1</span>:<br>                <span class="hljs-keyword">raise</span> RuntimeError(<br>                    <span class="hljs-string">f&quot;The size of (<span class="hljs-subst">&#123;name&#125;</span>) is (<span class="hljs-subst">&#123;self.name_to_size[name]&#125;</span>), but you haven&#x27;t&quot;</span><br>                    <span class="hljs-string">f&quot;specified the order (<span class="hljs-subst">&#123;self.order&#125;</span>).&quot;</span><br>                )<br>            <span class="hljs-keyword">elif</span> name <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> order:<br>                order = order + <span class="hljs-string">&quot;-&quot;</span> + name<br><br>        self.order = order<br>        self.ordered_size = []<br><br>        <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> order.split(<span class="hljs-string">&quot;-&quot;</span>):<br>            self.ordered_size.append(self.name_to_size[token])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_mask</span>(<span class="hljs-params">self, order: <span class="hljs-built_in">str</span>, token: <span class="hljs-built_in">str</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Create a mask for the specified tokens based on the given order.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            order (str): The order of parallelism types (e.g., &#x27;tp-dp-pp&#x27;).</span><br><span class="hljs-string">            token (str): The specific parallelism types to include in the mask,</span><br><span class="hljs-string">                         separated by hyphens (e.g., &#x27;tp-dp&#x27;).</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        ordered_token = order.split(<span class="hljs-string">&quot;-&quot;</span>)<br>        token_list = token.split(<span class="hljs-string">&quot;-&quot;</span>)<br>        mask = [<span class="hljs-literal">False</span>] * <span class="hljs-built_in">len</span>(ordered_token)<br>        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> token_list:<br>            mask[ordered_token.index(t)] = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">return</span> mask<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_ranks</span>(<span class="hljs-params">self, token</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Get rank group by input token.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            token (str):</span><br><span class="hljs-string">                Specify the ranks type that want to get. If we want</span><br><span class="hljs-string">                to obtain multiple parallel types, we can use a hyphen</span><br><span class="hljs-string">                &#x27;-&#x27; to separate them. For example, if we want to obtain</span><br><span class="hljs-string">                the TP_DP group, the token should be &#x27;tp-dp&#x27;.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        mask = self.get_mask(self.order, token)<br>        ranks = generate_masked_orthogonal_rank_groups(self.world_size, self.ordered_size, mask)<br>        <span class="hljs-keyword">if</span> self.rank_offset &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">for</span> rank_group <span class="hljs-keyword">in</span> ranks:<br>                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(rank_group)):<br>                    rank_group[i] += self.rank_offset<br>        <span class="hljs-keyword">return</span> ranks<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_masked_orthogonal_rank_groups</span>(<span class="hljs-params"></span><br><span class="hljs-params">    world_size: <span class="hljs-built_in">int</span>, parallel_size: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], mask: <span class="hljs-type">List</span>[<span class="hljs-built_in">bool</span>]</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>    <span class="hljs-string">r&quot;&quot;&quot;Generate orthogonal parallel groups based on the parallel size and mask.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">        world_size (int): world size</span><br><span class="hljs-string"></span><br><span class="hljs-string">        parallel_size (List[int]):</span><br><span class="hljs-string">            The parallel size of each orthogonal parallel type. For example, if</span><br><span class="hljs-string">            tensor_parallel_size = 2, pipeline_model_parallel_group = 3, data_parallel_size = 4,</span><br><span class="hljs-string">            and the parallel mapping order is tp-pp-dp, then the parallel_size = [2, 3, 4].</span><br><span class="hljs-string"></span><br><span class="hljs-string">        mask (List[bool]):</span><br><span class="hljs-string">            The mask controls which parallel methods the generated groups represent. If mask[i] is</span><br><span class="hljs-string">            True, it means the generated group contains the i-th parallelism method. For example,</span><br><span class="hljs-string">            if parallel_size = [tp_size, pp_size, dp_size], and mask = [True, False , True], then</span><br><span class="hljs-string">            the generated group is the `tp-dp` group, if the mask = [False, True, False], then the</span><br><span class="hljs-string">            generated group is the `pp` group.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Algorithm:</span><br><span class="hljs-string">        For orthogonal parallelism, such as tp/dp/pp/cp, the global_rank and</span><br><span class="hljs-string">        local_rank satisfy the following equation:</span><br><span class="hljs-string">            global_rank = tp_rank + dp_rank * tp_size + pp_rank * tp_size * dp_size (1)</span><br><span class="hljs-string">                tp_rank \in [0, tp_size)</span><br><span class="hljs-string">                dp_rank \in [0, dp_size)</span><br><span class="hljs-string">                pp_rank \in [0, pp_size)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        If we want to get the `dp_group` (tp_size * pp_size groups of dp_size ranks each.</span><br><span class="hljs-string">        For example,  if the gpu size is 8 and order is &#x27;tp-pp-dp&#x27;, size is &#x27;2-2-2&#x27;, and the</span><br><span class="hljs-string">        dp_group here is [[0, 4], [1, 5], [2, 6], [3, 7]].)</span><br><span class="hljs-string">        The tp_rank and pp_rank will be combined to form the `dp_group_index`.</span><br><span class="hljs-string">            dp_group_index = tp_rank + pp_rank * tp_size (2)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        So, Given that tp_rank and pp_rank satisfy equation (2), and dp_rank in</span><br><span class="hljs-string">        range(0, dp_size), the ranks in dp_group[dp_group_index] satisfies the</span><br><span class="hljs-string">        equation (1).</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This function solve this math problem.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    For example, if the parallel_size = [tp_size, dp_size, pp_size] = [2, 3, 4],</span><br><span class="hljs-string">    and the mask = [False, True, False]. Then,</span><br><span class="hljs-string">        dp_group_index(0) = tp_rank(0) + pp_rank(0) * 2</span><br><span class="hljs-string">        dp_group_index(1) = tp_rank(1) + pp_rank(0) * 2</span><br><span class="hljs-string">        ...</span><br><span class="hljs-string">        dp_group_index(7) = tp_rank(1) + pp_rank(3) * 2</span><br><span class="hljs-string"></span><br><span class="hljs-string">        dp_group[0] = 0 + range(0, 3) * 2 + 0 = [0, 2, 4]</span><br><span class="hljs-string">        dp_group[1] = 1 + range(0, 3) * 2 + 0 = [1, 3, 5]</span><br><span class="hljs-string">        ...</span><br><span class="hljs-string">        dp_group[7] = 1 + range(0, 3) * 2 + 3 * 2 * 3 = [19, 21, 23]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">prefix_product</span>(<span class="hljs-params">a: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], init=<span class="hljs-number">1</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        r = [init]<br>        <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> a:<br>            init = init * v<br>            r.append(init)<br>        <span class="hljs-keyword">return</span> r<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">inner_product</span>(<span class="hljs-params">a: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], b: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>([x * y <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(a, b)])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">decompose</span>(<span class="hljs-params">index, shape, stride=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        This function solve the math problem below:</span><br><span class="hljs-string">            There is an equation:</span><br><span class="hljs-string">                index = sum(idx[i] * stride[i])</span><br><span class="hljs-string">            And given the value of index, stride.</span><br><span class="hljs-string">            Return the idx.</span><br><span class="hljs-string">        This function will be used to get the pp/dp/pp_rank</span><br><span class="hljs-string">        from group_index and rank_in_group.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> stride <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            stride = prefix_product(shape)<br>        idx = [(index // d) % s <span class="hljs-keyword">for</span> s, d <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(shape, stride)]<br>        <span class="hljs-comment"># stride is a prefix_product result. And the value of stride[-1]</span><br>        <span class="hljs-comment"># is not used.</span><br>        <span class="hljs-keyword">assert</span> (<br>            <span class="hljs-built_in">sum</span>([x * y <span class="hljs-keyword">for</span> x, y <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(idx, stride[:-<span class="hljs-number">1</span>])]) == index<br>        ), <span class="hljs-string">&quot;idx &#123;&#125; with shape &#123;&#125; mismatch the return idx &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(index, shape, idx)<br>        <span class="hljs-keyword">return</span> idx<br><br>    masked_shape = [s <span class="hljs-keyword">for</span> s, m <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(parallel_size, mask) <span class="hljs-keyword">if</span> m]<br>    unmasked_shape = [s <span class="hljs-keyword">for</span> s, m <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(parallel_size, mask) <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> m]<br><br>    global_stride = prefix_product(parallel_size)<br>    masked_stride = [d <span class="hljs-keyword">for</span> d, m <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(global_stride, mask) <span class="hljs-keyword">if</span> m]<br>    unmasked_stride = [d <span class="hljs-keyword">for</span> d, m <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(global_stride, mask) <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> m]<br><br>    group_size = prefix_product(masked_shape)[-<span class="hljs-number">1</span>]<br>    num_of_group = world_size // group_size<br><br>    ranks = []<br>    <span class="hljs-keyword">for</span> group_index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_of_group):<br>        <span class="hljs-comment"># get indices from unmaksed for group_index.</span><br>        decomposed_group_idx = decompose(group_index, unmasked_shape)<br>        rank = []<br>        <span class="hljs-keyword">for</span> rank_in_group <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(group_size):<br>            <span class="hljs-comment"># get indices from masked for rank_in_group.</span><br>            decomposed_rank_idx = decompose(rank_in_group, masked_shape)<br>            rank.append(<br>                inner_product(decomposed_rank_idx, masked_stride)<br>                + inner_product(decomposed_group_idx, unmasked_stride)<br>            )<br>        ranks.append(rank)<br>    <span class="hljs-keyword">return</span> ranks<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p><code>RankGenerator</code>需要先获取到各个并行方法的并行度，此外还需要获得一个rank计数的顺序，这是一个字符串，如<code>&quot;tp-dp-pp&quot;</code>，说明先计数tp再是dp再是pp，<code>RankGenerator</code>在初始化时还会进行一定程度的补全与解析。</p>
</li>
<li><p><code>RankGenerator</code>有两个函数</p>
<ul>
<li><p>一个是<code>get_mask</code>,负责根据order和token返回mask。例如order是’tp-dp-pp’，token是’tp-dp’，那么就会返回[true, false, true]</p>
</li>
<li><p>一个是<code>get_ranks</code>,负责依据token返回对应的rank group。例如现在的order是’tp-pp-dp’，tp_size&#x3D;2,pp_size&#x3D;2,dp_size&#x3D;2，现在global rank的计算公式为tp_rank+pp_rank*tp_size+dp_rank*tp_size*pp_size，现在token是’dp’，说明想要知道tp所属rank、pp所属rank相同，但是所属dp不同的rank的集合，即tp_rank+pp_rank*tp_size+rang(dp_rank)*tp_size*pp_size，rang(dp_rank)&#x3D;{0,1}，也就是需要知道哪些rank需要进行dp间通信以共享对应相同模型参数的梯度计算结果等，在这个例子中我们得到的就是[[0,4],[1,5],[2,6],[3,7]]，如下图所示，同颜色的就是同一个dp_group内的rank。</p>
<p><img src="/2025/12/22/megatron-lm-pre-train-process/%E6%9C%AA%E5%91%BD%E5%90%8D%E6%96%87%E4%BB%B6.jpg" srcset="/img/loading.gif" lazyload></p>
</li>
<li><p>同理对于<code>get_ranks</code>，如果现在的order是’tp-dp-pp’，tp_size&#x3D;2,dp_size&#x3D;3,pp_size&#x3D;4，现在global rank的计算公式为tp_rank+dp_rank*tp_size+pp_rank*tp_size*dp_size，如果token依旧是’dp’，那么dp group的计算公式为tp_rank+rang(dp_rank)*tp_size+pp_rank*tp_size*dp_size，即[[0,2,4],[1,3,5]…[19,21,23]]</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>然后借助<code>RankGenerator</code>，我们就可以创建各个通信组，其创建流程基本与如下代码类似，即得到不同并行策略的groups，然后遍历这些group，对每个group创建<code>torch.distributed.new_group</code>，然后查看如果本进程的rank在这个group里，那么就设置其相关全局变量为这个group。注意这里每次都创建了new_group，但是本进程接下来可能并不会保存它，这么做是为了在分布式执行中让所有worker都执行同样的new_group，以保证分布式通信的正确，防止死锁等问题。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> rank_generator.get_ranks(<span class="hljs-string">&#x27;xxx&#x27;</span>):<br>    group = create_group(ranks, backend=..., pg_options=...)<br>    <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>        GLOBAL_GROUP = group<br>        GLOBAL_RANKS = ranks<br></code></pre></td></tr></table></figure>

<ul>
<li><p>非 Expert（Decoder &#x2F; Attention）并行组对应关系</p>
</li>
<li><p>Expert（MoE）相关并行组对应关系：</p>
</li>
<li><p>Distributed Optimizer 相关组对应关系</p>
</li>
<li></li>
</ul>
</li>
</ul>
<h2 id="setup-model-and-optimizer"><a href="#setup-model-and-optimizer" class="headerlink" title="setup_model_and_optimizer"></a>setup_model_and_optimizer</h2><p>setup_model_and_optimizer的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">setup_model_and_optimizer</span>(<span class="hljs-params"></span><br><span class="hljs-params">    model_provider_func,</span><br><span class="hljs-params">    model_type,</span><br><span class="hljs-params">    no_wd_decay_cond=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    scale_lr_cond=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    lr_mult=<span class="hljs-number">1.0</span>,</span><br><span class="hljs-params">    checkpointing_context=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Setup model and optimizer.&quot;&quot;&quot;</span><br>    args = get_args()<br>    timers = get_timers()<br>    one_logger = get_one_logger()<br><br>    model = get_model(model_provider_func, model_type)<br>    unwrapped_model = unwrap_model(model)<br><br>    one_logger <span class="hljs-keyword">and</span> one_logger.log_metrics(&#123;<span class="hljs-string">&quot;app_build_optimzer_start_time&quot;</span>: one_logger_utils.get_timestamp_in_ms()&#125;)<br>    kwargs = &#123;&#125;<br>    <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> dataclasses.fields(OptimizerConfig):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(args, f.name):<br>            kwargs[f.name] = <span class="hljs-built_in">getattr</span>(args, f.name)<br>    config = OptimizerConfig(**kwargs)<br>    config.timers = timers<br>    optimizer = get_megatron_optimizer(<br>        config,<br>        model,<br>        no_wd_decay_cond,<br>        scale_lr_cond,<br>        lr_mult,<br>        use_gloo_process_groups=args.enable_gloo_process_groups,<br>        <span class="hljs-comment"># If the user is asking for a non-zero embedding init std, skip weight decay for embeddings</span><br>        <span class="hljs-comment">#  to avoid embeddings from shrinking to zero as recommended in https://arxiv.org/abs/2312.16903</span><br>        default_skip_embedding_weight_decay=args.embedding_init_method_std <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>,<br>    )<br>    opt_param_scheduler = get_optimizer_param_scheduler(optimizer)<br>    one_logger <span class="hljs-keyword">and</span> one_logger.log_metrics(&#123;<span class="hljs-string">&quot;app_build_optimzer_finish_time&quot;</span>: one_logger_utils.get_timestamp_in_ms()&#125;)<br><br>    <span class="hljs-keyword">if</span> args.moe_use_upcycling:<br>        torch.distributed.barrier()<br>        <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> checkpoint_exists(args.save), (<br>            <span class="hljs-string">&quot;The upcycling destination directory already exists. &quot;</span><br>            <span class="hljs-string">&quot;Please check if --moe-use-upcycling is mistakenly enabled. &quot;</span><br>            <span class="hljs-string">&quot;Upcycling should only be set for the first run when converting the dense model. &quot;</span><br>            <span class="hljs-string">&quot;All subsequent runs should remove this flag. &quot;</span><br>        )<br>        <span class="hljs-comment"># before changing moe related global args, save them in local variables</span><br>        num_experts = args.num_experts<br>        expert_model_parallel_size = args.expert_model_parallel_size<br>        moe_ffn_hidden_size = args.ffn_hidden_size<br><br>        <span class="hljs-comment"># set dense model related args in to global args before getting dense model</span><br>        args.num_experts = <span class="hljs-literal">None</span><br>        args.expert_model_parallel_size = <span class="hljs-number">1</span><br>        args.ffn_hidden_size = moe_ffn_hidden_size * args.moe_upcycling_granularity <br><br>        <span class="hljs-comment"># get dense model</span><br>        dense_model_for_upcycling = get_model(model_provider_func, model_type)<br><br>        <span class="hljs-comment"># recover moe upcycling related args in global args before executing upcycling</span><br>        args.num_experts = num_experts<br>        args.expert_model_parallel_size = expert_model_parallel_size<br>        args.ffn_hidden_size = moe_ffn_hidden_size<br><br>        <span class="hljs-comment"># execute upcycling</span><br>        _, args.num_floating_point_operations_so_far = upcycling_utils.load_and_upcycle_model(<br>            load_checkpoint,<br>            unwrapped_model,<br>            dense_model_for_upcycling,<br>            load_kwargs=&#123;<br>                <span class="hljs-string">&#x27;model&#x27;</span>: dense_model_for_upcycling,<br>                <span class="hljs-string">&#x27;optimizer&#x27;</span>: <span class="hljs-literal">None</span>,<br>                <span class="hljs-string">&#x27;opt_param_scheduler&#x27;</span>: <span class="hljs-literal">None</span>,<br>            &#125;,<br>        )<br>        args.iteration = <span class="hljs-number">1</span><br>        save_checkpoint(<br>            args.iteration, model, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, args.num_floating_point_operations_so_far<br>        )<br>        torch.distributed.barrier()<br>        <span class="hljs-keyword">del</span> dense_model_for_upcycling<br>        <span class="hljs-keyword">if</span> (args.fp16 <span class="hljs-keyword">or</span> args.bf16) <span class="hljs-keyword">and</span> optimizer <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            optimizer.reload_model_params()<br>        print_rank_0(<span class="hljs-string">f&#x27;Upcycled checkpoint saved to <span class="hljs-subst">&#123;args.save&#125;</span>&#x27;</span>)<br><br>    <span class="hljs-keyword">if</span> (<br>        args.load <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> args.pretrained_checkpoint <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>    ) <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> args.moe_use_upcycling:<br>        one_logger <span class="hljs-keyword">and</span> one_logger.log_metrics(<br>            &#123;<span class="hljs-string">&#x27;load_checkpoint_start_time&#x27;</span>: one_logger_utils.get_timestamp_in_ms()&#125;<br>        )<br>        timers(<span class="hljs-string">&#x27;load-checkpoint&#x27;</span>, log_level=<span class="hljs-number">0</span>).start(barrier=<span class="hljs-literal">True</span>)<br><br>        args.iteration, args.num_floating_point_operations_so_far = load_checkpoint(<br>            model,<br>            optimizer,<br>            opt_param_scheduler,<br>            checkpointing_context=checkpointing_context,<br>            skip_load_to_model_and_opt=HAVE_FSDP2<br>            <span class="hljs-keyword">and</span> <span class="hljs-built_in">getattr</span>(args, <span class="hljs-string">&quot;use_torch_fsdp2&quot;</span>, <span class="hljs-literal">False</span>)<br>            <span class="hljs-keyword">and</span> args.ckpt_format == <span class="hljs-string">&quot;torch_dist&quot;</span>,<br>        )<br>        timers(<span class="hljs-string">&#x27;load-checkpoint&#x27;</span>).stop(barrier=<span class="hljs-literal">True</span>)<br>        timers.log([<span class="hljs-string">&#x27;load-checkpoint&#x27;</span>])<br>        one_logger <span class="hljs-keyword">and</span> one_logger.log_metrics(<br>            &#123;<br>                <span class="hljs-string">&#x27;load_checkpoint_finish_time&#x27;</span>: one_logger_utils.get_timestamp_in_ms(),<br>                <span class="hljs-string">&#x27;load_checkpoint_time&#x27;</span>: timers(<span class="hljs-string">&#x27;load-checkpoint&#x27;</span>).active_time(),<br>            &#125;<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        args.iteration = <span class="hljs-number">0</span><br>        args.num_floating_point_operations_so_far = <span class="hljs-number">0</span><br><br>    <span class="hljs-comment"># get model without FP16 and/or DDP wrappers</span><br>    <span class="hljs-keyword">if</span> (<br>        args.iteration == <span class="hljs-number">0</span><br>        <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(unwrapped_model) == <span class="hljs-number">1</span><br>        <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(unwrapped_model[<span class="hljs-number">0</span>], <span class="hljs-string">&#x27;init_state_dict_from_bert&#x27;</span>)<br>    ):<br>        print_rank_0(<span class="hljs-string">&quot;Initializing ICT from pretrained BERT model&quot;</span>)<br>        unwrapped_model[<span class="hljs-number">0</span>].init_state_dict_from_bert()<br>        <span class="hljs-keyword">if</span> args.fp16:<br>            optimizer.reload_model_params()<br><br>    <span class="hljs-comment"># Convert checkpoint format.</span><br>    <span class="hljs-keyword">if</span> args.ckpt_convert_format <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        load_ckpt_format = args.ckpt_format<br>        args.ckpt_format = args.ckpt_convert_format<br>        args.save = os.path.join(args.ckpt_convert_save, args.ckpt_convert_format)<br>        update_use_dist_ckpt(args)<br><br>        save_checkpoint(<br>            args.iteration,<br>            model,<br>            optimizer,<br>            opt_param_scheduler,<br>            args.num_floating_point_operations_so_far,<br>            preprocess_common_state_dict_fn=preprocess_common_state_dict,<br>        )<br><br>        print_rank_0(<span class="hljs-string">&quot;&gt; converted checkpoint: %s -&gt; %s.&quot;</span> % (load_ckpt_format, args.ckpt_format))<br>        torch.distributed.barrier()<br>        exit()<br><br>    <span class="hljs-keyword">return</span> model, optimizer, opt_param_scheduler<br><br></code></pre></td></tr></table></figure>

<p>其整体流程如下：</p>
<ol>
<li><p>通过get_model获取本worker上的模型</p>
</li>
<li><p>通过<code>unwrap_model</code>来获取DDP包装下的原始模型</p>
</li>
<li><p>通过<code>get_megatron_optimizer</code>获取optimizer</p>
</li>
<li><p>通过<code>get_optimizer_param_scheduler</code>获取optimizer学习率参数调度器</p>
</li>
<li><p>如有配置args.moe_use_upcycling，执行MoE upcycling，把 Dense FFN 模型转成 MoE 模型，然后从检查点中获取模型并保存，还会调整优化器</p>
</li>
<li><p>如果没有配置args.moe_use_upcycling并且配置了检查点，那么就从检查点中加载模型、优化器等</p>
</li>
<li><p>如果iteration &#x3D; 0 时的特殊初始化（BERT），执行optimizer.reload_model_params()</p>
</li>
<li><p>如果配置了args.ckpt_convert_format，就加载旧格式的模型检查点，然后保存为新格式的模型检查点</p>
</li>
<li><p>最终return model, optimizer, opt_param_scheduler</p>
</li>
</ol>
<h3 id="get-model"><a href="#get-model" class="headerlink" title="get_model"></a>get_model</h3><p>get_model代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model</span>(<span class="hljs-params">model_provider_func, model_type=ModelType.encoder_or_decoder, wrap_with_ddp=<span class="hljs-literal">True</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build the model.&quot;&quot;&quot;</span><br>    args = get_args()<br>    args.model_type = model_type<br><br>    <span class="hljs-comment"># Build model.</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build_model</span>():<br>        <span class="hljs-keyword">if</span> (<br>            mpu.get_pipeline_model_parallel_world_size() &gt; <span class="hljs-number">1</span><br>            <span class="hljs-keyword">and</span> args.virtual_pipeline_model_parallel_size <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        ):<br>            model = []<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(args.virtual_pipeline_model_parallel_size):<br>                <span class="hljs-comment"># Set pre_process and post_process only after virtual rank is set.</span><br>                pre_process = mpu.is_pipeline_first_stage(ignore_virtual=<span class="hljs-literal">False</span>, vp_stage=i)<br>                post_process = mpu.is_pipeline_last_stage(ignore_virtual=<span class="hljs-literal">False</span>, vp_stage=i)<br>                this_model = model_provider_func(<br>                    pre_process=pre_process, post_process=post_process, vp_stage=i)<br>                this_model.model_type = model_type<br>                this_model.vp_stage = i<br>                model.append(this_model)<br>        <span class="hljs-keyword">else</span>:<br>            pre_process = mpu.is_pipeline_first_stage()<br>            post_process = mpu.is_pipeline_last_stage()<br>            model = model_provider_func(pre_process=pre_process, post_process=post_process)<br>            model.model_type = model_type<br>        <span class="hljs-keyword">return</span> model<br><br>    <span class="hljs-keyword">if</span> args.init_model_with_meta_device:<br>        <span class="hljs-keyword">with</span> torch.device(<span class="hljs-string">&#x27;meta&#x27;</span>):<br>            model = build_model()<br>    <span class="hljs-keyword">else</span>:<br>        model = build_model()<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(model, <span class="hljs-built_in">list</span>):<br>        model = [model]<br><br>    <span class="hljs-comment"># Set tensor model parallel attributes if not set.</span><br>    <span class="hljs-comment"># Only parameters that are already tensor model parallel have these</span><br>    <span class="hljs-comment"># attributes set for them. We should make sure the default attributes</span><br>    <span class="hljs-comment"># are set for all params so the optimizer can use them.</span><br>    <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model:<br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model_module.parameters():<br>            tensor_parallel.set_defaults_if_not_set_tensor_model_parallel_attributes(param)<br><br>    <span class="hljs-comment"># Print number of parameters.</span><br>    num_parameters = <span class="hljs-built_in">sum</span>(<br>        [<span class="hljs-built_in">sum</span>([p.nelement() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model_module.parameters()]) <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model]<br>    )<br>    <span class="hljs-keyword">if</span> mpu.get_data_parallel_rank() == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> mpu.get_context_parallel_rank() == <span class="hljs-number">0</span>:<br>        <span class="hljs-built_in">print</span>(<br>            <span class="hljs-string">&#x27; &gt; number of parameters on (tensor, pipeline) &#x27;</span><br>            <span class="hljs-string">&#x27;model parallel rank (&#123;&#125;, &#123;&#125;): &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                mpu.get_tensor_model_parallel_rank(),<br>                mpu.get_pipeline_model_parallel_rank(),<br>                num_parameters,<br>            ),<br>            flush=<span class="hljs-literal">True</span>,<br>        )<br><br>    <span class="hljs-comment"># GPU allocation.</span><br>    <span class="hljs-comment"># For FSDP2, we don&#x27;t allocate GPU memory here. We allocate GPU memory</span><br>    <span class="hljs-comment"># in the fully_shard function of FSDP2 instead.</span><br>    <span class="hljs-keyword">if</span> (<br>        <span class="hljs-keyword">not</span> (args.use_torch_fsdp2 <span class="hljs-keyword">and</span> args.use_cpu_initialization)<br>        <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> args.init_model_with_meta_device<br>    ):<br>        <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model:<br>            model_module.cuda(torch.cuda.current_device())<br><br>    <span class="hljs-comment"># Fp16 conversion.</span><br>    <span class="hljs-keyword">if</span> args.fp16 <span class="hljs-keyword">or</span> args.bf16:<br>        config = get_model_config(model[<span class="hljs-number">0</span>])<br>        model = [Float16Module(config, model_module) <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model]<br><br>    <span class="hljs-comment"># Before TE2.x: The model_module.bfloat16()/model_module.half() above will call the inplace</span><br>    <span class="hljs-comment">#               copy of TE&#x27;s Float8Tensor, which will write an unwanted value (amax calculated</span><br>    <span class="hljs-comment">#               from the current fp8 param) to its amax_history. The below function will correct</span><br>    <span class="hljs-comment">#               the amax_history back.</span><br>    <span class="hljs-comment"># After TE2.x: Below function is an empty function and does nothing.</span><br>    correct_amax_history_if_needed(model)<br><br>    <span class="hljs-keyword">if</span> wrap_with_ddp:<br>        <span class="hljs-keyword">if</span> args.use_torch_fsdp2:<br>            <span class="hljs-keyword">assert</span> HAVE_FSDP2, <span class="hljs-string">&quot;Torch FSDP2 requires torch&gt;=2.4.0&quot;</span><br>            DP = torch_FSDP<br>        <span class="hljs-keyword">elif</span> args.use_megatron_fsdp:<br>            DP = megatron_FSDP<br>        <span class="hljs-keyword">else</span>:<br>            DP = DDP<br><br>        config = get_model_config(model[<span class="hljs-number">0</span>])<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">getattr</span>(args, <span class="hljs-string">&quot;use_torch_fsdp2&quot;</span>, <span class="hljs-literal">False</span>):<br>            reshard_after_forward = <span class="hljs-built_in">getattr</span>(args, <span class="hljs-string">&quot;torch_fsdp2_reshard_after_forward&quot;</span>, <span class="hljs-literal">True</span>)<br>            ddp_config = TorchFullyShardedDataParallelConfig(reshard_after_forward=reshard_after_forward)<br>        <span class="hljs-keyword">else</span>:<br>            kwargs = &#123;&#125;<br>            <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> dataclasses.fields(DistributedDataParallelConfig):<br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(args, f.name):<br>                    kwargs[f.name] = <span class="hljs-built_in">getattr</span>(args, f.name)<br>            kwargs[<span class="hljs-string">&#x27;grad_reduce_in_fp32&#x27;</span>] = args.accumulate_allreduce_grads_in_fp32<br>            kwargs[<span class="hljs-string">&#x27;check_for_nan_in_grad&#x27;</span>] = args.check_for_nan_in_loss_and_grad<br>            kwargs[<span class="hljs-string">&#x27;check_for_large_grads&#x27;</span>] = args.check_for_large_grads<br>            <span class="hljs-keyword">if</span> args.ddp_num_buckets <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">assert</span> args.ddp_bucket_size <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, \<br>                    <span class="hljs-string">&quot;Cannot specify both --ddp-num-buckets and --ddp-bucket-size&quot;</span><br>                <span class="hljs-keyword">assert</span> args.ddp_num_buckets &gt; <span class="hljs-number">0</span>, \<br>                    <span class="hljs-string">&quot;--ddp-num-buckets must be greater than 0&quot;</span><br>                kwargs[<span class="hljs-string">&#x27;bucket_size&#x27;</span>] = num_parameters // args.ddp_num_buckets<br>            <span class="hljs-keyword">else</span>:<br>                kwargs[<span class="hljs-string">&#x27;bucket_size&#x27;</span>] = args.ddp_bucket_size<br>            kwargs[<span class="hljs-string">&#x27;pad_buckets_for_high_nccl_busbw&#x27;</span>] = args.ddp_pad_buckets_for_high_nccl_busbw<br>            kwargs[<span class="hljs-string">&#x27;average_in_collective&#x27;</span>] = args.ddp_average_in_collective<br>            <span class="hljs-keyword">if</span> args.use_megatron_fsdp <span class="hljs-keyword">and</span> args.use_precision_aware_optimizer:<br>                kwargs[<span class="hljs-string">&quot;preserve_fp32_weights&quot;</span>] = <span class="hljs-literal">False</span><br>            ddp_config = DistributedDataParallelConfig(**kwargs)<br><br>            <span class="hljs-comment"># In the Megatron FSDP and DDP use path, we need to initialize the bucket size.</span><br>            <span class="hljs-comment"># If bucket_size is not provided as an input, use sane default.</span><br>            <span class="hljs-comment"># If using very large dp_sizes, make buckets larger to ensure that chunks used in NCCL</span><br>            <span class="hljs-comment"># ring-reduce implementations are large enough to remain bandwidth-bound rather than</span><br>            <span class="hljs-comment"># latency-bound.</span><br>            <span class="hljs-keyword">if</span> ddp_config.bucket_size <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                ddp_config.bucket_size = <span class="hljs-built_in">max</span>(<br>                    <span class="hljs-number">40000000</span>, <span class="hljs-number">1000000</span> * mpu.get_data_parallel_world_size(with_context_parallel=<span class="hljs-literal">True</span>)<br>                )<br>            <span class="hljs-comment"># Set bucket_size to infinity if overlap_grad_reduce is False.</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> ddp_config.overlap_grad_reduce:<br>                ddp_config.bucket_size = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">with</span> torch.cuda.stream(torch.cuda.Stream()):<br>            model = [<br>                DP(<br>                    config=config,<br>                    ddp_config=ddp_config,<br>                    module=model_chunk,<br>                    <span class="hljs-comment"># Turn off bucketing for model_chunk 2 onwards, since communication for these</span><br>                    <span class="hljs-comment"># model chunks is overlapped with compute anyway.</span><br>                    disable_bucketing=(model_chunk_idx &gt; <span class="hljs-number">0</span>)<br>                    <span class="hljs-keyword">or</span> args.overlap_param_gather_with_optimizer_step,<br>                )<br>                <span class="hljs-keyword">for</span> (model_chunk_idx, model_chunk) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(model)<br>            ]<br><br>        <span class="hljs-comment"># Broadcast params from data parallel src rank to other data parallel ranks.</span><br>        <span class="hljs-keyword">if</span> args.data_parallel_random_init:<br>            <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model:<br>                model_module.broadcast_params()<br><br>    <span class="hljs-keyword">return</span> model<br><br></code></pre></td></tr></table></figure>

<p>其整体流程如下</p>
<ol>
<li><p>构建了一个<code>build_model()</code>函数，并使用其获得各个worker上应有的模型。其主要是负责根据当前是否是第一阶段、是否是最后一阶段以及当前的pp维度、vp维度构建在当前worker上的模型。例如当前layer数量是8，pp_size是4，vp_size是2，则4个worker获得的模型分别是[embedding+layer_0, layer_4], [layer_1, layer_5], [layer_2,layer_6], [layer_3,layer_7+lm head+loss]。</p>
</li>
<li><p>给各个model的parameters设置tp的默认参数：</p>
</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">_MODEL_PARALLEL_ATTRIBUTE_DEFAULTS = &#123;<br>    <span class="hljs-string">&quot;tensor_model_parallel&quot;</span>: False,<br>    <span class="hljs-string">&quot;partition_dim&quot;</span>: -1,<br>    <span class="hljs-string">&quot;partition_stride&quot;</span>: 1,<br>&#125;<br></code></pre></td></tr></table></figure>

<ul>
<li><p>如果没有使用fsdp2并且不使用cpu初始化并且没有<code>init_model_with_meta_device</code>，那么就将模型搬运到GPU显存上</p>
</li>
<li><p>如果配置了fp16、bf16，那么就对model进行低精度转换</p>
</li>
<li><p>然后如果启用了DDP就进行DDP包装：</p>
<ol>
<li><p>首先定义DP，这里支持3类DDP包装，分别是torch_FSDP、megatron_FSDP和DDP</p>
</li>
<li><p>然后构造ddp_config：</p>
<ol>
<li><p>如果使用的是fsdp2，就直接TorchFullyShardedDataParallelConfig作为ddp_config</p>
</li>
<li><p>不然就自行填写参数构造DistributedDataParallelConfig作为ddp_config</p>
</li>
</ol>
</li>
<li><p>然后对于当前rank的各个model，使用DP对其进行包装</p>
</li>
<li><p>如果使用了data_parallel_random_init，还需要在ddp内进行broadcast_params以统一参数。</p>
</li>
</ol>
</li>
</ul>
<h3 id="get-megatron-optimizer"><a href="#get-megatron-optimizer" class="headerlink" title="get_megatron_optimizer"></a>get_megatron_optimizer</h3><p><code>get_megatron_optimizer</code>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_megatron_optimizer</span>(<span class="hljs-params"></span><br><span class="hljs-params">    config: OptimizerConfig,</span><br><span class="hljs-params">    model_chunks: <span class="hljs-type">List</span>[MegatronModule],</span><br><span class="hljs-params">    no_weight_decay_cond: <span class="hljs-type">Optional</span>[<span class="hljs-type">Callable</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    scale_lr_cond: <span class="hljs-type">Optional</span>[<span class="hljs-type">Callable</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    lr_mult: <span class="hljs-built_in">float</span> = <span class="hljs-number">1.0</span>,</span><br><span class="hljs-params">    use_gloo_process_groups: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params">    default_skip_embedding_weight_decay: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    grad_comm_pgs: <span class="hljs-type">Optional</span>[GradCommProcessGroups] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    model_comm_pgs: <span class="hljs-type">Optional</span>[ModelCommProcessGroups] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>) -&gt; MegatronOptimizer:<br>    <span class="hljs-string">&quot;&quot;&quot;Retrieve the Megatron optimizer for model chunks.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    We use separate optimizers for expert parameters and non-expert parameters.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        config (OptimizerConfig): optimizer configuration object.</span><br><span class="hljs-string">        model_chunks (List[MegatronModule]): model chunks to get optimizer for.</span><br><span class="hljs-string">        no_weight_decay_cond (func, optional): function to determine whether a parameter</span><br><span class="hljs-string">            should not perform weight decay. Defaults to None.</span><br><span class="hljs-string">        scale_lr_cond (func, optional): function to determine whether a parameter</span><br><span class="hljs-string">            should have a scaled learning rate. Defaults to None.</span><br><span class="hljs-string">        lr_mult (float, optional): learning rate multiplier for parameters that</span><br><span class="hljs-string">            satisfy scale_lr_cond. Defaults to 1.0.</span><br><span class="hljs-string">        use_gloo_process_groups (bool): if false, disable use of Gloo process groups</span><br><span class="hljs-string">            in underlying Megatron optimizers.</span><br><span class="hljs-string">        default_skip_embedding_weight_decay (bool): whether to skip weight decay for</span><br><span class="hljs-string">            embedding parameters by default, if no_weight_decay_cond is not provided.</span><br><span class="hljs-string">            This is useful if you do not want embeddings to shrink to zero in training</span><br><span class="hljs-string">            as recommended in https://arxiv.org/abs/2312.16903</span><br><span class="hljs-string">        grad_comm_pgs (Optional[GradCommProcessGroups]): gradient communication process groups.</span><br><span class="hljs-string">            If None, uses default parallel_state groups.</span><br><span class="hljs-string">        model_comm_pgs (Optional[ModelCommProcessGroups]): model communication process groups.</span><br><span class="hljs-string">            If None, uses default parallel_state groups.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        Instance of MegatronOptimizer.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    log_single_rank(logger, logging.INFO, <span class="hljs-string">f&#x27;Setting up optimizer with config <span class="hljs-subst">&#123;config&#125;</span>&#x27;</span>)<br><br>    <span class="hljs-comment"># Separate out first model chunk if overlapping param AG with optimizer step.</span><br>    <span class="hljs-keyword">if</span> config.overlap_param_gather_with_optimizer_step:<br>        all_dense_model_chunks = [[model_chunks[<span class="hljs-number">0</span>]], model_chunks[<span class="hljs-number">1</span>:]]<br>        overlap_param_gather_with_optimizer_step_flags = [<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>]<br>    <span class="hljs-keyword">else</span>:<br>        all_dense_model_chunks = [model_chunks]<br>        overlap_param_gather_with_optimizer_step_flags = [<span class="hljs-literal">False</span>]<br><br>    <span class="hljs-keyword">if</span> grad_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> model_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># Gradient communication groups</span><br>        dp_cp_group = parallel_state.get_data_parallel_group(<br>            with_context_parallel=<span class="hljs-literal">True</span>, partial_data_parallel=<span class="hljs-literal">False</span><br>        )<br>        intra_dp_cp_group = parallel_state.get_data_parallel_group(<br>            with_context_parallel=<span class="hljs-literal">True</span>, partial_data_parallel=<span class="hljs-literal">True</span><br>        )<br><br>        intra_expt_dp_group = parallel_state.get_expert_data_parallel_group(<br>            partial_expert_data_parallel=<span class="hljs-literal">True</span><br>        )<br><br>        <span class="hljs-comment"># Gloo groups</span><br>        <span class="hljs-keyword">if</span> use_gloo_process_groups:<br>            intra_dp_cp_group_gloo = parallel_state.get_data_parallel_group_gloo(<br>                with_context_parallel=<span class="hljs-literal">True</span>, partial_data_parallel=<span class="hljs-literal">True</span><br>            )<br>            intra_expt_dp_group_gloo = parallel_state.get_expert_data_parallel_group_gloo(<br>                partial_expert_data_parallel=<span class="hljs-literal">True</span><br>            )<br>        <span class="hljs-keyword">else</span>:<br>            intra_dp_cp_group_gloo = <span class="hljs-literal">None</span><br>            intra_expt_dp_group_gloo = <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># Model communication groups</span><br>        mp_group = parallel_state.get_model_parallel_group()<br>        expt_tp_pp_group = parallel_state.get_expert_tensor_model_pipeline_parallel_group()<br>    <span class="hljs-keyword">elif</span> grad_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> model_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># 1. dp group - this is always required</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(grad_comm_pgs, <span class="hljs-string">&#x27;dp&#x27;</span>):<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;dp process group is required but not provided in grad_comm_pgs&quot;</span>)<br>        dp_group = grad_comm_pgs.dp<br><br>        <span class="hljs-comment"># 2. dp_cp group:</span><br>        <span class="hljs-comment"># - If provided in grad_comm_pgs, use it</span><br>        <span class="hljs-comment"># - Otherwise check context_parallel_size</span><br>        <span class="hljs-comment">#   - If cp_size is 1, use same as dp</span><br>        <span class="hljs-comment">#   - If cp_size &gt; 1, raise error as dp_cp is needed</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(grad_comm_pgs, <span class="hljs-string">&#x27;dp_cp&#x27;</span>):<br>            dp_cp_group = grad_comm_pgs.dp_cp<br>        <span class="hljs-keyword">else</span>:<br>            model_config = get_model_config(model_chunks[<span class="hljs-number">0</span>])<br>            cp_size = <span class="hljs-built_in">getattr</span>(model_config, <span class="hljs-string">&#x27;context_parallel_size&#x27;</span>, <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> cp_size == <span class="hljs-number">1</span>:<br>                <span class="hljs-comment"># If no context parallelism, dp_cp is same as dp</span><br>                dp_cp_group = dp_group<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">raise</span> ValueError(<br>                    <span class="hljs-string">&quot;dp_cp process group is required when context_parallel_size &gt; 1 &quot;</span><br>                    <span class="hljs-string">&quot;but not provided in grad_comm_pgs&quot;</span><br>                )<br><br>        <span class="hljs-comment"># 3. Handle expert data parallel group</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_comm_pgs, <span class="hljs-string">&#x27;expt_dp&#x27;</span>), (<br>            <span class="hljs-string">&quot;expt_dp process group is required but not provided in grad_comm_pgs&quot;</span>,<br>            <span class="hljs-string">&quot;please explicitly set it to None if you don&#x27;t need it&quot;</span>,<br>        )<br>        expt_dp_group = grad_comm_pgs.expt_dp<br><br>        <span class="hljs-comment"># 4. Handle intra_dp_cp, intra_expt_dp, and inter_dist_opt</span><br>        <span class="hljs-comment">#    based on optimizer instances:</span><br>        <span class="hljs-comment"># Get ddp_config from model chunks to determine optimizer instances</span><br>        ddp_config = model_chunks[<span class="hljs-number">0</span>].ddp_config<br>        <span class="hljs-keyword">if</span> ddp_config.num_distributed_optimizer_instances == <span class="hljs-number">1</span>:<br>            <span class="hljs-comment"># With a single optimizer instance:</span><br>            <span class="hljs-comment"># - intra_dp_cp is same as dp_cp</span><br>            <span class="hljs-comment"># - intra_expt_dp is same as expt_dp</span><br>            <span class="hljs-comment"># - inter_dist_opt is not needed (set to None)</span><br>            intra_dp_cp_group = dp_cp_group<br>            intra_expt_dp_group = expt_dp_group<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># With multiple optimizer instances, both groups must be provided</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> (<br>                <span class="hljs-built_in">hasattr</span>(grad_comm_pgs, <span class="hljs-string">&#x27;intra_dp_cp&#x27;</span>)<br>                <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(grad_comm_pgs, <span class="hljs-string">&#x27;intra_expt_dp&#x27;</span>)<br>                <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(grad_comm_pgs, <span class="hljs-string">&#x27;inter_dist_opt&#x27;</span>)<br>            ):<br>                <span class="hljs-keyword">raise</span> ValueError(<br>                    <span class="hljs-string">&quot;intra_dp_cp, intra_expt_dp, and inter_dist_opt &quot;</span><br>                    <span class="hljs-string">&quot;process groups are required when using multiple optimizer &quot;</span><br>                    <span class="hljs-string">&quot;instances (&gt;1) but not provided in grad_comm_pgs&quot;</span><br>                )<br>            intra_dp_cp_group = grad_comm_pgs.intra_dp_cp<br>            intra_expt_dp_group = grad_comm_pgs.intra_expt_dp<br><br>        <span class="hljs-comment"># 5. Model communication groups</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(model_comm_pgs, <span class="hljs-string">&#x27;mp&#x27;</span>), (<br>            <span class="hljs-string">&quot;mp process group is required but not provided in model_comm_pgs&quot;</span>,<br>            <span class="hljs-string">&quot;please explicitly set it to None if you don&#x27;t need it&quot;</span>,<br>        )<br>        mp_group = model_comm_pgs.mp<br><br>        <span class="hljs-comment"># Expert tensor-model-pipeline group for MoE</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(model_comm_pgs, <span class="hljs-string">&#x27;tp_ep_pp&#x27;</span>), (<br>            <span class="hljs-string">&quot;tp_ep_pp process group is required but not provided in model_comm_pgs&quot;</span>,<br>            <span class="hljs-string">&quot;please explicitly set it to None if you don&#x27;t need it&quot;</span>,<br>        )<br>        expt_tp_pp_group = model_comm_pgs.tp_ep_pp<br><br>        <span class="hljs-comment"># Set up gloo groups - these might not be provided in process groups config</span><br>        <span class="hljs-comment"># so we need to create them or set to None</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> use_gloo_process_groups, (<br>            <span class="hljs-string">&quot;Gloo process groups are not supported when grad_comm_pgs and model_comm_pgs are &quot;</span><br>            <span class="hljs-string">&quot;provided. Please set use_gloo_process_groups to False.&quot;</span><br>        )<br>        intra_dp_cp_group_gloo = <span class="hljs-literal">None</span><br>        intra_expt_dp_group_gloo = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Grad and model comm process groups must be provided or both must be None&quot;</span>)<br><br>    model_parallel_rank = get_pg_rank(mp_group)<br><br>    <span class="hljs-keyword">if</span> get_pg_size(dp_cp_group) &gt; get_pg_size(intra_dp_cp_group):<br>        <span class="hljs-keyword">if</span> grad_comm_pgs <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            inter_dist_opt_group = grad_comm_pgs.inter_dist_opt<br>        <span class="hljs-keyword">else</span>:<br>            inter_dist_opt_group = parallel_state.get_inter_distributed_optimizer_instance_group()<br>        distributed_optimizer_instance_id = get_pg_rank(inter_dist_opt_group)<br>    <span class="hljs-keyword">else</span>:<br>        distributed_optimizer_instance_id = <span class="hljs-number">0</span><br><br>    optimizers = []<br>    model_chunk_offset = <span class="hljs-number">0</span><br>    ddp_config = model_chunks[<span class="hljs-number">0</span>].ddp_config  <span class="hljs-comment"># Use the first model chunk&#x27;s DDP config</span><br>    <span class="hljs-keyword">if</span> ddp_config.use_megatron_fsdp:<br>        <span class="hljs-keyword">for</span> model_chunk, overlap_param_gather_with_optimizer_step <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<br>            all_dense_model_chunks, overlap_param_gather_with_optimizer_step_flags<br>        ):<br>            param_groups, buffers = _get_param_groups_and_buffers(<br>                model_chunk,<br>                model_chunk_offset=model_chunk_offset,<br>                config=config,<br>                no_weight_decay_cond=no_weight_decay_cond,<br>                scale_lr_cond=scale_lr_cond,<br>                lr_mult=lr_mult,<br>                filter_fn=<span class="hljs-keyword">lambda</span> g: <span class="hljs-literal">True</span>,<br>                buffer_name=<span class="hljs-string">&#x27;buffers&#x27;</span>,<br>                default_skip_embedding_weight_decay=default_skip_embedding_weight_decay,<br>            )<br><br>            optimizers.append(<br>                _get_megatron_optimizer_based_on_param_groups(<br>                    config,<br>                    model_chunks=model_chunk,<br>                    param_groups=param_groups,<br>                    per_model_buffers=buffers,<br>                    model_parallel_group=mp_group,<br>                    data_parallel_group=dp_cp_group,<br>                    data_parallel_group_gloo=intra_dp_cp_group_gloo,<br>                    data_parallel_group_idx=model_parallel_rank,<br>                    distributed_optimizer_instance_id=distributed_optimizer_instance_id,<br>                )<br>            )<br>            model_chunk_offset += <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(optimizers) == <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> optimizers[<span class="hljs-number">0</span>]<br><br>        <span class="hljs-keyword">return</span> ChainedOptimizer(optimizers)<br><br>    <span class="hljs-keyword">for</span> dense_model_chunks, overlap_param_gather_with_optimizer_step <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<br>        all_dense_model_chunks, overlap_param_gather_with_optimizer_step_flags<br>    ):<br>        param_groups, buffers = _get_param_groups_and_buffers(<br>            dense_model_chunks,<br>            model_chunk_offset=model_chunk_offset,<br>            config=config,<br>            no_weight_decay_cond=no_weight_decay_cond,<br>            scale_lr_cond=scale_lr_cond,<br>            lr_mult=lr_mult,<br>            filter_fn=<span class="hljs-keyword">lambda</span> g: <span class="hljs-keyword">not</span> g[<span class="hljs-string">&#x27;is_expert_parallel&#x27;</span>],<br>            buffer_name=<span class="hljs-string">&#x27;buffers&#x27;</span>,<br>            default_skip_embedding_weight_decay=default_skip_embedding_weight_decay,<br>        )<br>        <span class="hljs-keyword">for</span> model_chunk <span class="hljs-keyword">in</span> dense_model_chunks:<br>            model_chunk.overlap_param_gather_with_optimizer_step = (<br>                overlap_param_gather_with_optimizer_step<br>            )<br><br>        <span class="hljs-comment"># Pass Gloo process groups into optimizer only if needed.</span><br>        optimizers.append(<br>            _get_megatron_optimizer_based_on_param_groups(<br>                config,<br>                model_chunks=dense_model_chunks,<br>                param_groups=param_groups,<br>                per_model_buffers=buffers,<br>                model_parallel_group=mp_group,<br>                data_parallel_group=intra_dp_cp_group,<br>                data_parallel_group_gloo=intra_dp_cp_group_gloo,<br>                data_parallel_group_idx=model_parallel_rank,<br>                distributed_optimizer_instance_id=distributed_optimizer_instance_id,<br>            )<br>        )<br>        model_chunk_offset += <span class="hljs-number">1</span><br><br>    moe_param_groups, moe_buffers = _get_param_groups_and_buffers(<br>        model_chunks,<br>        model_chunk_offset=<span class="hljs-number">0</span>,<br>        config=config,<br>        no_weight_decay_cond=no_weight_decay_cond,<br>        scale_lr_cond=scale_lr_cond,<br>        lr_mult=lr_mult,<br>        filter_fn=<span class="hljs-keyword">lambda</span> g: g[<span class="hljs-string">&#x27;is_expert_parallel&#x27;</span>],<br>        buffer_name=<span class="hljs-string">&#x27;expert_parallel_buffers&#x27;</span>,<br>        default_skip_embedding_weight_decay=default_skip_embedding_weight_decay,<br>    )<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(moe_param_groups) &gt; <span class="hljs-number">0</span>:<br>        expt_model_parallel_rank = get_pg_rank(expt_tp_pp_group)<br>        <span class="hljs-comment"># Pass Gloo process groups into optimizer only if needed.</span><br>        <span class="hljs-keyword">if</span> use_gloo_process_groups:<br>            expt_data_parallel_group_gloo = intra_expt_dp_group_gloo<br>        <span class="hljs-keyword">else</span>:<br>            expt_data_parallel_group_gloo = <span class="hljs-literal">None</span><br>        optimizers.append(<br>            _get_megatron_optimizer_based_on_param_groups(<br>                config,<br>                model_chunks=model_chunks,<br>                param_groups=moe_param_groups,<br>                per_model_buffers=moe_buffers,<br>                model_parallel_group=expt_tp_pp_group,<br>                data_parallel_group=intra_expt_dp_group,<br>                data_parallel_group_gloo=expt_data_parallel_group_gloo,<br>                data_parallel_group_idx=expt_model_parallel_rank,<br>                distributed_optimizer_instance_id=distributed_optimizer_instance_id,<br>            )<br>        )<br><br>    <span class="hljs-keyword">return</span> ChainedOptimizer(optimizers)<br><br></code></pre></td></tr></table></figure>

<h3 id="get-optimizer-param-scheduler"><a href="#get-optimizer-param-scheduler" class="headerlink" title="get_optimizer_param_scheduler"></a>get_optimizer_param_scheduler</h3><p><code>get_optimizer_param_scheduler</code>负责给optimizer计算每一步使用什么学习率，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_optimizer_param_scheduler</span>(<span class="hljs-params">optimizer</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build the learning rate scheduler.&quot;&quot;&quot;</span><br>    args = get_args()<br><br>    <span class="hljs-comment"># Iteration-based training.</span><br>    <span class="hljs-keyword">if</span> args.train_iters:<br>        <span class="hljs-keyword">if</span> args.lr_decay_iters <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            args.lr_decay_iters = args.train_iters<br>        lr_decay_steps = args.lr_decay_iters * args.global_batch_size<br>        wd_incr_steps = args.train_iters * args.global_batch_size<br>        wsd_decay_steps = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> args.lr_wsd_decay_iters <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            wsd_decay_steps = args.lr_wsd_decay_iters * args.global_batch_size<br>        <span class="hljs-keyword">if</span> args.lr_warmup_fraction <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            lr_warmup_steps = args.lr_warmup_fraction * lr_decay_steps<br>        <span class="hljs-keyword">else</span>:<br>            lr_warmup_steps = args.lr_warmup_iters * args.global_batch_size<br>    <span class="hljs-comment"># Sample-based training.</span><br>    <span class="hljs-keyword">elif</span> args.train_samples:<br>        <span class="hljs-comment"># We need to set training iters for later use. Technically</span><br>        <span class="hljs-comment"># we need to adjust the training samples too (due to last</span><br>        <span class="hljs-comment"># batch being incomplete) but we leave it as is for now.</span><br>        update_train_iters(args)<br>        <span class="hljs-keyword">if</span> args.lr_decay_samples <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            args.lr_decay_samples = args.train_samples<br>        lr_decay_steps = args.lr_decay_samples<br>        wd_incr_steps = args.train_samples<br>        wsd_decay_steps = args.lr_wsd_decay_samples<br>        <span class="hljs-keyword">if</span> args.lr_warmup_fraction <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            lr_warmup_steps = args.lr_warmup_fraction * lr_decay_steps<br>        <span class="hljs-keyword">else</span>:<br>            lr_warmup_steps = args.lr_warmup_samples<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">&#x27;either train-iters or train-samples should be provided.&#x27;</span>)<br><br>    opt_param_scheduler = OptimizerParamScheduler(<br>        optimizer,<br>        init_lr=args.lr_warmup_init,<br>        max_lr=args.lr,<br>        min_lr=args.min_lr,<br>        lr_warmup_steps=lr_warmup_steps,<br>        lr_decay_steps=lr_decay_steps,<br>        lr_decay_style=args.lr_decay_style,<br>        start_wd=args.start_weight_decay,<br>        end_wd=args.end_weight_decay,<br>        wd_incr_steps=wd_incr_steps,<br>        wd_incr_style=args.weight_decay_incr_style,<br>        use_checkpoint_opt_param_scheduler=args.use_checkpoint_opt_param_scheduler,<br>        override_opt_param_scheduler=args.override_opt_param_scheduler,<br>        wsd_decay_steps=wsd_decay_steps,<br>        lr_wsd_decay_style=args.lr_wsd_decay_style,<br>    )<br><br>    <span class="hljs-keyword">return</span> opt_param_scheduler<br><br></code></pre></td></tr></table></figure>

<p>其有两类训练描述方式：</p>
<ul>
<li><p>Iteration-based training，即设置了类似<code>--train-iters 500000</code></p>
<ul>
<li><p>因为Megatron 的 <code>OptimizerParamScheduler</code> 内部是以已处理的 sample 数作为横轴，所以1 iteration &#x3D; global_batch_size 个 samples</p>
</li>
<li><p>然后其支持设置warm up步数、Warm Start Decay步数</p>
</li>
</ul>
</li>
<li><p>Sample-based training，即设置了类似<code>--train-samples 300B</code></p>
<ul>
<li><p>首先其通过<code>update_train_iters(args)</code>来反推<code>train_iters</code>，因为可能其他地方还需要使用iters</p>
</li>
<li><p>然后可以直接得到lr_decay_steps &#x3D; args.lr_decay_samples，wd_incr_steps &#x3D; args.train_samples</p>
</li>
</ul>
</li>
<li><p>最后借助这些参数构造<code>OptimizerParamScheduler</code></p>
</li>
</ul>
<h2 id="train"><a href="#train" class="headerlink" title="train"></a>train</h2><p>train的代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params"></span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    model,</span><br><span class="hljs-params">    optimizer,</span><br><span class="hljs-params">    opt_param_scheduler,</span><br><span class="hljs-params">    train_data_iterator,</span><br><span class="hljs-params">    valid_data_iterator,</span><br><span class="hljs-params">    process_non_loss_data_func,</span><br><span class="hljs-params">    config,</span><br><span class="hljs-params">    checkpointing_context,</span><br><span class="hljs-params">    non_loss_data_func,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Training function: run train_step desired number of times, run validation, checkpoint.&quot;&quot;&quot;</span><br>    args = get_args()<br>    timers = get_timers()<br>    energy_monitor = get_energy_monitor()<br>    one_logger = get_one_logger()<br><br>    <span class="hljs-keyword">if</span> args.run_workload_inspector_server:<br>        <span class="hljs-keyword">try</span>:<br>            <span class="hljs-keyword">from</span> workload_inspector.utils.webserver <span class="hljs-keyword">import</span> run_server<br>            <span class="hljs-keyword">import</span> threading<br><br>            threading.Thread(<br>                target=run_server, daemon=<span class="hljs-literal">True</span>, args=(torch.distributed.get_rank(),)<br>            ).start()<br>        <span class="hljs-keyword">except</span> ModuleNotFoundError:<br>            print_rank_0(<span class="hljs-string">&quot;workload inspector module not found.&quot;</span>)<br><br>    <span class="hljs-comment"># Write args to tensorboard</span><br>    write_args_to_tensorboard()<br><br>    <span class="hljs-comment"># Turn on training mode which enables dropout.</span><br>    <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model:<br>        model_module.train()<br><br>    <span class="hljs-comment"># Tracking loss.</span><br>    total_loss_dict = &#123;&#125;<br><br>    <span class="hljs-comment"># Iterations.</span><br>    iteration = args.iteration<br>    <span class="hljs-comment"># Make sure rerun_state_machine has the right iteration loaded from checkpoint.</span><br>    rerun_state_machine = get_rerun_state_machine()<br>    <span class="hljs-keyword">if</span> rerun_state_machine.current_iteration != iteration:<br>        print_rank_0(<span class="hljs-string">f&quot;Overwriting rerun_state_machine.current_iteration from &quot;</span><br>                     <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;rerun_state_machine.current_iteration&#125;</span> to <span class="hljs-subst">&#123;iteration&#125;</span>...&quot;</span>)<br>        rerun_state_machine.current_iteration = iteration<br><br>    <span class="hljs-comment"># Track E2E metrics at the start of training.</span><br>    one_logger_utils.on_train_start(<br>        iteration=iteration,<br>        consumed_train_samples=args.consumed_train_samples,<br>        train_samples=args.train_samples,<br>        seq_length=args.seq_length,<br>        train_iters=args.train_iters,<br>        save=args.save,<br>        async_save=args.async_save,<br>        log_throughput=args.log_throughput,<br>        num_floating_point_operations_so_far=args.num_floating_point_operations_so_far,<br>    )<br><br>    num_floating_point_operations_so_far = args.num_floating_point_operations_so_far<br><br>    <span class="hljs-comment"># Setup some training config params.</span><br>    config.grad_scale_func = optimizer.scale_loss<br>    config.timers = timers<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(model[<span class="hljs-number">0</span>], (megatron_FSDP, DDP)) <span class="hljs-keyword">and</span> args.overlap_grad_reduce:<br>        <span class="hljs-keyword">assert</span> config.no_sync_func <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, (<br>            <span class="hljs-string">&#x27;When overlap_grad_reduce is True, config.no_sync_func must be None; &#x27;</span><br>            <span class="hljs-string">&#x27;a custom no_sync_func is not supported when overlapping grad-reduce&#x27;</span><br>        )<br>        config.no_sync_func = [model_chunk.no_sync <span class="hljs-keyword">for</span> model_chunk <span class="hljs-keyword">in</span> model]<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(model) == <span class="hljs-number">1</span>:<br>            config.no_sync_func = config.no_sync_func[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">if</span> args.align_grad_reduce:<br>            config.grad_sync_func = [model_chunk.start_grad_sync <span class="hljs-keyword">for</span> model_chunk <span class="hljs-keyword">in</span> model]<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(model) == <span class="hljs-number">1</span>:<br>                config.grad_sync_func = config.grad_sync_func[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">if</span> args.overlap_param_gather <span class="hljs-keyword">and</span> args.align_param_gather:<br>        config.param_sync_func = [model_chunk.start_param_sync <span class="hljs-keyword">for</span> model_chunk <span class="hljs-keyword">in</span> model]<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(model) == <span class="hljs-number">1</span>:<br>            config.param_sync_func = config.param_sync_func[<span class="hljs-number">0</span>]<br>    config.finalize_model_grads_func = finalize_model_grads<br><br>    <span class="hljs-keyword">if</span> args.log_energy:<br>        energy_monitor.setup()<br>        energy_monitor.resume()<br><br>    timers(<span class="hljs-string">&#x27;interval-time&#x27;</span>, log_level=<span class="hljs-number">0</span>).start(barrier=<span class="hljs-literal">True</span>)<br>    print_datetime(<span class="hljs-string">&#x27;before the start of training step&#x27;</span>)<br>    report_memory_flag = <span class="hljs-literal">True</span><br>    pre_hook_enabled = <span class="hljs-literal">False</span><br>    should_exit = <span class="hljs-literal">False</span><br>    exit_code = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">if</span> args.manual_gc:<br>        <span class="hljs-comment"># Disable the default garbage collector and perform the collection manually.</span><br>        <span class="hljs-comment"># This is to align the timing of garbage collection across ranks.</span><br>        <span class="hljs-keyword">assert</span> (<br>            args.manual_gc_interval &gt;= <span class="hljs-number">0</span><br>        ), <span class="hljs-string">&#x27;Manual garbage collection interval should be larger than or equal to 0&#x27;</span><br>        gc.disable()<br>        gc.collect()<br><br>    <span class="hljs-comment"># Singleton initialization of straggler detector.</span><br>    <span class="hljs-keyword">if</span> args.log_straggler:<br>        <span class="hljs-keyword">global</span> stimer<br>        world = torch.distributed.get_world_size()<br>        rank = torch.distributed.get_rank()<br>        mmcnt = args.straggler_minmax_count<br>        stimer.configure(<br>            world,<br>            rank,<br>            mmcnt=mmcnt,<br>            enabled=<span class="hljs-keyword">not</span> args.disable_straggler_on_startup,<br>            port=args.straggler_ctrlr_port,<br>        )<br>    num_floating_point_operations_since_last_log_event = <span class="hljs-number">0.0</span><br><br>    num_microbatches = get_num_microbatches()<br>    eval_duration = <span class="hljs-number">0.0</span><br>    eval_iterations = <span class="hljs-number">0</span><br>    <span class="hljs-comment"># Wrap forward_backward_func for Full iteration CUDA graph</span><br>    forward_backward_func = get_forward_backward_func()<br>    <span class="hljs-keyword">if</span> args.enable_cuda_graph <span class="hljs-keyword">and</span> args.cuda_graph_scope==<span class="hljs-string">&quot;full_iteration&quot;</span>:<br>        forward_backward_func = FullCudaGraphWrapper(forward_backward_func, cuda_graph_warmup_steps=args.cuda_graph_warmup_steps)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_e2e_base_metrics</span>():<br>        <span class="hljs-string">&quot;&quot;&quot;Get base metrics values for one-logger to calculate E2E tracking metrics.&quot;&quot;&quot;</span><br>        num_floating_point_operations_since_current_train_start = (<br>            num_floating_point_operations_so_far - args.num_floating_point_operations_so_far<br>        )<br>        <span class="hljs-keyword">return</span> &#123;<br>            <span class="hljs-string">&#x27;iteration&#x27;</span>: iteration,<br>            <span class="hljs-string">&#x27;train_duration&#x27;</span>: timers(<span class="hljs-string">&#x27;interval-time&#x27;</span>).active_time(),<br>            <span class="hljs-string">&#x27;eval_duration&#x27;</span>: eval_duration,<br>            <span class="hljs-string">&#x27;eval_iterations&#x27;</span>: eval_iterations,<br>            <span class="hljs-string">&#x27;total_flops_since_current_train_start&#x27;</span>: num_floating_point_operations_since_current_train_start,<br>            <span class="hljs-string">&#x27;num_floating_point_operations_so_far&#x27;</span>: num_floating_point_operations_so_far,<br>            <span class="hljs-string">&#x27;consumed_train_samples&#x27;</span>: args.consumed_train_samples,<br>            <span class="hljs-string">&#x27;world_size&#x27;</span>: args.world_size,<br>            <span class="hljs-string">&#x27;seq_length&#x27;</span>: args.seq_length,<br>        &#125;<br><br>    <span class="hljs-comment"># Cache into one-logger for callback.</span><br>    <span class="hljs-keyword">if</span> one_logger:<br>        <span class="hljs-keyword">with</span> one_logger.get_context_manager():<br>            one_logger.store_set(<span class="hljs-string">&#x27;get_e2e_base_metrics&#x27;</span>, get_e2e_base_metrics)<br><br>    prof = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> (<br>        args.profile<br>        <span class="hljs-keyword">and</span> torch.distributed.get_rank() <span class="hljs-keyword">in</span> args.profile_ranks<br>        <span class="hljs-keyword">and</span> args.use_pytorch_profiler<br>    ):<br>        prof = torch.profiler.profile(<br>            schedule=torch.profiler.schedule(<br>                wait=<span class="hljs-built_in">max</span>(args.profile_step_start - <span class="hljs-number">1</span>, <span class="hljs-number">0</span>),<br>                warmup=<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> args.profile_step_start &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>,<br>                active=args.profile_step_end - args.profile_step_start,<br>                repeat=<span class="hljs-number">1</span>,<br>            ),<br>            on_trace_ready=torch.profiler.tensorboard_trace_handler(args.tensorboard_dir),<br>            record_shapes=<span class="hljs-literal">True</span>,<br>            with_stack=<span class="hljs-literal">True</span>,<br>        )<br>        prof.start()<br><br>    start_iteration = iteration<br>    <span class="hljs-comment"># Disable forward pre-hook to start training to ensure that errors in checkpoint loading</span><br>    <span class="hljs-comment"># or random initialization don&#x27;t propagate to all ranks in first all-gather (which is a</span><br>    <span class="hljs-comment"># no-op if things work correctly).</span><br>    <span class="hljs-keyword">if</span> should_disable_forward_pre_hook(args):<br>        disable_forward_pre_hook(model, param_sync=<span class="hljs-literal">False</span>)<br>        <span class="hljs-comment"># Also remove param_sync_func temporarily so that sync calls made in</span><br>        <span class="hljs-comment"># `forward_backward_func` are no-ops.</span><br>        param_sync_func = config.param_sync_func<br>        config.param_sync_func = <span class="hljs-literal">None</span><br>        pre_hook_enabled = <span class="hljs-literal">False</span><br>    <span class="hljs-comment"># Also, check weight hash across DP replicas to be very pedantic.</span><br>    <span class="hljs-keyword">if</span> args.check_weight_hash_across_dp_replicas_interval <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">assert</span> check_param_hashes_across_dp_replicas(<br>            model, cross_check=<span class="hljs-literal">True</span><br>        ), <span class="hljs-string">&quot;Parameter hashes not matching across DP replicas&quot;</span><br>        torch.distributed.barrier()<br>        print_rank_0(<span class="hljs-string">f&quot;&gt;&gt;&gt; Weight hashes match after <span class="hljs-subst">&#123;iteration&#125;</span> iterations...&quot;</span>)<br><br>    <span class="hljs-comment"># Capture CUDA Graphs.</span><br>    <span class="hljs-keyword">if</span> args.external_cuda_graph:<br>        cuda_graph_helper = TECudaGraphHelper(<br>            model=model,<br>            config=config,<br>            seq_length=args.seq_length,<br>            micro_batch_size=args.micro_batch_size,<br>            optimizers=[optimizer],<br>        )<br>        cuda_graph_helper.create_cudagraphs()<br><br>    <span class="hljs-comment"># Run training iterations till done.</span><br>    <span class="hljs-keyword">while</span> iteration &lt; args.train_iters:<br>        <span class="hljs-keyword">if</span> args.profile <span class="hljs-keyword">and</span> torch.distributed.get_rank() <span class="hljs-keyword">in</span> args.profile_ranks:<br>            <span class="hljs-keyword">if</span> args.use_pytorch_profiler:<br>                prof.step()<br>            <span class="hljs-keyword">elif</span> iteration == args.profile_step_start:<br>                torch.cuda.cudart().cudaProfilerStart()<br>                torch.autograd.profiler.emit_nvtx(record_shapes=<span class="hljs-literal">True</span>).__enter__()<br><br>        ft_integration.on_checkpointing_start()<br>        maybe_finalize_async_save(blocking=<span class="hljs-literal">False</span>)<br>        ft_integration.on_checkpointing_end(is_async_finalization=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-comment"># Update number of microbatches first without consistency check to decide if a</span><br>        <span class="hljs-comment"># checkpoint should be saved. If the number of microbatches is different</span><br>        <span class="hljs-comment"># from the previous iteration, save a checkpoint. Then run consistency check</span><br>        <span class="hljs-comment"># to make sure training configuration is still valid.</span><br>        update_num_microbatches(args.consumed_train_samples, consistency_check=<span class="hljs-literal">False</span>, verbose=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">if</span> get_num_microbatches() != num_microbatches <span class="hljs-keyword">and</span> iteration != <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">assert</span> get_num_microbatches() &gt; num_microbatches, (<br>                <span class="hljs-string">f&quot;Number of microbatches should be increasing due to batch size rampup; &quot;</span><br>                <span class="hljs-string">f&quot;instead going from <span class="hljs-subst">&#123;num_microbatches&#125;</span> to <span class="hljs-subst">&#123;get_num_microbatches()&#125;</span>&quot;</span><br>            )<br>            <span class="hljs-keyword">if</span> args.save <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                save_checkpoint_and_time(<br>                    iteration,<br>                    model,<br>                    optimizer,<br>                    opt_param_scheduler,<br>                    num_floating_point_operations_so_far,<br>                    checkpointing_context,<br>                    train_data_iterator=train_data_iterator,<br>                )<br>        num_microbatches = get_num_microbatches()<br>        update_num_microbatches(args.consumed_train_samples, consistency_check=<span class="hljs-literal">True</span>, verbose=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-comment"># Completely skip iteration if needed.</span><br>        <span class="hljs-keyword">if</span> iteration <span class="hljs-keyword">in</span> args.iterations_to_skip:<br>            <span class="hljs-comment"># Dummy train_step to fast forward train_data_iterator.</span><br>            dummy_train_step(train_data_iterator)<br>            iteration += <span class="hljs-number">1</span><br>            batch_size = (<br>                mpu.get_data_parallel_world_size() * args.micro_batch_size * get_num_microbatches()<br>            )<br>            args.consumed_train_samples += batch_size<br>            args.skipped_train_samples += batch_size<br>            <span class="hljs-keyword">continue</span><br><br>        <span class="hljs-comment"># Run training step.</span><br>        args.curr_iteration = iteration<br>        ft_integration.on_training_step_start()<br>        (<br>            loss_dict,<br>            skipped_iter,<br>            should_checkpoint,<br>            should_exit,<br>            exit_code,<br>            grad_norm,<br>            num_zeros_in_grad,<br>        ) = train_step(<br>            forward_step_func, train_data_iterator, model, optimizer, opt_param_scheduler, config, forward_backward_func<br>        )<br>        ft_integration.on_training_step_end()<br>        <span class="hljs-keyword">if</span> should_checkpoint:<br>            save_checkpoint_and_time(<br>                iteration,<br>                model,<br>                optimizer,<br>                opt_param_scheduler,<br>                num_floating_point_operations_so_far,<br>                checkpointing_context,<br>                train_data_iterator=train_data_iterator,<br>            )<br>        <span class="hljs-keyword">if</span> should_exit:<br>            <span class="hljs-keyword">break</span><br><br>        <span class="hljs-comment"># Enable forward pre-hooks after first set of forward and backward passes.</span><br>        <span class="hljs-comment"># When running in fp16, skip all NaN iterations until steady-state loss scaling value</span><br>        <span class="hljs-comment"># is reached.</span><br>        <span class="hljs-keyword">if</span> iteration == start_iteration:<br>            <span class="hljs-keyword">if</span> skipped_iter:<br>                <span class="hljs-comment"># Only enable forward pre-hook after a training step has successfully run. Relevant</span><br>                <span class="hljs-comment"># for fp16 codepath where first XX iterations are skipped until steady-state loss</span><br>                <span class="hljs-comment"># scale value is reached.</span><br>                start_iteration = iteration + <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># Enable forward pre-hook after training step has successfully run. All subsequent</span><br>                <span class="hljs-comment"># forward passes will use the forward pre-hook / `param_sync_func` in</span><br>                <span class="hljs-comment"># `forward_backward_func`.</span><br>                <span class="hljs-keyword">if</span> should_disable_forward_pre_hook(args):<br>                    enable_forward_pre_hook(model)<br>                    config.param_sync_func = param_sync_func<br>                    pre_hook_enabled = <span class="hljs-literal">True</span><br>                    <span class="hljs-comment"># Set the manual hooks when CUDA Graphs are used.</span><br>                    <span class="hljs-keyword">if</span> args.external_cuda_graph:<br>                        cuda_graph_helper.cuda_graph_set_manual_hooks()<br><br>        iteration += <span class="hljs-number">1</span><br>        batch_size = (<br>            mpu.get_data_parallel_world_size() * args.micro_batch_size * get_num_microbatches()<br>        )<br>        args.consumed_train_samples += batch_size<br>        num_skipped_samples_in_batch = (<br>            get_current_global_batch_size() - get_current_running_global_batch_size()<br>        )<br>        <span class="hljs-keyword">if</span> args.decrease_batch_size_if_needed:<br>            <span class="hljs-keyword">assert</span> num_skipped_samples_in_batch &gt;= <span class="hljs-number">0</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">assert</span> num_skipped_samples_in_batch == <span class="hljs-number">0</span><br>        args.skipped_train_samples += num_skipped_samples_in_batch<br>        num_floating_point_operations_in_batch = num_floating_point_operations(args, batch_size)<br>        num_floating_point_operations_so_far += num_floating_point_operations_in_batch<br>        num_floating_point_operations_since_last_log_event += num_floating_point_operations_in_batch<br><br>        <span class="hljs-comment"># Logging.</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> optimizer.is_stub_optimizer:<br>            loss_scale = optimizer.get_loss_scale().item()<br>        <span class="hljs-keyword">else</span>:<br>            loss_scale = <span class="hljs-number">1.0</span><br>        params_norm = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">if</span> args.log_params_norm:<br>            params_norm = calc_params_l2_norm(model)<br>        learning_rate = <span class="hljs-literal">None</span><br>        decoupled_learning_rate = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">for</span> param_group <span class="hljs-keyword">in</span> optimizer.param_groups:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(param_group[<span class="hljs-string">&#x27;params&#x27;</span>]) == <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">if</span> param_group[<span class="hljs-string">&#x27;is_decoupled_lr&#x27;</span>]:<br>                decoupled_learning_rate = param_group[<span class="hljs-string">&#x27;lr&#x27;</span>]<br>            <span class="hljs-keyword">else</span>:<br>                learning_rate = param_group[<span class="hljs-string">&#x27;lr&#x27;</span>]<br>        report_memory_flag = training_log(<br>            loss_dict,<br>            total_loss_dict,<br>            learning_rate,<br>            decoupled_learning_rate,<br>            iteration,<br>            loss_scale,<br>            report_memory_flag,<br>            skipped_iter,<br>            grad_norm,<br>            params_norm,<br>            num_zeros_in_grad,<br>        )<br><br>        <span class="hljs-comment"># Evaluation.</span><br>        <span class="hljs-keyword">if</span> args.eval_interval <span class="hljs-keyword">and</span> iteration % args.eval_interval == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> args.do_valid:<br>            <span class="hljs-keyword">if</span> args.log_energy:<br>                energy_monitor.pause()<br>            timers(<span class="hljs-string">&#x27;interval-time&#x27;</span>).stop()<br>            <span class="hljs-keyword">if</span> should_disable_forward_pre_hook(args):<br>                disable_forward_pre_hook(model)<br>                pre_hook_enabled = <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">if</span> args.manual_gc <span class="hljs-keyword">and</span> args.manual_gc_eval:<br>                <span class="hljs-comment"># Collect all objects.</span><br>                gc.collect()<br>            prefix = <span class="hljs-string">f&#x27;iteration <span class="hljs-subst">&#123;iteration&#125;</span>&#x27;</span><br>            timers(<span class="hljs-string">&#x27;eval-time&#x27;</span>, log_level=<span class="hljs-number">0</span>).start(barrier=<span class="hljs-literal">True</span>)<br>            evaluate_and_print_results(<br>                prefix,<br>                forward_step_func,<br>                valid_data_iterator,<br>                model,<br>                iteration,<br>                process_non_loss_data_func,<br>                config,<br>                verbose=<span class="hljs-literal">False</span>,<br>                write_to_tensorboard=<span class="hljs-literal">True</span>,<br>                non_loss_data_func=non_loss_data_func,<br>            )<br>            eval_duration += timers(<span class="hljs-string">&#x27;eval-time&#x27;</span>).elapsed()<br>            eval_iterations += <span class="hljs-built_in">sum</span>(args.eval_iters) <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(args.eval_iters, <span class="hljs-built_in">list</span>) <span class="hljs-keyword">else</span> args.eval_iters<br>            timers(<span class="hljs-string">&#x27;eval-time&#x27;</span>).stop()<br>            one_logger_utils.track_e2e_metrics()<br><br>            <span class="hljs-keyword">if</span> args.manual_gc <span class="hljs-keyword">and</span> args.manual_gc_eval:<br>                <span class="hljs-comment"># Collect only the objects created and used in evaluation.</span><br>                gc.collect(generation=<span class="hljs-number">0</span>)<br>            <span class="hljs-keyword">if</span> should_disable_forward_pre_hook(args):<br>                enable_forward_pre_hook(model)<br>                pre_hook_enabled = <span class="hljs-literal">True</span><br>            timers(<span class="hljs-string">&#x27;interval-time&#x27;</span>, log_level=<span class="hljs-number">0</span>).start(barrier=<span class="hljs-literal">True</span>)<br>            <span class="hljs-keyword">if</span> args.log_energy:<br>                energy_monitor.resume()<br><br>        <span class="hljs-comment"># Miscellaneous post-training-step functions (e.g., FT heartbeats, GC).</span><br>        <span class="hljs-comment"># Some of these only happen at specific iterations.</span><br>        post_training_step_callbacks(<br>            model,<br>            optimizer,<br>            opt_param_scheduler,<br>            iteration,<br>            prof,<br>            num_floating_point_operations_since_last_log_event,<br>        )<br><br>        <span class="hljs-comment"># Checkpoint and decide whether to exit.</span><br>        should_exit = checkpoint_and_decide_exit(<br>            model,<br>            optimizer,<br>            opt_param_scheduler,<br>            iteration,<br>            num_floating_point_operations_so_far,<br>            checkpointing_context,<br>            train_data_iterator,<br>        )<br>        <span class="hljs-keyword">if</span> should_exit:<br>            <span class="hljs-keyword">break</span><br><br>    one_logger_utils.track_e2e_metrics()<br><br>    <span class="hljs-comment"># Flush TensorBoard, WandB writers and one-logger.</span><br>    writer = get_tensorboard_writer()<br>    <span class="hljs-keyword">if</span> writer:<br>        writer.flush()<br><br>    <span class="hljs-comment"># Close out pre-hooks if using distributed optimizer and overlapped param gather.</span><br>    <span class="hljs-keyword">if</span> pre_hook_enabled:<br>        disable_forward_pre_hook(model)<br><br>    ft_integration.on_checkpointing_start()<br>    <span class="hljs-comment"># This will finalize all unfinalized async request and terminate</span><br>    <span class="hljs-comment"># a persistent async worker if persistent ckpt worker is enabled</span><br>    maybe_finalize_async_save(blocking=<span class="hljs-literal">True</span>, terminate=<span class="hljs-literal">True</span>)<br>    ft_integration.on_checkpointing_end(is_async_finalization=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">if</span> args.enable_ft_package <span class="hljs-keyword">and</span> ft_integration.get_rank_monitor_client() <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        ft_integration.get_rank_monitor_client().shutdown_workload_monitoring()<br><br>    <span class="hljs-keyword">if</span> args.log_energy:<br>        energy_monitor.lap()<br>        total_energy = energy_monitor.get_total()<br>        print_rank_0(<span class="hljs-string">f&quot;Total training energy (GPU): <span class="hljs-subst">&#123;total_energy / <span class="hljs-number">1e6</span>&#125;</span> MJ&quot;</span>)<br>        energy_monitor.shutdown()<br><br>    <span class="hljs-comment"># If any exit conditions (signal handler, duration, iterations) have been reached, exit.</span><br>    <span class="hljs-keyword">if</span> should_exit:<br>        wandb_writer = get_wandb_writer()<br>        <span class="hljs-keyword">if</span> wandb_writer:<br>            wandb_writer.finish()<br>        ft_integration.shutdown()<br>        one_logger_utils.finish()<br>        sys.exit(exit_code)<br><br>    <span class="hljs-keyword">return</span> iteration, num_floating_point_operations_so_far<br><br></code></pre></td></tr></table></figure>

<p>其主要流程为：</p>
<ol>
<li><p>获取全局配置</p>
</li>
<li><p>切换到 train() 模式</p>
</li>
<li><p>将迭代次数与rerun_state_machine对齐</p>
</li>
<li><p>给 <code>config</code> 注入“分布式重叠通信&#x2F;同步”钩子，<code>config</code> 会被 <code>forward_backward_func</code> &#x2F; <code>train_step</code> 使用，用于控制 no_sync（梯度累积）、梯度同步重叠、参数 all-gather 重叠 等。</p>
</li>
<li><p>还有一些可选的配置，如控制gc、开启profiler等</p>
</li>
<li><p>进入主循环：<code>while iteration &lt; args.train_iters</code>：</p>
<ol>
<li><p>更新<code>microbatch</code>，因为存在动态batch size的场景</p>
</li>
<li><p>使用<code>train_step</code>进行一轮训练</p>
</li>
<li><p>按需保存checkpoint</p>
</li>
<li><p>第一轮成功后再启用 forward pre-hook</p>
</li>
<li><p>更新 iteration&#x2F;样本数&#x2F;FLOPs</p>
</li>
<li><p>按需进行评估</p>
</li>
</ol>
</li>
<li><p>训练收尾，flush writer（TensorBoard&#x2F;WandB&#x2F;one-logger），若 pre-hook 还开着，关闭它（避免退出时还有挂钩或后台同步），<strong>最终 finalize 异步检查点等</strong></p>
</li>
</ol>
<h3 id="train-step"><a href="#train-step" class="headerlink" title="train_step"></a><code>train_step</code></h3><h4 id="参数forward-backward-func"><a href="#参数forward-backward-func" class="headerlink" title="参数forward_backward_func"></a>参数<code>forward_backward_func</code></h4><p><code>train_step</code>的关键参数还包含了<code>forward_backward_func</code>，其通过如下获得：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Wrap forward_backward_func for Full iteration CUDA graph</span><br>forward_backward_func = get_forward_backward_func()<br><span class="hljs-keyword">if</span> args.enable_cuda_graph and args.cuda_graph_scope==<span class="hljs-string">&quot;full_iteration&quot;</span>:<br>    forward_backward_func = FullCudaGraphWrapper(forward_backward_func, cuda_graph_warmup_steps=args.cuda_graph_warmup_steps)<br></code></pre></td></tr></table></figure>

<p><code>get_forward_backward_func</code>函数如下：</p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><code class="hljs markdown">def get<span class="hljs-emphasis">_forward_</span>backward<span class="hljs-emphasis">_func():</span><br><span class="hljs-emphasis">    &quot;&quot;&quot;Retrieves the appropriate forward_</span>backward function given the<br><span class="hljs-code">    configuration of parallel_state.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    Returns a function that will perform all of the forward and</span><br><span class="hljs-code">    backward passes of the model given the pipeline model parallel</span><br><span class="hljs-code">    world size and virtual pipeline model parallel world size in the</span><br><span class="hljs-code">    global parallel_state.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    Note that if using sequence parallelism, the sequence length component of</span><br><span class="hljs-code">    the tensor shape is updated to original_sequence_length /</span><br><span class="hljs-code">    tensor_model_parallel_world_size.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    The function returned takes the following arguments:</span><br><span class="hljs-code"></span><br><span class="hljs-code">    forward_step_func (required): A function that takes a data</span><br><span class="hljs-code">        iterator and a model as its arguments and return the model&#x27;s</span><br><span class="hljs-code">        forward output and the loss function. The loss function should</span><br><span class="hljs-code">        take one torch.Tensor and return a torch.Tensor of loss and a</span><br><span class="hljs-code">        dictionary of string -&gt; torch.Tensor.</span><br><span class="hljs-code"></span><br><span class="hljs-code">        A third argument, checkpoint_activations_microbatch, indicates</span><br><span class="hljs-code">        that the activations for this microbatch should be</span><br><span class="hljs-code">        checkpointed. A None value for this argument indicates that</span><br><span class="hljs-code">        the default from the configuration should be used. This is</span><br><span class="hljs-code">        used when the</span><br><span class="hljs-code">        num_microbatches_with_partial_activation_checkpoints is used.</span><br><span class="hljs-code"></span><br><span class="hljs-code">        For example:</span><br><span class="hljs-code"></span><br><span class="hljs-code">        def loss_func(loss_mask, output_tensor):</span><br><span class="hljs-code">            losses = output_tensor.float()</span><br><span class="hljs-code">            loss_mask = loss_mask.view(-1).float()</span><br><span class="hljs-code">            loss = torch.sum(losses.view(-1) * loss_mask) / loss_mask.sum()</span><br><span class="hljs-code"></span><br><span class="hljs-code">            # Reduce loss for logging.</span><br><span class="hljs-code">            averaged_loss = average_losses_across_data_parallel_group([loss])</span><br><span class="hljs-code"></span><br><span class="hljs-code">            return loss, &#123;&#x27;lm loss&#x27;: averaged_loss[0]&#125;</span><br><span class="hljs-code"></span><br><span class="hljs-code">        def forward_step(data_iterator, model):</span><br><span class="hljs-code">            data, loss_mask = next(data_iterator)</span><br><span class="hljs-code">            output = model(data)</span><br><span class="hljs-code">            return output, partial(loss_func, loss_mask)</span><br><span class="hljs-code"></span><br><span class="hljs-code">        forward_backward_func(forward_step_func=forward_step, ...)</span><br><span class="hljs-code"></span><br><span class="hljs-code">    data_iterator (required): an iterator over the data, will be</span><br><span class="hljs-code">        passed as is to forward_step_func. Expected to be a list of</span><br><span class="hljs-code">        iterators in the case of interleaved pipeline parallelism.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    model (required): the actual model. Expected to be a list of modules in the case of interleaved</span><br><span class="hljs-code">        pipeline parallelism. Must be a (potentially wrapped) megatron.core.models.MegatronModule.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    num_microbatches (int, required):</span><br><span class="hljs-code">        The number of microbatches to go through</span><br><span class="hljs-code"></span><br><span class="hljs-code">    seq_length (int, required): Sequence length of the current global batch. If this is a dual-stack</span><br><span class="hljs-code">        transformer, this is the encoder&#x27;s sequence length. This is ignored if variable_seq_lengths</span><br><span class="hljs-code">        in the config is True. Otherwise, each microbatch in the current global batch size must use</span><br><span class="hljs-code">        this sequence length.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    micro_batch_size (int, required): The number of sequences in a microbatch.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    decoder_seq_length (int, optional): The sequence length for the decoder in a dual-stack</span><br><span class="hljs-code">        transformer. This is ignored for a single-stack transformer.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    forward_only (optional, default = False): Perform only the forward step</span><br><span class="hljs-code"></span><br><span class="hljs-code">    collect_non_loss_data (optional, bool, default=False): TODO</span><br><span class="hljs-code"></span><br><span class="hljs-code">    first_val_step (bool, optional): Is the first step of the validation phase. Used by</span><br><span class="hljs-code">        Transformer Engine modules to only update their fp8 weights only on the first validation</span><br><span class="hljs-code">        step.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    adjust_tensor_shapes_fn (Callable, optional): A function that adjusts the receive and send</span><br><span class="hljs-code">        tensor shapes. Only applicable in forward_backward_pipelining_without_interleaving for now.</span><br><span class="hljs-code">        Takes in a list of receive shapes and a list of send shapes and returns the adjusted</span><br><span class="hljs-code">        respective list of shapes. Thus it is not used in the other forward-backward functions</span><br><span class="hljs-code">        which have different shape handling.</span><br><span class="hljs-code"></span><br><span class="hljs-code">    &quot;&quot;&quot;</span><br><span class="hljs-code">    pipeline_model_parallel_size = parallel_state.get_pipeline_model_parallel_world_size()</span><br><span class="hljs-code">    if pipeline_model_parallel_size &gt; 1:</span><br><span class="hljs-code">        if parallel_state.get_virtual_pipeline_model_parallel_world_size() is not None:</span><br><span class="hljs-code">            forward_backward_func = forward_backward_pipelining_with_interleaving</span><br><span class="hljs-code">        else:</span><br><span class="hljs-code">            forward_backward_func = forward_backward_pipelining_without_interleaving</span><br><span class="hljs-code">    else:</span><br><span class="hljs-code">        forward_backward_func = forward_backward_no_pipelining</span><br><span class="hljs-code">    return forward_backward_func</span><br></code></pre></td></tr></table></figure>

<p>其依据pipeline划分为了多个类别</p>
<ul>
<li><p><code>forward_backward_pipelining_with_interleaving</code>：开了pp和vp</p>
</li>
<li><p><code>forward_backward_pipelining_without_interleaving</code>：开了pp但是没有vp</p>
</li>
<li><p><code>forward_backward_no_pipelining</code>：没有开pp</p>
</li>
</ul>
<p>先只看最简单的<code>forward_backward_no_pipelining</code>，代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_backward_no_pipelining</span>(<span class="hljs-params"></span><br><span class="hljs-params">    *,</span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    data_iterator: <span class="hljs-type">Union</span>[Iterator, <span class="hljs-type">List</span>[Iterator]],</span><br><span class="hljs-params">    model: <span class="hljs-type">Union</span>[torch.nn.Module, <span class="hljs-type">List</span>[torch.nn.Module]],</span><br><span class="hljs-params">    num_microbatches: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    seq_length: <span class="hljs-built_in">int</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    micro_batch_size: <span class="hljs-built_in">int</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    decoder_seq_length: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    forward_only: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    collect_non_loss_data: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    first_val_step: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    adjust_tensor_shapes_fn: <span class="hljs-type">Optional</span>[<span class="hljs-type">Callable</span>] = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    grad_finalize_pgs: <span class="hljs-type">Optional</span>[GradFinalizeProcessGroups] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Run forward and backward passes with no pipeline parallelism&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">if</span> grad_finalize_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        tp_group = parallel_state.get_tensor_model_parallel_group()<br>        cp_group = parallel_state.get_context_parallel_group()<br>        embd_group = parallel_state.get_embedding_group(check_initialized=<span class="hljs-literal">False</span>)<br>        pp_group = parallel_state.get_pipeline_model_parallel_group()<br>        pos_emb_group = parallel_state.get_position_embedding_group(check_initialized=<span class="hljs-literal">False</span>)<br>        grad_finalize_pgs = GradFinalizeProcessGroups()<br>        grad_finalize_pgs.tp = tp_group<br>        grad_finalize_pgs.cp = cp_group<br>        grad_finalize_pgs.embd = embd_group<br>        grad_finalize_pgs.pos_embd = pos_emb_group<br>        grad_finalize_pgs.pp = pp_group<br>        grad_finalize_pgs.dp_cp = parallel_state.get_data_parallel_group(<br>            with_context_parallel=<span class="hljs-literal">True</span>, partial_data_parallel=<span class="hljs-literal">False</span><br>        )<br><br>    <span class="hljs-keyword">elif</span> grad_finalize_pgs <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;tp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;cp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;embd&#x27;</span>), (<br>            <span class="hljs-string">&quot;grad_finalize_pgs must have a embd. In previous version, it is used default &quot;</span><br>            <span class="hljs-string">&quot;`parallel_state.default_embedding_ranks` to create the process group. If you are &quot;</span><br>            <span class="hljs-string">&quot;using the default process group, please use `parallel_state.get_embedding_group()` &quot;</span><br>            <span class="hljs-string">&quot;to get the process group. If you don&#x27;t need explicitly set it to None.&quot;</span><br>        )<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;pos_embd&#x27;</span>), (<br>            <span class="hljs-string">&quot;grad_finalize_pgs must have a pos_embd. In previous version, it is used default &quot;</span><br>            <span class="hljs-string">&quot;`parallel_state.default_position_embedding_ranks` to create the process group. &quot;</span><br>            <span class="hljs-string">&quot;If you are using the default process group, &quot;</span><br>            <span class="hljs-string">&quot;please use `parallel_state.get_position_embedding_group()` &quot;</span><br>            <span class="hljs-string">&quot;to get the process group. If you don&#x27;t need explicitly set it to None.&quot;</span><br>        )<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;pp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;dp_cp&#x27;</span>)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(model, <span class="hljs-built_in">list</span>):<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(model) == <span class="hljs-number">1</span>, <span class="hljs-string">&quot;non-pipeline-parallel schedule does not support model chunking&quot;</span><br>        model = model[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(data_iterator, <span class="hljs-built_in">list</span>):<br>        <span class="hljs-keyword">assert</span> (<br>            <span class="hljs-built_in">len</span>(data_iterator) == <span class="hljs-number">1</span><br>        ), <span class="hljs-string">&quot;non-pipeline-parallel schedule does not support model chunking&quot;</span><br>        data_iterator = data_iterator[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">assert</span> (<br>        adjust_tensor_shapes_fn <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;adjust_tensor_shapes_fn is not supported for non-pipeline-parallel schedule&quot;</span><br><br>    config = get_model_config(model)<br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;forward-backward&#x27;</span>, log_level=<span class="hljs-number">1</span>).start(barrier=config.barrier_with_L1_time)<br><br>    no_sync_func = config.no_sync_func<br>    <span class="hljs-keyword">if</span> no_sync_func <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        no_sync_func = contextlib.nullcontext<br><br>    model_type = get_model_type(model)<br><br>    forward_data_store = []<br>    input_tensor, output_tensor_grad = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    total_num_tokens = torch.zeros([], dtype=torch.<span class="hljs-built_in">int</span>, device=<span class="hljs-string">&quot;cuda&quot;</span>)<br><br>    <span class="hljs-keyword">if</span> config.overlap_moe_expert_parallel_comm <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> forward_only:<br>        forward_data_store, total_num_tokens = combined_1f1b_schedule_for_no_pipelining(<br>            forward_step_func,<br>            data_iterator,<br>            model,<br>            num_microbatches,<br>            input_tensor,<br>            output_tensor_grad,<br>            forward_data_store,<br>            config,<br>            collect_non_loss_data,<br>            first_val_step,<br>            forward_only,<br>            no_sync_func,<br>            total_num_tokens,<br>            partial(check_first_val_step, first_val_step, forward_only),<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">with</span> no_sync_func():<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_microbatches - <span class="hljs-number">1</span>):<br>                output_tensor, num_tokens = forward_step(<br>                    forward_step_func,<br>                    data_iterator,<br>                    model,<br>                    num_microbatches,<br>                    input_tensor,<br>                    forward_data_store,<br>                    config,<br>                    grad_finalize_pgs.cp.size(),<br>                    collect_non_loss_data,<br>                    is_first_microbatch=check_first_val_step(first_val_step, forward_only, i == <span class="hljs-number">0</span>),<br>                    current_microbatch=i,<br>                )<br>                total_num_tokens += num_tokens<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> forward_only:<br>                    backward_step(<br>                        input_tensor, output_tensor, output_tensor_grad, model_type, config<br>                    )<br>        <span class="hljs-comment"># Run computation for last microbatch out of context handler (want to</span><br>        <span class="hljs-comment"># synchronize gradients).</span><br>        output_tensor, num_tokens = forward_step(<br>            forward_step_func,<br>            data_iterator,<br>            model,<br>            num_microbatches,<br>            input_tensor,<br>            forward_data_store,<br>            config,<br>            grad_finalize_pgs.cp.size(),<br>            collect_non_loss_data,<br>            is_first_microbatch=check_first_val_step(<br>                first_val_step, forward_only, num_microbatches == <span class="hljs-number">1</span><br>            ),<br>            current_microbatch=num_microbatches - <span class="hljs-number">1</span>,<br>        )<br><br>        total_num_tokens += num_tokens<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> forward_only:<br>            backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)<br><br>    <span class="hljs-keyword">if</span> config.finalize_model_grads_func <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> forward_only:<br>        <span class="hljs-comment"># Finalize model grads (perform full grad all-reduce / reduce-scatter for</span><br>        <span class="hljs-comment"># data parallelism and layernorm all-reduce for sequence parallelism).</span><br>        config.finalize_model_grads_func(<br>            [model],<br>            total_num_tokens <span class="hljs-keyword">if</span> config.calculate_per_token_loss <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>            grad_finalize_pgs=grad_finalize_pgs,<br>        )<br><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;forward-backward&#x27;</span>).stop()<br><br>    <span class="hljs-keyword">if</span> (<br>        <span class="hljs-built_in">hasattr</span>(config, <span class="hljs-string">&#x27;enable_cuda_graph&#x27;</span>)<br>        <span class="hljs-keyword">and</span> config.enable_cuda_graph<br>        <span class="hljs-keyword">and</span> config.cuda_graph_scope != <span class="hljs-string">&quot;full_iteration&quot;</span><br>    ):<br>        create_cudagraphs()<br><br>    <span class="hljs-keyword">return</span> forward_data_store<br><br></code></pre></td></tr></table></figure>

<p>其流程如下：</p>
<ol>
<li><p>如果没有设置<code>grad_finalize_pgs</code>，就构造默认值，从而在做梯度规约&#x2F;归并时，需要明确“在哪些通信域里做什么规约”，例如DP&#x2F;DP×CP：参数梯度的 all-reduce 或 reduce-scatter</p>
</li>
<li><p>做一些参数检查</p>
</li>
<li><p>然后分为MoE overlap 与普通路径：</p>
<ol>
<li><p>MoE会调用<code>combined_1f1b_schedule_for_no_pipelining</code></p>
</li>
<li><p>普通路径就是执行<code>num_microbatches</code>次<code>forward_step</code>，如果不是<code>forward_only</code>就执行<code>backward_step</code>，注意后一个 micro-batch 在 no_sync 外，从而让“梯度同步&#x2F;规约”在这一轮的尾部发生（典型的梯度累积实现方式）</p>
</li>
</ol>
</li>
<li><p>最后执行<code>finalize_model_grads_func</code>，这一步通常包含：</p>
<ul>
<li><p>DP 的全梯度 all-reduce &#x2F; reduce-scatter（取决于 optimizer&#x2F;ZeRO 类策略）</p>
</li>
<li><p>sequence parallel 的 LayerNorm 等规约</p>
</li>
<li><p>embedding&#x2F;pos embedding 的特定规约</p>
</li>
</ul>
</li>
</ol>
<p>其中一个micro batch的<code>forward_step</code>代码如下所示，注意到这里就会调用到用户传入的<code>forward_step_func</code>函数了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_step</span>(<span class="hljs-params"></span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    data_iterator,</span><br><span class="hljs-params">    model,</span><br><span class="hljs-params">    num_microbatches,</span><br><span class="hljs-params">    input_tensor,</span><br><span class="hljs-params">    forward_data_store,</span><br><span class="hljs-params">    config,</span><br><span class="hljs-params">    cp_group_size,</span><br><span class="hljs-params">    collect_non_loss_data=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    checkpoint_activations_microbatch=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    is_first_microbatch=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    current_microbatch=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    vp_stage=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    is_last_stage=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Forward step for passed-in model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    If it is the first stage, the input tensor is obtained from the data_iterator.</span><br><span class="hljs-string">    Otherwise, the passed-in input_tensor is used.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        forward_step_func (callable):</span><br><span class="hljs-string">            The forward step function for the model that takes the</span><br><span class="hljs-string">            data iterator as the first argument, and model as the second.</span><br><span class="hljs-string">            This user&#x27;s forward step is expected to output a tuple of two elements:</span><br><span class="hljs-string"></span><br><span class="hljs-string">                1. The output object from the forward step. This output object needs to be a</span><br><span class="hljs-string">                    tensor or some kind of collection of tensors. The only hard requirement</span><br><span class="hljs-string">                    for this object is that it needs to be acceptible as input into the second</span><br><span class="hljs-string">                    function.</span><br><span class="hljs-string">                2. A function to reduce (optionally) the output from the forward step. This</span><br><span class="hljs-string">                    could be a reduction over the loss from the model, it could be a function that</span><br><span class="hljs-string">                    grabs the output from the model and reformats, it could be a function that just</span><br><span class="hljs-string">                    passes through the model output. This function must have one of the following</span><br><span class="hljs-string">                    patterns, and depending on the pattern different things happen internally:</span><br><span class="hljs-string"></span><br><span class="hljs-string">                        a. A tuple of reduced loss and some other data. Note that in this case</span><br><span class="hljs-string">                            the first argument is divided by the number of global microbatches,</span><br><span class="hljs-string">                            assuming it is a loss, so that the loss is stable as a function of</span><br><span class="hljs-string">                            the number of devices the step is split across.</span><br><span class="hljs-string">                        b. A triple of reduced loss, number of tokens, and some other data. This</span><br><span class="hljs-string">                            is similar to case (a), but the loss is further averaged across the</span><br><span class="hljs-string">                            number of tokens in the batch. If the user is not already averaging</span><br><span class="hljs-string">                            across the number of tokens, this pattern is useful to use.</span><br><span class="hljs-string">                        c. Any arbitrary data the user wants (eg a dictionary of tensors, a list</span><br><span class="hljs-string">                            of tensors, etc in the case of inference). To trigger case 3 you need</span><br><span class="hljs-string">                            to specify `collect_non_loss_data=True` and you may also want to</span><br><span class="hljs-string">                            specify `forward_only=True` in the call to the parent forward_backward</span><br><span class="hljs-string">                            function.</span><br><span class="hljs-string">        data_iterator (iterator):</span><br><span class="hljs-string">            The data iterator.</span><br><span class="hljs-string">        model (nn.Module):</span><br><span class="hljs-string">            The model to perform the forward step on.</span><br><span class="hljs-string">        num_microbatches (int):</span><br><span class="hljs-string">            The number of microbatches.</span><br><span class="hljs-string">        input_tensor (Tensor or list[Tensor]):</span><br><span class="hljs-string">            The input tensor(s) for the forward step.</span><br><span class="hljs-string">        forward_data_store (list):</span><br><span class="hljs-string">            The list to store the forward data. If you go down path 2.a or</span><br><span class="hljs-string">            2.b for the return of your forward reduction function then this will store only the</span><br><span class="hljs-string">            final dimension of the output, for example the metadata output by the loss function.</span><br><span class="hljs-string">            If you go down the path of 2.c then this will store the entire output of the forward</span><br><span class="hljs-string">            reduction function applied to the model output.</span><br><span class="hljs-string">        config (object):</span><br><span class="hljs-string">            The configuration object.</span><br><span class="hljs-string">        collect_non_loss_data (bool, optional):</span><br><span class="hljs-string">            Whether to collect non-loss data. Defaults to False.</span><br><span class="hljs-string">            This is the path to use if you want to collect arbitrary output from the model forward,</span><br><span class="hljs-string">            such as with inference use cases. Defaults to False.</span><br><span class="hljs-string">        checkpoint_activations_microbatch (int, optional):</span><br><span class="hljs-string">            The microbatch to checkpoint activations.</span><br><span class="hljs-string">            Defaults to None.</span><br><span class="hljs-string">        is_first_microbatch (bool, optional):</span><br><span class="hljs-string">            Whether it is the first microbatch. Defaults to False.</span><br><span class="hljs-string">        current_microbatch (int, optional):</span><br><span class="hljs-string">            The current microbatch. Defaults to None.</span><br><span class="hljs-string">        vp_stage (int, optional):</span><br><span class="hljs-string">            The virtual pipeline stage. Defaults to None.</span><br><span class="hljs-string">        is_last_stage (bool, optional):</span><br><span class="hljs-string">            Whether it is the last stage. Defaults to True.</span><br><span class="hljs-string">            Also considering virtual stages.</span><br><span class="hljs-string">            In case of PP/VPP, is_last_stage/is_vp_last_stage.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        Tensor or list[Tensor]: The output object(s) from the forward step.</span><br><span class="hljs-string">        Tensor: The number of tokens.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">from</span> megatron.core.transformer.multi_token_prediction <span class="hljs-keyword">import</span> MTPLossAutoScaler<br><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;forward-compute&#x27;</span>, log_level=<span class="hljs-number">2</span>).start()<br><br>    <span class="hljs-keyword">if</span> is_first_microbatch <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(model, <span class="hljs-string">&#x27;set_is_first_microbatch&#x27;</span>):<br>        model.set_is_first_microbatch()<br>    <span class="hljs-keyword">if</span> current_microbatch <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        set_current_microbatch(model, current_microbatch)<br><br>    unwrap_output_tensor = <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(input_tensor, <span class="hljs-built_in">list</span>):<br>        input_tensor = [input_tensor]<br>        unwrap_output_tensor = <span class="hljs-literal">True</span><br><br>    set_input_tensor = get_attr_wrapped_model(model, <span class="hljs-string">&quot;set_input_tensor&quot;</span>)<br>    set_input_tensor(input_tensor)<br><br>    <span class="hljs-keyword">if</span> config.enable_autocast:<br>        context_manager = torch.autocast(<span class="hljs-string">&quot;cuda&quot;</span>, dtype=config.autocast_dtype)<br>    <span class="hljs-keyword">else</span>:<br>        context_manager = contextlib.nullcontext()<br>    <span class="hljs-keyword">with</span> context_manager:<br>        <span class="hljs-keyword">if</span> checkpoint_activations_microbatch <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            output_tensor, loss_func = forward_step_func(data_iterator, model)<br>        <span class="hljs-keyword">else</span>:<br>            output_tensor, loss_func = forward_step_func(<br>                data_iterator, model, checkpoint_activations_microbatch<br>            )<br>    output_tensor, num_tokens = forward_step_calc_loss(<br>        model,<br>        output_tensor,<br>        loss_func,<br>        config,<br>        vp_stage,<br>        collect_non_loss_data,<br>        num_microbatches,<br>        forward_data_store,<br>        cp_group_size,<br>        is_last_stage,<br>    )<br><br>    <span class="hljs-keyword">if</span> unwrap_output_tensor:<br>        <span class="hljs-keyword">return</span> output_tensor, num_tokens<br>    <span class="hljs-keyword">return</span> [output_tensor], num_tokens<br><br></code></pre></td></tr></table></figure>

<h4 id="train-step代码-流程"><a href="#train-step代码-流程" class="headerlink" title="train_step代码&amp;流程"></a><code>train_step</code>代码&amp;流程</h4><p><code>train_step</code>的代码如下，它是训练循环里真正执行一次参数更新（或跳过更新） 的函数。它把 “梯度清零 → 前后向（可能重跑）→ optimizer.step → LR scheduler.step → 统计&#x2F;归约 loss” 串起来，并与 容错重跑机制、分布式优化器&#x2F;通信重叠、视觉预训练特殊逻辑交织在一起。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">train_step<br></code></pre></td></tr></table></figure>

<p>其流程如下：</p>
<ol>
<li><p>其计算Forward的代码外包了一层<code>while rerun_state_machine.should_run_forward_backward(data_iterator)</code>，这是为了当检测到某些 transient 错误或需要重新取 batch 时可以进行容错与重跑。具体Forward如下</p>
<ol>
<li><p>清空model和optimizer的grad</p>
</li>
<li><p>一些特殊逻辑，包含形状调整函数（仅 ModelOpt 蒸馏 + PP）和mxfp8 参数复用 grad buffer 时的参数 buffer 拷贝</p>
</li>
<li><p>调用<code>forward_backward_func</code>执行一次forward并拿到loss</p>
</li>
</ol>
</li>
<li><p>统一判断是否要 checkpoint或exit</p>
</li>
<li><p>清空cuda cache，判断是否要走视觉预训练的特殊逻辑</p>
</li>
<li><p>调用<code>optimizer.step()</code>更新模型参数</p>
</li>
<li><p>进一步调整opt_param_scheduler的学习率</p>
</li>
<li><p>如果当前是 pipeline last stage 则汇总 microbatches 的 loss，主要是用于日志数据记录等</p>
</li>
</ol>
<h2 id="evaluate-and-print-results"><a href="#evaluate-and-print-results" class="headerlink" title="evaluate_and_print_results"></a>evaluate_and_print_results</h2><p><code>evaluate_and_print_results</code>的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_and_print_results</span>(<span class="hljs-params"></span><br><span class="hljs-params">    prefix,</span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    data_iterator,</span><br><span class="hljs-params">    model,</span><br><span class="hljs-params">    iteration,</span><br><span class="hljs-params">    process_non_loss_data_func,</span><br><span class="hljs-params">    config,</span><br><span class="hljs-params">    verbose=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    write_to_tensorboard=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">    non_loss_data_func=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Helper function to evaluate and dump results on screen.&quot;&quot;&quot;</span><br>    args = get_args()<br>    <span class="hljs-keyword">if</span> write_to_tensorboard:<br>        writer = get_tensorboard_writer()<br>    <span class="hljs-keyword">else</span>:<br>        writer = <span class="hljs-literal">None</span><br><br>    wandb_writer = get_wandb_writer()<br><br>    data_iterators = data_iterator <span class="hljs-keyword">if</span> args.multiple_validation_sets <span class="hljs-keyword">else</span> [data_iterator]<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> args.multiple_validation_sets:<br>        eval_iters = [args.eval_iters]<br>    <span class="hljs-keyword">else</span>:<br>        eval_iters = args.eval_iters<br>        <br>    <span class="hljs-keyword">if</span> args.full_validation:<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(eval_iters) == <span class="hljs-built_in">len</span>(data_iterators)<br><br>        <span class="hljs-comment"># with full validation we need to distribute eval_iters to all ranks</span><br>        <span class="hljs-keyword">if</span> mpu.get_tensor_model_parallel_rank() == <span class="hljs-number">0</span>:<br>            eval_iters = torch.tensor(args.eval_iters, dtype=torch.long, device=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            eval_iters = torch.tensor([<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(eval_iters), dtype=torch.long, device=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>        torch.distributed.broadcast(eval_iters, <span class="hljs-number">0</span>)<br>        eval_iters = eval_iters.tolist()<br>        args.eval_iters = eval_iters[<span class="hljs-number">0</span>] <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> args.multiple_validation_sets <span class="hljs-keyword">else</span> eval_iters<br>    <br>    <span class="hljs-keyword">for</span> index, (iterator, iterations) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(data_iterators, eval_iters)):<br>        suffix = <span class="hljs-string">&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> args.multiple_validation_sets:<br>            suffix = <span class="hljs-string">f&quot;-<span class="hljs-subst">&#123;index&#125;</span>&quot;</span><br>        total_loss_dict, collected_non_loss_data, timelimit = evaluate(<br>            forward_step_func,<br>            iterator,<br>            model,<br>            process_non_loss_data_func,<br>            config,<br>            verbose,<br>            non_loss_data_func,<br>            eval_iters=iterations,<br>        )<br>        <span class="hljs-comment"># Timelimit hit during evaluation</span><br>        <span class="hljs-keyword">if</span> timelimit:<br>            <span class="hljs-keyword">return</span><br>        string = <span class="hljs-string">f&#x27; validation<span class="hljs-subst">&#123;suffix&#125;</span> loss at <span class="hljs-subst">&#123;prefix&#125;</span> | &#x27;</span><br>        <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> total_loss_dict:<br>            string += <span class="hljs-string">&#x27;&#123;&#125; value: &#123;:.6E&#125; | &#x27;</span>.<span class="hljs-built_in">format</span>(key, total_loss_dict[key].item())<br>            ppl = math.exp(<span class="hljs-built_in">min</span>(<span class="hljs-number">20</span>, total_loss_dict[key].item()))<br>            string += <span class="hljs-string">&#x27;&#123;&#125; PPL: &#123;:.6E&#125; | &#x27;</span>.<span class="hljs-built_in">format</span>(key, ppl)<br>            <span class="hljs-keyword">if</span> writer:<br>                writer.add_scalar(<span class="hljs-string">&#x27;&#123;&#125; validation&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(key, suffix), total_loss_dict[key].item(), iteration)<br>                writer.add_scalar(<br>                    <span class="hljs-string">&#x27;&#123;&#125; validation&#123;&#125; vs samples&#x27;</span>.<span class="hljs-built_in">format</span>(key, suffix),<br>                    total_loss_dict[key].item(),<br>                    args.consumed_train_samples,<br>                )<br>                <span class="hljs-keyword">if</span> args.log_validation_ppl_to_tensorboard:<br>                    writer.add_scalar(<span class="hljs-string">&#x27;&#123;&#125; validation&#123;&#125; ppl&#x27;</span>.<span class="hljs-built_in">format</span>(key, suffix), ppl, iteration)<br>                    writer.add_scalar(<br>                        <span class="hljs-string">&#x27;&#123;&#125; validation&#123;&#125; ppl vs samples&#x27;</span>.<span class="hljs-built_in">format</span>(key, suffix), ppl, args.consumed_train_samples<br>                    )<br>                <span class="hljs-keyword">if</span> wandb_writer <span class="hljs-keyword">and</span> is_last_rank():<br>                    wandb_writer.log(<br>                        &#123;<span class="hljs-string">&#x27;&#123;&#125; validation&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(key, suffix): total_loss_dict[key].item()&#125;, iteration<br>                    )<br><br>        <span class="hljs-keyword">if</span> process_non_loss_data_func <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> writer <span class="hljs-keyword">and</span> is_last_rank():<br>            process_non_loss_data_func(collected_non_loss_data, iteration, writer)<br><br>        length = <span class="hljs-built_in">len</span>(string) + <span class="hljs-number">1</span><br>        print_rank_last(<span class="hljs-string">&#x27;-&#x27;</span> * length)<br>        print_rank_last(string)<br>        print_rank_last(<span class="hljs-string">&#x27;-&#x27;</span> * length)<br><br></code></pre></td></tr></table></figure>

<p>其流程如下：</p>
<ol>
<li><p>进行TensorBoard 与 WandB的初始化</p>
</li>
<li><p>如果是多验证集，就获取对应的<code>eval_iters</code>列表</p>
</li>
<li><p>如果设置了<code>full_validation</code>，就把验证集发给所有rank</p>
</li>
<li><p>对每个验证集调用 evaluate(…)</p>
</li>
<li><p>打印对应的日志，写入到WandB</p>
</li>
<li><p>调用<code>process_non_loss_data_func</code>进行自定义的非 loss 数据的处理</p>
</li>
</ol>
<h3 id="evaluate"><a href="#evaluate" class="headerlink" title="evaluate"></a>evaluate</h3><p>evaluate的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params"></span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    data_iterator,</span><br><span class="hljs-params">    model,</span><br><span class="hljs-params">    process_non_loss_data_func,</span><br><span class="hljs-params">    config,</span><br><span class="hljs-params">    verbose=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    non_loss_data_func=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    eval_iters=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Evaluation.&quot;&quot;&quot;</span><br>    args = get_args()<br>    timers = get_timers()<br><br>    timers(<span class="hljs-string">&#x27;evaluate&#x27;</span>, log_level=<span class="hljs-number">0</span>).start(barrier=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">if</span> args.vision_pretraining <span class="hljs-keyword">and</span> args.vision_pretraining_type == <span class="hljs-string">&quot;dino&quot;</span>:<br>        <span class="hljs-keyword">from</span> megatron.legacy.model.vision.knn_monitor <span class="hljs-keyword">import</span> compute_feature_bank<br><br>        compute_feature_bank(model)<br><br>    <span class="hljs-comment"># Turn on evaluation mode which disables dropout.</span><br>    <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model:<br>        model_module.<span class="hljs-built_in">eval</span>()<br><br>    <span class="hljs-comment"># Disable result validation during evaluation</span><br>    rerun_state_machine = get_rerun_state_machine()<br>    rerun_mode = rerun_state_machine.get_mode()<br>    rerun_state_machine.set_mode(RerunMode.DISABLED)<br><br>    total_loss_dict = &#123;&#125;<br><br>    <span class="hljs-comment"># make validation batch size independent from training batch size</span><br>    eval_batch_size = args.global_batch_size<br>    eval_num_microbatches = eval_batch_size // (args.micro_batch_size * args.data_parallel_size)<br>    forward_backward_func = get_forward_backward_func()<br>    <span class="hljs-keyword">if</span> args.enable_cuda_graph <span class="hljs-keyword">and</span> args.cuda_graph_scope==<span class="hljs-string">&quot;full_iteration&quot;</span>:<br>        forward_backward_func = FullCudaGraphWrapper(forward_backward_func, cuda_graph_warmup_steps=args.cuda_graph_warmup_steps)<br><br>    <span class="hljs-keyword">if</span> eval_iters <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        eval_iters = args.eval_iters<br><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        iteration = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> verbose:<br>            print_rank_0(<span class="hljs-string">f&#x27;Evaluating on <span class="hljs-subst">&#123;eval_iters * eval_batch_size&#125;</span> samples&#x27;</span>)<br>        <span class="hljs-keyword">while</span> iteration &lt; eval_iters:<br>            iteration += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> verbose:<br>                print_rank_0(<span class="hljs-string">f&#x27;Evaluating iter <span class="hljs-subst">&#123;iteration&#125;</span>/<span class="hljs-subst">&#123;eval_iters&#125;</span>&#x27;</span>)<br><br>            <span class="hljs-comment"># Don&#x27;t care about timing during evaluation</span><br>            config.timers = <span class="hljs-literal">None</span><br>            ft_integration.on_eval_step_start()<br>            loss_dicts = forward_backward_func(<br>                forward_step_func=forward_step_func,<br>                data_iterator=data_iterator,<br>                model=model,<br>                num_microbatches=eval_num_microbatches,<br>                seq_length=args.seq_length,<br>                micro_batch_size=args.micro_batch_size,<br>                decoder_seq_length=args.decoder_seq_length,<br>                forward_only=<span class="hljs-literal">True</span>,<br>            )<br>            ft_integration.on_eval_step_end()<br>            config.timers = get_timers()<br><br>            <span class="hljs-comment"># Empty unused memory</span><br>            <span class="hljs-keyword">if</span> args.empty_unused_memory_level &gt;= <span class="hljs-number">1</span>:<br>                torch.cuda.empty_cache()<br><br>            <span class="hljs-keyword">if</span> mpu.is_pipeline_last_stage(ignore_virtual=<span class="hljs-literal">True</span>):<br>                <span class="hljs-comment"># Reduce across processes.</span><br>                <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> loss_dicts[<span class="hljs-number">0</span>].keys():<br>                    <span class="hljs-keyword">if</span> key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> total_loss_dict:<br>                        total_loss_dict[key] = torch.tensor(<br>                            [<span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>], dtype=torch.<span class="hljs-built_in">float</span><br>                        ).cuda()<br>                    val = [x[key].view(-<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> loss_dicts]<br><br>                    <span class="hljs-keyword">if</span> val[<span class="hljs-number">0</span>].numel() == <span class="hljs-number">2</span>:<br>                        <span class="hljs-keyword">if</span> args.sft:<br>                            <span class="hljs-comment"># normalize over micro batch instead of global</span><br>                            val = torch.vstack(val)<br>                            val = val[:, <span class="hljs-number">0</span>] / val[:, <span class="hljs-number">1</span>]<br>                            val = val.mean()<br>                            torch.distributed.all_reduce(<br>                                val,<br>                                group=mpu.get_data_parallel_group(with_context_parallel=<span class="hljs-literal">True</span>)<br>                            )<br>                            val /= torch.distributed.get_world_size(<br>                                group=mpu.get_data_parallel_group(with_context_parallel=<span class="hljs-literal">True</span>)<br>                            )<br>                            total_loss_dict[key][<span class="hljs-number">0</span>] += val<br>                            total_loss_dict[key][<span class="hljs-number">1</span>] += <span class="hljs-number">1</span><br>                        <span class="hljs-keyword">else</span> :<br>                            val = torch.vstack(val).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">0</span>)<br>                            torch.distributed.all_reduce(<br>                                val,<br>                                group=mpu.get_data_parallel_group(with_context_parallel=<span class="hljs-literal">True</span>)<br>                            )<br>                            total_loss_dict[key] += val<br>                    <span class="hljs-keyword">elif</span> val[<span class="hljs-number">0</span>].numel() == <span class="hljs-number">1</span>:<br>                        val = torch.cat(val).<span class="hljs-built_in">sum</span>()<br>                        total_loss_dict[key][<span class="hljs-number">0</span>] += val<br>                        total_loss_dict[key][<span class="hljs-number">1</span>] += <span class="hljs-built_in">len</span>(loss_dicts)<br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;Invalid value shape: <span class="hljs-subst">&#123;val[<span class="hljs-number">0</span>].shape&#125;</span> for key <span class="hljs-subst">&#123;key&#125;</span>&quot;</span>)<br><br>            args.consumed_valid_samples += eval_batch_size<br><br>            <span class="hljs-keyword">if</span> args.exit_duration_in_mins:<br>                train_time = (time.time() - _TRAIN_START_TIME) / <span class="hljs-number">60.0</span><br>                done_cuda = torch.tensor(<br>                    [train_time &gt; args.exit_duration_in_mins], dtype=torch.<span class="hljs-built_in">int</span>, device=<span class="hljs-string">&#x27;cuda&#x27;</span><br>                )<br>                torch.distributed.all_reduce(done_cuda, op=torch.distributed.ReduceOp.MAX)<br>                done = done_cuda.item()<br>                <span class="hljs-keyword">if</span> done:<br>                    rerun_state_machine.set_mode(rerun_mode)<br>                    print_rank_0(<span class="hljs-string">&#x27;Exiting during evaluation, timelimit reached&#x27;</span>)<br>                    <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">True</span><br><br>        collected_non_loss_data = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> non_loss_data_func <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            collected_non_loss_data = non_loss_data_func(model)<br>        <span class="hljs-keyword">elif</span> process_non_loss_data_func <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> is_last_rank():<br>            collected_non_loss_data = forward_backward_func(<br>                forward_step_func=forward_step_func,<br>                data_iterator=data_iterator,<br>                model=model,<br>                num_microbatches=get_num_microbatches(),<br>                seq_length=args.seq_length,<br>                micro_batch_size=args.micro_batch_size,<br>                decoder_seq_length=args.decoder_seq_length,<br>                forward_only=<span class="hljs-literal">True</span>,<br>                collect_non_loss_data=<span class="hljs-literal">True</span>,<br>            )<br><br>    <span class="hljs-comment"># Move model back to the train mode.</span><br>    <span class="hljs-keyword">for</span> model_module <span class="hljs-keyword">in</span> model:<br>        model_module.train()<br><br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> total_loss_dict:<br>        numerator, denominator = total_loss_dict[key]<br>        total_loss_dict[key] = numerator / denominator<br><br>    timers(<span class="hljs-string">&#x27;evaluate&#x27;</span>).stop()<br>    timers.log([<span class="hljs-string">&#x27;evaluate&#x27;</span>])<br><br>    rerun_state_machine.set_mode(rerun_mode)<br><br>    rerun_state_machine.set_mode(rerun_mode)<br><br>    <span class="hljs-keyword">return</span> total_loss_dict, collected_non_loss_data, <span class="hljs-literal">False</span><br><br></code></pre></td></tr></table></figure>

<p>其整体流程如下所示：</p>
<ol>
<li><p>切换模型到 eval()，并临时禁用<code>rerun_state_machine</code></p>
</li>
<li><p>计算evaluate用的 microbatch 大小</p>
</li>
<li><p>与train类似，得到<code>forward_backward_func</code></p>
</li>
<li><p>在no_grad下依次执行各个eval_iter：</p>
<ol>
<li><p>调用<code>forward_backward_func</code>，注意调用时专门设置了forward_only&#x3D;True</p>
</li>
<li><p>在 pipeline last stage 收集与规约 loss</p>
</li>
<li><p>通过评估样本计数与 time limit 计算是否需要提前退出</p>
</li>
<li><p>执行<code>non_loss_data_func</code>与<code>forward_backward_func</code></p>
</li>
</ol>
</li>
<li><p>将model恢复训练模式，并把累计的 numerator&#x2F;denominator 转成标量</p>
</li>
<li><p>恢复<code>rerun_state_machine</code>、返回结果</p>
</li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/LLM/" class="category-chain-item">LLM</a>
  
  
    <span>></span>
    
  <a href="/categories/LLM/Megatron-LM/" class="category-chain-item">Megatron-LM</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/LLM/" class="print-no-link">#LLM</a>
      
        <a href="/tags/Megatron-LM/" class="print-no-link">#Megatron-LM</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>【Megatron-LM源码分析（二）】-GPT模型pretrain流程</div>
      <div>http://example.com/2025/12/22/megatron-lm-pre-train-process/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>滑滑蛋</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年12月22日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/12/22/ScheMoE-paper-note/" title="【论文阅读】ScheMoE:An Extensible Mixture-of-Experts  Distributed Training System with Tasks Scheduling">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">【论文阅读】ScheMoE:An Extensible Mixture-of-Experts  Distributed Training System with Tasks Scheduling</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/12/14/megatron-lm-env-and-example/" title="【Megatron-LM源码分析（一）】-环境配置与训练示例跑通">
                        <span class="hidden-mobile">【Megatron-LM源码分析（一）】-环境配置与训练示例跑通</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"WMtHomhQYlrbIodTwoPU3gTY-MdYXbMMI","appKey":"pZeun9WfI1yaQrIoUbvTQrXv","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"https://wmthomhq.api.lncldglobal.com","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         次
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
