

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.jpg">
  <link rel="icon" href="/img/favicon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#1C262C">
  <meta name="author" content="滑滑蛋">
  <meta name="keywords" content="">
  
    <meta name="description" content="数据并行存在多种，最简单的就是DDP，每个DP都拥有完整的模型，然后在每个batch结束后在DP间同步梯度，最后统一进行优化器更新。再复杂一些的数据宾县会上ZERO技术，将模型、梯度、优化器状态等进行切分。 这里主要关注最简单的DDP，主要关注DP并行组如何划分，如果做个各个DP读取不同的数据，以及训练过程中如何做到梯度同步。 DP并行组查看megatron&#x2F;core&#x2F;parallel_state">
<meta property="og:type" content="article">
<meta property="og:title" content="【Megatron-LM源码分析（四）】-DDP数据并行">
<meta property="og:url" content="http://example.com/2025/12/28/megatron-lm-ddp/index.html">
<meta property="og:site_name" content="滑滑蛋的个人博客">
<meta property="og:description" content="数据并行存在多种，最简单的就是DDP，每个DP都拥有完整的模型，然后在每个batch结束后在DP间同步梯度，最后统一进行优化器更新。再复杂一些的数据宾县会上ZERO技术，将模型、梯度、优化器状态等进行切分。 这里主要关注最简单的DDP，主要关注DP并行组如何划分，如果做个各个DP读取不同的数据，以及训练过程中如何做到梯度同步。 DP并行组查看megatron&#x2F;core&#x2F;parallel_state">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2025/12/28/megatron-lm-ddp/image.png">
<meta property="og:image" content="http://example.com/2025/12/28/megatron-lm-ddp/image-1.png">
<meta property="article:published_time" content="2025-12-28T07:50:09.000Z">
<meta property="article:modified_time" content="2025-12-30T07:58:32.161Z">
<meta property="article:author" content="滑滑蛋">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Megatron-LM">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2025/12/28/megatron-lm-ddp/image.png">
  
  
  
  <title>【Megatron-LM源码分析（四）】-DDP数据并行 - 滑滑蛋的个人博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"d38d21fca521d897798e5bdd940a90d0","google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"WMtHomhQYlrbIodTwoPU3gTY-MdYXbMMI","app_key":"pZeun9WfI1yaQrIoUbvTQrXv","server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?d38d21fca521d897798e5bdd940a90d0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', '');
        });
      }
    </script>
  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>滑滑蛋</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="【Megatron-LM源码分析（四）】-DDP数据并行"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-12-28 15:50" pubdate>
          2025年12月28日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          85k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          426 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">【Megatron-LM源码分析（四）】-DDP数据并行</h1>
            
            
              <div class="markdown-body">
                
                <p>数据并行存在多种，最简单的就是DDP，每个DP都拥有完整的模型，然后在每个batch结束后在DP间同步梯度，最后统一进行优化器更新。再复杂一些的数据宾县会上ZERO技术，将模型、梯度、优化器状态等进行切分。</p>
<p>这里主要关注最简单的DDP，主要关注DP并行组如何划分，如果做个各个DP读取不同的数据，以及训练过程中如何做到梯度同步。</p>
<h1 id="DP并行组"><a href="#DP并行组" class="headerlink" title="DP并行组"></a>DP并行组</h1><p>查看<code>megatron/core/parallel_state.py</code>中的<code>initialize_model_parallel</code>函数，可以看到其model_size计算的公式为<code>model_size = tensor_model_parallel_size * pipeline_model_parallel_size * context_parallel_size</code>，然后其<code>data_parallel_size</code>并不是直接定义出来的，而是通过<code>data_parallel_size: int = world_size // model_size</code>计算出来。</p>
<p>其构建DP并行组的代码如下，这里是通过<code>decoder_rank_generator</code>将所有除所属dp不同外都相同的rank组成一个dp group通信组：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-keyword">for</span> ranks <span class="hljs-keyword">in</span> decoder_rank_generator.get_ranks(<span class="hljs-string">&#x27;dp&#x27;</span>):<br>    group = create_group(<br>        ranks,<br>        <span class="hljs-built_in">timeout</span>=<span class="hljs-built_in">timeout</span>,<br>        pg_options=get_nccl_options(<span class="hljs-string">&quot;dp&quot;</span>, nccl_comm_cfgs),<br>        group_desc=<span class="hljs-string">&quot;DATA_PARALLEL_GROUP&quot;</span>,<br>    )<br>    <span class="hljs-keyword">if</span> create_gloo_process_groups:<br>        group_gloo = create_group(<br>            ranks, <span class="hljs-built_in">timeout</span>=<span class="hljs-built_in">timeout</span>, backend=<span class="hljs-string">&quot;gloo&quot;</span>, group_desc=<span class="hljs-string">&quot;DATA_PARALLEL_GROUP_GLOO&quot;</span><br>        )<br>    <span class="hljs-keyword">else</span>:<br>        group_gloo = None<br>    <span class="hljs-keyword">if</span> rank <span class="hljs-keyword">in</span> ranks:<br>        _DATA_PARALLEL_GROUP = group<br>        _DATA_PARALLEL_GROUP_GLOO = group_gloo<br>        _DATA_PARALLEL_GLOBAL_RANKS = ranks<br></code></pre></td></tr></table></figure>

<h1 id="数据切分"><a href="#数据切分" class="headerlink" title="数据切分"></a>数据切分</h1><p>在DP并行下，每个dp都应该获得不同的数据。</p>
<h2 id="数据集读取器构造流程"><a href="#数据集读取器构造流程" class="headerlink" title="数据集读取器构造流程"></a>数据集读取器构造流程</h2><p>数据集读取器的整体构造流程为：</p>
<ol>
<li><p>用户提供数据集原始文件，以及token化所需要的merge和vocab文件，然后提供split划分train、valid、test的比例，并设置<code>micro-batch-size</code>和<code>global-batch-size</code></p>
</li>
<li><p>用户通过定义<code>BlendedMegatronDatasetBuilder</code>得到将文件转化为数据集的train_ds, valid_ds, test_ds</p>
</li>
<li><p>然后将这些数据集转化为支持迭代获取一批一批数据的rain_dataloader, valid_dataloaders, test_dataloader</p>
</li>
</ol>
<p>Megatron-LM构建数据集读取器的具体构造顺序如下：</p>
<ol>
<li>用户需要自定义一个<code>train_valid_test_datasets_provider</code>，在该函数中构建<code>BlendedMegatronDatasetBuilder</code>获得train_ds, valid_ds, test_ds并返回，这类ds对原始数据集文件进行了包裹，支持从中读取数据。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">is_dataset_built_on_rank</span>():<br>    <span class="hljs-keyword">return</span> (<br>        parallel_state.is_pipeline_first_stage(ignore_virtual=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">or</span> parallel_state.is_pipeline_last_stage(ignore_virtual=<span class="hljs-literal">True</span>)<br>    ) <span class="hljs-keyword">and</span> parallel_state.get_tensor_model_parallel_rank() == <span class="hljs-number">0</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">core_gpt_dataset_config_from_args</span>(<span class="hljs-params">args</span>):<br>    tokenizer = get_tokenizer()<br><br>    <span class="hljs-comment"># Sometimes --data-path is too long, instead we parse it from a file.</span><br>    blend: <span class="hljs-type">Optional</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>], <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]]]]<br>    blend_per_split: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-type">Optional</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>], <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]]]]]]<br>    blend, blend_per_split = get_blend_and_blend_per_split(args)<br><br>    <span class="hljs-keyword">return</span> GPTDatasetConfig(<br>        random_seed=args.seed,<br>        sequence_length=args.seq_length,<br>        blend=blend,<br>        blend_per_split=blend_per_split,<br>        split=args.split,<br>        multiple_validation_sets=args.multiple_validation_sets,<br>        full_validation=args.full_validation,<br>        num_dataset_builder_threads=args.num_dataset_builder_threads,<br>        path_to_cache=args.data_cache_path,<br>        mmap_bin_files=args.mmap_bin_files,<br>        tokenizer=tokenizer,<br>        reset_position_ids=args.reset_position_ids,<br>        reset_attention_mask=args.reset_attention_mask,<br>        eod_mask_loss=args.eod_mask_loss,<br>        create_attention_mask=args.create_attention_mask_in_dataloader,<br>        object_storage_cache_path=args.object_storage_cache_path,<br>        mid_level_dataset_surplus=args.mid_level_dataset_surplus,<br>    )<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_valid_test_datasets_provider</span>(<span class="hljs-params">train_val_test_num_samples</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build the train test and validation datasets.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        train_val_test_num_samples : A list containing the number of samples in train test and validation.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    args = get_args()<br><br>    config = core_gpt_dataset_config_from_args(args)<br><br>    <span class="hljs-keyword">if</span> args.sft:<br>        dataset_type = SFTDataset<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">if</span> args.mock_data:<br>            dataset_type = MockGPTDataset<br>        <span class="hljs-keyword">else</span>:<br>            dataset_type = GPTDataset<br><br>    print_rank_0(<span class="hljs-string">&quot;&gt; building train, validation, and test datasets for GPT ...&quot;</span>)<br><br>    train_ds, valid_ds, test_ds = BlendedMegatronDatasetBuilder(<br>        dataset_type, train_val_test_num_samples, is_dataset_built_on_rank, config<br>    ).build()<br><br>    print_rank_0(<span class="hljs-string">&quot;&gt; finished creating GPT datasets ...&quot;</span>)<br><br>    <span class="hljs-keyword">return</span> train_ds, valid_ds, test_ds<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>将自定义的<code>train_valid_test_datasets_provider</code>作为参数传递给<code>pretrain</code>核心训练函数，然后在<code>pretrain</code>中借助<code>build_train_valid_test_data_iterators</code>函数构造出数据集迭代器：train_data_iterator,valid_data_iterator, test_data_iterator。注意如果是pp并行中启用了vp，那么需要多个iterators。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br><br>    <span class="hljs-comment"># Temporary for transition to core datasets</span><br>    train_valid_test_datasets_provider.is_distributed = <span class="hljs-literal">True</span><br><br>    <span class="hljs-comment"># Optionally enable inprocess restart on pretrain</span><br>    pretrain, store = inprocess_restart.maybe_wrap_for_inprocess_restart(pretrain)<br><br>    pretrain(<br>        train_valid_test_datasets_provider,<br>        model_provider,<br>        ModelType.encoder_or_decoder,<br>        forward_step,<br>        args_defaults=&#123;<span class="hljs-string">&#x27;tokenizer_type&#x27;</span>: <span class="hljs-string">&#x27;GPT2BPETokenizer&#x27;</span>&#125;,<br>        extra_args_provider=add_modelopt_args <span class="hljs-keyword">if</span> has_nvidia_modelopt <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>        store=store,<br>    )<br><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pretrain</span>(<span class="hljs-params">...</span>):<br>    ...<br>    <span class="hljs-keyword">if</span> args.virtual_pipeline_model_parallel_size <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        train_data_iterator = []<br>        valid_data_iterator = []<br>        test_data_iterator = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(model)):<br>            iterators = build_train_valid_test_data_iterators(train_valid_test_dataset_provider)<br>            train_data_iterator.append(iterators[<span class="hljs-number">0</span>])<br>            valid_data_iterator.append(iterators[<span class="hljs-number">1</span>])<br>            test_data_iterator.append(iterators[<span class="hljs-number">2</span>])<br>    <span class="hljs-keyword">else</span>:<br>        train_data_iterator, valid_data_iterator, test_data_iterator = (<br>            build_train_valid_test_data_iterators(train_valid_test_dataset_provider)<br>        )<br>    ...<br><br></code></pre></td></tr></table></figure>

<ol>
<li><p><code>build_train_valid_test_data_iterators</code>中首先会通过<code>build_train_valid_test_data_loaders</code>获取 train_dataloader, valid_dataloaders, test_dataloader</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_train_valid_test_data_iterators</span>(<span class="hljs-params">build_train_valid_test_datasets_provider</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build pretraining data iterators.&quot;&quot;&quot;</span><br><br>    args = get_args()<br><br>    <span class="hljs-comment"># Build loaders.</span><br>    train_dataloader, valid_dataloaders, test_dataloader = build_train_valid_test_data_loaders(<br>        build_train_valid_test_datasets_provider<br>    )<br><br>    <span class="hljs-comment"># Build iterators.</span><br>    dl_type = args.dataloader_type<br>    <span class="hljs-keyword">assert</span> dl_type <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;single&#x27;</span>, <span class="hljs-string">&#x27;cyclic&#x27;</span>, <span class="hljs-string">&#x27;external&#x27;</span>]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_iterator</span>(<span class="hljs-params">dataloader_type, dataloader</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Return dataset iterator.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> dataloader_type == <span class="hljs-string">&quot;single&quot;</span>:<br>            <span class="hljs-keyword">return</span> RerunDataIterator(<span class="hljs-built_in">iter</span>(dataloader))<br>        <span class="hljs-keyword">elif</span> dataloader_type == <span class="hljs-string">&quot;cyclic&quot;</span>:<br>            <span class="hljs-keyword">return</span> RerunDataIterator(<span class="hljs-built_in">iter</span>(cyclic_iter(dataloader)))<br>        <span class="hljs-keyword">elif</span> dataloader_type == <span class="hljs-string">&quot;external&quot;</span>:<br>            <span class="hljs-comment"># External dataloader is passed through. User is expected to define how to iterate.</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(dataloader, <span class="hljs-built_in">list</span>):<br>                <span class="hljs-keyword">return</span> [RerunDataIterator(d) <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> dataloader]<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">return</span> RerunDataIterator(dataloader)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&quot;unexpected dataloader type&quot;</span>)<br><br>    <span class="hljs-keyword">if</span> train_dataloader <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        train_data_iterator = _get_iterator(dl_type, train_dataloader)<br>    <span class="hljs-keyword">else</span>:<br>        train_data_iterator = <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># when using full validation, we need to override eval iters with the correct</span><br>    <span class="hljs-comment"># number of iterations on tp rank 0 so that it can be distributed to the other </span><br>    <span class="hljs-comment"># ranks later</span><br>    <span class="hljs-keyword">if</span> args.full_validation:<br>        <span class="hljs-keyword">if</span> args.multiple_validation_sets:<br>            <span class="hljs-keyword">if</span> valid_dataloaders[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                args.eval_iters = [<span class="hljs-literal">None</span>]*<span class="hljs-built_in">len</span>(valid_dataloaders)<br>            <span class="hljs-keyword">else</span>:<br>                args.eval_iters = [<span class="hljs-built_in">len</span>(dl) <span class="hljs-keyword">for</span> dl <span class="hljs-keyword">in</span> valid_dataloaders]<br>        <span class="hljs-keyword">else</span>:<br>            args.eval_iters = <span class="hljs-built_in">len</span>(valid_dataloaders[<span class="hljs-number">0</span>])<br><br>    <span class="hljs-keyword">if</span> args.multiple_validation_sets:<br>        <span class="hljs-keyword">if</span> valid_dataloaders[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            valid_data_iterators = [<span class="hljs-literal">None</span>] * <span class="hljs-built_in">len</span>(valid_dataloaders)<br>        <span class="hljs-keyword">else</span>:<br>            valid_dl_type = <span class="hljs-string">&quot;cyclic&quot;</span> <span class="hljs-keyword">if</span> args.full_validation <span class="hljs-keyword">else</span> dl_type<br>            <span class="hljs-built_in">print</span>(<br>                <span class="hljs-string">f&quot;[VALID DATA LOADER LENGTHS] &quot;</span><br>                <span class="hljs-string">&quot;, &quot;</span>.join(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;idx&#125;</span>: <span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(dl)&#125;</span>&quot;</span> <span class="hljs-keyword">for</span> idx, dl <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(valid_dataloaders))<br>            )<br>            valid_data_iterators = [<br>                _get_iterator(valid_dl_type, dl) <span class="hljs-keyword">for</span> dl <span class="hljs-keyword">in</span> valid_dataloaders<br>            ]<br>    <span class="hljs-keyword">elif</span> valid_dataloaders[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        valid_data_iterators = _get_iterator(dl_type, valid_dataloaders[<span class="hljs-number">0</span>])<br>    <span class="hljs-keyword">else</span>:<br>        valid_data_iterators = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">if</span> test_dataloader <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        test_data_iterator = _get_iterator(dl_type, test_dataloader)<br>    <span class="hljs-keyword">else</span>:<br>        test_data_iterator = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">return</span> train_data_iterator, valid_data_iterators, test_data_iterator<br><br></code></pre></td></tr></table></figure>

<ol>
<li><code>build_train_valid_test_data_loaders</code>函数如下所示：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_train_valid_test_data_loaders</span>(<span class="hljs-params">build_train_valid_test_datasets_provider</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build pretraining data loaders.&quot;&quot;&quot;</span><br><br>    args = get_args()<br><br>    (train_dataloader, valid_dataloaders, test_dataloader) = (<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>)<br><br>    print_rank_0(<span class="hljs-string">&#x27;&gt; building train, validation, and test datasets ...&#x27;</span>)<br><br>    <span class="hljs-comment"># Backward compatibility, assume fixed batch size.</span><br>    <span class="hljs-keyword">if</span> args.iteration &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> args.consumed_train_samples == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">assert</span> (<br>            args.train_samples <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>        ), <span class="hljs-string">&#x27;Only backward compatiblity support for iteration-based training&#x27;</span><br>        args.consumed_train_samples = args.iteration * args.global_batch_size<br>    <span class="hljs-keyword">if</span> args.iteration &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> args.consumed_valid_samples == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">if</span> args.train_samples <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            args.consumed_valid_samples = (<br>                (args.iteration // args.eval_interval) * args.eval_iters * args.global_batch_size<br>            )<br><br>    <span class="hljs-comment"># Rely on distributed-aware core datasets, temporary</span><br>    is_distributed = <span class="hljs-built_in">getattr</span>(build_train_valid_test_datasets_provider, <span class="hljs-string">&quot;is_distributed&quot;</span>, <span class="hljs-literal">False</span>)<br><br>    <span class="hljs-comment"># Construct the data pipeline</span><br>    <span class="hljs-keyword">if</span> is_distributed <span class="hljs-keyword">or</span> mpu.get_tensor_model_parallel_rank() == <span class="hljs-number">0</span>:<br><br>        <span class="hljs-comment"># Build datasets.</span><br>        train_ds, valid_ds, test_ds = build_train_valid_test_datasets(<br>            build_train_valid_test_datasets_provider<br>        )<br>        valid_ds = [valid_ds] <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(valid_ds, <span class="hljs-built_in">list</span>) <span class="hljs-keyword">else</span> valid_ds<br>        <br>        <span class="hljs-comment"># Build dataloders.</span><br>        train_dataloader = build_pretraining_data_loader(train_ds, args.consumed_train_samples)<br><br>        valid_dataloaders = []<br>        <span class="hljs-keyword">for</span> valid_d <span class="hljs-keyword">in</span> valid_ds:<br>            <span class="hljs-keyword">if</span> args.skip_train <span class="hljs-keyword">or</span> args.full_validation:<br>                valid_dataloaders.append(build_pretraining_data_loader(valid_d, <span class="hljs-number">0</span>))<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-keyword">if</span> args.multiple_validation_sets:<br>                    <span class="hljs-comment"># TODO(bnorick): for multiple validation sets without full validation, args.consumed_valid_samples is not</span><br>                    <span class="hljs-comment"># correct and needs to be calculated/set per validation set</span><br>                    <span class="hljs-keyword">raise</span> NotImplementedError(<span class="hljs-string">&quot;--multiple-validation-sets currently requires --full-validation&quot;</span>)<br>                valid_dataloaders.append(build_pretraining_data_loader(valid_d, args.consumed_valid_samples))<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> args.multiple_validation_sets:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(valid_dataloaders) == <span class="hljs-number">1</span><br>        test_dataloader = build_pretraining_data_loader(test_ds, <span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># Flags to know if we need to do training/validation/testing.</span><br>        do_train = train_dataloader <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> args.train_iters &gt; <span class="hljs-number">0</span><br>        do_valid = valid_dataloaders <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> (args.full_validation <span class="hljs-keyword">or</span> args.eval_iters &gt; <span class="hljs-number">0</span>)<br>        do_test = test_dataloader <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> (args.full_validation <span class="hljs-keyword">or</span> args.eval_iters &gt; <span class="hljs-number">0</span>)<br>        flags = torch.tensor(<br>            [<span class="hljs-built_in">int</span>(do_train), <span class="hljs-built_in">int</span>(do_valid), <span class="hljs-built_in">int</span>(do_test)], dtype=torch.long, device=<span class="hljs-string">&#x27;cuda&#x27;</span><br>        )<br>    <span class="hljs-keyword">else</span>:<br>        flags = torch.tensor([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], dtype=torch.long, device=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br><br>    torch.distributed.broadcast(flags, <span class="hljs-number">0</span>)<br><br>    args.do_train = <span class="hljs-built_in">getattr</span>(args, <span class="hljs-string">&quot;do_train&quot;</span>, <span class="hljs-literal">False</span>) <span class="hljs-keyword">or</span> flags[<span class="hljs-number">0</span>].item()<br>    args.do_valid = <span class="hljs-built_in">getattr</span>(args, <span class="hljs-string">&quot;do_valid&quot;</span>, <span class="hljs-literal">False</span>) <span class="hljs-keyword">or</span> flags[<span class="hljs-number">1</span>].item()<br>    args.do_test = <span class="hljs-built_in">getattr</span>(args, <span class="hljs-string">&quot;do_test&quot;</span>, <span class="hljs-literal">False</span>) <span class="hljs-keyword">or</span> flags[<span class="hljs-number">2</span>].item()<br><br>    <span class="hljs-keyword">return</span> train_dataloader, valid_dataloaders, test_dataloader<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>其首先会补全出当前已消耗的train、valid样本数量，以避免断点重训后还使用同样的数据。</p>
</li>
<li><p>然后获取<code>build_train_valid_test_datasets_provider</code>的<code>is_distributed</code>参数，如果为true才认为需要构建数据集。</p>
</li>
<li><p>然后其调用<code>build_train_valid_test_datasets</code>，通过简单计算train、valid、test需要的样本数量借助用户自定义的<code>train_valid_test_datasets_provider</code>获得train_ds, valid_ds, test_ds</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_train_valid_test_datasets</span>(<span class="hljs-params">build_train_valid_test_datasets_provider</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build pretraining datasets.&quot;&quot;&quot;</span><br>    train_valid_test_num_samples = get_train_valid_test_num_samples()<br>    print_rank_0(<span class="hljs-string">&#x27; &gt; datasets target sizes (minimum size):&#x27;</span>)<br>    print_rank_0(<span class="hljs-string">&#x27;    train:      &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(train_valid_test_num_samples[<span class="hljs-number">0</span>]))<br>    print_rank_0(<span class="hljs-string">&#x27;    validation: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(train_valid_test_num_samples[<span class="hljs-number">1</span>]))<br>    print_rank_0(<span class="hljs-string">&#x27;    test:       &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(train_valid_test_num_samples[<span class="hljs-number">2</span>]))<br>    <span class="hljs-keyword">return</span> build_train_valid_test_datasets_provider(train_valid_test_num_samples)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_train_valid_test_num_samples</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;Train/valid/test num samples.&quot;&quot;&quot;</span><br><br>    args = get_args()<br><br>    <span class="hljs-comment"># Number of train/valid/test samples.</span><br>    <span class="hljs-keyword">if</span> args.train_samples:<br>        train_samples = args.train_samples<br>    <span class="hljs-keyword">else</span>:<br>        train_samples = args.train_iters * args.global_batch_size<br>    <span class="hljs-keyword">if</span> args.full_validation:<br>        eval_samples = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">else</span>:<br>        eval_iters = (args.train_iters // args.eval_interval + <span class="hljs-number">1</span>) * args.eval_iters<br>        eval_samples = eval_iters * args.global_batch_size<br>    test_iters = args.eval_iters<br><br>    <span class="hljs-keyword">return</span> (train_samples, eval_samples, test_iters * args.global_batch_size)<br></code></pre></td></tr></table></figure>

<ul>
<li><p>然后再将train_ds, valid_ds, test_ds与当前已消耗的train、valid样本数量结合利用<code>build_pretraining_data_loader</code>构造出train_dataloader、valid_dataloaders、test_dataloader ，如果对应的dataloader不为空就设置对应args.do_train、args.do_valid、args.do_test。</p>
<ol>
<li><p><code>build_pretraining_data_loader</code>代码如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_pretraining_data_loader</span>(<span class="hljs-params">dataset, consumed_samples</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Build dataloader given an input dataset.&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">if</span> dataset <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br>    args = get_args()<br>    <br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(dataset,<span class="hljs-string">&#x27;split&#x27;</span>):<br>        split = dataset.split<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">hasattr</span>(dataset,<span class="hljs-string">&#x27;index_split&#x27;</span>):<br>        split = dataset.index_split<br>    <span class="hljs-keyword">else</span>:<br>        split = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">if</span> split == Split.valid <span class="hljs-keyword">and</span> args.full_validation:<br>        batch_sampler = MegatronPretrainingSampler(<br>            total_samples=<span class="hljs-built_in">len</span>(dataset),<br>            consumed_samples=<span class="hljs-number">0</span>,<br>            micro_batch_size=args.micro_batch_size,<br>            data_parallel_rank=mpu.get_data_parallel_rank(),<br>            data_parallel_size=mpu.get_data_parallel_world_size())<br>    <span class="hljs-keyword">elif</span> args.dataloader_type == <span class="hljs-string">&#x27;single&#x27;</span>:<br>        <span class="hljs-comment"># Megatron sampler</span><br>        batch_sampler = MegatronPretrainingSampler(<br>            total_samples=<span class="hljs-built_in">len</span>(dataset),<br>            consumed_samples=consumed_samples,<br>            micro_batch_size=args.micro_batch_size,<br>            data_parallel_rank=mpu.get_data_parallel_rank(),<br>            data_parallel_size=mpu.get_data_parallel_world_size())<br>    <span class="hljs-keyword">elif</span> args.dataloader_type == <span class="hljs-string">&#x27;cyclic&#x27;</span>:<br>        batch_sampler = MegatronPretrainingRandomSampler(<br>            dataset,<br>            total_samples=<span class="hljs-built_in">len</span>(dataset),<br>            consumed_samples=consumed_samples,<br>            micro_batch_size=args.micro_batch_size,<br>            data_parallel_rank=mpu.get_data_parallel_rank(),<br>            data_parallel_size=mpu.get_data_parallel_world_size(),<br>            data_sharding=args.data_sharding)<br>    <span class="hljs-keyword">elif</span> args.dataloader_type == <span class="hljs-string">&quot;external&quot;</span>:<br>        <span class="hljs-comment"># External dataloaders are passed through. User is expected to provide a</span><br>        <span class="hljs-comment"># torch-compatible dataloader and define samplers, if needed.</span><br>        <span class="hljs-keyword">return</span> dataset<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">&#x27;&#123;&#125; dataloader type is not supported.&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                args.dataloader_type))<br><br>    <span class="hljs-comment"># Torch dataloader.</span><br>    <span class="hljs-keyword">return</span> torch.utils.data.DataLoader(dataset,<br>                                       batch_sampler=batch_sampler,<br>                                       num_workers=args.num_workers,<br>                                       pin_memory=<span class="hljs-literal">True</span>,<br>                                       persistent_workers=<span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> args.num_workers &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span>,<br>                                       )<br></code></pre></td></tr></table></figure>
</li>
<li><p>这里有Smapler，其中MegatronPretrainingSampler是按序读取、可恢复的Sampler，MegatronPretrainingRandomSampler是随机采样（基于当前epoch做随机数）、可无限循环的Smapler。</p>
</li>
<li><p>最后基于Samplerydataset返回了标准的<code>torch.utils.data.DataLoader</code></p>
</li>
<li><p>注意这里对于valid数据集且需要full_validation，即数据全跑一遍的情况，构建了consumed_samples&#x3D;0的MegatronPretrainingSampler。此外也支持通过<code>args.dataloader_type == &quot;external&quot;</code>自定义Dataloader</p>
</li>
</ol>
</li>
</ul>
</li>
<li><p>如果dataloader非空就通过<code>_get_iterator</code>依据<code>dataloader_type</code>对齐进行包装，包装成<code>RerunDataIterator</code>以支持rerun容错重跑</p>
</li>
<li><p>注意对于valid_dataloaders，如果参数配置了<code>full_validation</code>需要更新<code>eval_iters</code>为全部的iters</p>
</li>
</ol>
</li>
<li><p>train_data_iterator与valid_data_iterator会被传入到<code>train</code>函数中进行训练，test_data_iterator会在训练完后如果配置了<code>args.do_test</code>就最最终的测试。</p>
</li>
</ul>
<h2 id="数据集构造关键类介绍"><a href="#数据集构造关键类介绍" class="headerlink" title="数据集构造关键类介绍"></a>数据集构造关键类介绍</h2><h3 id="BlendedMegatronDatasetBuilder"><a href="#BlendedMegatronDatasetBuilder" class="headerlink" title="BlendedMegatronDatasetBuilder"></a><code>BlendedMegatronDatasetBuilder</code></h3><p><code>BlendedMegatronDatasetBuilder</code>主要是支持数据集混合功能，例如将常识数据集与代码数据集混合，其代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BlendedMegatronDatasetBuilder</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Builder class for the BlendedDataset and MegatronDataset classes</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        cls (Type[MegatronDataset]): The class to instantiate, must inherit from MegatronDataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        sizes (List[Optional[int]]): The minimum total number of samples to draw, or None, per split</span><br><span class="hljs-string"></span><br><span class="hljs-string">        is_built_on_rank (Callable): A callable which returns True if the dataset should be built on</span><br><span class="hljs-string">            the current rank and False otherwise. It should be Megatron Core parallelism aware i.e.</span><br><span class="hljs-string">            global rank, local group rank, and virtual rank may inform its return value.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        config (BlendedMegatronDatasetConfig): The config object which informs dataset creation</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        cls: <span class="hljs-type">Type</span>[MidLevelDataset],</span><br><span class="hljs-params">        sizes: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>],</span><br><span class="hljs-params">        is_built_on_rank: <span class="hljs-type">Callable</span>,</span><br><span class="hljs-params">        config: BlendedMegatronDatasetConfig,</span><br><span class="hljs-params">    </span>):<br>        self.cls = cls<br>        self.sizes = sizes<br>        self.is_built_on_rank = is_built_on_rank<br>        self.config = config<br><br>        log_single_rank(<br>            logger,<br>            logging.INFO,<br>            <span class="hljs-string">f&quot;Building <span class="hljs-subst">&#123;cls.__name__&#125;</span> splits with sizes=<span class="hljs-subst">&#123;self.sizes&#125;</span> and config=<span class="hljs-subst">&#123;self.config&#125;</span>&quot;</span>,<br>        )<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.config.mock:<br>            <span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> Split:<br>                size_is_none = self.sizes[split.value] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">if</span> self.config.blend_per_split <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                    weights_are_none = self.config.blend[<span class="hljs-number">1</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">if</span> self.config.blend_per_split[split.value] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                        <span class="hljs-keyword">continue</span><br>                    weights_are_none = self.config.blend_per_split[split.value][<span class="hljs-number">1</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">if</span> size_is_none:<br>                    <span class="hljs-keyword">assert</span> weights_are_none, <span class="hljs-string">f&quot;&quot;&quot;size_is_none =&gt; weights_are_none fails </span><br><span class="hljs-string">                    for <span class="hljs-subst">&#123;split.name&#125;</span> split</span><br><span class="hljs-string">                    This can occur with multiple validation sets if datasets have weights&quot;&quot;&quot;</span><br><br>        <span class="hljs-keyword">if</span> torch.distributed.is_initialized():<br>            gb_rank = torch.distributed.get_rank()<br>            <span class="hljs-keyword">if</span> gb_rank == <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">assert</span> (<br>                    self.is_built_on_rank()<br>                ), <span class="hljs-string">&quot;is_built_on_rank must return True when global rank = 0&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">Optional</span>[TopLevelDataset]]:<br>        <span class="hljs-string">&quot;&quot;&quot;Build all dataset splits according to the provided blend(s)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This method is distributed-aware and must be called on all ranks.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        The dataset splits returned can vary according to the config. Supply config.blend and</span><br><span class="hljs-string">        config.split to build BlendedDataset and/or MegatronDataset splits from the same</span><br><span class="hljs-string">        distribution. Supply config.blend_per_split to build BlendedDataset and/or MegatronDataset</span><br><span class="hljs-string">        splits from separate distributions. In either case, for each split, handle the following</span><br><span class="hljs-string">        cases:</span><br><span class="hljs-string"></span><br><span class="hljs-string">        (1) The split is None</span><br><span class="hljs-string">            - do nothing</span><br><span class="hljs-string"></span><br><span class="hljs-string">        (2) The split has one contributing dataset, and...</span><br><span class="hljs-string"></span><br><span class="hljs-string">            (a) &#x27;size&#x27; is not None</span><br><span class="hljs-string">                - Build a mid-level dataset with low-level dataset sampling in proportion to the</span><br><span class="hljs-string">                size</span><br><span class="hljs-string"></span><br><span class="hljs-string">            (b) &#x27;size&#x27; is None</span><br><span class="hljs-string">                - Build mid-level datasets with no excess low-level dataset sampling</span><br><span class="hljs-string"></span><br><span class="hljs-string">        (3) The split has multiple contributing datasets, and...</span><br><span class="hljs-string"></span><br><span class="hljs-string">            (a) &#x27;weights&#x27; is not None and &#x27;size&#x27; is not None</span><br><span class="hljs-string">                - Build mid-level datasets with low-level dataset sampling in proportion to their</span><br><span class="hljs-string">                weights and the size</span><br><span class="hljs-string">                - Build a top-level dataset of length marginally greater than &#x27;size&#x27; with mid-level</span><br><span class="hljs-string">                dataset sampling in proportion to their weights and the size</span><br><span class="hljs-string"></span><br><span class="hljs-string">            (b) &#x27;weights&#x27; is not None and &#x27;size&#x27; is None</span><br><span class="hljs-string">                - Error</span><br><span class="hljs-string"></span><br><span class="hljs-string">            (c) &#x27;weights&#x27; is None and &#x27;size&#x27; is not None</span><br><span class="hljs-string">                - Build mid-level datasets with no excess low-level dataset sampling</span><br><span class="hljs-string">                - Build a top-level dataset of length &#x27;size&#x27; (capped at the sum of the mid-level</span><br><span class="hljs-string">                dataset lengths) with mid-level dataset sampling in proportion to their lengths</span><br><span class="hljs-string">                and the size</span><br><span class="hljs-string"></span><br><span class="hljs-string">            (d) &#x27;weights&#x27; is None and &#x27;size&#x27; is None</span><br><span class="hljs-string">                - Build mid-level datasets with no excess low-level dataset sampling</span><br><span class="hljs-string">                - Build a top-level dataset with no excess mid-level dataset sampling</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            List[Optional[TopLevelDataset]]: A list containing a dataset instance (or None) per</span><br><span class="hljs-string">                split</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        datasets = self._build_blended_dataset_splits()<br><br>        <span class="hljs-keyword">for</span> dataset <span class="hljs-keyword">in</span> datasets:<br>            <span class="hljs-keyword">if</span> dataset <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(dataset) &gt; <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(dataset, BlendedDataset):<br>                    <span class="hljs-keyword">assert</span> dataset.size <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> dataset.size == <span class="hljs-built_in">len</span>(dataset)<br>                <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(dataset, MegatronDataset):<br>                    <span class="hljs-keyword">assert</span> dataset.num_samples <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> dataset.num_samples &lt;= <span class="hljs-built_in">len</span>(dataset)<br><br>        <span class="hljs-keyword">return</span> datasets<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_blended_dataset_splits</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">Optional</span>[TopLevelDataset]]:<br>        <span class="hljs-string">&quot;&quot;&quot;Build all dataset splits according to the provided blend(s)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        See the BlendedMegatronDatasetBuilder.build alias for more information.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            List[Optional[TopLevelDataset]]: A list containing a dataset instance (or None) per</span><br><span class="hljs-string">                split</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment">##</span><br>        <span class="hljs-comment"># Return fake &quot;mock&quot; datasets</span><br>        <span class="hljs-comment">##</span><br>        <span class="hljs-keyword">if</span> self.config.mock:<br>            split = self.config.split_matrix<br>            <span class="hljs-keyword">try</span>:<br>                <span class="hljs-keyword">return</span> self._build_megatron_dataset_splits(<span class="hljs-literal">None</span>, split, self.sizes)<br>            <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> error:<br>                <span class="hljs-keyword">raise</span> Exception(<br>                    <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;self.cls.__name__&#125;</span> failed to build as a mock data generator&quot;</span><br>                ) <span class="hljs-keyword">from</span> error<br><br>        <span class="hljs-comment">##</span><br>        <span class="hljs-comment"># All splits come from the same distribution</span><br>        <span class="hljs-comment">##</span><br>        <span class="hljs-keyword">elif</span> self.config.blend:<br>            prefixes, weights = self.config.blend<br>            <span class="hljs-keyword">if</span> weights <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                weights = normalize(weights)<br><br>            split = self.config.split_matrix<br><br>            <span class="hljs-comment"># Blend consists of a single prefix</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(prefixes) == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> weights <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">return</span> self._build_megatron_dataset_splits(prefixes[<span class="hljs-number">0</span>], split, self.sizes)<br><br>            <span class="hljs-comment"># Build the mid-level datasets</span><br>            <span class="hljs-keyword">if</span> weights <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-comment"># Build only one &quot;epoch&quot;</span><br>                sizes_per_dataset_buffer = [[<span class="hljs-literal">None</span> <span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> Split] <span class="hljs-keyword">for</span> prefix <span class="hljs-keyword">in</span> prefixes]<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># The number of samples we plan to use per dataset</span><br>                sizes_per_dataset_target = _get_size_per_split_per_dataset(weights, self.sizes)<br>                <span class="hljs-comment"># The number of samples we plan to build per dataset</span><br>                sizes_per_dataset_buffer = _get_size_per_split_per_dataset(<br>                    weights, self.sizes, surplus=self.config.mid_level_dataset_surplus<br>                )<br><br>            <span class="hljs-comment"># Build each dataset in parallel</span><br>            megatron_datasets = self._build_megatron_datasets_parallel(<br>                prefixes, split, sizes_per_dataset_buffer<br>            )<br><br>            <span class="hljs-comment"># Build the top-level datasets</span><br>            blended_datasets = [<span class="hljs-literal">None</span>] * <span class="hljs-built_in">len</span>(Split)<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(Split)):<br>                <span class="hljs-keyword">if</span> split[i] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    weights_i = weights<br>                    <span class="hljs-keyword">if</span> weights_i <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> self.sizes[i] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                        <span class="hljs-comment"># Blend according to client-specified weights and client-specified size</span><br>                        size_per_dataset = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(*sizes_per_dataset_target))[i]<br>                        size_i = <span class="hljs-built_in">sum</span>(size_per_dataset)<br>                    <span class="hljs-keyword">elif</span> weights_i <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                        <span class="hljs-comment"># Blend according to dataset sizes as-is and (maybe) client-specified size</span><br>                        <span class="hljs-keyword">try</span>:<br>                            weights_i = [<br>                                <span class="hljs-built_in">len</span>(megatron_dataset) <span class="hljs-keyword">for</span> megatron_dataset <span class="hljs-keyword">in</span> megatron_datasets[i]<br>                            ]<br>                        <span class="hljs-keyword">except</span> TypeError:<br>                            weights_i = [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> prefixes]<br>                        <span class="hljs-keyword">if</span> self.sizes[i] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                            size_i = <span class="hljs-built_in">min</span>(self.sizes[i], <span class="hljs-built_in">sum</span>(weights_i))<br>                        <span class="hljs-keyword">else</span>:<br>                            <span class="hljs-comment"># Build exhaustive indices</span><br>                            size_i = <span class="hljs-literal">None</span><br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-keyword">raise</span> ValueError(<br>                            <span class="hljs-string">&quot;Using client-specified weights requires client-specified size&quot;</span><br>                        )<br>                    blended_datasets[i] = self.build_generic_dataset(<br>                        BlendedDataset,<br>                        self.is_built_on_rank,<br>                        <span class="hljs-literal">True</span>,  <span class="hljs-comment"># synchronize_ranks, default behavior to build on rank-0 first</span><br>                        megatron_datasets[i],<br>                        weights_i,<br>                        size_i,<br>                        self.config,<br>                    )<br><br>            <span class="hljs-keyword">return</span> blended_datasets<br><br>        <span class="hljs-comment">##</span><br>        <span class="hljs-comment"># Each split comes from a separate distribution</span><br>        <span class="hljs-comment">##</span><br>        <span class="hljs-keyword">else</span>:<br>            blended_datasets = [<span class="hljs-literal">None</span>] * <span class="hljs-built_in">len</span>(Split)<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(Split)):<br>                split_spoof = [<span class="hljs-literal">None</span>] * <span class="hljs-built_in">len</span>(Split)<br>                split_spoof[i] = (<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>)<br>                sizes_spoof = [<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(Split)<br>                sizes_spoof[i] = self.sizes[i]<br><br>                <span class="hljs-comment"># Blend is provided for the split</span><br>                blend = self.config.blend_per_split[i]<br>                <span class="hljs-keyword">if</span> blend <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    prefixes, weights = blend<br>                    <span class="hljs-keyword">if</span> weights <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                        weights = normalize(weights)<br><br>                    <span class="hljs-comment"># Blend consists of a sigle prefix</span><br>                    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(prefixes) == <span class="hljs-number">1</span>:<br>                        blended_datasets[i] = self._build_megatron_dataset_splits(<br>                            prefixes[<span class="hljs-number">0</span>], split_spoof, sizes_spoof<br>                        )[i]<br>                        <span class="hljs-keyword">continue</span><br>                    <span class="hljs-keyword">elif</span> self.config.multiple_validation_sets <span class="hljs-keyword">and</span> i == Split.valid.value:<br>                        <span class="hljs-comment"># handle multiple validation sets</span><br>                        validation_datasets = []<br>                        <span class="hljs-keyword">if</span> self.config.full_validation:<br>                            <span class="hljs-comment"># verify that size is None, which causes a single epoch dataset</span><br>                            <span class="hljs-comment"># to be built</span><br>                            <span class="hljs-keyword">assert</span> sizes_spoof[i] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>                        <span class="hljs-keyword">for</span> prefix <span class="hljs-keyword">in</span> prefixes:<br>                            ds = self._build_megatron_dataset_splits(<br>                                prefix, split_spoof, sizes_spoof<br>                            )[i]<br>                            validation_datasets.append(ds)<br>                        blended_datasets[i] = validation_datasets<br>                        <span class="hljs-keyword">continue</span><br><br>                    <span class="hljs-comment"># Build mid-level datasets</span><br>                    <span class="hljs-keyword">if</span> weights <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                        sizes_per_dataset_buffer = [<br>                            [<span class="hljs-literal">None</span> <span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> Split] <span class="hljs-keyword">for</span> prefix <span class="hljs-keyword">in</span> prefixes<br>                        ]<br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-comment"># The number of samples we plan to use per dataset</span><br>                        sizes_per_dataset_target = _get_size_per_split_per_dataset(<br>                            weights, sizes_spoof<br>                        )<br>                        <span class="hljs-comment"># The number of samples we plan to build per dataset</span><br>                        sizes_per_dataset_buffer = _get_size_per_split_per_dataset(<br>                            weights, sizes_spoof, surplus=self.config.mid_level_dataset_surplus<br>                        )<br><br>                    <span class="hljs-comment"># Build each dataset in parallel</span><br>                    megatron_datasets = self._build_megatron_datasets_parallel(<br>                        prefixes, split_spoof, sizes_per_dataset_buffer<br>                    )[i]<br><br>                    <span class="hljs-comment"># Build top-level dataset</span><br>                    <span class="hljs-keyword">if</span> weights <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> self.sizes[i] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                        <span class="hljs-comment"># Blend according to client-specified weights and client-specified size</span><br>                        size_per_dataset = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(*sizes_per_dataset_target))[i]<br>                        size = <span class="hljs-built_in">sum</span>(size_per_dataset)<br>                    <span class="hljs-keyword">elif</span> weights <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                        <span class="hljs-comment"># Blend according to dataset sizes as-is and (maybe) client-specified size</span><br>                        <span class="hljs-keyword">try</span>:<br>                            weights = [<br>                                <span class="hljs-built_in">len</span>(megatron_dataset) <span class="hljs-keyword">for</span> megatron_dataset <span class="hljs-keyword">in</span> megatron_datasets<br>                            ]<br>                        <span class="hljs-keyword">except</span> TypeError:<br>                            weights = [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> prefixes]<br>                        <span class="hljs-keyword">if</span> self.sizes[i] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                            size = <span class="hljs-built_in">min</span>(self.sizes[i], <span class="hljs-built_in">sum</span>(weights))<br>                        <span class="hljs-keyword">else</span>:<br>                            <span class="hljs-comment"># Build exhaustive indices</span><br>                            size = <span class="hljs-literal">None</span><br>                    <span class="hljs-keyword">else</span>:<br>                        <span class="hljs-keyword">raise</span> RuntimeError<br>                    blended_datasets[i] = self.build_generic_dataset(<br>                        BlendedDataset,<br>                        self.is_built_on_rank,<br>                        <span class="hljs-literal">True</span>,  <span class="hljs-comment"># synchronize_ranks, default behavior to build on rank-0 first</span><br>                        megatron_datasets,<br>                        weights,<br>                        size,<br>                        self.config,<br>                    )<br><br>            <span class="hljs-keyword">return</span> blended_datasets<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_megatron_datasets_parallel</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, prefixes: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>], split: <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>], sizes_per_dataset: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-type">Optional</span>[MegatronDataset]]]:<br>        <span class="hljs-string">&quot;&quot;&quot;Build the megatron datasets for a list of prefixes in parallel</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            prefixes (List[str]): The list of prefix strings</span><br><span class="hljs-string"></span><br><span class="hljs-string">            split (List[float]): The dataset split ratios (must sum to 1.00)</span><br><span class="hljs-string"></span><br><span class="hljs-string">            sizes_per_dataset (List[List[int]]): The number of samples to request</span><br><span class="hljs-string">            per MegatronDataset per spilt</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            List[List[Optional[MegatronDataset]]]: For each split, have a list of</span><br><span class="hljs-string">            MegatronDataset per prefix</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># Helper function to wrap the threading logic</span><br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">_threading_helper</span>(<span class="hljs-params"></span><br><span class="hljs-params">            megatron_datasets: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-type">Optional</span>[MegatronDataset]]],</span><br><span class="hljs-params">            num_workers: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">            prefixes: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>],</span><br><span class="hljs-params">            split: <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>],</span><br><span class="hljs-params">            sizes_per_dataset: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]],</span><br><span class="hljs-params">        </span>) -&gt; <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">with</span> ThreadPoolExecutor(max_workers=num_workers) <span class="hljs-keyword">as</span> executor:<br>                all_futures = []<br>                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(prefixes)):<br>                    all_futures.append(<br>                        executor.submit(<br>                            self._build_megatron_dataset_splits,<br>                            prefixes[i],<br>                            split,<br>                            sizes_per_dataset[i],<br>                            <span class="hljs-literal">False</span>,  <span class="hljs-comment"># synchronize_ranks, barrier is called in this function</span><br>                        )<br>                    )<br>                <span class="hljs-keyword">for</span> future <span class="hljs-keyword">in</span> all_futures:<br>                    <span class="hljs-keyword">try</span>:<br>                        megatron_datasets_split = future.result()<br>                        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(megatron_datasets_split)):<br>                            megatron_datasets[j].append(megatron_datasets_split[j])<br>                    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> err:<br>                        <span class="hljs-keyword">raise</span> err<br><br>        megatron_datasets = [[] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(Split))]<br>        num_dataset_builder_threads = self.config.num_dataset_builder_threads<br><br>        <span class="hljs-keyword">if</span> torch.distributed.is_initialized():<br>            rank = torch.distributed.get_rank()<br>            <span class="hljs-comment"># First, build on rank 0</span><br>            <span class="hljs-keyword">if</span> rank == <span class="hljs-number">0</span>:<br>                num_workers = num_dataset_builder_threads<br>                <span class="hljs-keyword">if</span> num_workers &gt; <span class="hljs-number">1</span>:<br>                    <span class="hljs-comment"># since only rank 0 is running, scale up the thread count</span><br>                    <span class="hljs-comment"># but not too much to avoid overloading storage on miss path.</span><br>                    <span class="hljs-comment"># if user set num_dataset_builder_threads to 1,</span><br>                    <span class="hljs-comment"># i.e. meant for serial build, do not scale up.</span><br>                    num_workers *= <span class="hljs-built_in">min</span>(<span class="hljs-number">2</span>, <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, torch.cuda.device_count()))<br>                _threading_helper(<br>                    megatron_datasets, num_workers, prefixes, split, sizes_per_dataset<br>                )<br><br>            torch.distributed.barrier()<br><br>            <span class="hljs-comment"># Then, build on other ranks; guaranteed to be data_cache hit</span><br>            <span class="hljs-keyword">if</span> rank != <span class="hljs-number">0</span>:<br>                _threading_helper(<br>                    megatron_datasets,<br>                    num_dataset_builder_threads,<br>                    prefixes,<br>                    split,<br>                    sizes_per_dataset,<br>                )<br>        <span class="hljs-keyword">else</span>:<br>            _threading_helper(<br>                megatron_datasets, num_dataset_builder_threads, prefixes, split, sizes_per_dataset<br>            )<br><br>        <span class="hljs-keyword">return</span> megatron_datasets<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_megatron_dataset_splits</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        dataset_path: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>],</span><br><span class="hljs-params">        split: <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>],</span><br><span class="hljs-params">        sizes: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>],</span><br><span class="hljs-params">        synchronize_ranks: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">Optional</span>[MidLevelDataset]]:<br>        <span class="hljs-string">&quot;&quot;&quot;Build each MidLevelDataset split from a single LowLevelDataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            dataset_path (Optional[str]): The path on disk which defines the underlying</span><br><span class="hljs-string">                LowLevelDataset, or None for mock dataset classes</span><br><span class="hljs-string"></span><br><span class="hljs-string">            split (List[Tuple[float, float]]): The dataset split matrix</span><br><span class="hljs-string"></span><br><span class="hljs-string">            sizes (List[int]): The number of total samples to draw from each split</span><br><span class="hljs-string"></span><br><span class="hljs-string">            synchronize_ranks (bool): Whether to call barrier for rank-0 / barrier / other-ranks</span><br><span class="hljs-string">                behavior. Set to False when we enforce this behavior at higher level.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            List[Optional[MidLevelDataset]]: The MidLevelDataset (or None) per split</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># short-cut if we are not building on this rank</span><br>        <span class="hljs-keyword">if</span> torch.distributed.is_initialized() <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.is_built_on_rank():<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(Split)):<br>                <span class="hljs-keyword">if</span> split[i] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> synchronize_ranks:<br>                    torch.distributed.barrier()<br>            <span class="hljs-keyword">return</span> [<span class="hljs-literal">None</span>] * <span class="hljs-built_in">len</span>(Split)<br><br>        <span class="hljs-comment"># Build the low level dataset</span><br>        low_level_dataset = self.cls.build_low_level_dataset(dataset_path, self.config)<br><br>        <span class="hljs-comment"># Build the split indices for the low level dataset</span><br>        num_elements = self.cls.numel_low_level_dataset(low_level_dataset)<br>        split_indices = []<br>        <span class="hljs-keyword">for</span> i, _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(Split):<br>            <span class="hljs-keyword">if</span> split[i] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                beg = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">round</span>(split[i][<span class="hljs-number">0</span>] * <span class="hljs-built_in">float</span>(num_elements)))<br>                end = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">round</span>(split[i][<span class="hljs-number">1</span>] * <span class="hljs-built_in">float</span>(num_elements)))<br>                split_indices.append(numpy.arange(start=beg, stop=end, step=<span class="hljs-number">1</span>, dtype=numpy.int32))<br>            <span class="hljs-keyword">else</span>:<br>                split_indices.append(<span class="hljs-literal">None</span>)<br><br>        <span class="hljs-comment"># Build the mid level dataset</span><br>        mid_level_datasets = []<br>        <span class="hljs-keyword">for</span> i, _split <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(Split):<br>            <span class="hljs-keyword">if</span> split[i] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                mid_level_datasets.append(<span class="hljs-literal">None</span>)<br>            <span class="hljs-keyword">else</span>:<br>                mid_level_datasets.append(<br>                    self.build_generic_dataset(<br>                        self.cls,<br>                        self.is_built_on_rank,<br>                        synchronize_ranks,<br>                        low_level_dataset,<br>                        dataset_path,<br>                        split_indices[i],<br>                        sizes[i],<br>                        _split,<br>                        self.config,<br>                    )<br>                )<br><br>        <span class="hljs-keyword">return</span> mid_level_datasets<br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build_generic_dataset</span>(<span class="hljs-params"></span><br><span class="hljs-params">        cls: <span class="hljs-type">Union</span>[<span class="hljs-type">Type</span>[DistributedDataset], <span class="hljs-type">Callable</span>],</span><br><span class="hljs-params">        is_built_on_rank: <span class="hljs-type">Callable</span>,</span><br><span class="hljs-params">        synchronize_ranks: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        *args: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">Optional</span>[<span class="hljs-type">Union</span>[DistributedDataset, Iterable]]:<br>        <span class="hljs-string">&quot;&quot;&quot;Build the DistributedDataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Return None if and only if the underlying dataset class is not built on the current rank</span><br><span class="hljs-string">        and torch.distributed is initialized.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            cls (Union[Type[DistributedDataset], Callable]): The DistributedDataset class to be</span><br><span class="hljs-string">                built. In special cases, e.g. when we are building the low level dataset for a</span><br><span class="hljs-string">                RawMegatronDataset instance, we can accept a Callable which returns an Iterable.</span><br><span class="hljs-string"></span><br><span class="hljs-string">            synchronize_ranks (bool): Whether to call barrier for rank-0 / barrier / other-ranks</span><br><span class="hljs-string">                behavior. Set to False when we enforce this behavior at higher level.</span><br><span class="hljs-string"></span><br><span class="hljs-string">            args (Tuple[Any]): The positional arguments used to build the provided</span><br><span class="hljs-string">                DistributedDataset class</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Raises:</span><br><span class="hljs-string">            Exception: When the dataset constructor raises an OSError</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Optional[Union[DistributedDataset, Iterable]]: The DistributedDataset instantion, the</span><br><span class="hljs-string">                Iterable instantiation, or None</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> torch.distributed.is_initialized():<br>            rank = torch.distributed.get_rank()<br><br>            dataset = <span class="hljs-literal">None</span><br><br>            <span class="hljs-comment"># First, build on rank 0</span><br>            <span class="hljs-keyword">if</span> rank == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> is_built_on_rank():<br>                <span class="hljs-keyword">try</span>:<br>                    dataset = cls(*args)<br>                <span class="hljs-keyword">except</span> OSError <span class="hljs-keyword">as</span> err:<br>                    log = (<br>                        <span class="hljs-string">f&quot;Failed to write dataset materials to the data cache directory. Please &quot;</span><br>                        <span class="hljs-string">f&quot;supply a directory to which you have write access via the path_to_cache &quot;</span><br>                        <span class="hljs-string">f&quot;attribute in BlendedMegatronDatasetConfig and retry. Refer to the &quot;</span><br>                        <span class="hljs-string">f&quot;preserved traceback above for more information.&quot;</span><br>                    )<br>                    <span class="hljs-keyword">raise</span> Exception(log) <span class="hljs-keyword">from</span> err<br><br>            <span class="hljs-keyword">if</span> synchronize_ranks:<br>                torch.distributed.barrier()<br><br>            <span class="hljs-comment"># After, build on other ranks</span><br>            <span class="hljs-keyword">if</span> rank != <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> is_built_on_rank():<br>                dataset = cls(*args)<br><br>            <span class="hljs-keyword">return</span> dataset<br><br>        <span class="hljs-keyword">return</span> cls(*args)<br><br></code></pre></td></tr></table></figure>

<p>不过这里我们暂时不考虑数据集混合的情况，而是先看单一数据集下如何处理的。</p>
<p>单一数据集下会走进<code>_build_blended_dataset_splits</code>的如下代码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">Blend consists of a single prefix</span><br>if len(prefixes) == 1 and weights is None:<br>    return self._build_megatron_dataset_splits(prefixes[0], split, self.sizes)<br><br></code></pre></td></tr></table></figure>

<p>在<code>_build_megatron_dataset_splits</code>中的处理流程如下：</p>
<ol>
<li>如果当前是不需要创建数据集的rank（<code>is_dataset_built_on_rank</code>）就进行同步等待，即只有最前和最后的pp并行的rank以及tp的第一位才需要构建，从而避免资源浪费。</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">def is_dataset_built_on_rank():<br>    return (<br>        parallel_state.is_pipeline_first_stage(ignore_virtual=True)<br>        or parallel_state.is_pipeline_last_stage(ignore_virtual=True)<br>    ) and parallel_state.get_tensor_model_parallel_rank() == 0<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>然后通过<code>self.cls.build_low_level_dataset(dataset_path, self.config)</code>构建 low-level dataset，这里我们查看的是<code>GPTDataset</code>，如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-meta">@staticmethod</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_low_level_dataset</span>(<span class="hljs-params">dataset_path: <span class="hljs-built_in">str</span>, config: GPTDatasetConfig</span>) -&gt; IndexedDataset:<br>    <span class="hljs-string">&quot;&quot;&quot;Abstract method implementation</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        dataset_path (str): The real path prefix to the IndexedDataset .bin and .idx files</span><br><span class="hljs-string"></span><br><span class="hljs-string">        config (GPTDatasetConfig): The config</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        IndexedDataset: The underlying IndexedDataset</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> is_object_storage_path(dataset_path):<br>        <span class="hljs-keyword">assert</span> config.object_storage_cache_path <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">return</span> IndexedDataset(<br>            dataset_path,<br>            multimodal=<span class="hljs-literal">False</span>,<br>            mmap=config.mmap_bin_files,<br>            object_storage_config=ObjectStorageConfig(<br>                path_to_idx_cache=config.object_storage_cache_path<br>            ),<br>        )<br>    <span class="hljs-keyword">return</span> IndexedDataset(dataset_path, multimodal=<span class="hljs-literal">False</span>, mmap=config.mmap_bin_files)<br><br></code></pre></td></tr></table></figure>

<ol>
<li><p>从代码可以看到其本质是构建了一个<code>IndexedDataset</code>，代码如下所示</p>
</li>
<li><p>它使用的数据是一对文件：</p>
<ul>
<li><p>path_prefix.idx：索引文件，记录每条样本（sequence）的长度、在 <code>.bin</code> 里的字节偏移（pointer）、以及（可选）multimodal 的 mode；还记录文档边界（document_indices）。</p>
</li>
<li><p>path_prefix.bin：真实 token 数据（连续存储的定长 dtype 数组）。</p>
</li>
</ul>
</li>
<li><p>其核心能力是：</p>
<ul>
<li><p><strong>提供O(1) 级别的随机访问能力：</strong>&#x901A;过 <code>.idx</code> 找到第 i 条样本在 <code>.bin</code> 中的 offset 和 length，然后从 <code>.bin</code> 读取对应 token 序列。</p>
</li>
<li><p><strong>支持切片读取</strong>：<code>dataset[start:stop]</code> 允许一次读连续多条样本（step 必须为 1），避免逐条调用导致频繁 IO。</p>
</li>
<li><p><strong>部分读取</strong>：<code>get(idx, offset, length)</code> 可以只取某条样本的一段 token（用于截断、窗口等场景）。</p>
</li>
<li><p><strong>高效读取策略可选</strong>：通过 mmap&#x3D;True&#x2F;False 选择用内存映射（_MMapBinReader）或文件读（_FileBinReader）；如果数据在对象存储（S3&#x2F;MSC），则用对应 reader 分块拉取，并把 <code>.idx</code> 缓存到本地。</p>
</li>
<li><p>**exists(path_prefix)**：检查 <code>.idx/.bin</code> 是否存在（本地或对象存储）。</p>
</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">IndexedDataset</span>(torch.utils.data.Dataset):<br>    <span class="hljs-string">&quot;&quot;&quot;The low-level interface dataset class</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        path_prefix (str): The index (.idx) and data (.bin) prefix</span><br><span class="hljs-string"></span><br><span class="hljs-string">        multimodal (bool): Whether the dataset is multimodal. Defaults to False.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        mmap (bool): Whether to mmap the .bin files. Defaults to True.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        object_storage_config (Optional[ObjectStorageConfig]): Supplied only for data stored on S3</span><br><span class="hljs-string">            or MSC. IndexedDataset downloads the index (.idx) file to</span><br><span class="hljs-string">            `object_storage_config.path_to_idx_cache` and streams data from the data (.bin) file</span><br><span class="hljs-string">            in `object_storage_config.bin_chunk_nbytes` blocks. Note that `mmap` must be disabled</span><br><span class="hljs-string">            for S3 data loading. Defaults to None.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        path_prefix: <span class="hljs-built_in">str</span>,</span><br><span class="hljs-params">        multimodal: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">        mmap: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        object_storage_config: <span class="hljs-type">Optional</span>[ObjectStorageConfig] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        s3_config: <span class="hljs-type">Optional</span>[S3Config] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.path_prefix: <span class="hljs-built_in">str</span><br>        self.multimodal: <span class="hljs-built_in">bool</span><br>        self.mmap: <span class="hljs-built_in">bool</span><br>        self.object_storage_config: <span class="hljs-type">Optional</span>[ObjectStorageConfig]<br><br>        self.bin_reader: _BinReader<br>        self.index: _IndexReader<br><br>        <span class="hljs-comment"># Deprecated: s3_config is deprecated, use object_storage_config instead</span><br>        object_storage_config = object_storage_config <span class="hljs-keyword">or</span> s3_config<br><br>        <span class="hljs-comment"># Cache the index file if it is stored on object storage</span><br>        <span class="hljs-keyword">if</span> is_object_storage_path(path_prefix) <span class="hljs-keyword">and</span> object_storage_config <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            idx_path = get_idx_path(path_prefix)<br>            cache_idx_path = get_index_cache_path(idx_path, object_storage_config)<br>            cache_index_file(idx_path, cache_idx_path)<br><br>        self.initialize(path_prefix, multimodal, mmap, object_storage_config)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        path_prefix: <span class="hljs-built_in">str</span>,</span><br><span class="hljs-params">        multimodal: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        mmap: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        object_storage_config: <span class="hljs-type">Optional</span>[ObjectStorageConfig],</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Initialize the dataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This method is called by IndexedDataset.__init__ during object creation and by</span><br><span class="hljs-string">        IndexedDataset.__setstate__ during un-pickling</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            path_prefix (str): The index (.idx) and data (.bin) prefix</span><br><span class="hljs-string"></span><br><span class="hljs-string">            multimodal (bool): Whether the dataset is multimodal</span><br><span class="hljs-string"></span><br><span class="hljs-string">            mmap (bool): Whether to mmap the .bin file</span><br><span class="hljs-string"></span><br><span class="hljs-string">            object_storage_config (Optional[ObjectStorageConfig]): See IndexedDataset docstring</span><br><span class="hljs-string">                for details.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        idx_path = get_idx_path(path_prefix)<br>        bin_path = get_bin_path(path_prefix)<br>        <span class="hljs-keyword">if</span> object_storage_config <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">assert</span> os.path.exists(idx_path) <span class="hljs-keyword">and</span> os.path.exists(<br>                bin_path<br>            ), <span class="hljs-string">&quot;One or both of the .idx and .bin files cannot be found at the &quot;</span><br>            <span class="hljs-string">f&quot;path prefix <span class="hljs-subst">&#123;path_prefix&#125;</span>&quot;</span><br>        self.path_prefix = path_prefix<br>        self.multimodal = multimodal<br>        self.mmap = mmap<br>        self.object_storage_config = object_storage_config<br>        <span class="hljs-keyword">if</span> mmap:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> object_storage_config<br>            self.bin_reader = _MMapBinReader(bin_path)<br>        <span class="hljs-keyword">elif</span> object_storage_config:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> mmap<br>            self.bin_reader = OBJECT_STORAGE_BIN_READERS[get_object_storage_access(path_prefix)](<br>                bin_path, object_storage_config<br>            )<br>            idx_path = get_index_cache_path(get_idx_path(path_prefix), object_storage_config)<br>        <span class="hljs-keyword">else</span>:<br>            self.bin_reader = _FileBinReader(bin_path)<br>        self.index = _IndexReader(idx_path, self.multimodal)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getstate__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">bool</span>, <span class="hljs-built_in">bool</span>, <span class="hljs-type">Optional</span>[ObjectStorageConfig]]:<br>        <span class="hljs-string">&quot;&quot;&quot;Get the state during pickling</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Tuple[str, bool, bool, Optional[ObjectStorageConfig]]: The state tuple</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> self.path_prefix, self.multimodal, self.mmap, self.object_storage_config<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__setstate__</span>(<span class="hljs-params">self, state: <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">str</span>, <span class="hljs-built_in">bool</span>, <span class="hljs-built_in">bool</span>, <span class="hljs-type">Optional</span>[ObjectStorageConfig]]</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Set the state during un-pickling</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            state (Tuple[str, bool, bool, Optional[ObjectStorageConfig]]): The state tuple</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        path_prefix, multimodal, mmap, object_storage_config = state<br>        self.initialize(path_prefix, multimodal, mmap, object_storage_config)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__del__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Clean up the object&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">del</span> self.bin_reader<br>        <span class="hljs-keyword">del</span> self.index<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Return the length of the dataset i.e. the number of sequences in the index</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            int: The length of the dataset</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.index)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, idx: <span class="hljs-type">Union</span>[<span class="hljs-built_in">int</span>, numpy.integer, <span class="hljs-built_in">slice</span>]</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">Union</span>[<br>        numpy.ndarray,<br>        <span class="hljs-type">Tuple</span>[numpy.ndarray, numpy.number],<br>        <span class="hljs-type">List</span>[numpy.ndarray],<br>        <span class="hljs-type">Tuple</span>[<span class="hljs-type">List</span>[numpy.ndarray], numpy.ndarray],<br>    ]:<br>        <span class="hljs-string">&quot;&quot;&quot;Return from the dataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            idx (Union[int, numpy.integer, slice]): The index or index slice into the dataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Raises:</span><br><span class="hljs-string">            ValueError: When the index slice is non-contiguous</span><br><span class="hljs-string"></span><br><span class="hljs-string">            TypeError: When the index is of an unexpected type</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Union[</span><br><span class="hljs-string">                numpy.ndarray,</span><br><span class="hljs-string">                Tuple[numpy.ndarray, numpy.number],</span><br><span class="hljs-string">                List[numpy.ndarray],</span><br><span class="hljs-string">                Tuple[List[numpy.ndarray], numpy.ndarray],</span><br><span class="hljs-string">            ]: The sequence tokens and modes at the index or index slice</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(idx, (<span class="hljs-built_in">int</span>, numpy.integer)):<br>            sequence_pointer, sequence_length, sequence_mode = self.index[idx]<br>            sequence = self.bin_reader.read(<br>                dtype=self.index.dtype, count=sequence_length, offset=sequence_pointer<br>            )<br>            <span class="hljs-keyword">return</span> (sequence, sequence_mode) <span class="hljs-keyword">if</span> sequence_mode <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> sequence<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(idx, <span class="hljs-built_in">slice</span>):<br>            start, stop, step = idx.indices(<span class="hljs-built_in">len</span>(self))<br>            <span class="hljs-keyword">if</span> step != <span class="hljs-number">1</span>:<br>                <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Slices into indexed_dataset must be contiguous&quot;</span>)<br>            sequence_lengths = self.index.sequence_lengths[idx]<br>            sequence_modes = (<br>                self.index.sequence_modes[idx] <span class="hljs-keyword">if</span> self.multimodal <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>  <span class="hljs-comment"># type: ignore[index]</span><br>            )<br>            sequence_offsets = <span class="hljs-built_in">list</span>(accumulate(sequence_lengths))<br>            sequences = numpy.split(<br>                self.bin_reader.read(<br>                    dtype=self.index.dtype,<br>                    count=<span class="hljs-built_in">sum</span>(sequence_lengths),<br>                    offset=self.index.sequence_pointers[start],<br>                ),<br>                sequence_offsets[:-<span class="hljs-number">1</span>],<br>            )<br>            <span class="hljs-keyword">return</span> (sequences, sequence_modes) <span class="hljs-keyword">if</span> sequence_modes <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> sequences<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> TypeError(<span class="hljs-string">&quot;Unexpected type received for idx: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">type</span>(idx)))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, idx: <span class="hljs-built_in">int</span>, offset: <span class="hljs-built_in">int</span> = <span class="hljs-number">0</span>, length: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span></span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">Union</span>[numpy.ndarray, <span class="hljs-type">Tuple</span>[numpy.ndarray, numpy.number]]:<br>        <span class="hljs-string">&quot;&quot;&quot;Retrieve a single item from the dataset with the option to only</span><br><span class="hljs-string">        return a portion of the item.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        get(idx) is the same as [idx] but get() does not support slicing.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            idx (Union[int, numpy.integer]): The index into the dataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">            offset (int): The integer token offset in the sequence</span><br><span class="hljs-string"></span><br><span class="hljs-string">            length (int): The number of tokens to grab from the sequence</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Union[numpy.ndarray, Tuple[numpy.ndarray, numpy.number]]: The sequence tokens and mode</span><br><span class="hljs-string">                at the index</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        sequence_pointer, sequence_length, sequence_mode = self.index[idx]<br>        <span class="hljs-keyword">if</span> length <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            length = sequence_length - offset<br>        sequence_pointer += offset * DType.size(self.index.dtype)<br>        sequence = self.bin_reader.read(<br>            dtype=self.index.dtype, count=length, offset=sequence_pointer<br>        )<br>        <span class="hljs-keyword">return</span> (sequence, sequence_mode) <span class="hljs-keyword">if</span> sequence_mode <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> sequence<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sequence_lengths</span>(<span class="hljs-params">self</span>) -&gt; numpy.ndarray:<br>        <span class="hljs-string">&quot;&quot;&quot;Get the sequence lengths</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            numpy.ndarray: The sequence lengths</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> self.index.sequence_lengths<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">document_indices</span>(<span class="hljs-params">self</span>) -&gt; numpy.ndarray:<br>        <span class="hljs-string">&quot;&quot;&quot;Get the document indices</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            numpy.ndarray: The document indices</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> self.index.document_indices<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_document_indices</span>(<span class="hljs-params">self</span>) -&gt; numpy.ndarray:<br>        <span class="hljs-string">&quot;&quot;&quot;Get the document indices</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This method is slated for deprecation.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            numpy.ndarray: The document indices</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> self.index.document_indices<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">set_document_indices</span>(<span class="hljs-params">self, document_indices: numpy.ndarray</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Set the document indices</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This method is slated for deprecation.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            document_indices (numpy.ndarray): The document indices</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.index.document_indices = document_indices<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sequence_modes</span>(<span class="hljs-params">self</span>) -&gt; numpy.ndarray:<br>        <span class="hljs-string">&quot;&quot;&quot;Get the sequence modes</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            numpy.ndarray: The sequence modes</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">assert</span> self.index.sequence_modes<br>        <span class="hljs-keyword">return</span> self.index.sequence_modes<br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">exists</span>(<span class="hljs-params">path_prefix: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Return whether the IndexedDataset exists on disk at the prefix</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            path_prefix (str): The prefix to the index (.idx) and data (.bin) files</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            bool: Whether the IndexedDataset exists on disk at the prefix</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> is_object_storage_path(path_prefix):<br>            <span class="hljs-keyword">return</span> dataset_exists(path_prefix, get_idx_path(path_prefix), get_bin_path(path_prefix))<br><br>        <span class="hljs-keyword">return</span> os.path.exists(get_idx_path(path_prefix)) <span class="hljs-keyword">and</span> os.path.exists(<br>            get_bin_path(path_prefix)<br>        )<br><br></code></pre></td></tr></table></figure>
</li>
<li><p>读取这个low-level dataset中一共有多少样本，然后依据split来计算按比例分割下的各实际范围</p>
</li>
<li><p>然后调用<code>build_generic_dataset</code>为各范围构建mid-level dataset并返回，这也就是我们得到的 train_ds, valid_ds, test_ds</p>
<ol>
<li><p>其首先是让rank 0构建mid-level dataset也就是实际的<code>GPTDataset</code>，然后让其他rank都等待</p>
</li>
<li><p>再让rank不为 0并且<code>is_built_on_rank</code>的rank构建<code>GPTDataset</code>，并对应返回该<code>GPTDataset</code>。</p>
</li>
</ol>
<blockquote>
<p><strong>rank 0 先构建</strong>是为了把“构建 dataset 时可能产生的共享缓存写入”变成“单进程写入 + 多进程读取”，从而避免竞态、提升缓存命中、保证 barrier 同步与流程一致性。</p>
</blockquote>
<ul>
<li><p><code>GPTDataset</code>的作用是：在底层 IndexedDataset（存放 <code>.bin/.idx</code> 的 token 序列）之上，构建<strong>可直接用于 GPT 自回归训练</strong>的 PyTorch <code>Dataset</code>。其主要作用有：</p>
<ul>
<li><p><strong>把原始序列拼接&#x2F;切片成固定长度样本</strong>：通过构建 document_index &#x2F; sample_index &#x2F; shuffle_index，把很多条变长序列按文档顺序拼接，然后切成长度为 sequence_length（可带 1 个 extra token）的训练样本。</p>
</li>
<li><p><strong>提供训练所需张量</strong>：getitem 返回 tokens&#x2F;labels，并生成（或复用缓存的）<code>attention_mask / loss_mask / position_ids</code>，满足左到右（causal）语言模型训练。</p>
</li>
<li><p><strong>支持可复现的 shuffle 与多 epoch 采样</strong>：用 shuffle_index 控制样本随机顺序；当 num_samples 大于一个 epoch 的可用样本时，会计算需要重复多少个 epoch。</p>
</li>
<li><p><strong>支持索引缓存</strong>：会把构建出来的 document_index.npy &#x2F; sample_index.npy &#x2F; shuffle_index.npy 写到 path_to_cache，下次启动直接加载，避免重复计算。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GPTDataset</span>(<span class="hljs-title class_ inherited__">MegatronDataset</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;The base GPT dataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        indexed_dataset (IndexedDataset): The IndexedDataset around which to build the GPTDataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        dataset_path (Optional[str]): The real path on disk to the dataset, for bookkeeping</span><br><span class="hljs-string"></span><br><span class="hljs-string">        indexed_indices (numpy.ndarray): The set of the documents indices to expose</span><br><span class="hljs-string"></span><br><span class="hljs-string">        num_samples (Optional[int]): The number of samples to draw from the indexed dataset. When</span><br><span class="hljs-string">            None, build as many samples as correspond to one epoch.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        index_split (Split): The indexed_indices Split</span><br><span class="hljs-string"></span><br><span class="hljs-string">        config (GPTDatasetConfig): The config</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        indexed_dataset: IndexedDataset,</span><br><span class="hljs-params">        dataset_path: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">str</span>],</span><br><span class="hljs-params">        indexed_indices: numpy.ndarray,</span><br><span class="hljs-params">        num_samples: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>],</span><br><span class="hljs-params">        index_split: Split,</span><br><span class="hljs-params">        config: GPTDatasetConfig,</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__(<br>            indexed_dataset, dataset_path, indexed_indices, num_samples, index_split, config<br>        )<br>        self.masks_and_position_ids_are_cacheable = <span class="hljs-keyword">not</span> <span class="hljs-built_in">any</span>(<br>            [<br>                self.config.reset_position_ids,<br>                self.config.reset_attention_mask,<br>                self.config.eod_mask_loss,<br>            ]<br>        )<br>        self.masks_and_position_ids_are_cached = <span class="hljs-literal">False</span><br>        self.cached_attention_mask = <span class="hljs-literal">None</span><br>        self.cached_loss_mask = <span class="hljs-literal">None</span><br>        self.cached_position_ids = <span class="hljs-literal">None</span><br><br>        <span class="hljs-keyword">try</span>:<br>            self._pad_token_id = self.config.tokenizer.pad<br>        <span class="hljs-keyword">except</span> Exception:<br>            self._pad_token_id = _PAD_TOKEN_ID<br><br>        (self.document_index, self.sample_index, self.shuffle_index) = (<br>            self._build_document_sample_shuffle_indices()<br>        )<br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">numel_low_level_dataset</span>(<span class="hljs-params">low_level_dataset: IndexedDataset</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Abstract method implementation</span><br><span class="hljs-string"></span><br><span class="hljs-string">        For GPT, the underlying IndexedDataset should be split by sequence, as opposed to, say,</span><br><span class="hljs-string">        BERT, which should be split by document</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            low_level_dataset (IndexedDataset): The underlying IndexedDataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            int: The number of unique elements in the underlying IndexedDataset</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> low_level_dataset.sequence_lengths.shape[<span class="hljs-number">0</span>]<br><br><span class="hljs-meta">    @staticmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build_low_level_dataset</span>(<span class="hljs-params">dataset_path: <span class="hljs-built_in">str</span>, config: GPTDatasetConfig</span>) -&gt; IndexedDataset:<br>        <span class="hljs-string">&quot;&quot;&quot;Abstract method implementation</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            dataset_path (str): The real path prefix to the IndexedDataset .bin and .idx files</span><br><span class="hljs-string"></span><br><span class="hljs-string">            config (GPTDatasetConfig): The config</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            IndexedDataset: The underlying IndexedDataset</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> is_object_storage_path(dataset_path):<br>            <span class="hljs-keyword">assert</span> config.object_storage_cache_path <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">return</span> IndexedDataset(<br>                dataset_path,<br>                multimodal=<span class="hljs-literal">False</span>,<br>                mmap=config.mmap_bin_files,<br>                object_storage_config=ObjectStorageConfig(<br>                    path_to_idx_cache=config.object_storage_cache_path<br>                ),<br>            )<br>        <span class="hljs-keyword">return</span> IndexedDataset(dataset_path, multimodal=<span class="hljs-literal">False</span>, mmap=config.mmap_bin_files)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Abstract method implementation</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            int: The length of the dataset</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> self.sample_index.shape[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, torch.Tensor]:<br>        <span class="hljs-string">&quot;&quot;&quot;Abstract method implementation</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            idx (Optioal[int]): The index into the dataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Dict[str, torch.Tensor]: The sample information wrapped in a dictionary</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> idx <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># Batch padding sequence so the index does not matter</span><br>            text, _ = self._query_document_sample_shuffle_indices(<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">else</span>:<br>            text, _ = self._query_document_sample_shuffle_indices(idx)<br><br>        text = torch.from_numpy(text).long()<br>        <span class="hljs-keyword">if</span> self.config.add_extra_token_to_sequence:<br>            tokens = text[:-<span class="hljs-number">1</span>].contiguous()<br>            labels = text[<span class="hljs-number">1</span>:].contiguous()<br>        <span class="hljs-keyword">else</span>:<br>            tokens = text<br>            labels = torch.roll(text, shifts=-<span class="hljs-number">1</span>, dims=<span class="hljs-number">0</span>)<br>            labels[-<span class="hljs-number">1</span>] = self._pad_token_id<br><br>        <span class="hljs-keyword">if</span> (<br>            <span class="hljs-keyword">not</span> self.masks_and_position_ids_are_cacheable<br>            <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> self.masks_and_position_ids_are_cached<br>        ):<br>            attention_mask, loss_mask, position_ids = _get_ltor_masks_and_position_ids(<br>                tokens,<br>                self.config.tokenizer.eod,<br>                self.config.reset_position_ids,<br>                self.config.reset_attention_mask,<br>                self.config.eod_mask_loss,<br>                self.config.create_attention_mask,<br>            )<br>            <span class="hljs-keyword">if</span> self.masks_and_position_ids_are_cacheable:<br>                self.cached_attention_mask = attention_mask<br>                self.cached_loss_mask = loss_mask<br>                self.cached_position_ids = position_ids<br>                self.masks_and_position_ids_are_cached = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">else</span>:<br>            attention_mask = self.cached_attention_mask<br>            loss_mask = self.cached_loss_mask<br>            position_ids = self.cached_position_ids<br><br>        <span class="hljs-comment"># For padded sequences, mask the loss</span><br>        loss_mask[labels == self._pad_token_id] = <span class="hljs-number">0.0</span><br><br>        <span class="hljs-comment"># For padded sequences, ensure the embedding layer can map the token ID</span><br>        tokens[tokens == self._pad_token_id] = <span class="hljs-number">0</span><br>        labels[labels == self._pad_token_id] = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># Batch padding sequence so we mask the loss</span><br>        <span class="hljs-keyword">if</span> idx <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            loss_mask = torch.zeros_like(loss_mask)<br><br>        <span class="hljs-keyword">if</span> self.config.create_attention_mask:<br>            <span class="hljs-keyword">return</span> &#123;<br>                <span class="hljs-string">&quot;tokens&quot;</span>: tokens,<br>                <span class="hljs-string">&quot;labels&quot;</span>: labels,<br>                <span class="hljs-string">&quot;attention_mask&quot;</span>: attention_mask,<br>                <span class="hljs-string">&quot;loss_mask&quot;</span>: loss_mask,<br>                <span class="hljs-string">&quot;position_ids&quot;</span>: position_ids,<br>            &#125;<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> &#123;<br>                <span class="hljs-string">&quot;tokens&quot;</span>: tokens,<br>                <span class="hljs-string">&quot;labels&quot;</span>: labels,<br>                <span class="hljs-string">&quot;loss_mask&quot;</span>: loss_mask,<br>                <span class="hljs-string">&quot;position_ids&quot;</span>: position_ids,<br>            &#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_query_document_sample_shuffle_indices</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self, idx: <span class="hljs-built_in">int</span></span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">Tuple</span>[numpy.ndarray, numpy.ndarray]:<br>        <span class="hljs-string">&quot;&quot;&quot;Get the text (token ids) and document ids for a given index</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            idx (int): The index into the dataset</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Tuple[numpy.ndarray, numpy.ndarray]: The text ids and document ids</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Do the shuffle mapping</span><br>        idx = self.shuffle_index[idx]<br><br>        <span class="hljs-comment"># Get the beginning and end documents and offsets</span><br>        doc_index_beg, doc_index_beg_offset = self.sample_index[idx]<br>        doc_index_end, doc_index_end_offset = self.sample_index[idx + <span class="hljs-number">1</span>]<br><br>        document_ids = []<br>        sample_parts = []<br><br>        <span class="hljs-comment"># Sample spans a single document</span><br>        <span class="hljs-keyword">if</span> doc_index_beg == doc_index_end:<br>            <span class="hljs-comment"># Add the document id</span><br>            document_ids.append(self.document_index[doc_index_beg])<br><br>            <span class="hljs-comment"># Add the entire sample</span><br>            sample_parts.append(<br>                self.dataset.get(<br>                    self.document_index[doc_index_beg],<br>                    offset=doc_index_beg_offset,<br>                    length=doc_index_end_offset<br>                    - doc_index_beg_offset<br>                    + self.config.add_extra_token_to_sequence,<br>                )<br>            )<br><br>        <span class="hljs-comment"># Sample spans multiple documents</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(doc_index_beg, doc_index_end + <span class="hljs-number">1</span>):<br>                <span class="hljs-comment"># Add the document id</span><br>                document_ids.append(self.document_index[i])<br><br>                <span class="hljs-comment"># Add the sample part</span><br>                offset = <span class="hljs-number">0</span> <span class="hljs-keyword">if</span> i &gt; doc_index_beg <span class="hljs-keyword">else</span> doc_index_beg_offset<br>                length = (<br>                    <span class="hljs-literal">None</span><br>                    <span class="hljs-keyword">if</span> i &lt; doc_index_end<br>                    <span class="hljs-keyword">else</span> doc_index_end_offset + self.config.add_extra_token_to_sequence<br>                )<br>                sample_parts.append(<br>                    self.dataset.get(self.document_index[i], offset=offset, length=length)<br>                )<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(document_ids) == <span class="hljs-built_in">len</span>(<br>            sample_parts<br>        ), <span class="hljs-string">f&quot;len(document_ids) (<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(document_ids)&#125;</span>) != len(sample_parts) (<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(sample_parts)&#125;</span>)&quot;</span><br><br>        length = <span class="hljs-built_in">sum</span>(<span class="hljs-built_in">map</span>(<span class="hljs-built_in">len</span>, sample_parts))<br><br>        <span class="hljs-comment"># Pad the sample if necessary</span><br>        <span class="hljs-keyword">if</span> length &lt; (self.config.sequence_length + self.config.add_extra_token_to_sequence):<br>            sample_parts.append(<br>                [self._pad_token_id]<br>                * (self.config.sequence_length + self.config.add_extra_token_to_sequence - length)<br>            )<br><br>        <span class="hljs-keyword">return</span> (<br>            numpy.concatenate(sample_parts, dtype=numpy.int64),<br>            numpy.array(document_ids, dtype=numpy.int64),<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_build_document_sample_shuffle_indices</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-type">Tuple</span>[numpy.ndarray, numpy.ndarray, numpy.ndarray]:<br>        <span class="hljs-string">&quot;&quot;&quot;Build the document index, the sample index, and the shuffle index</span><br><span class="hljs-string"></span><br><span class="hljs-string">        The document index:</span><br><span class="hljs-string">            -- 1-D</span><br><span class="hljs-string">            -- An ordered array of document ids</span><br><span class="hljs-string"></span><br><span class="hljs-string">        The sample index:</span><br><span class="hljs-string">            -- 2-D</span><br><span class="hljs-string">            -- The document indices and offsets which mark the start of every sample</span><br><span class="hljs-string"></span><br><span class="hljs-string">        The shuffle index:</span><br><span class="hljs-string">            -- 1-D</span><br><span class="hljs-string">            -- A random permutation of index range of the sample index</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            Tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray]: The document index, the sample</span><br><span class="hljs-string">            index, and the shuffle index</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        path_to_cache = self.config.path_to_cache<br>        <span class="hljs-keyword">if</span> path_to_cache <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.config.mock:<br>            path_to_cache = os.path.join(<br>                self.dataset.path_prefix, <span class="hljs-string">&quot;cache&quot;</span>, <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(self).__name__&#125;</span>_indices&quot;</span><br>            )<br><br>        <span class="hljs-keyword">if</span> path_to_cache:<br>            base = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;self.unique_description_hash&#125;</span>-<span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(self).__name__&#125;</span>-<span class="hljs-subst">&#123;self.index_split.name&#125;</span>&quot;</span><br>            get_path_to = <span class="hljs-keyword">lambda</span> affix: os.path.join(path_to_cache, <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;base&#125;</span>-<span class="hljs-subst">&#123;affix&#125;</span>&quot;</span>)<br>            path_to_description = get_path_to(<span class="hljs-string">&quot;description.txt&quot;</span>)<br>            path_to_document_index = get_path_to(<span class="hljs-string">&quot;document_index.npy&quot;</span>)<br>            path_to_sample_index = get_path_to(<span class="hljs-string">&quot;sample_index.npy&quot;</span>)<br>            path_to_shuffle_index = get_path_to(<span class="hljs-string">&quot;shuffle_index.npy&quot;</span>)<br>            cache_hit = <span class="hljs-built_in">all</span>(<br>                <span class="hljs-built_in">map</span>(<br>                    os.path.isfile,<br>                    [<br>                        path_to_description,<br>                        path_to_document_index,<br>                        path_to_sample_index,<br>                        path_to_shuffle_index,<br>                    ],<br>                )<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            cache_hit = <span class="hljs-literal">False</span><br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> path_to_cache <span class="hljs-keyword">or</span> (<br>            <span class="hljs-keyword">not</span> cache_hit<br>            <span class="hljs-keyword">and</span> (<span class="hljs-keyword">not</span> torch.distributed.is_initialized() <span class="hljs-keyword">or</span> torch.distributed.get_rank() == <span class="hljs-number">0</span>)<br>        ):<br>            log_single_rank(<br>                logger,<br>                logging.INFO,<br>                <span class="hljs-string">f&quot;Build and save the <span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(self).__name__&#125;</span> <span class="hljs-subst">&#123;self.index_split.name&#125;</span> indices&quot;</span>,<br>            )<br>            t_beg = time.time()<br><br>            sequence_length = self.config.sequence_length<br>            num_tokens_per_epoch = self._get_num_tokens_per_epoch()<br>            num_epochs = self._get_num_epochs(num_tokens_per_epoch)<br><br>            <span class="hljs-keyword">if</span> num_epochs == <span class="hljs-number">1</span>:<br>                separate_final_epoch = <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># Get the number of samples for the last epoch</span><br>                num_samples_sans_final_epoch = (<br>                    (num_epochs - <span class="hljs-number">1</span>) * num_tokens_per_epoch<br>                    - self.config.add_extra_token_to_sequence<br>                ) // sequence_length<br>                num_samples_from_final_epoch = self.num_samples - num_samples_sans_final_epoch<br>                num_samples_per_epoch = (<br>                    num_tokens_per_epoch - self.config.add_extra_token_to_sequence<br>                ) // sequence_length<br><br>                <span class="hljs-comment"># num_samples_from_final_epoch should be non-negative</span><br>                <span class="hljs-keyword">assert</span> num_samples_from_final_epoch &gt;= <span class="hljs-number">0</span><br><br>                <span class="hljs-comment"># num_samples_from_final_epoch should not exceed max value</span><br>                <span class="hljs-keyword">assert</span> num_samples_from_final_epoch &lt;= num_samples_per_epoch + <span class="hljs-number">1</span><br><br>                <span class="hljs-comment"># Separate the final epoch if it falls below the threshold</span><br>                threshold = <span class="hljs-number">0.80</span><br>                separate_final_epoch = num_samples_from_final_epoch &lt; <span class="hljs-built_in">int</span>(<br>                    threshold * num_samples_per_epoch<br>                )<br><br>                log_single_rank(<br>                    logger,<br>                    logging.DEBUG,<br>                    <span class="hljs-string">f&quot;&gt; num_samples_from_final_epoch: <span class="hljs-subst">&#123;num_samples_from_final_epoch&#125;</span>&quot;</span>,<br>                )<br>                log_single_rank(logger, logging.DEBUG, <span class="hljs-string">f&quot;&gt; threshold: <span class="hljs-subst">&#123;threshold&#125;</span>&quot;</span>)<br>                log_single_rank(<br>                    logger, logging.DEBUG, <span class="hljs-string">f&quot;&gt; num_samples_per_epoch: <span class="hljs-subst">&#123;num_samples_per_epoch&#125;</span>&quot;</span><br>                )<br><br>            log_single_rank(<br>                logger, logging.DEBUG, <span class="hljs-string">f&quot;&gt; separate_final_epoch: <span class="hljs-subst">&#123;separate_final_epoch&#125;</span>&quot;</span><br>            )<br><br>            numpy_random_state = numpy.random.RandomState(self.config.random_seed)<br><br>            <span class="hljs-comment"># Build the document index</span><br>            document_index = _build_document_index(<br>                self.indices, num_epochs, numpy_random_state, separate_final_epoch<br>            )<br><br>            <span class="hljs-comment"># Build the sample index</span><br>            <span class="hljs-keyword">from</span> megatron.core.datasets <span class="hljs-keyword">import</span> helpers<br><br>            <span class="hljs-keyword">if</span> self.index_split == Split.valid:<br>                drop_last_partial_sequence = self.config.drop_last_partial_validation_sequence<br>            <span class="hljs-keyword">else</span>:<br>                drop_last_partial_sequence = <span class="hljs-literal">True</span><br><br>            <span class="hljs-keyword">assert</span> document_index.dtype == numpy.int32<br>            <span class="hljs-keyword">assert</span> self.dataset.sequence_lengths.dtype == numpy.int32<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(document_index) * <span class="hljs-number">2</span> &gt; <span class="hljs-built_in">len</span>(self.dataset.sequence_lengths):<br>                <span class="hljs-comment"># If &quot;access density&quot; of sequence_lengths is high, force load the mmap-ed array</span><br>                <span class="hljs-comment"># into memory by making a copy.</span><br>                <span class="hljs-comment">#</span><br>                <span class="hljs-comment"># System performance benefits come from two aspects:</span><br>                <span class="hljs-comment">#   1. We sequentially pre-load the whole file, most of which we expect to read</span><br>                <span class="hljs-comment">#   2. The GIL is held when entering the c++ program, improving the speed of which</span><br>                <span class="hljs-comment">#      improves parallelism</span><br>                sequence_lengths_for_cpp = self.dataset.sequence_lengths.copy()<br>            <span class="hljs-keyword">else</span>:<br>                sequence_lengths_for_cpp = self.dataset.sequence_lengths<br>            sample_index = helpers.build_sample_idx(<br>                sequence_lengths_for_cpp,<br>                document_index,<br>                sequence_length,<br>                num_epochs,<br>                num_tokens_per_epoch,<br>                drop_last_partial_sequence,<br>                self.config.add_extra_token_to_sequence,<br>            )<br><br>            <span class="hljs-comment"># Build the shuffle index</span><br>            <span class="hljs-keyword">if</span> separate_final_epoch:<br>                shuffle_index = _build_shuffle_index(<br>                    num_samples_sans_final_epoch, sample_index.shape[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>, numpy_random_state<br>                )<br>            <span class="hljs-keyword">else</span>:<br>                shuffle_index = _build_shuffle_index(<br>                    sample_index.shape[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>, sample_index.shape[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>, numpy_random_state<br>                )<br><br>            <span class="hljs-keyword">if</span> path_to_cache:<br>                os.makedirs(path_to_cache, exist_ok=<span class="hljs-literal">True</span>)<br>                <span class="hljs-comment"># Write the description</span><br>                <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(path_to_description, <span class="hljs-string">&quot;wt&quot;</span>) <span class="hljs-keyword">as</span> writer:<br>                    writer.write(self.unique_description)<br>                numpy.save(path_to_document_index, document_index, allow_pickle=<span class="hljs-literal">True</span>)<br>                numpy.save(path_to_sample_index, sample_index, allow_pickle=<span class="hljs-literal">True</span>)<br>                numpy.save(path_to_shuffle_index, shuffle_index, allow_pickle=<span class="hljs-literal">True</span>)<br>            <span class="hljs-keyword">else</span>:<br>                log_single_rank(<br>                    logger,<br>                    logging.WARNING,<br>                    <span class="hljs-string">f&quot;Unable to save <span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(self).__name__&#125;</span> indexes because path_to_cache is None&quot;</span>,<br>                )<br><br>            t_end = time.time()<br>            log_single_rank(logger, logging.DEBUG, <span class="hljs-string">f&quot;\t&gt; time elapsed: <span class="hljs-subst">&#123;t_end - t_beg:4f&#125;</span> seconds&quot;</span>)<br><br>            log_single_rank(<br>                logger, logging.INFO, <span class="hljs-string">f&quot;&gt; total number of samples: <span class="hljs-subst">&#123;sample_index.shape[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>&#125;</span>&quot;</span><br>            )<br>            log_single_rank(logger, logging.INFO, <span class="hljs-string">f&quot;&gt; total number of epochs: <span class="hljs-subst">&#123;num_epochs&#125;</span>&quot;</span>)<br><br>            <span class="hljs-keyword">return</span> document_index, sample_index, shuffle_index<br><br>        log_single_rank(<br>            logger, logging.INFO, <span class="hljs-string">f&quot;Load the <span class="hljs-subst">&#123;<span class="hljs-built_in">type</span>(self).__name__&#125;</span> <span class="hljs-subst">&#123;self.index_split.name&#125;</span> indices&quot;</span><br>        )<br><br>        log_single_rank(<br>            logger,<br>            logging.INFO,<br>            <span class="hljs-string">f&quot;\tLoad the document index from <span class="hljs-subst">&#123;os.path.basename(path_to_document_index)&#125;</span>&quot;</span>,<br>        )<br>        t_beg = time.time()<br>        document_index = numpy.load(path_to_document_index, allow_pickle=<span class="hljs-literal">True</span>, mmap_mode=<span class="hljs-string">&quot;r&quot;</span>)<br>        t_end = time.time()<br>        log_single_rank(logger, logging.DEBUG, <span class="hljs-string">f&quot;\t&gt; time elapsed: <span class="hljs-subst">&#123;t_end - t_beg:4f&#125;</span> seconds&quot;</span>)<br><br>        log_single_rank(<br>            logger,<br>            logging.INFO,<br>            <span class="hljs-string">f&quot;\tLoad the sample index from <span class="hljs-subst">&#123;os.path.basename(path_to_sample_index)&#125;</span>&quot;</span>,<br>        )<br>        t_beg = time.time()<br>        sample_index = numpy.load(path_to_sample_index, allow_pickle=<span class="hljs-literal">True</span>, mmap_mode=<span class="hljs-string">&quot;r&quot;</span>)<br>        t_end = time.time()<br>        log_single_rank(logger, logging.DEBUG, <span class="hljs-string">f&quot;\t&gt; time elapsed: <span class="hljs-subst">&#123;t_end - t_beg:4f&#125;</span> seconds&quot;</span>)<br><br>        log_single_rank(<br>            logger,<br>            logging.INFO,<br>            <span class="hljs-string">f&quot;\tLoad the shuffle index from <span class="hljs-subst">&#123;os.path.basename(path_to_shuffle_index)&#125;</span>&quot;</span>,<br>        )<br>        t_beg = time.time()<br>        shuffle_index = numpy.load(path_to_shuffle_index, allow_pickle=<span class="hljs-literal">True</span>, mmap_mode=<span class="hljs-string">&quot;r&quot;</span>)<br>        t_end = time.time()<br>        log_single_rank(logger, logging.DEBUG, <span class="hljs-string">f&quot;\t&gt; time elapsed: <span class="hljs-subst">&#123;t_end - t_beg:4f&#125;</span> seconds&quot;</span>)<br><br>        log_single_rank(<br>            logger, logging.INFO, <span class="hljs-string">f&quot;&gt; total number of samples: <span class="hljs-subst">&#123;sample_index.shape[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>&#125;</span>&quot;</span><br>        )<br><br>        <span class="hljs-keyword">return</span> document_index, sample_index, shuffle_index<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_num_tokens_per_epoch</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Calculate the number of tokens in a single epoch</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            int: The number of tokens in a single epoch</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">int</span>(numpy.<span class="hljs-built_in">sum</span>(self.dataset.sequence_lengths[self.indices]))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_num_epochs</span>(<span class="hljs-params">self, num_tokens_per_epoch: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;Calculate the number of epochs</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            num_tokens_per_epoch (int): The number of tokens in a single epoch</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            int: The number of epochs</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        num_epochs = <span class="hljs-number">1</span><br>        num_tokens = num_tokens_per_epoch<br>        <span class="hljs-keyword">if</span> self.num_samples <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span> num_epochs<br>        <span class="hljs-keyword">else</span>:<br>            num_tokens_requested = (<br>                self.num_samples * self.config.sequence_length<br>            ) + self.config.add_extra_token_to_sequence<br>            <span class="hljs-keyword">while</span> num_tokens &lt; num_tokens_requested:<br>                num_epochs += <span class="hljs-number">1</span><br>                num_tokens += num_tokens_per_epoch<br>        <span class="hljs-keyword">return</span> num_epochs<br><br></code></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="MegatronPretrainingSampler"><a href="#MegatronPretrainingSampler" class="headerlink" title="MegatronPretrainingSampler"></a><code>MegatronPretrainingSampler</code></h3><p>在得到可以获取单个GPT训练条目的mid-level dataset(train_ds, valid_ds, test_ds)后，<code>MegatronPretrainingSampler</code>会对其进行包裹，<code>MegatronPretrainingSampler</code>作为一个<strong>批采样器（batch sampler）</strong>：它不返回单个样本索引，而是每次 <code>yield</code> 一组索引（一个 micro-batch），供 PyTorch DataLoader(…, batch_sampler&#x3D;…) 直接使用。它的核心目标是：</p>
<ul>
<li><p>在 <strong>Data Parallel</strong> 训练中，把“全局 batch”（&#x3D; micro_batch_size * data_parallel_size）按 <strong>DP rank</strong> 切分成每个 rank 自己的 micro-batch 索引。</p>
</li>
<li><p>支持从某个 consumed_samples 开始继续取样（用于 resume &#x2F; 断点续训 &#x2F; 跳过已训练样本）。</p>
</li>
</ul>
<p>代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MegatronPretrainingSampler</span>:<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, total_samples, consumed_samples, micro_batch_size,</span><br><span class="hljs-params">                 data_parallel_rank, data_parallel_size, drop_last=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-comment"># Keep a copy of input params for later use.</span><br>        self.total_samples = total_samples<br>        self.consumed_samples = consumed_samples<br>        self.micro_batch_size = micro_batch_size<br>        self.data_parallel_rank = data_parallel_rank<br>        self.micro_batch_times_data_parallel_size = \<br>            self.micro_batch_size * data_parallel_size<br>        self.drop_last = drop_last<br><br>        <span class="hljs-comment"># Sanity checks.</span><br>        <span class="hljs-keyword">assert</span> self.total_samples &gt; <span class="hljs-number">0</span>, \<br>            <span class="hljs-string">&#x27;no sample to consume: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.total_samples)<br>        <span class="hljs-keyword">assert</span> self.consumed_samples &lt; self.total_samples, \<br>            <span class="hljs-string">&#x27;no samples left to consume: &#123;&#125;, &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.consumed_samples,<br>                                                        self.total_samples)<br>        <span class="hljs-keyword">assert</span> self.micro_batch_size &gt; <span class="hljs-number">0</span><br>        <span class="hljs-keyword">assert</span> data_parallel_size &gt; <span class="hljs-number">0</span><br>        <span class="hljs-keyword">assert</span> self.data_parallel_rank &lt; data_parallel_size, \<br>            <span class="hljs-string">&#x27;data_parallel_rank should be smaller than data size: &#123;&#125;, &#x27;</span> \<br>            <span class="hljs-string">&#x27;&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.data_parallel_rank, data_parallel_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self.total_samples<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_start_end_idx</span>(<span class="hljs-params">self</span>):<br>        start_idx = self.data_parallel_rank * self.micro_batch_size<br>        end_idx = start_idx + self.micro_batch_size<br>        <span class="hljs-keyword">return</span> start_idx, end_idx<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__iter__</span>(<span class="hljs-params">self</span>):<br>        batch = []<br>        <span class="hljs-comment"># Last batch will be dropped if drop_last is not set False</span><br>        <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.consumed_samples, self.total_samples):<br>            batch.append(idx)<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(batch) == self.micro_batch_times_data_parallel_size:<br>                start_idx, end_idx = self.get_start_end_idx()<br>                <span class="hljs-keyword">yield</span> batch[start_idx:end_idx]<br>                batch = []<br><br>        <span class="hljs-comment"># Check the last partial batch and see drop_last is set</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(batch) &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self.drop_last:<br>            start_idx, end_idx = self.get_start_end_idx()<br>            <span class="hljs-keyword">yield</span> batch[start_idx:end_idx]<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>在初始化阶段，其记录了当前所属的dp rank，并且通过<code>self.micro_batch_times_data_parallel_size =  self.micro_batch_size * data_parallel_size</code>计算了在数据并行下一次需要读取的实际数量</p>
</li>
<li><p>其获取迭代数据时，遍历范围为(self.consumed_samples, self.total_samples)，它会在其中获取<code>micro_batch_times_data_parallel_size</code>个训练样本的idx，然后依据当前所属的dp rank计算得到本rank实际需要的miro_batch个idx</p>
</li>
</ul>
<h3 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a><code>DataLoader</code></h3><p>在<code>build_pretraining_data_loader</code>中最终会结合mid-level dataset（<code>GPTDataset</code>）以及<code>MegatronPretrainingSampler</code>得到<code>torch.utils.data.DataLoader</code>。从而支持通过该DataLoader获取一个mocro_batch大小的训练样本。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">return torch.utils.data.DataLoader(dataset,<br>                                   batch_sampler=batch_sampler,<br>                                   num_workers=args.num_workers,<br>                                   pin_memory=True,<br>                                   persistent_workers=True if args.num_workers &gt; 0 else False,<br>                                   )<br></code></pre></td></tr></table></figure>

<h2 id="数据集使用流程"><a href="#数据集使用流程" class="headerlink" title="数据集使用流程"></a>数据集使用流程</h2><ol>
<li>在用户提供的forward_step函数中，就会传入类别为<code>torch.utils.data.DataLoader</code>的<code>data_iterator</code>，如下所示：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_step</span>(<span class="hljs-params">data_iterator, model: GPTModel, return_schedule_plan: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Forward training step.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        data_iterator : Input data iterator</span><br><span class="hljs-string">        model (GPTModel): The GPT Model</span><br><span class="hljs-string">        return_schedule_plan (bool): Whether to return the schedule plan instead of the output tensor</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    args = get_args()<br>    timers = get_timers()<br><br>    <span class="hljs-comment"># Get the batch.</span><br>    timers(<span class="hljs-string">&#x27;batch-generator&#x27;</span>, log_level=<span class="hljs-number">2</span>).start()<br>    <span class="hljs-keyword">global</span> stimer<br>    <span class="hljs-keyword">with</span> stimer(bdata=<span class="hljs-literal">True</span>):<br>        tokens, labels, loss_mask, attention_mask, position_ids = get_batch(data_iterator)<br>    timers(<span class="hljs-string">&#x27;batch-generator&#x27;</span>).stop()<br><br>    <span class="hljs-keyword">with</span> stimer:<br>        <span class="hljs-keyword">if</span> args.use_legacy_models:<br>            output_tensor = model(tokens, position_ids, attention_mask, labels=labels)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> return_schedule_plan:<br>                <span class="hljs-keyword">assert</span> args.overlap_moe_expert_parallel_comm, \<br>                    <span class="hljs-string">&quot;overlap_moe_expert_parallel_comm must be enabled to return the schedule plan&quot;</span><br>                schedule_plan = model.build_schedule_plan(<br>                    tokens, position_ids, attention_mask, labels=labels, loss_mask=loss_mask<br>                )<br>                <span class="hljs-keyword">return</span> schedule_plan, partial(loss_func, loss_mask, model=model)<br>            <span class="hljs-keyword">else</span>:<br>                output_tensor = model(<br>                    tokens, position_ids, attention_mask, labels=labels, loss_mask=loss_mask<br>                )<br><br>    <span class="hljs-comment"># [ModelOpt]: model is needed to access ModelOpt distillation losses</span><br>    <span class="hljs-keyword">return</span> output_tensor, partial(loss_func, loss_mask, model=model)<br>  <br></code></pre></td></tr></table></figure>

<ul>
<li>然后在获取训练数据的get_batch函数中会查看是否是pp并行的第一个或最后一个，如此才会去获取训练数据，获取数据依赖的是get_batch_on_this_tp_rank和get_batch_on_this_cp_rank，如下所示。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_batch</span>(<span class="hljs-params">data_iterator</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Generate a batch.&quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> this is pretty hacky, find a better way</span><br>    <span class="hljs-keyword">if</span> (<span class="hljs-keyword">not</span> parallel_state.is_pipeline_first_stage(ignore_virtual=<span class="hljs-literal">True</span>)) <span class="hljs-keyword">and</span> (<br>        <span class="hljs-keyword">not</span> parallel_state.is_pipeline_last_stage(ignore_virtual=<span class="hljs-literal">True</span>)<br>    ):<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># get batches based on the TP rank you are on</span><br>    batch = get_batch_on_this_tp_rank(data_iterator)<br><br>    <span class="hljs-comment"># slice batch along sequence dimension for context parallelism</span><br>    batch = get_batch_on_this_cp_rank(batch)<br><br>    <span class="hljs-keyword">return</span> batch.values()<br><br></code></pre></td></tr></table></figure>

<ul>
<li>首先是<code>get_batch_on_this_tp_rank</code>，由于前述在构造数据集的时候只会在tp rank&#x3D;0的时候构造，所以这里会查看当前rank，如果是tp rank&#x3D;0的worker就通过<code>next(data_iterator)</code>获取一批micro_batch数据，然后与其他tp rank≠0的worker进行broadcast，传播各数据。代码如下所示：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_batch_on_this_tp_rank</span>(<span class="hljs-params">data_iterator</span>):<br><br>    args = get_args()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_broadcast</span>(<span class="hljs-params">item</span>):<br>        <span class="hljs-keyword">if</span> item <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            torch.distributed.broadcast(<br>                item,<br>                mpu.get_tensor_model_parallel_src_rank(),<br>                group=mpu.get_tensor_model_parallel_group(),<br>            )<br><br>    <span class="hljs-keyword">if</span> mpu.get_tensor_model_parallel_rank() == <span class="hljs-number">0</span>:<br><br>        <span class="hljs-keyword">if</span> data_iterator <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            data = <span class="hljs-built_in">next</span>(data_iterator)<br>        <span class="hljs-keyword">else</span>:<br>            data = <span class="hljs-literal">None</span><br><br>        batch = &#123;<br>            <span class="hljs-string">&#x27;tokens&#x27;</span>: data[<span class="hljs-string">&quot;tokens&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>            <span class="hljs-string">&#x27;labels&#x27;</span>: data[<span class="hljs-string">&quot;labels&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>            <span class="hljs-string">&#x27;loss_mask&#x27;</span>: data[<span class="hljs-string">&quot;loss_mask&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>            <span class="hljs-string">&#x27;attention_mask&#x27;</span>: (<br>                <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;attention_mask&quot;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> data<br>                <span class="hljs-keyword">else</span> data[<span class="hljs-string">&quot;attention_mask&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>)<br>            ),<br>            <span class="hljs-string">&#x27;position_ids&#x27;</span>: data[<span class="hljs-string">&quot;position_ids&quot;</span>].cuda(non_blocking=<span class="hljs-literal">True</span>),<br>        &#125;<br><br>        <span class="hljs-keyword">if</span> args.pipeline_model_parallel_size == <span class="hljs-number">1</span>:<br>            _broadcast(batch[<span class="hljs-string">&#x27;tokens&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;labels&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;loss_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;position_ids&#x27;</span>])<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_first_stage():<br>            _broadcast(batch[<span class="hljs-string">&#x27;tokens&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;position_ids&#x27;</span>])<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_last_stage():<br>            <span class="hljs-comment"># Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.</span><br>            <span class="hljs-comment"># Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need</span><br>            <span class="hljs-comment"># to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.</span><br>            <span class="hljs-keyword">if</span> args.mtp_num_layers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                _broadcast(batch[<span class="hljs-string">&#x27;tokens&#x27;</span>])<br>                _broadcast(batch[<span class="hljs-string">&#x27;position_ids&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;labels&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;loss_mask&#x27;</span>])<br>            _broadcast(batch[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])<br><br>    <span class="hljs-keyword">else</span>:<br><br>        tokens = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.int64,<br>            device=torch.cuda.current_device(),<br>        )<br>        labels = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.int64,<br>            device=torch.cuda.current_device(),<br>        )<br>        loss_mask = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.float32,<br>            device=torch.cuda.current_device(),<br>        )<br>        <span class="hljs-keyword">if</span> args.create_attention_mask_in_dataloader:<br>            attention_mask = torch.empty(<br>                (args.micro_batch_size, <span class="hljs-number">1</span>, args.seq_length, args.seq_length),<br>                dtype=torch.<span class="hljs-built_in">bool</span>,<br>                device=torch.cuda.current_device(),<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            attention_mask = <span class="hljs-literal">None</span><br>        position_ids = torch.empty(<br>            (args.micro_batch_size, args.seq_length),<br>            dtype=torch.int64,<br>            device=torch.cuda.current_device(),<br>        )<br><br>        <span class="hljs-keyword">if</span> args.pipeline_model_parallel_size == <span class="hljs-number">1</span>:<br>            _broadcast(tokens)<br>            _broadcast(labels)<br>            _broadcast(loss_mask)<br>            _broadcast(attention_mask)<br>            _broadcast(position_ids)<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_first_stage():<br>            labels = <span class="hljs-literal">None</span><br>            loss_mask = <span class="hljs-literal">None</span><br><br>            _broadcast(tokens)<br>            _broadcast(attention_mask)<br>            _broadcast(position_ids)<br><br>        <span class="hljs-keyword">elif</span> mpu.is_pipeline_last_stage():<br>            <span class="hljs-comment"># Multi-Token Prediction (MTP) layers need tokens and position_ids to calculate embedding.</span><br>            <span class="hljs-comment"># Currently the Multi-Token Prediction (MTP) layers is fixed on the last stage, so we need</span><br>            <span class="hljs-comment"># to broadcast tokens and position_ids to all of the tensor parallel ranks on the last stage.</span><br>            <span class="hljs-keyword">if</span> args.mtp_num_layers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                _broadcast(tokens)<br>                _broadcast(position_ids)<br>            <span class="hljs-keyword">else</span>:<br>                tokens = <span class="hljs-literal">None</span><br>                position_ids = <span class="hljs-literal">None</span><br><br>            _broadcast(labels)<br>            _broadcast(loss_mask)<br>            _broadcast(attention_mask)<br><br>        batch = &#123;<br>            <span class="hljs-string">&#x27;tokens&#x27;</span>: tokens,<br>            <span class="hljs-string">&#x27;labels&#x27;</span>: labels,<br>            <span class="hljs-string">&#x27;loss_mask&#x27;</span>: loss_mask,<br>            <span class="hljs-string">&#x27;attention_mask&#x27;</span>: attention_mask,<br>            <span class="hljs-string">&#x27;position_ids&#x27;</span>: position_ids,<br>        &#125;<br><br>    <span class="hljs-keyword">return</span> batch<br><br></code></pre></td></tr></table></figure>

<ul>
<li>然后是执行<code>get_batch_on_this_cp_rank</code>，在上下文并行中，为了GPU负载均衡，我们往往采用的是之字型计算划分，如下所示。所以这里进行CP维度切分的核心思想是将序列切分为2*CP份，然后每第i个cp rank拿走前面的第i份以及倒数第i份，从而平衡计算负载。代码如下</li>
</ul>
<p><img src="/2025/12/28/megatron-lm-ddp/image.png" srcset="/img/loading.gif" lazyload></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_batch_on_this_cp_rank</span>(<span class="hljs-params">batch: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Slice batch input along sequence dimension into multiple chunks,</span><br><span class="hljs-string">    which are parallelized across GPUs in a context parallel group.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># With causal masking, each token only attends to its prior tokens. Simply split</span><br>    <span class="hljs-comment"># sequence into CP chunks can result in severe load imbalance. That&#x27;s to say, chunks</span><br>    <span class="hljs-comment"># at the end of sequence have bigger workload than others. To address this issue,</span><br>    <span class="hljs-comment"># we split sequence into 2*CP ranks. Assuming CP=2, we then get 4 chunks, chunk_0</span><br>    <span class="hljs-comment"># and chunk_3 are assigned to GPU0, chunk_1 and chunk_2 are assigned to GPU1, so</span><br>    <span class="hljs-comment"># that we can get balanced workload among GPUs in a context parallel group.</span><br>    cp_size = parallel_state.get_context_parallel_world_size()<br>    <span class="hljs-keyword">if</span> cp_size &gt; <span class="hljs-number">1</span>:<br>        cp_rank = parallel_state.get_context_parallel_rank()<br>        <span class="hljs-keyword">for</span> key, val <span class="hljs-keyword">in</span> batch.items():<br>            <span class="hljs-keyword">if</span> val <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                seq_dim = <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> key != <span class="hljs-string">&quot;attention_mask&quot;</span> <span class="hljs-keyword">else</span> <span class="hljs-number">2</span><br>                val = val.view(<br>                    *val.shape[<span class="hljs-number">0</span>:seq_dim],<br>                    <span class="hljs-number">2</span> * cp_size,<br>                    val.shape[seq_dim] // (<span class="hljs-number">2</span> * cp_size),<br>                    *val.shape[(seq_dim + <span class="hljs-number">1</span>) :],<br>                )<br>                index = torch.zeros(<span class="hljs-number">2</span>, dtype=torch.int64, device=val.device)<br>                index[<span class="hljs-number">0</span>].fill_(cp_rank)<br>                index[<span class="hljs-number">1</span>].fill_(<span class="hljs-number">2</span> * cp_size - cp_rank - <span class="hljs-number">1</span>)<br>                val = val.index_select(seq_dim, index)<br>                val = val.view(*val.shape[<span class="hljs-number">0</span>:seq_dim], -<span class="hljs-number">1</span>, *val.shape[(seq_dim + <span class="hljs-number">2</span>) :])<br>                batch[key] = val<br><br>    <span class="hljs-keyword">return</span> batch<br><br></code></pre></td></tr></table></figure>

<ul>
<li>得到batch中的tokens, labels, loss_mask, attention_mask, position_ids数据后，就可以调用model进行前向传播计算了。最终的结果就是每个dp会使用不同的micro_batch数据，同一个dp中只有pp并行中的第一位和最后一位获取了数据集，这些pp中的各tp获取的都是同一份数据，如果有cp会进一步对这数据进行切分。</li>
</ul>
<h1 id="数据并行训练"><a href="#数据并行训练" class="headerlink" title="数据并行训练"></a>数据并行训练</h1><p>上述已经分析了数据并行中数据集是如何划分给各个worker的以及在一个训练step中是如何获取数据集的，现在看在训练过程中是如何进行dp间梯度同步的。</p>
<p>在一次训练迭代中，其<code>train_step</code>会完成一次完整的step，如下所示，其会调用<code>forward_backward_func</code>完成一个step的训练：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">losses_reduced = forward_backward_func(<br>    forward_step_func=forward_step_func,<br>    data_iterator=data_iterator,<br>    model=model,<br>    num_microbatches=get_num_microbatches(),<br>    seq_length=args.seq_length,<br>    micro_batch_size=args.micro_batch_size,<br>    decoder_seq_length=args.decoder_seq_length,<br>    forward_only=False,<br>    adjust_tensor_shapes_fn=adjust_tensor_shapes_fn,<br>)<br></code></pre></td></tr></table></figure>

<p><code>forward_backward_func</code>依据pp并行有多种，我们这里查看没有pp并行的<code>forward_backward_no_pipelining</code>，其代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_backward_no_pipelining</span>(<span class="hljs-params"></span><br><span class="hljs-params">    *,</span><br><span class="hljs-params">    forward_step_func,</span><br><span class="hljs-params">    data_iterator: <span class="hljs-type">Union</span>[Iterator, <span class="hljs-type">List</span>[Iterator]],</span><br><span class="hljs-params">    model: <span class="hljs-type">Union</span>[torch.nn.Module, <span class="hljs-type">List</span>[torch.nn.Module]],</span><br><span class="hljs-params">    num_microbatches: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    seq_length: <span class="hljs-built_in">int</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    micro_batch_size: <span class="hljs-built_in">int</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    decoder_seq_length: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    forward_only: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    collect_non_loss_data: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    first_val_step: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">bool</span>] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    adjust_tensor_shapes_fn: <span class="hljs-type">Optional</span>[<span class="hljs-type">Callable</span>] = <span class="hljs-literal">None</span>,  <span class="hljs-comment"># unused</span></span><br><span class="hljs-params">    grad_finalize_pgs: <span class="hljs-type">Optional</span>[GradFinalizeProcessGroups] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Run forward and backward passes with no pipeline parallelism&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">if</span> grad_finalize_pgs <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        tp_group = parallel_state.get_tensor_model_parallel_group()<br>        cp_group = parallel_state.get_context_parallel_group()<br>        embd_group = parallel_state.get_embedding_group(check_initialized=<span class="hljs-literal">False</span>)<br>        pp_group = parallel_state.get_pipeline_model_parallel_group()<br>        pos_emb_group = parallel_state.get_position_embedding_group(check_initialized=<span class="hljs-literal">False</span>)<br>        grad_finalize_pgs = GradFinalizeProcessGroups()<br>        grad_finalize_pgs.tp = tp_group<br>        grad_finalize_pgs.cp = cp_group<br>        grad_finalize_pgs.embd = embd_group<br>        grad_finalize_pgs.pos_embd = pos_emb_group<br>        grad_finalize_pgs.pp = pp_group<br>        grad_finalize_pgs.dp_cp = parallel_state.get_data_parallel_group(<br>            with_context_parallel=<span class="hljs-literal">True</span>, partial_data_parallel=<span class="hljs-literal">False</span><br>        )<br><br>    <span class="hljs-keyword">elif</span> grad_finalize_pgs <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;tp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;cp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;embd&#x27;</span>), (<br>            <span class="hljs-string">&quot;grad_finalize_pgs must have a embd. In previous version, it is used default &quot;</span><br>            <span class="hljs-string">&quot;`parallel_state.default_embedding_ranks` to create the process group. If you are &quot;</span><br>            <span class="hljs-string">&quot;using the default process group, please use `parallel_state.get_embedding_group()` &quot;</span><br>            <span class="hljs-string">&quot;to get the process group. If you don&#x27;t need explicitly set it to None.&quot;</span><br>        )<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;pos_embd&#x27;</span>), (<br>            <span class="hljs-string">&quot;grad_finalize_pgs must have a pos_embd. In previous version, it is used default &quot;</span><br>            <span class="hljs-string">&quot;`parallel_state.default_position_embedding_ranks` to create the process group. &quot;</span><br>            <span class="hljs-string">&quot;If you are using the default process group, &quot;</span><br>            <span class="hljs-string">&quot;please use `parallel_state.get_position_embedding_group()` &quot;</span><br>            <span class="hljs-string">&quot;to get the process group. If you don&#x27;t need explicitly set it to None.&quot;</span><br>        )<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;pp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;dp_cp&#x27;</span>)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(model, <span class="hljs-built_in">list</span>):<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(model) == <span class="hljs-number">1</span>, <span class="hljs-string">&quot;non-pipeline-parallel schedule does not support model chunking&quot;</span><br>        model = model[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(data_iterator, <span class="hljs-built_in">list</span>):<br>        <span class="hljs-keyword">assert</span> (<br>            <span class="hljs-built_in">len</span>(data_iterator) == <span class="hljs-number">1</span><br>        ), <span class="hljs-string">&quot;non-pipeline-parallel schedule does not support model chunking&quot;</span><br>        data_iterator = data_iterator[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">assert</span> (<br>        adjust_tensor_shapes_fn <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;adjust_tensor_shapes_fn is not supported for non-pipeline-parallel schedule&quot;</span><br><br>    config = get_model_config(model)<br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;forward-backward&#x27;</span>, log_level=<span class="hljs-number">1</span>).start(barrier=config.barrier_with_L1_time)<br><br>    no_sync_func = config.no_sync_func<br>    <span class="hljs-keyword">if</span> no_sync_func <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        no_sync_func = contextlib.nullcontext<br><br>    model_type = get_model_type(model)<br><br>    forward_data_store = []<br>    input_tensor, output_tensor_grad = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br>    total_num_tokens = torch.zeros([], dtype=torch.<span class="hljs-built_in">int</span>, device=<span class="hljs-string">&quot;cuda&quot;</span>)<br><br>    <span class="hljs-keyword">if</span> config.overlap_moe_expert_parallel_comm <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> forward_only:<br>        forward_data_store, total_num_tokens = combined_1f1b_schedule_for_no_pipelining(<br>            forward_step_func,<br>            data_iterator,<br>            model,<br>            num_microbatches,<br>            input_tensor,<br>            output_tensor_grad,<br>            forward_data_store,<br>            config,<br>            collect_non_loss_data,<br>            first_val_step,<br>            forward_only,<br>            no_sync_func,<br>            total_num_tokens,<br>            partial(check_first_val_step, first_val_step, forward_only),<br>        )<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">with</span> no_sync_func():<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_microbatches - <span class="hljs-number">1</span>):<br>                output_tensor, num_tokens = forward_step(<br>                    forward_step_func,<br>                    data_iterator,<br>                    model,<br>                    num_microbatches,<br>                    input_tensor,<br>                    forward_data_store,<br>                    config,<br>                    grad_finalize_pgs.cp.size(),<br>                    collect_non_loss_data,<br>                    is_first_microbatch=check_first_val_step(first_val_step, forward_only, i == <span class="hljs-number">0</span>),<br>                    current_microbatch=i,<br>                )<br>                total_num_tokens += num_tokens<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> forward_only:<br>                    backward_step(<br>                        input_tensor, output_tensor, output_tensor_grad, model_type, config<br>                    )<br>        <span class="hljs-comment"># Run computation for last microbatch out of context handler (want to</span><br>        <span class="hljs-comment"># synchronize gradients).</span><br>        output_tensor, num_tokens = forward_step(<br>            forward_step_func,<br>            data_iterator,<br>            model,<br>            num_microbatches,<br>            input_tensor,<br>            forward_data_store,<br>            config,<br>            grad_finalize_pgs.cp.size(),<br>            collect_non_loss_data,<br>            is_first_microbatch=check_first_val_step(<br>                first_val_step, forward_only, num_microbatches == <span class="hljs-number">1</span><br>            ),<br>            current_microbatch=num_microbatches - <span class="hljs-number">1</span>,<br>        )<br><br>        total_num_tokens += num_tokens<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> forward_only:<br>            backward_step(input_tensor, output_tensor, output_tensor_grad, model_type, config)<br><br>    <span class="hljs-keyword">if</span> config.finalize_model_grads_func <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> forward_only:<br>        <span class="hljs-comment"># Finalize model grads (perform full grad all-reduce / reduce-scatter for</span><br>        <span class="hljs-comment"># data parallelism and layernorm all-reduce for sequence parallelism).</span><br>        config.finalize_model_grads_func(<br>            [model],<br>            total_num_tokens <span class="hljs-keyword">if</span> config.calculate_per_token_loss <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>            grad_finalize_pgs=grad_finalize_pgs,<br>        )<br><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;forward-backward&#x27;</span>).stop()<br><br>    <span class="hljs-keyword">if</span> (<br>        <span class="hljs-built_in">hasattr</span>(config, <span class="hljs-string">&#x27;enable_cuda_graph&#x27;</span>)<br>        <span class="hljs-keyword">and</span> config.enable_cuda_graph<br>        <span class="hljs-keyword">and</span> config.cuda_graph_scope != <span class="hljs-string">&quot;full_iteration&quot;</span><br>    ):<br>        create_cudagraphs()<br><br>    <span class="hljs-keyword">return</span> forward_data_store<br><br></code></pre></td></tr></table></figure>

<ol>
<li>在前microbatch-1次训练中，会套上<code>with no_sync_func()</code>,并且只要不是<code>forward_only</code>模式就会在backward_step中，这里其实主要都是pp的逻辑，关键的是会使用<code>torch.autograd.backward</code>来传播梯度，在梯度传播中会对各个micro_batch产生的梯度进行累加。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward_step</span>(<span class="hljs-params"></span><br><span class="hljs-params">    input_tensor,</span><br><span class="hljs-params">    output_tensor,</span><br><span class="hljs-params">    output_tensor_grad,</span><br><span class="hljs-params">    model_type,</span><br><span class="hljs-params">    config,</span><br><span class="hljs-params">    pipeline_model_parallel_size=<span class="hljs-number">1</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Backward step through passed-in output tensor.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    If last stage, output_tensor_grad is None, otherwise gradient of loss</span><br><span class="hljs-string">    with respect to stage&#x27;s output tensor.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns gradient of loss with respect to input tensor (None if first</span><br><span class="hljs-string">    stage).&quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># <span class="hljs-doctag">NOTE:</span> This code currently can handle at most one skip connection. It</span><br>    <span class="hljs-comment"># needs to be modified slightly to support arbitrary numbers of skip</span><br>    <span class="hljs-comment"># connections.</span><br><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;backward-compute&#x27;</span>, log_level=<span class="hljs-number">2</span>).start()<br><br>    <span class="hljs-comment"># Retain the grad on the input_tensor.</span><br>    unwrap_input_tensor_grad = <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(input_tensor, <span class="hljs-built_in">list</span>):<br>        input_tensor = [input_tensor]<br>        unwrap_input_tensor_grad = <span class="hljs-literal">True</span><br>    <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> input_tensor:<br>        <span class="hljs-keyword">if</span> x <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            x.retain_grad()<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(output_tensor, <span class="hljs-built_in">list</span>):<br>        output_tensor = [output_tensor]<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(output_tensor_grad, <span class="hljs-built_in">list</span>):<br>        output_tensor_grad = [output_tensor_grad]<br><br>    <span class="hljs-comment"># Backward pass.</span><br>    <span class="hljs-keyword">if</span> output_tensor_grad[<span class="hljs-number">0</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> config.grad_scale_func <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        output_tensor[<span class="hljs-number">0</span>] = config.grad_scale_func(output_tensor[<span class="hljs-number">0</span>])<br><br>    <span class="hljs-comment"># In multi-modal models like VLM, some batches may not have images.</span><br>    <span class="hljs-comment"># When no image is present, the vision encoder (as a separate pipeline stage)</span><br>    <span class="hljs-comment"># will not participate in the computation.</span><br>    <span class="hljs-comment"># This results in a tensor that does not require gradients.</span><br>    <span class="hljs-comment"># In such cases, we intentionally skip the backward pass while preserving zero gradients.</span><br>    <span class="hljs-keyword">if</span> output_tensor[<span class="hljs-number">0</span>].requires_grad:<br>        <span class="hljs-keyword">if</span> config.deallocate_pipeline_outputs:<br>            custom_backward(output_tensor[<span class="hljs-number">0</span>], output_tensor_grad[<span class="hljs-number">0</span>])<br>        <span class="hljs-keyword">else</span>:<br>            torch.autograd.backward(output_tensor[<span class="hljs-number">0</span>], grad_tensors=output_tensor_grad[<span class="hljs-number">0</span>])<br><br>    <span class="hljs-comment"># Collect the grad of the input_tensor.</span><br>    input_tensor_grad = [<span class="hljs-literal">None</span>]<br>    <span class="hljs-keyword">if</span> input_tensor <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        input_tensor_grad = []<br>        <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> input_tensor:<br>            <span class="hljs-keyword">if</span> x <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                input_tensor_grad.append(<span class="hljs-literal">None</span>)<br>            <span class="hljs-keyword">else</span>:<br>                input_tensor_grad.append(x.grad)<br><br>    <span class="hljs-keyword">if</span> unwrap_input_tensor_grad:<br>        input_tensor_grad = input_tensor_grad[<span class="hljs-number">0</span>]<br><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;backward-compute&#x27;</span>).stop()<br><br>    <span class="hljs-keyword">return</span> input_tensor_grad<br><br></code></pre></td></tr></table></figure>

<ul>
<li><p>在最后第microbatch次进行训练时没有套上<code>with no_sync_func()</code>从而允许一些同步操作</p>
</li>
<li><p>而数据并行的梯度间同步其实是在后续的config.finalize_model_grads_func中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> config.finalize_model_grads_func <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> forward_only:<br>    <span class="hljs-comment"># Finalize model grads (perform full grad all-reduce / reduce-scatter for</span><br>    <span class="hljs-comment"># data parallelism and layernorm all-reduce for sequence parallelism).</span><br>    config.finalize_model_grads_func(<br>        [model],<br>        total_num_tokens <span class="hljs-keyword">if</span> config.calculate_per_token_loss <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>        grad_finalize_pgs=grad_finalize_pgs,<br>    )<br></code></pre></td></tr></table></figure>

<ol>
<li><p>config来自get_model_config(model)中，而<code>config.finalize_model_grads_func</code>是在<code>train</code>函数中添加的功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_model_config</span>(<span class="hljs-params">model</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;Returns the config attribute, allowed to return None&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> get_attr_wrapped_model(model, <span class="hljs-string">&quot;config&quot;</span>, allow_none=<span class="hljs-literal">False</span>)<br><br></code></pre></td></tr></table></figure>

<ol>
<li>添加的<code>finalize_model_grads</code>代码如下</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">finalize_model_grads</span>(<span class="hljs-params"></span><br><span class="hljs-params">    model: <span class="hljs-type">List</span>[torch.nn.Module],</span><br><span class="hljs-params">    num_tokens: <span class="hljs-type">Optional</span>[torch.Tensor] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    grad_finalize_pgs: <span class="hljs-type">Optional</span>[GradFinalizeProcessGroups] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    All-reduce all model grads across DP replicas, layernorm grads for sequence parallelism,</span><br><span class="hljs-string">    embedding grads across first and last pipeline stages (if not tied),</span><br><span class="hljs-string">    scale gradients by `num_tokens`.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    config = get_model_config(model[<span class="hljs-number">0</span>])<br>    <span class="hljs-keyword">if</span> grad_finalize_pgs <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;tp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;pp&#x27;</span>)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;embd&#x27;</span>), (<br>            <span class="hljs-string">&quot;grad_finalize_pgs must have a embd. In previous version, it is used default &quot;</span><br>            <span class="hljs-string">&quot;`parallel_state.default_embedding_ranks` to create the process group.&quot;</span><br>            <span class="hljs-string">&quot; If you are using the default process group, please use&quot;</span><br>            <span class="hljs-string">&quot; `parallel_state.get_embedding_group()` &quot;</span><br>            <span class="hljs-string">&quot;If you don&#x27;t need embd_group, you need to explicitly set it to None.&quot;</span><br>        )<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;pos_embd&#x27;</span>), (<br>            <span class="hljs-string">&quot;grad_finalize_pgs must have a pos_embd. In previous version, it is used default &quot;</span><br>            <span class="hljs-string">&quot;`parallel_state.default_position_embedding_ranks` to create the process group.&quot;</span><br>            <span class="hljs-string">&quot; If you are using the default process group, please use &quot;</span><br>            <span class="hljs-string">&quot; `parallel_state.get_position_embedding_group()` &quot;</span><br>            <span class="hljs-string">&quot;If you don&#x27;t need pos_embd_group, you need to explicitly set it to None.&quot;</span><br>        )<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">hasattr</span>(grad_finalize_pgs, <span class="hljs-string">&#x27;dp_cp&#x27;</span>)<br>        tp_group = grad_finalize_pgs.tp<br>        pp_group = grad_finalize_pgs.pp<br>        embd_group = grad_finalize_pgs.embd<br>        pos_emb_group = grad_finalize_pgs.pos_embd<br>        dp_cp_group = grad_finalize_pgs.dp_cp<br>    <span class="hljs-keyword">else</span>:<br>        tp_group = parallel_state.get_tensor_model_parallel_group()<br>        pp_group = parallel_state.get_pipeline_model_parallel_group()<br>        embd_group = parallel_state.get_embedding_group(check_initialized=<span class="hljs-literal">False</span>)<br>        pos_emb_group = parallel_state.get_position_embedding_group(check_initialized=<span class="hljs-literal">False</span>)<br>        dp_cp_group = parallel_state.get_data_parallel_group(with_context_parallel=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-comment"># All-reduce / reduce-scatter across DP replicas.</span><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;all-grads-sync&#x27;</span>, log_level=<span class="hljs-number">1</span>).start(barrier=config.barrier_with_L1_time)<br>    <span class="hljs-keyword">for</span> model_chunk <span class="hljs-keyword">in</span> model:<br>        model_chunk.finish_grad_sync()<br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;all-grads-sync&#x27;</span>).stop()<br><br>    <span class="hljs-comment"># All-reduce t_embedder grads (for pp &amp; vpp of DiT).</span><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;conditional-embedder-grads-all-reduce&#x27;</span>, log_level=<span class="hljs-number">1</span>).start(<br>            barrier=config.barrier_with_L1_time<br>        )<br>    _allreduce_conditional_embedding_grads(model, config, pp_group)<br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;conditional-embedder-grads-all-reduce&#x27;</span>).stop()<br><br>    <span class="hljs-comment"># All-reduce layer-norm grads (for sequence parallelism) and non-tensor parallel modules.</span><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;non-tensor-parallel-grads-all-reduce&#x27;</span>, log_level=<span class="hljs-number">1</span>).start(<br>            barrier=config.barrier_with_L1_time<br>        )<br>    _allreduce_non_tensor_model_parallel_grads(model, config, tp_group)<br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;non-tensor-parallel-grads-all-reduce&#x27;</span>).stop()<br><br>    <span class="hljs-comment"># All-reduce embedding grads (for pipeline parallelism).</span><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;embedding-grads-all-reduce&#x27;</span>, log_level=<span class="hljs-number">1</span>).start(<br>            barrier=config.barrier_with_L1_time<br>        )<br>    _allreduce_word_embedding_grads(model, config, embd_group, pp_group)<br>    _allreduce_position_embedding_grads(model, config, pos_emb_group, pp_group)<br><br>    <span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        config.timers(<span class="hljs-string">&#x27;embedding-grads-all-reduce&#x27;</span>).stop()<br><br>    <span class="hljs-keyword">if</span> config.moe_router_enable_expert_bias:<br>        _update_router_expert_bias(model, config)<br><br>    <span class="hljs-comment"># normalize gradients for per-token loss normalization.</span><br>    <span class="hljs-comment"># if we are using by the number of tokens, then we use that as a divisor. this number</span><br>    <span class="hljs-comment"># will be the total number of non-padded tokens in the global batch.</span><br>    <span class="hljs-keyword">if</span> num_tokens <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br><br>        <span class="hljs-comment"># the number of tokens is only present on the last stage, so broadcast it</span><br>        <span class="hljs-comment"># to the other ranks in the pipeline parallel group.</span><br>        <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(pp_group, <span class="hljs-built_in">list</span>)<br>        last_rank = get_pp_last_rank(pp_group)<br>        torch.distributed.broadcast(num_tokens, src=last_rank, group=pp_group)<br><br>        <span class="hljs-comment"># all-reduce across DP ranks.</span><br>        torch.distributed.all_reduce(num_tokens, group=dp_cp_group)<br>        <span class="hljs-keyword">for</span> model_chunk <span class="hljs-keyword">in</span> model:<br>            <span class="hljs-keyword">if</span> num_tokens &gt; <span class="hljs-number">0</span>:<br>                scaling = <span class="hljs-number">1.0</span> / num_tokens<br>                model_chunk.scale_gradients(scaling)<br><br></code></pre></td></tr></table></figure>

<ul>
<li>其中最关键的是如下触发各个model_chunk的<code>finish_grad_sync</code>梯度同步的代码：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># All-reduce / reduce-scatter across DP replicas.</span><br><span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    config.timers(<span class="hljs-string">&#x27;all-grads-sync&#x27;</span>, log_level=<span class="hljs-number">1</span>).start(barrier=config.barrier_with_L1_time)<br><span class="hljs-keyword">for</span> model_chunk <span class="hljs-keyword">in</span> model:<br>    model_chunk.finish_grad_sync()<br><span class="hljs-keyword">if</span> config.timers <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    config.timers(<span class="hljs-string">&#x27;all-grads-sync&#x27;</span>).stop()<br></code></pre></td></tr></table></figure>

<ul>
<li>而这函数的执行就与model的类型有关了，model的类型由<code>get_model</code>函数决定，如下所示，这里我们以最简单的DDP（<code>DistributedDataParallel</code>）进行举例分析</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs shell">if wrap_with_ddp:<br>    if args.use_torch_fsdp2:<br>        assert HAVE_FSDP2, &quot;Torch FSDP2 requires torch&gt;=2.4.0&quot;<br>        DP = torch_FSDP<br>    elif args.use_megatron_fsdp:<br>        DP = megatron_FSDP<br>    else:<br>        DP = DDP<br>        <br>    ...<br>    <br>    with torch.cuda.stream(torch.cuda.Stream()):<br>        model = [<br>            DP(<br>                config=config,<br>                ddp_config=ddp_config,<br>                module=model_chunk,<br>                # Turn off bucketing for model_chunk 2 onwards, since communication for these<br>                # model chunks is overlapped with compute anyway.<br>                disable_bucketing=(model_chunk_idx &gt; 0)<br>                or args.overlap_param_gather_with_optimizer_step,<br>            )<br>            for (model_chunk_idx, model_chunk) in enumerate(model)<br>        ]<br></code></pre></td></tr></table></figure>

<ul>
<li><code>DistributedDataParallel</code>中的<code>finish_grad_sync</code>函数如下所示，其对每个bucket_group 调用了<code>finish_grad_sync</code>。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">finish_grad_sync</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Finishes grad sync (all-reduce or reduce-scatter) communication operations</span><br><span class="hljs-string">    for all model gradients.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    When overlap_grad_reduce is set to True, waits for asynchronous communication</span><br><span class="hljs-string">    calls to complete. When overlap_grad_reduce is set to False, calls synchronous</span><br><span class="hljs-string">    communication ops.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">for</span> bucket_group <span class="hljs-keyword">in</span> self.bucket_groups + self.expert_parallel_bucket_groups:<br>        bucket_group.finish_grad_sync()<br></code></pre></td></tr></table></figure>

<ul>
<li>bucket_group 是<code>_ParamAndGradBucketGroup</code>类，其<code>finish_grad_sync</code>函数如下所示，这里看一般情况，也就是<code>self.ddp_config.overlap_grad_reduce</code>为False，即直接调用<code>start_grad_sync</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">finish_grad_sync</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Finishes grad sync (all-reduce or reduce-scatter) communication operations</span><br><span class="hljs-string">    for all buckets in the bucket group.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    When ddp_config.overlap_grad_reduce is set to True, waits for asynchronous</span><br><span class="hljs-string">    communication call to complete. When ddp_config.overlap_grad_reduce is set to False,</span><br><span class="hljs-string">    makes synchronous call.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    self.param_gather_dispatched = <span class="hljs-literal">False</span><br>    <span class="hljs-comment"># If overlap_grad_reduce is False, start (and finish) synchronous communication call here.</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.ddp_config.overlap_grad_reduce:<br>        self.start_grad_sync()<br>        <span class="hljs-keyword">return</span><br>    <span class="hljs-comment"># When using multiple DistOpt instances, we don&#x27;t need to sync here as we launch</span><br>    <span class="hljs-comment"># communications on a separate communication stream.</span><br>    <span class="hljs-keyword">if</span> self.ddp_config.num_distributed_optimizer_instances &gt; <span class="hljs-number">1</span>:<br>        torch.cuda.default_stream().wait_stream(self.communication_stream)<br>        <span class="hljs-keyword">return</span><br>    <span class="hljs-keyword">assert</span> self.grad_reduce_handle <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>, (<br>        <span class="hljs-string">f&quot;Communication call has not been issued for this bucket &quot;</span><br>        <span class="hljs-string">f&quot;(<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(self.params_with_grad)&#125;</span>/<span class="hljs-subst">&#123;<span class="hljs-built_in">len</span>(self.params)&#125;</span> params have grad available)&quot;</span><br>    )<br>    self.grad_reduce_handle.wait()<br>    self.grad_reduce_handle = <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure>

<ul>
<li><code>start_grad_sync</code>函数如下所示，其核心是<strong>把一组 bucket（连续 grad buffer 的若干切片）触发数据并行梯度同步</strong>，并支持“是否与反向计算重叠（overlap）”、“是否用分布式优化器（reduce-scatter）”、“是否多 DistOpt 实例（两级通信）”、“是否做梯度检查&#x2F;缩放”等。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">start_grad_sync</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Initiates grad sync (all-reduce or reduce-scatter) communication operations</span><br><span class="hljs-string">    for all buckets in the bucket group.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    When ddp_config.overlap_grad_reduce is set to True, dispatches an asynchronous</span><br><span class="hljs-string">    communication call. When ddp_config.overlap_grad_reduce is set to False, makes</span><br><span class="hljs-string">    synchronous call.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">assert</span> (<br>        self.grad_reduce_handle <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>    ), <span class="hljs-string">&quot;Should not have multiple communication calls outstanding at once&quot;</span><br><br>    <span class="hljs-keyword">if</span> self.ddp_config.check_for_nan_in_grad <span class="hljs-keyword">or</span> self.ddp_config.check_for_large_grads:<br>        self.check_grads(<br>            check_for_nan_or_inf=self.ddp_config.check_for_nan_in_grad,<br>            check_for_large=self.ddp_config.check_for_large_grads,<br>        )<br><br>    <span class="hljs-comment"># gradient_scaling_factor already takes into account whether we are computing</span><br>    <span class="hljs-comment"># an average or sum in the data-parallel collective.</span><br>    <span class="hljs-keyword">for</span> bucket <span class="hljs-keyword">in</span> self.buckets:<br>        <span class="hljs-keyword">if</span> bucket.gradient_scaling_factor != <span class="hljs-number">1.0</span>:<br>            bucket.grad_data *= bucket.gradient_scaling_factor<br><br>    <span class="hljs-comment"># Decide reduce_op.</span><br>    reduce_op = torch.distributed.ReduceOp.SUM<br>    <span class="hljs-keyword">if</span> self.ddp_config.average_in_collective:<br>        reduce_op = torch.distributed.ReduceOp.AVG<br><br>    <span class="hljs-comment"># We use the following stream synchronization for the gradient reduction</span><br>    <span class="hljs-comment"># within and across DistOpt instances.</span><br><br>    <span class="hljs-comment"># Compute Stream: -------------Gradient compute-------------------</span><br>    <span class="hljs-comment"># Comm. Stream:   ------(wait for NCCL)-----(wait for NCCL)-------</span><br>    <span class="hljs-comment"># NCCL Stream:          -------RS------     -------AR------</span><br><br>    <span class="hljs-comment"># Use async communications only when overlap_grad_reduce is True.</span><br>    async_op = (<br>        self.ddp_config.overlap_grad_reduce<br>        <span class="hljs-keyword">and</span> self.ddp_config.num_distributed_optimizer_instances == <span class="hljs-number">1</span><br>    )<br>    <span class="hljs-keyword">if</span> (<br>        self.ddp_config.num_distributed_optimizer_instances &gt; <span class="hljs-number">1</span><br>        <span class="hljs-keyword">and</span> self.ddp_config.overlap_grad_reduce<br>    ):<br>        <span class="hljs-comment"># Assign a communication stream if we have multiple DistOpt instances and we</span><br>        <span class="hljs-comment"># need to overlap communication.</span><br>        stream_context = torch.cuda.stream(self.communication_stream)<br><br>        <span class="hljs-comment"># The RS/AR communication stream needs to wait for the default stream</span><br>        <span class="hljs-comment"># to complete its gradient computation before launching the next</span><br>        <span class="hljs-comment"># gradient reduction collective.</span><br>        self.communication_stream.wait_stream(torch.cuda.default_stream())<br>    <span class="hljs-keyword">else</span>:<br>        stream_context = nullcontext()<br><br>    <span class="hljs-keyword">if</span> self.ddp_config.use_distributed_optimizer:<br>        communication_group = self.intra_distributed_optimizer_instance_group<br>    <span class="hljs-keyword">else</span>:<br>        communication_group = self.data_parallel_group<br><br>    <span class="hljs-comment"># Coalesce communication kernels across buckets in the bucket group.</span><br>    <span class="hljs-keyword">with</span> stream_context, _coalescing_manager(communication_group, async_ops=async_op) <span class="hljs-keyword">as</span> cm:<br>        <span class="hljs-keyword">for</span> idx, bucket <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.buckets):<br>            <span class="hljs-keyword">if</span> self.ddp_config.use_distributed_optimizer:<br>                <span class="hljs-keyword">if</span> self.cached_grad_buffer_shard_list[idx] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                    self.cached_grad_buffer_shard_list[idx] = shard_buffer(<br>                        bucket.grad_data, self.intra_distributed_optimizer_instance_size<br>                    )<br>                local_data_view = self.cached_grad_buffer_shard_list[idx][<br>                    self.intra_distributed_optimizer_instance_rank<br>                ]<br>                dist_reduce_scatter_func(<br>                    local_data_view,<br>                    bucket.grad_data,<br>                    op=reduce_op,<br>                    group=communication_group,<br>                    async_op=async_op,<br>                )<br>            <span class="hljs-keyword">else</span>:<br>                torch.distributed.all_reduce(<br>                    bucket.grad_data, op=reduce_op, group=communication_group, async_op=async_op<br>                )<br><br>    <span class="hljs-comment"># With multiple DistOpt instances, we need to all-reduce across instances.</span><br>    <span class="hljs-keyword">if</span> (<br>        self.ddp_config.use_distributed_optimizer<br>        <span class="hljs-keyword">and</span> self.ddp_config.num_distributed_optimizer_instances &gt; <span class="hljs-number">1</span><br>    ):<br>        <span class="hljs-keyword">assert</span> self.inter_distributed_optimizer_instance_group <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>        <span class="hljs-comment"># Create a new coalescing manager for the inter-instance all-reduce.</span><br>        <span class="hljs-keyword">with</span> (<br>            stream_context,<br>            _coalescing_manager(<br>                self.inter_distributed_optimizer_instance_group, async_ops=async_op<br>            ) <span class="hljs-keyword">as</span> cm,<br>        ):<br>            <span class="hljs-keyword">for</span> idx, bucket <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.buckets):<br>                <span class="hljs-keyword">if</span> self.cached_grad_buffer_shard_list[idx] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                    self.cached_grad_buffer_shard_list[idx] = shard_buffer(<br>                        bucket.grad_data, self.intra_distributed_optimizer_instance_size<br>                    )<br>                local_data_view = self.cached_grad_buffer_shard_list[idx][<br>                    self.intra_distributed_optimizer_instance_rank<br>                ]<br><br>                torch.distributed.all_reduce(<br>                    local_data_view,<br>                    op=reduce_op,<br>                    group=self.inter_distributed_optimizer_instance_group,<br>                    async_op=async_op,<br>                )<br><br>    <span class="hljs-keyword">if</span> async_op:<br>        self.grad_reduce_handle = cm<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># When using `_coalescing_manager`, even if a synchronous op (async_op=False) is used,</span><br>        <span class="hljs-comment"># `cm` is not None, which is different from when `_coalescing_manager` is not used in</span><br>        <span class="hljs-comment"># which case the torch.distributed._reduce_scatter_base() will return None. In order to</span><br>        <span class="hljs-comment"># maintain consistency with prior code, we need to manually set communication handle to</span><br>        <span class="hljs-comment"># None.</span><br>        self.grad_reduce_handle = <span class="hljs-literal">None</span><br><br></code></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<p>看DDP数据并行的torch profile结果也能证实这一系列链路的正确性：</p>
<p><img src="/2025/12/28/megatron-lm-ddp/image-1.png" srcset="/img/loading.gif" lazyload></p>
<p>后续还可以再看看FSDP下的数据并行实现。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/LLM/" class="category-chain-item">LLM</a>
  
  
    <span>></span>
    
  <a href="/categories/LLM/Megatron-LM/" class="category-chain-item">Megatron-LM</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/LLM/" class="print-no-link">#LLM</a>
      
        <a href="/tags/Megatron-LM/" class="print-no-link">#Megatron-LM</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>【Megatron-LM源码分析（四）】-DDP数据并行</div>
      <div>http://example.com/2025/12/28/megatron-lm-ddp/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>滑滑蛋</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年12月28日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/12/28/ByteScale-paper-note/" title="【论文阅读】ByteScale:Efficient Scaling of LLM Training with a  2048K Context Length on More Than 12,000">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">【论文阅读】ByteScale:Efficient Scaling of LLM Training with a  2048K Context Length on More Than 12,000</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/12/26/megatron-lm-profiler/" title="【Megatron-LM源码分析（三）】-性能分析">
                        <span class="hidden-mobile">【Megatron-LM源码分析（三）】-性能分析</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"WMtHomhQYlrbIodTwoPU3gTY-MdYXbMMI","appKey":"pZeun9WfI1yaQrIoUbvTQrXv","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"https://wmthomhq.api.lncldglobal.com","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         次
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
