

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.jpg">
  <link rel="icon" href="/img/favicon.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#1C262C">
  <meta name="author" content="滑滑蛋">
  <meta name="keywords" content="">
  
    <meta name="description" content="专有名词解释 warp：对模型进行包裹，使其具备fsdp的相关的分布式能力  shard: 对参数进行切分，得到每个rank sharded的参数  unshard: 将切分的参数allgather，得到完整的参数  reshard：将完整的参数释放，只保留每个rank的sharded的参数  sharded：切分后的参数  unsharded：完整的参数   fsdp概览如下图所示，首先对于Ze">
<meta property="og:type" content="article">
<meta property="og:title" content="【pytorch-fsdp 源代码阅读（一）】-全流程概览">
<meta property="og:url" content="http://example.com/2025/07/02/pytorch-fsdp-1/index.html">
<meta property="og:site_name" content="滑滑蛋的个人博客">
<meta property="og:description" content="专有名词解释 warp：对模型进行包裹，使其具备fsdp的相关的分布式能力  shard: 对参数进行切分，得到每个rank sharded的参数  unshard: 将切分的参数allgather，得到完整的参数  reshard：将完整的参数释放，只保留每个rank的sharded的参数  sharded：切分后的参数  unsharded：完整的参数   fsdp概览如下图所示，首先对于Ze">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2025/07/02/pytorch-fsdp-1/image-2.png">
<meta property="og:image" content="http://example.com/2025/07/02/pytorch-fsdp-1/image-3.png">
<meta property="og:image" content="http://example.com/2025/07/02/pytorch-fsdp-1/image.png">
<meta property="og:image" content="http://example.com/2025/07/02/pytorch-fsdp-1/image-1.png">
<meta property="og:image" content="http://example.com/2025/07/02/pytorch-fsdp-1/diagram.png">
<meta property="article:published_time" content="2025-07-02T12:53:03.000Z">
<meta property="article:modified_time" content="2025-10-17T06:34:59.904Z">
<meta property="article:author" content="滑滑蛋">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/2025/07/02/pytorch-fsdp-1/image-2.png">
  
  
  
  <title>【pytorch-fsdp 源代码阅读（一）】-全流程概览 - 滑滑蛋的个人博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.5","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":true,"baidu":"d38d21fca521d897798e5bdd940a90d0","google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"WMtHomhQYlrbIodTwoPU3gTY-MdYXbMMI","app_key":"pZeun9WfI1yaQrIoUbvTQrXv","server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- Baidu Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?d38d21fca521d897798e5bdd940a90d0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
      }
    </script>
  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', '');
        });
      }
    </script>
  

  

  

  

  
    
  



  
<meta name="generator" content="Hexo 7.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>滑滑蛋</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="【pytorch-fsdp 源代码阅读（一）】-全流程概览"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-07-02 20:53" pubdate>
          2025年7月2日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          74k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          371 分钟
        
      </span>
    

    
    
      
        <span id="leancloud-page-views-container" class="post-meta" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="leancloud-page-views"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">【pytorch-fsdp 源代码阅读（一）】-全流程概览</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="专有名词解释"><a href="#专有名词解释" class="headerlink" title="专有名词解释"></a>专有名词解释</h1><ul>
<li><p>warp：对模型进行包裹，使其具备fsdp的相关的分布式能力</p>
</li>
<li><p>shard: 对参数进行切分，得到每个rank sharded的参数</p>
</li>
<li><p>unshard: 将切分的参数allgather，得到完整的参数</p>
</li>
<li><p>reshard：将完整的参数释放，只保留每个rank的sharded的参数</p>
</li>
<li><p>sharded：切分后的参数</p>
</li>
<li><p>unsharded：完整的参数</p>
</li>
</ul>
<h1 id="fsdp概览"><a href="#fsdp概览" class="headerlink" title="fsdp概览"></a>fsdp概览</h1><p>如下图所示，首先对于Zero算法来说：</p>
<ul>
<li><p>Zero-1切分了优化器状态</p>
</li>
<li><p>Zero-2切分了优化器状态和梯度</p>
</li>
<li><p>Zero-3切分了优化器状态和梯度和参数</p>
</li>
</ul>
<p><img src="/2025/07/02/pytorch-fsdp-1/image-2.png" srcset="/img/loading.gif" lazyload></p>
<p>对于fsdp来说，它实际上就是Zero-3。传统的数据并行会在每个GPU上维护一份模型参数，梯度，优化器状态的副本，但是FSDP将这些状态分片到所有的数据并行worker中，并且可以选择将分片的模型参数卸载到CPU上，从而使得若现在有 $n$个GPU，某一层的参数量为 $m$，那么每个GPU会维护这一层 $m&#x2F;n$个参数。</p>
<p><img src="/2025/07/02/pytorch-fsdp-1/image-3.png" srcset="/img/loading.gif" lazyload></p>
<p>通常，模型层以嵌套方式用 FSDP 包装，因此在前向或后向计算期间，只有单个 FSDP 实例中的层需要将完整参数收集到单个设备。聚合到的完整参数会在计算后立即释放，释放的内存可以用于下一层的计算。通过这种方式，可以节省峰值 GPU 内存，因此可以扩展训练以使用更大的模型大小或更大的批量大小。为了进一步最大化内存效率，当实例在计算中不活动时，FSDP 可以将参数、梯度和优化器状态卸载到 CPU。</p>
<h1 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h1><p>如下是一个使用示例，简单来说有这几个关键步骤：</p>
<ol>
<li><p>定义自动 wrap 策略：只 wrap nn.Linear层</p>
</li>
<li><p>将模型用FSDP进行包裹，从而转变为fsdp_model</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># torchrun --nproc_per_node=2 --master_port=47123 test.py</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> torch.distributed.fsdp <span class="hljs-keyword">import</span> FullyShardedDataParallel <span class="hljs-keyword">as</span> FSDP<br><span class="hljs-keyword">from</span> torch.distributed.fsdp.wrap <span class="hljs-keyword">import</span> lambda_auto_wrap_policy<br><span class="hljs-keyword">from</span> torch.distributed.fsdp <span class="hljs-keyword">import</span> ShardingStrategy<br><span class="hljs-keyword">import</span> torch.distributed <span class="hljs-keyword">as</span> dist<br><span class="hljs-keyword">import</span> torch.multiprocessing <span class="hljs-keyword">as</span> mp<br><span class="hljs-keyword">from</span> torch.nn.parallel <span class="hljs-keyword">import</span> DistributedDataParallel <span class="hljs-keyword">as</span> DDP<br><span class="hljs-keyword">from</span> torch.distributed.fsdp.wrap <span class="hljs-keyword">import</span> lambda_auto_wrap_policy<br><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> partial<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">my_wrap_criteria</span>(<span class="hljs-params">module</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">isinstance</span>(module, nn.Linear)<br><br><span class="hljs-comment"># 定义模型</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, H</span>):<br>        <span class="hljs-built_in">super</span>(Net, self).__init__()<br>        self.fc0 = nn.Linear(H, H, bias=<span class="hljs-literal">False</span>)<br>        self.fc1 = nn.Linear(H, H, bias=<span class="hljs-literal">False</span>)<br>        self.fc2 = nn.Linear(H, H, bias=<span class="hljs-literal">False</span>)<br>        self.fc3 = nn.Linear(H, H, bias=<span class="hljs-literal">False</span>)<br>        self.fc4 = nn.Linear(H, H, bias=<span class="hljs-literal">False</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.fc0(x)<br>        x = self.fc1(x)<br>        x = self.fc2(x)<br>        x = self.fc3(x)<br>        x = self.fc4(x)<br>        <span class="hljs-keyword">return</span> x<br><br><span class="hljs-comment"># 启动函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    rank = <span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;RANK&quot;</span>])<br>    world_size = <span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;WORLD_SIZE&quot;</span>])<br>    local_rank = <span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;LOCAL_RANK&quot;</span>])<br><br>    dist.init_process_group(<span class="hljs-string">&quot;nccl&quot;</span>, rank=rank, world_size=world_size)<br>    torch.cuda.set_device(rank)<br><br>    H = <span class="hljs-number">512</span><br>    model = Net(H).cuda()<br><br>    <span class="hljs-comment"># 定义自动 wrap 策略：只 wrap nn.Linear</span><br>    policy = partial(lambda_auto_wrap_policy, lambda_fn=my_wrap_criteria)<br><br>    fsdp_model = FSDP(<br>        model,<br>        device_id=torch.cuda.current_device(),<br>        sharding_strategy=ShardingStrategy.FULL_SHARD,<br>        auto_wrap_policy=policy<br>    )<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;fsdp_model:&quot;</span>, fsdp_model)<br><br>    optimizer = optim.Adam(fsdp_model.parameters(), lr=<span class="hljs-number">1e-3</span>)<br><br>    <span class="hljs-comment"># 模拟数据</span><br>    x = torch.randn(<span class="hljs-number">32</span>, H).cuda()<br>    target = torch.randn(<span class="hljs-number">32</span>, H).cuda()<br>    criterion = nn.MSELoss()<br><br>    fsdp_model.train()<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):<br>        optimizer.zero_grad()<br>        output = fsdp_model(x)<br>        loss = criterion(output, target)<br>        loss.backward()<br>        optimizer.step()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Rank <span class="hljs-subst">&#123;rank&#125;</span>] Epoch <span class="hljs-subst">&#123;epoch&#125;</span> Loss: <span class="hljs-subst">&#123;loss.item()&#125;</span>&quot;</span>)<br><br>    dist.destroy_process_group()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    main()<br><br></code></pre></td></tr></table></figure>

<h1 id="fsdp初始化"><a href="#fsdp初始化" class="headerlink" title="fsdp初始化"></a>fsdp初始化</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FullyShardedDataParallel</span>(nn.Module, _FSDPState):<br><br></code></pre></td></tr></table></figure>

<p>注意fsdp的初始化过程是“惰性”的（lazy），只有在forward调用的时候才会进行初始化，从而对模型进行shard。</p>
<h2 id="Warp"><a href="#Warp" class="headerlink" title="Warp"></a>Warp</h2><p>具体的_auto_wrap的调用是在使用FullyShardedDataParallel包裹module后进行初始化的时候实现的，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">_auto_wrap(<br>    module,<br>    auto_wrap_policy,<br>    self._ignored_modules,<br>    self._ignored_params,<br>    root_kwargs,<br>    FullyShardedDataParallel,<br>)<br></code></pre></td></tr></table></figure>

<h3 id="判断模块是否需要划分的函数"><a href="#判断模块是否需要划分的函数" class="headerlink" title="判断模块是否需要划分的函数"></a>判断模块是否需要划分的函数</h3><p>pytorch中提供了多种对模型进行自动切分和包装的方法，下面介绍几个常用的：</p>
<h4 id="CustomPolicy"><a href="#CustomPolicy" class="headerlink" title="CustomPolicy"></a>CustomPolicy</h4><p>这支持自定义包装策略，关键是允许通过lambda_fn来进行自定义。lambda_fn可以返回bool值，这代表和root执行同样的分片参数，也可以返回args，这代表自定义的分片参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomPolicy</span>(<span class="hljs-title class_ inherited__">_Policy</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    这是一个高度灵活的 FSDP（Fully Sharded Data Parallel）自动包装策略。</span><br><span class="hljs-string">    它允许用户通过提供一个自定义的 lambda 函数来精确控制对哪个模块应用 FSDP 包装，</span><br><span class="hljs-string">    以及使用什么样的参数。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    工作机制：</span><br><span class="hljs-string">    策略的核心是用户传入的 `lambda_fn`。FSDP 会遍历模型中的每一个模块，并用该模块</span><br><span class="hljs-string">    作为参数调用 `lambda_fn`。根据 `lambda_fn` 的返回值，决定如何操作：</span><br><span class="hljs-string">    - 返回 `False`：不包装当前模块。</span><br><span class="hljs-string">    - 返回 `True`：使用 FSDP 的默认参数包装当前模块。</span><br><span class="hljs-string">    - 返回一个非空字典：包装当前模块，并使用该字典中的键值对来覆盖或补充 FSDP 的默认参数。</span><br><span class="hljs-string">      这允许为特定模块设置不同的分片策略（ShardingStrategy）或其他配置。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    使用场景：</span><br><span class="hljs-string">    当你需要比 `ModuleWrapPolicy`（基于类名包装）更精细的控制时，此策略非常有用。</span><br><span class="hljs-string">    例如，你可能想为模型的大部分 Transformer 层使用默认包装，但为最后的输出层（如 lm_head）</span><br><span class="hljs-string">    指定一个不同的分片策略，或者完全不包装某个特定的层。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, lambda_fn: <span class="hljs-type">Callable</span>[[nn.Module], <span class="hljs-type">Union</span>[<span class="hljs-built_in">bool</span>, <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]]]</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        构造函数。</span><br><span class="hljs-string"></span><br><span class="hljs-string">        参数:</span><br><span class="hljs-string">        - lambda_fn (Callable): 一个函数，它接受一个 `nn.Module` 实例作为输入，</span><br><span class="hljs-string">          并返回一个布尔值或一个字典，用于决定包装行为。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self._lambda_fn = lambda_fn<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_run_policy</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        root_module: nn.Module,</span><br><span class="hljs-params">        ignored_modules: <span class="hljs-built_in">set</span>[nn.Module],</span><br><span class="hljs-params">        root_kwargs: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>],</span><br><span class="hljs-params">    </span>) -&gt; <span class="hljs-built_in">dict</span>[nn.Module, <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]]:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        （内部方法）执行策略的核心逻辑，遍历所有模块并应用 lambda 函数来决定包装方案。</span><br><span class="hljs-string"></span><br><span class="hljs-string">        参数:</span><br><span class="hljs-string">        - root_module (nn.Module): 整个模型。</span><br><span class="hljs-string">        - ignored_modules (set[nn.Module]): 需要忽略的模块集合。</span><br><span class="hljs-string">        - root_kwargs (dict[str, Any]): 应用于根 FSDP 模块的参数，作为包装子模块时的默认参数。</span><br><span class="hljs-string"></span><br><span class="hljs-string">        返回:</span><br><span class="hljs-string">        一个字典，键是需要被包装的目标模块实例，值是应用于该模块的 FSDP 参数。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        target_module_to_kwargs: <span class="hljs-built_in">dict</span>[nn.Module, <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]] = &#123;&#125;<br>        <span class="hljs-comment"># 遍历整个模型的所有子模块</span><br>        <span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> root_module.modules():<br>            <span class="hljs-comment"># 如果模块在忽略列表中，则跳过</span><br>            <span class="hljs-keyword">if</span> module <span class="hljs-keyword">in</span> ignored_modules:<br>                <span class="hljs-keyword">continue</span><br>            <br>            <span class="hljs-comment"># 对当前模块调用用户提供的 lambda 函数</span><br>            res = self._lambda_fn(module)<br>            <br>            <span class="hljs-comment"># 验证 lambda 函数的返回值是否合法（必须是布尔型或字典）</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(res, (<span class="hljs-built_in">dict</span>, <span class="hljs-built_in">bool</span>)):<br>                <span class="hljs-keyword">raise</span> ValueError(<br>                    <span class="hljs-string">f&quot;传递给 CustomPolicy 的 lambda_fn 应返回 &quot;</span><br>                    <span class="hljs-string">f&quot;False/True 或一个 kwarg 字典，但它返回了 <span class="hljs-subst">&#123;res&#125;</span>&quot;</span><br>                )<br>            <br>            <span class="hljs-comment"># 如果返回 False 或一个空字典，表示不包装该模块，直接跳过</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> res:<br>                <span class="hljs-keyword">continue</span><br>            <br>            <span class="hljs-comment"># 如果需要包装，首先浅拷贝根 FSDP 的参数作为默认值</span><br>            <span class="hljs-comment"># 这样做是为了防止不同 FSDP 实例间共享和意外修改同一份配置</span><br>            kwargs = copy.copy(root_kwargs)<br>            <br>            <span class="hljs-comment"># 如果 lambda 函数返回的是一个字典，用它的内容更新（覆盖）默认参数</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(res, <span class="hljs-built_in">dict</span>):<br>                kwargs.update(res)<br>            <br>            <span class="hljs-comment"># 将最终确定要包装的模块及其配置参数存入结果字典</span><br>            target_module_to_kwargs[module] = kwargs<br>            <br>        <span class="hljs-keyword">return</span> target_module_to_kwargs<br></code></pre></td></tr></table></figure>

<p>使用示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">model = init_transformer_model(...)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">lambda_fn</span>(<span class="hljs-params">module: nn.Module</span>):<br>    <span class="hljs-keyword">if</span> module <span class="hljs-keyword">is</span> model.lm_head:<br>        <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&quot;sharding_strategy&quot;</span>: ShardingStrategy.SHARD_GRAD_OP&#125;<br>    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(module, TransformerBlock):<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>policy = CustomPolicy(lambda_fn)<br>fsdp_model = FSDP(model, auto_wrap_policy=policy)<br></code></pre></td></tr></table></figure>

<h5 id="Module结构学习"><a href="#Module结构学习" class="headerlink" title="Module结构学习"></a>Module结构学习</h5><p>对于module，其记录子结构的变量为<code>_modules: dict[str, Optional[&quot;Module&quot;]]</code>，即对于函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">self.block1 = SomeModule()<br></code></pre></td></tr></table></figure>

<p>底层实际做了如下事情（在 <code>setattr</code> 中）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">self._modules[<span class="hljs-string">&quot;block1&quot;</span>] = SomeModule()<br></code></pre></td></tr></table></figure>

<p>也就是说所有子模块都保存在 <code>self._modules</code> 中（有顺序的字典）。</p>
<p>这里调用了root_module.modules()来获取root_module的子modules()，该函数实际上就是用深度优先遍历的方式去遍历<code>self._modules</code> ，具体的方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">modules</span>(<span class="hljs-params">self</span>) -&gt; Iterator[<span class="hljs-string">&quot;Module&quot;</span>]:<br>    <span class="hljs-string">r&quot;&quot;&quot;返回一个遍历网络中所有模块的迭代器。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    功能:</span><br><span class="hljs-string">    - 这是一个便捷方法，用于获取模型及其所有子模块的实例，而不需要它们的名称。</span><br><span class="hljs-string">    - 它在内部调用 `self.named_modules()`，但忽略了每个元组中的名称部分。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    产生:</span><br><span class="hljs-string">        Module: 网络中的一个模块。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    注意:</span><br><span class="hljs-string">        与 `named_modules` 一样，重复的模块实例默认只返回一次。在下面的例子中，</span><br><span class="hljs-string">        `l` 只会被返回一次。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    示例::</span><br><span class="hljs-string"></span><br><span class="hljs-string">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span><br><span class="hljs-string">        &gt;&gt;&gt; net = nn.Sequential(l, l)</span><br><span class="hljs-string">        &gt;&gt;&gt; for idx, m in enumerate(net.modules()):</span><br><span class="hljs-string">        ...     print(idx, &#x27;-&gt;&#x27;, m)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        0 -&gt; Sequential(</span><br><span class="hljs-string">          (0): Linear(in_features=2, out_features=2, bias=True)</span><br><span class="hljs-string">          (1): Linear(in_features=2, out_features=2, bias=True)</span><br><span class="hljs-string">        )</span><br><span class="hljs-string">        1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 核心实现：</span><br>    <span class="hljs-comment"># 1. 调用 `self.named_modules()`，这个方法会返回一个 (名称, 模块) 元组的迭代器。</span><br>    <span class="hljs-comment"># 2. 在 for 循环中，使用 `_` 来接收并“丢弃”元组中的第一个元素（即模块的名称）。</span><br>    <span class="hljs-comment"># 3. `module` 变量接收元组中的第二个元素（即模块对象本身）。</span><br>    <span class="hljs-comment"># 4. `yield module` 将模块对象作为生成器的下一个值返回。</span><br>    <span class="hljs-comment"># 这种实现方式非常优雅，因为它将所有复杂的遍历逻辑（如递归、处理重复）</span><br>    <span class="hljs-comment"># 全部委托给了 `named_modules` 方法，自身保持了极度的简洁。</span><br>    <span class="hljs-keyword">for</span> _, module <span class="hljs-keyword">in</span> self.named_modules():<br>        <span class="hljs-keyword">yield</span> module<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">named_modules</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    memo: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">set</span>[<span class="hljs-string">&quot;Module&quot;</span>]] = <span class="hljs-literal">None</span>, <span class="hljs-comment"># 用于记录已访问模块的集合，防止重复处理</span></span><br><span class="hljs-params">    prefix: <span class="hljs-built_in">str</span> = <span class="hljs-string">&quot;&quot;</span>, <span class="hljs-comment"># 当前模块的名称前缀</span></span><br><span class="hljs-params">    remove_duplicate: <span class="hljs-built_in">bool</span> = <span class="hljs-literal">True</span>, <span class="hljs-comment"># 是否移除重复的模块实例</span></span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">r&quot;&quot;&quot;返回一个迭代器，该迭代器遍历网络中的所有模块，同时产生模块的名称和模块本身。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这是一个深度优先（pre-order，先序）的遍历。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">        memo: 用于存储已添加到结果中的模块集合的备忘录。主要用于内部递归调用。</span><br><span class="hljs-string">        prefix: 将被添加到模块名称前面的前缀。主要用于内部递归调用。</span><br><span class="hljs-string">        remove_duplicate: 是否在结果中移除重复的模块实例。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    产生:</span><br><span class="hljs-string">        (str, Module): (名称, 模块) 的元组。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    注意:</span><br><span class="hljs-string">        默认情况下，重复的模块只返回一次。在下面的例子中，`l` 只会被返回一次。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    示例::</span><br><span class="hljs-string"></span><br><span class="hljs-string">        &gt;&gt;&gt; l = nn.Linear(2, 2)</span><br><span class="hljs-string">        &gt;&gt;&gt; net = nn.Sequential(l, l) # net 中有两个对同一 l 实例的引用</span><br><span class="hljs-string">        &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):</span><br><span class="hljs-string">        ...     print(idx, &#x27;-&gt;&#x27;, m)</span><br><span class="hljs-string"></span><br><span class="hljs-string">        0 -&gt; (&#x27;&#x27;, Sequential(</span><br><span class="hljs-string">          (0): Linear(in_features=2, out_features=2, bias=True)</span><br><span class="hljs-string">          (1): Linear(in_features=2, out_features=2, bias=True)</span><br><span class="hljs-string">        ))</span><br><span class="hljs-string">        1 -&gt; (&#x27;0&#x27;, Linear(in_features=2, out_features=2, bias=True))</span><br><span class="hljs-string">        # 注意：尽管有两个 l，但 (&#x27;1&#x27;, l) 不会再次出现，因为 l 已经被访问过。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 1. 初始化备忘录（memo）</span><br>    <span class="hljs-comment"># 如果是顶层调用（非递归调用），memo 为 None，此时创建一个新的集合。</span><br>    <span class="hljs-comment"># 在递归调用中，memo 会被传递下去。</span><br>    <span class="hljs-keyword">if</span> memo <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        memo = <span class="hljs-built_in">set</span>()<br>    <br>    <span class="hljs-comment"># 2. 处理当前模块（self）</span><br>    <span class="hljs-comment"># 检查当前模块实例是否已经被访问过。</span><br>    <span class="hljs-keyword">if</span> self <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> memo:<br>        <span class="hljs-comment"># 如果 remove_duplicate 为 True，则将当前模块添加到备忘录中，</span><br>        <span class="hljs-comment"># 以确保后续遇到同一个实例时不再处理。</span><br>        <span class="hljs-keyword">if</span> remove_duplicate:<br>            memo.add(self)<br>        <br>        <span class="hljs-comment"># 3. 产生当前模块的名称和实例</span><br>        <span class="hljs-comment"># 这是先序遍历的体现：先访问根节点（当前模块）。</span><br>        <span class="hljs-keyword">yield</span> prefix, self<br>        <br>        <span class="hljs-comment"># 4. 递归遍历所有子模块</span><br>        <span class="hljs-comment"># self._modules 是一个有序字典，存储了所有直接子模块（例如 self.layer1, self.conv2）。</span><br>        <span class="hljs-keyword">for</span> name, module <span class="hljs-keyword">in</span> self._modules.items():<br>            <span class="hljs-comment"># 如果某个子模块是 None（例如，通过 delattr 删除后），则跳过。</span><br>            <span class="hljs-keyword">if</span> module <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">continue</span><br>            <br>            <span class="hljs-comment"># 5. 构建子模块的完整名称</span><br>            <span class="hljs-comment"># 在当前前缀的基础上，添加子模块的名称。</span><br>            <span class="hljs-comment"># 例如，如果当前 prefix 是 &#x27;encoder&#x27;，子模块 name 是 &#x27;layer1&#x27;，</span><br>            <span class="hljs-comment"># 那么 submodule_prefix 就是 &#x27;encoder.layer1&#x27;。</span><br>            submodule_prefix = prefix + (<span class="hljs-string">&quot;.&quot;</span> <span class="hljs-keyword">if</span> prefix <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;&quot;</span>) + name<br>            <br>            <span class="hljs-comment"># 6. 递归调用</span><br>            <span class="hljs-comment"># 使用 `yield from` 将递归调用产生的生成器内容直接“转发”出去。</span><br>            <span class="hljs-comment"># 将 memo 和新构建的 submodule_prefix 传递给下一次递归。</span><br>            <span class="hljs-keyword">yield</span> <span class="hljs-keyword">from</span> module.named_modules(<br>                memo, submodule_prefix, remove_duplicate<br>            )<br></code></pre></td></tr></table></figure>





<h4 id="transformer-auto-wrap-policy"><a href="#transformer-auto-wrap-policy" class="headerlink" title="transformer_auto_wrap_policy"></a>transformer_auto_wrap_policy</h4><p>这是transformer模型中比较常用的策略，主要就是自定义要划分的模块的类型，然后自动划分。具体使用的时候需要用functools.partial进行包装。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">transformer_auto_wrap_policy</span>(<span class="hljs-params"></span><br><span class="hljs-params">    module: nn.Module,</span><br><span class="hljs-params">    recurse: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">    nonwrapped_numel: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    transformer_layer_cls: <span class="hljs-type">Set</span>[<span class="hljs-type">Type</span>[nn.Module]],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    这是一个专门为 Transformer 模型设计的便捷包装策略。它本质上是 `_module_wrap_policy` 的一个别名或封装。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    目的：</span><br><span class="hljs-string">    提供一个语义上更清晰的函数名 (`transformer_auto_wrap_policy`)，让用户在处理 Transformer 模型时，</span><br><span class="hljs-string">    能更直观地理解其作用。它特别适用于包装 Transformer 的编码器/解码器层（例如 `TransformerEncoderLayer`）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    要点：</span><br><span class="hljs-string">    - 它直接调用 `_module_wrap_policy`，并将 `transformer_layer_cls` 作为 `module_classes` 传递过去。</span><br><span class="hljs-string">    - 正确地包装共享参数（如词嵌入层）非常重要，因为它们必须位于同一个 FSDP 实例中。</span><br><span class="hljs-string">      这个策略通过将所有指定的层（通常是 Transformer block）包装起来，有助于确保模型中其他部分（如共享的嵌入层）</span><br><span class="hljs-string">      最终被包含在更高层级的 FSDP 实例中，从而被正确处理。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 直接调用通用的模块包装策略函数，实现完全相同的功能。</span><br>    <span class="hljs-keyword">return</span> _module_wrap_policy(module, recurse, nonwrapped_numel, transformer_layer_cls)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_module_wrap_policy</span>(<span class="hljs-params"></span><br><span class="hljs-params">    module: nn.Module,</span><br><span class="hljs-params">    recurse: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">    nonwrapped_numel: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    module_classes: <span class="hljs-type">Set</span>[<span class="hljs-type">Type</span>[nn.Module]],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    这是一个核心的辅助函数，用于实现基于模块类的自动包装策略。</span><br><span class="hljs-string">    FSDP 的自动包装过程是一个从上到下（top-down）的遍历，但包装决策是从下到上（bottom-up）做出的。</span><br><span class="hljs-string">    此函数在这个过程中被调用，以决定是否应该包装当前的模块。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    工作机制：</span><br><span class="hljs-string">    该函数的行为取决于 `recurse` 参数：</span><br><span class="hljs-string">    1. 当 `recurse=True` 时：表示 FSDP 正在递归地深入模块树（DFS 过程）。</span><br><span class="hljs-string">       在这种情况下，函数总是返回 `True`，告诉 FSDP 继续向下遍历，直到到达叶子模块或一个已经被包装的子模块。</span><br><span class="hljs-string">    2. 当 `recurse=False` 时：表示 FSDP 已经完成对当前模块所有子模块的遍历，现在需要对当前模块本身做出决策。</span><br><span class="hljs-string">       这时，函数会检查 `module` 是否是 `module_classes` 中指定的任何一个类的实例。</span><br><span class="hljs-string">       - 如果是，则返回 `True`，表示“请包装我这个模块”。</span><br><span class="hljs-string">       - 如果不是，则返回 `False`，表示“不要包装我，继续向上返回”。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">        module (nn.Module): 当前正在被考虑的模块。</span><br><span class="hljs-string">        recurse (bool): 控制函数行为的标志。`True` 表示继续递归，`False` 表示需要做出包装决策。</span><br><span class="hljs-string">        nonwrapped_numel (int): 尚未被包装的参数数量（在此函数中未使用，但在其他更复杂的策略中可能有用）。</span><br><span class="hljs-string">        module_classes (Set[Type[nn.Module]]): 一个包含模块类的集合。任何属于这些类的模块都将被包装。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">        一个布尔值。如果 `recurse=True`，总是返回 `True`。如果 `recurse=False`，返回是否应该包装 `module`。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 如果标志为 True，意味着我们仍在递归地深入模块树，所以总是返回 True 以继续递归。</span><br>    <span class="hljs-keyword">if</span> recurse:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>    <br>    <span class="hljs-comment"># 如果标志为 False，意味着已经到达决策点。</span><br>    <span class="hljs-comment"># 检查当前模块的类型是否在用户指定的需要包装的类型列表中。</span><br>    <span class="hljs-comment"># isinstance 的第二个参数需要是元组，所以我们将集合转换为元组。</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">isinstance</span>(module, <span class="hljs-built_in">tuple</span>(module_classes))<br><br></code></pre></td></tr></table></figure>

<p>使用示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">fsdp_m = FSDP(<br>    m,<br>    auto_wrap_policy=functools.partial(<br>        transformer_auto_wrap_policy, transformer_layer_cls=(nn.Linear,)<br>    ),<br>    use_orig_params=<span class="hljs-literal">True</span>,<br>)<br></code></pre></td></tr></table></figure>

<h3 id="具体进行自动划分"><a href="#具体进行自动划分" class="headerlink" title="具体进行自动划分"></a>具体进行自动划分</h3><p>注意观察下面的函数中的fsdp_fn为FullyShardedDataParallel，即这个fsdp_fn的作用是把module包装成FullyShardedDataParallel类型。</p>
<p>这里有两种划分的调用方式：</p>
<ol>
<li><p>如果 <code>policy</code>是 <code>_Policy</code>的实例（推荐方式），则使用策略对象来决定哪些模块需要被包装。</p>
</li>
<li><p>如果 <code>policy</code> 是一个可调用对象（旧版方式），则使用递归的方式进行包装。</p>
</li>
</ol>
<p>暂时先只看第一种<code>_Policy</code>的实例的方法，其执行顺序如下：</p>
<ol>
<li><p>执行<code>_run_policy</code>得到root_module下所有符合包装规则的module以及args</p>
</li>
<li><p>如果配置了混合精度就特殊处理一下</p>
</li>
<li><p>验证要包装的模块中的冻结参数（即 requires_grad&#x3D;False 的参数）</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_auto_wrap</span>(<span class="hljs-params"></span><br><span class="hljs-params">    root_module: nn.Module,</span><br><span class="hljs-params">    policy: <span class="hljs-type">Union</span>[<span class="hljs-type">Callable</span>, _Policy], <span class="hljs-comment"># 包装策略，可以是_Policy对象或一个可调用函数</span></span><br><span class="hljs-params">    ignored_modules: <span class="hljs-built_in">set</span>[nn.Module], <span class="hljs-comment"># 应该忽略不进行包装的模块集合</span></span><br><span class="hljs-params">    ignored_params: <span class="hljs-built_in">set</span>[nn.Parameter], <span class="hljs-comment"># 应该忽略不进行包装的参数集合</span></span><br><span class="hljs-params">    root_kwargs: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>], <span class="hljs-comment"># FSDP的根配置参数</span></span><br><span class="hljs-params">    fsdp_fn: <span class="hljs-type">Callable</span>,  <span class="hljs-comment"># FSDP的包装函数，例如 `FullyShardedDataParallel` 类或 `fully_shard` 函数</span></span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    根据 `policy`，以后序遍历的方式自动包装 `root_module` 模块树中的模块。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    此函数是 FSDP 自动包装功能的核心入口。</span><br><span class="hljs-string">    它根据传入的 `policy` 类型，选择不同的包装逻辑：</span><br><span class="hljs-string">    1.  如果 `policy` 是 `_Policy` 的实例（推荐方式），则使用策略对象来决定哪些模块需要被包装。</span><br><span class="hljs-string">    2.  如果 `policy` 是一个可调用对象（旧版方式），则使用递归的方式进行包装。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    前提条件: `root_kwargs` 应该包含除 `module` 之外的所有FSDP构造函数参数。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 检查模块是否已经被FSDP包装过。自动包装不支持对已包装的模块再次进行包装（嵌套包装）。</span><br>    _check_nested_wrapping(root_module)<br><br>    <span class="hljs-comment"># --- 分支1：基于 _Policy 对象的策略化自动包装 (推荐方式) ---</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(policy, _Policy):<br>        <span class="hljs-comment"># 运行策略，获取一个从目标模块到其对应FSDP参数的映射字典</span><br>        target_module_to_kwargs = policy._run_policy(<br>            root_module, ignored_modules, root_kwargs<br>        )<br>        <span class="hljs-comment"># 如果配置了混合精度(mixed_precision)</span><br>        <span class="hljs-keyword">if</span> root_kwargs.get(<span class="hljs-string">&quot;mixed_precision&quot;</span>) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># 运行混合精度覆盖策略，这可能会修改 target_module_to_kwargs，</span><br>            <span class="hljs-comment"># 例如，将某些模块的混合精度设置为特定的类型或禁用它。</span><br>            target_module_to_kwargs = _run_mixed_precision_override_policy(<br>                root_module,<br>                root_kwargs[<span class="hljs-string">&quot;mixed_precision&quot;</span>]._module_classes_to_ignore,<br>                ignored_modules,<br>                root_kwargs,<br>                target_module_to_kwargs,<br>            )<br>            <span class="hljs-comment"># 对指定的不使用混合精度的模块类别，通过注册前向钩子来覆盖其行为</span><br>            overridden_module_classes = _override_module_mixed_precision(<br>                root_module, root_kwargs[<span class="hljs-string">&quot;mixed_precision&quot;</span>]._module_classes_to_ignore<br>            )<br>            <span class="hljs-comment"># 如果有模块的混合精度设置被覆盖，发出警告</span><br>            _warn_on_overridden_mixed_precision(overridden_module_classes)<br>        <br>        <span class="hljs-comment"># 验证要包装的模块中的冻结参数（即 requires_grad=False 的参数）</span><br>        <span class="hljs-comment"># 确保所有参数的 `requires_grad` 状态在所有进程中是一致的</span><br>        _validate_frozen_params(<br>            root_module,<br>            <span class="hljs-built_in">set</span>(target_module_to_kwargs.keys()),<br>            ignored_params,<br>            root_kwargs.get(<span class="hljs-string">&quot;use_orig_params&quot;</span>, <span class="hljs-literal">False</span>),<br>        )<br>        <span class="hljs-comment"># 根据 target_module_to_kwargs 构建一个包装函数</span><br>        wrap_fn = _construct_wrap_fn(root_module, target_module_to_kwargs, fsdp_fn)<br>        <span class="hljs-comment"># 以后序遍历的方式，将包装函数应用到模块树上</span><br>        _post_order_apply(root_module, wrap_fn)<br>        <span class="hljs-keyword">return</span> <span class="hljs-comment"># 完成包装，直接返回</span><br><br>    <span class="hljs-comment"># --- 分支2：基于可调用函数的递归自动包装 (旧版方式) ---</span><br>    <span class="hljs-comment"># 准备递归包装所需的参数</span><br>    recursive_wrap_kwargs = &#123;<br>        <span class="hljs-string">&quot;module&quot;</span>: root_module,<br>        <span class="hljs-string">&quot;auto_wrap_policy&quot;</span>: policy,<br>        <span class="hljs-string">&quot;wrapper_cls&quot;</span>: fsdp_fn,<br>        <span class="hljs-string">&quot;ignored_modules&quot;</span>: ignored_modules,<br>        <span class="hljs-string">&quot;ignored_params&quot;</span>: ignored_params,<br>        <span class="hljs-string">&quot;only_wrap_children&quot;</span>: <span class="hljs-literal">True</span>,  <span class="hljs-comment"># 表示只对子模块进行递归包装</span><br>    &#125;<br>    <span class="hljs-comment"># 如果配置了混合精度</span><br>    <span class="hljs-keyword">if</span> root_kwargs.get(<span class="hljs-string">&quot;mixed_precision&quot;</span>) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># 对指定的不使用混合精度的模块类别，通过注册前向钩子来覆盖其行为</span><br>        overridden_module_classes = _override_module_mixed_precision(<br>            root_module, root_kwargs[<span class="hljs-string">&quot;mixed_precision&quot;</span>]._module_classes_to_ignore<br>        )<br>        <span class="hljs-comment"># 创建一个组合策略：它会同时应用用户提供的原始策略，</span><br>        <span class="hljs-comment"># 以及一个单独包装被忽略混合精度模块的策略。</span><br>        policy = functools.partial(<br>            _or_policy, <span class="hljs-comment"># _or_policy 会依次尝试列表中的每个策略</span><br>            policies=[<br>                policy, <span class="hljs-comment"># 用户原始策略</span><br>                partial( <span class="hljs-comment"># 一个新策略，用于单独包装需要忽略混合精度的模块</span><br>                    _wrap_module_cls_individually,<br>                    module_classes=root_kwargs[<span class="hljs-string">&quot;mixed_precision&quot;</span>]._module_classes_to_ignore,<br>                ),<br>            ],<br>        )<br>        recursive_wrap_kwargs[<span class="hljs-string">&quot;auto_wrap_policy&quot;</span>] = policy<br>        <span class="hljs-comment"># 如果有模块的混合精度设置被覆盖，发出警告</span><br>        _warn_on_overridden_mixed_precision(overridden_module_classes)<br>    <br>    <span class="hljs-comment"># 执行递归包装。将根配置和递归配置合并后传递给 _recursive_wrap</span><br>    _recursive_wrap(**recursive_wrap_kwargs, **root_kwargs)  <span class="hljs-comment"># type: ignore[arg-<span class="hljs-built_in">type</span>]</span><br><br></code></pre></td></tr></table></figure>

<h4 id="construct-wrap-fn-post-order-apply"><a href="#construct-wrap-fn-post-order-apply" class="headerlink" title="_construct_wrap_fn&amp;_post_order_apply"></a>_construct_wrap_fn&amp;_post_order_apply</h4><p>首先构建一个warp函数，该函数对于target model且不是root的会调用fsdp_fn执行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_construct_wrap_fn</span>(<span class="hljs-params"></span><br><span class="hljs-params">    root_module: nn.Module, <span class="hljs-comment"># 整个模型的根模块</span></span><br><span class="hljs-params">    target_module_to_kwargs: <span class="hljs-built_in">dict</span>[nn.Module, <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]], <span class="hljs-comment"># 一个字典，映射了需要被包装的模块到它们的FSDP参数</span></span><br><span class="hljs-params">    fsdp_fn: <span class="hljs-type">Callable</span>, <span class="hljs-comment"># 实际执行包装的函数，例如 `FullyShardedDataParallel`</span></span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Callable</span>[[nn.Module], <span class="hljs-type">Optional</span>[nn.Module]]:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    此函数是一个高阶函数，它的作用是构建并返回一个&quot;包装函数&quot;（`fn`）。</span><br><span class="hljs-string">    这个返回的函数将被传递给 `_post_order_apply`，用于在后序遍历中实际应用FSDP包装。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    - 它利用闭包捕获 `root_module`、`target_module_to_kwargs` 和 `fsdp_fn`。</span><br><span class="hljs-string">    - 返回的 `fn` 封装了决定是否包装一个特定模块以及如何包装它的全部逻辑。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">    - root_module (nn.Module): 模型的根模块，用于在包装时进行排除检查。</span><br><span class="hljs-string">    - target_module_to_kwargs (dict): 由包装策略生成的字典，指明了哪些模块需要被包装以及它们各自的FSDP配置。</span><br><span class="hljs-string">    - fsdp_fn (Callable): FSDP的包装器，如 `FullyShardedDataParallel` 类。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">    - 一个可调用对象 `fn`，该函数接受一个模块作为输入，如果该模块需要被包装，则返回包装后的模块，否则返回 `None`。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fn</span>(<span class="hljs-params">module: nn.Module</span>) -&gt; <span class="hljs-type">Optional</span>[nn.Module]:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        这个内部函数是实际执行替换逻辑的单元。</span><br><span class="hljs-string">        它会被 `_post_order_apply` 在遍历模型树时对每个模块调用。</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 检查当前遍历到的 `module` 是否在我们的目标包装列表 `target_module_to_kwargs` 中</span><br>        <span class="hljs-comment"># 同时，显式地避免包装根模块 `root_module`，因为根模块的包装通常由调用FSDP的用户在最外层手动完成。</span><br>        <span class="hljs-keyword">if</span> module <span class="hljs-keyword">in</span> target_module_to_kwargs <span class="hljs-keyword">and</span> module <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> root_module:<br>            <span class="hljs-comment"># 如果模块是目标模块，就从字典中获取其对应的FSDP参数</span><br>            kwargs = target_module_to_kwargs[module]<br>            <span class="hljs-comment"># 使用传入的 `fsdp_fn` (例如 `FullyShardedDataParallel`) 和对应的参数来包装当前模块</span><br>            <span class="hljs-comment"># `_post_order_apply` 会用这里返回的新模块替换掉原始模块</span><br>            <span class="hljs-keyword">return</span> fsdp_fn(module, **kwargs)<br>        <br>        <span class="hljs-comment"># 如果模块不是目标包装模块，或者它就是根模块，则返回 None。</span><br>        <span class="hljs-comment"># `_post_order_apply` 看到返回 `None` 时，不会对原始模块做任何改动。</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">return</span> fn<br></code></pre></td></tr></table></figure>

<p>然后对于_post_order_apply函数，他就是会执行上面构造的fn，然后以后序遍历的方式去遍历所有的子模块（注意不会替换root 模块），替换的方法就是setattr。</p>
<blockquote>
<p>为什么要避免替换root呢，因为root这时候已经是FullyShardedDataParallel类型了，不需要重复进行。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 注意：我们有意保持此函数简单，并将复杂性隔离到 `fn` 中，</span><br><span class="hljs-comment"># 以便能够通用地使用此函数。我们将来可能会将其移动到非 FSDP 特定的文件夹和/或使其公开。</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_order_apply</span>(<span class="hljs-params"></span><br><span class="hljs-params">    root_module: nn.Module,</span><br><span class="hljs-params">    fn: <span class="hljs-type">Callable</span>[[nn.Module], <span class="hljs-type">Optional</span>[nn.Module]],</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    此函数遵循后序遍历（post-order traversal）将 `fn` 应用于 `root_module` 模块树中的每个模块。</span><br><span class="hljs-string">    如果 `fn` 返回一个 :class:`nn.Module`，那么它将在树中用新返回的模块替换原始模块。</span><br><span class="hljs-string">    否则，`fn` 应返回 `None`，在这种情况下，模块不会被更改。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    后序遍历意味着，对于任何给定的模块，函数 `fn` 会先被应用于其所有子模块，然后再应用于该模块本身。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">    - root_module (nn.Module): 整个模型层级结构的根模块。</span><br><span class="hljs-string">    - fn (Callable[[nn.Module], Optional[nn.Module]]): 一个可调用对象，它接收一个模块作为输入。</span><br><span class="hljs-string">        - 如果需要替换该模块，则返回一个新的 nn.Module 实例。</span><br><span class="hljs-string">        - 如果不需要改变该模块，则返回 None。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 跟踪已访问的模块，以避免多次访问共享的模块实例。</span><br>    visited_modules: <span class="hljs-built_in">set</span>[nn.Module] = &#123;root_module&#125;<br><br>    <span class="hljs-comment"># 定义一个内部辅助函数来进行递归的后序遍历和应用。</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_order_apply_inner</span>(<span class="hljs-params"></span><br><span class="hljs-params">        module: nn.Module, <span class="hljs-comment"># 当前正在处理的模块</span></span><br><span class="hljs-params">        module_name: <span class="hljs-built_in">str</span>, <span class="hljs-comment"># 当前模块在其父模块中的属性名</span></span><br><span class="hljs-params">        parent_module: <span class="hljs-type">Optional</span>[nn.Module], <span class="hljs-comment"># 当前模块的父模块</span></span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-comment"># 1. 遍历当前模块的所有直接子模块。</span><br>        <span class="hljs-keyword">for</span> child_module_name, child_module <span class="hljs-keyword">in</span> module.named_children():<br>            <span class="hljs-comment"># 如果子模块还没有被访问过（防止重复处理共享模块）</span><br>            <span class="hljs-keyword">if</span> child_module <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> visited_modules:<br>                visited_modules.add(child_module) <span class="hljs-comment"># 标记为已访问</span><br>                <span class="hljs-comment"># 2. 对子模块进行递归调用。这是实现后序遍历的关键：先深入子树。</span><br>                _post_order_apply_inner(child_module, child_module_name, module)<br>        <br>        <span class="hljs-comment"># 3. 在所有子模块都处理完毕后，对当前模块应用 `fn` 函数。</span><br>        <span class="hljs-comment">#    这就是“后序”的含义：先处理子节点，再处理父节点。</span><br>        optional_module = fn(module)<br>        <br>        <span class="hljs-comment"># 4. 如果 `fn` 返回了一个新的模块实例（而不是 None），则替换原始模块。</span><br>        <span class="hljs-keyword">if</span> optional_module <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># 断言确保非根模块必须有一个父模块，否则替换操作无法进行。</span><br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">isinstance</span>(parent_module, nn.Module), (<br>                <span class="hljs-string">f&quot;非根模块应该设置其父模块，但对于 <span class="hljs-subst">&#123;module&#125;</span> 得到了 <span class="hljs-subst">&#123;parent_module&#125;</span>&quot;</span><br>            )<br>            <span class="hljs-comment"># 断言确保非根模块必须有一个名称，否则无法通过名称在父模块中找到并替换它。</span><br>            <span class="hljs-keyword">assert</span> module_name, (<br>                <span class="hljs-string">f&quot;非根模块应该设置其模块名称，但对于 <span class="hljs-subst">&#123;module&#125;</span> 得到了一个空模块名&quot;</span><br>            )<br>            <span class="hljs-comment"># 断言确保 `fn` 的返回值要么是 None，要么是 nn.Module 的实例。</span><br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">isinstance</span>(optional_module, nn.Module), (<br>                <span class="hljs-string">f&quot;fn 应返回 None 或 nn.Module，但得到了 <span class="hljs-subst">&#123;optional_module&#125;</span>&quot;</span><br>            )<br>            <span class="hljs-comment"># 使用 setattr 动态地将父模块中的 `module_name` 属性设置为新的 `optional_module`，</span><br>            <span class="hljs-comment"># 从而完成模块的替换。</span><br>            <span class="hljs-built_in">setattr</span>(parent_module, module_name, optional_module)<br><br>    <span class="hljs-comment"># 从根模块开始启动整个后序应用过程。</span><br>    <span class="hljs-comment"># 根模块没有父模块（None）和名称（&quot;&quot;）。</span><br>    _post_order_apply_inner(root_module, <span class="hljs-string">&quot;&quot;</span>, <span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure>

<h2 id="初始化param-handle"><a href="#初始化param-handle" class="headerlink" title="初始化param_handle"></a>初始化param_handle</h2><p>在Warp之后，初始化param_handle的顶层调用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">_init_param_handle_from_module(<br>    self,<br>    module,<br>    device_id,<br>    param_init_fn,<br>    sync_module_states,<br>)<br></code></pre></td></tr></table></figure>

<h3 id="init-param-handle-from-module"><a href="#init-param-handle-from-module" class="headerlink" title="_init_param_handle_from_module"></a>_init_param_handle_from_module</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_param_handle_from_module</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState, <span class="hljs-comment"># FSDP 状态对象，用于跟踪和管理 FSDP 实例的各种状态</span></span><br><span class="hljs-params">    fully_sharded_module: nn.Module, <span class="hljs-comment"># 需要被 FSDP 完全分片的模块</span></span><br><span class="hljs-params">    device_id: <span class="hljs-type">Optional</span>[<span class="hljs-type">Union</span>[<span class="hljs-built_in">int</span>, torch.device]], <span class="hljs-comment"># 目标设备 ID</span></span><br><span class="hljs-params">    param_init_fn: <span class="hljs-type">Optional</span>[<span class="hljs-type">Callable</span>[[nn.Module], <span class="hljs-literal">None</span>]], <span class="hljs-comment"># 可选的参数初始化函数，用于在物化（materialize）模块时调用</span></span><br><span class="hljs-params">    sync_module_states: <span class="hljs-built_in">bool</span>, <span class="hljs-comment"># 是否在初始化时同步模块的状态（参数和缓冲区）</span></span><br><span class="hljs-params"></span>) -&gt; _FSDPState:<br>    <span class="hljs-string">&quot;&quot;&quot;从一个模块 `fully_sharded_module` 初始化一个 `FlatParamHandle`。</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    `FlatParamHandle` 是 FSDP 的核心组件，它将模块的多个原始参数展平（flatten）</span><br><span class="hljs-string">    并合并成一个单一的、连续的 `FlatParameter`。这个函数负责完成这一过程。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><span class="hljs-string">&quot;&quot;</span><br>    <span class="hljs-comment"># 1. 检查和准备设备</span><br>    <span class="hljs-comment"># 确保模块的所有参数都在同一个设备上，或者在 CPU 上，或者在 &#x27;meta&#x27; 设备上</span><br>    _check_single_device_module(fully_sharded_module, state._ignored_params, device_id)<br>    <span class="hljs-comment"># 根据传入的 device_id 获取实际的 torch.device 对象</span><br>    device_from_device_id = _get_device_from_device_id(<br>        device_id, state.rank, state._device_handle<br>    )<br><br>    <span class="hljs-comment"># 2. 模块物化（Materialization）</span><br>    <span class="hljs-comment"># 检查模块是否需要在设备上进行物化。如果模块的参数在 &#x27;meta&#x27; 设备上，</span><br>    <span class="hljs-comment"># 或者使用了 torchdistX 的延迟初始化，就需要进行物化，即为参数分配实际的内存。</span><br>    is_meta_module, is_torchdistX_deferred_init = _need_to_materialize_module(<br>        fully_sharded_module, state._ignored_params, state._ignored_modules<br>    )<br>    <span class="hljs-comment"># 如果需要物化并且用户提供了自定义的参数初始化函数 `param_init_fn`</span><br>    <span class="hljs-keyword">if</span> (is_meta_module <span class="hljs-keyword">or</span> is_torchdistX_deferred_init) <span class="hljs-keyword">and</span> param_init_fn <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># 使用用户提供的函数来初始化并物化模块</span><br>        _materialize_with_param_init_fn(<br>            fully_sharded_module, param_init_fn, state._ignored_modules<br>        )<br>    <span class="hljs-comment"># 如果是 &#x27;meta&#x27; 模块，但没有提供初始化函数</span><br>    <span class="hljs-keyword">elif</span> is_meta_module:<br>        <span class="hljs-comment"># 使用默认方式在目标设备上物化模块</span><br>        _materialize_meta_module(<br>            fully_sharded_module,<br>            device_id,<br>            state._ignored_modules,<br>            state._device_handle,<br>        )<br>    <span class="hljs-comment"># 如果使用了 torchdistX 的延迟初始化</span><br>    <span class="hljs-keyword">elif</span> is_torchdistX_deferred_init:<br>        <span class="hljs-comment"># 使用 torchdistX 的 API 来物化模块</span><br>        deferred_init.materialize_module(<br>            fully_sharded_module,<br>            <span class="hljs-comment"># 检查函数确保我们不会重复物化已经被 FSDP 管理或被忽略的子模块</span><br>            check_fn=<span class="hljs-keyword">lambda</span> submodule: _get_module_fsdp_state(submodule) <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">and</span> submodule <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> state._ignored_modules,<br>        )<br><br>    <span class="hljs-comment"># 3. 将模块移动到目标设备</span><br>    <span class="hljs-comment"># 收集所有被忽略的模块中的缓冲区，这些缓冲区将不会被移动</span><br>    ignored_buffers = &#123;<br>        buffer<br>        <span class="hljs-keyword">for</span> ignored_module <span class="hljs-keyword">in</span> state._ignored_modules<br>        <span class="hljs-keyword">for</span> buffer <span class="hljs-keyword">in</span> ignored_module.buffers()<br>    &#125;<br>    <span class="hljs-comment"># 将整个模块（包括其参数和缓冲区）移动到目标设备</span><br>    _move_module_to_device(<br>        fully_sharded_module,<br>        state._ignored_params, <span class="hljs-comment"># 忽略指定参数</span><br>        ignored_buffers, <span class="hljs-comment"># 忽略指定缓冲区</span><br>        device_from_device_id,<br>    )<br>    <span class="hljs-comment"># 确定计算设备（通常是 GPU），并更新 FSDP 状态</span><br>    state.compute_device = _get_compute_device(<br>        fully_sharded_module,<br>        state._ignored_params,<br>        device_from_device_id,<br>        state.rank,<br>        state._device_handle,<br>    )<br><br>    <span class="hljs-comment"># 4. 参数同步</span><br>    <span class="hljs-comment"># 获取 FSDP 需要管理的所有原始参数</span><br>    managed_params = <span class="hljs-built_in">list</span>(_get_orig_params(fully_sharded_module, state._ignored_params))<br>    <span class="hljs-comment"># 验证这些参数是否符合 FSDP 的要求</span><br>    _verify_managed_params(fully_sharded_module, managed_params)<br>    <span class="hljs-comment"># 如果设置了 `sync_module_states`，则在所有 rank 之间同步参数和缓冲区</span><br>    <span class="hljs-comment"># 这确保了在训练开始前，所有进程上的模型状态是完全一致的</span><br>    <span class="hljs-keyword">if</span> sync_module_states:<br>        _sync_module_params_and_buffers(<br>            fully_sharded_module, managed_params, state.process_group<br>        )<br>        <span class="hljs-comment"># 对于混合分片策略，还需要在跨节点的进程组中进行同步</span><br>        <span class="hljs-keyword">if</span> state.sharding_strategy <span class="hljs-keyword">in</span> HYBRID_SHARDING_STRATEGIES:<br>            _sync_module_params_and_buffers(<br>                fully_sharded_module, managed_params, state._inter_node_pg<br>            )<br>            <br>    <span class="hljs-comment"># 5. 创建 FlatParamHandle</span><br>    <span class="hljs-comment"># 这是最后一步，使用准备好的参数列表来实际创建和初始化 FlatParamHandle</span><br>    _init_param_handle_from_params(state, managed_params, fully_sharded_module)<br>    <br>    <span class="hljs-comment"># 返回更新后的 FSDP 状态</span><br>    <span class="hljs-keyword">return</span> state<br></code></pre></td></tr></table></figure>

<h3 id="get-orig-params"><a href="#get-orig-params" class="headerlink" title="_get_orig_params"></a>_get_orig_params</h3><p>得到fully_sharded_module中所有的参数，也就是tensor矩阵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_orig_params</span>(<span class="hljs-params"></span><br><span class="hljs-params">    module: nn.Module, <span class="hljs-comment"># 要从中提取参数的模块</span></span><br><span class="hljs-params">    ignored_params: <span class="hljs-built_in">set</span>[nn.Parameter], <span class="hljs-comment"># 一个包含应被忽略的参数的集合</span></span><br><span class="hljs-params"></span>) -&gt; Iterator[nn.Parameter]: <span class="hljs-comment"># 返回一个参数的迭代器（生成器）</span><br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    返回一个遍历 `module` 中原始参数的迭代器。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这个迭代器不会返回以下几种参数：</span><br><span class="hljs-string">    1. 在 `ignored_params` 集合中的参数。</span><br><span class="hljs-string">    2. 任何 `FlatParameter` 实例（这可能因为嵌套使用 FSDP 而出现）。</span><br><span class="hljs-string">    3. 任何已经被展平的原始参数（这只在 `use_orig_params=True` 模式下有意义）。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 获取模块所有参数的生成器</span><br>    param_gen = module.parameters()<br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-comment"># 使用一个无限循环来手动迭代生成器</span><br>        <span class="hljs-comment"># 这样做是为了清晰地处理 StopIteration 异常</span><br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>            <span class="hljs-comment"># 从生成器中获取下一个参数</span><br>            param = <span class="hljs-built_in">next</span>(param_gen)<br>            <br>            <span class="hljs-comment"># 这是核心的过滤逻辑：</span><br>            <span class="hljs-comment"># 1. `param not in ignored_params`：确保该参数不是用户明确指定要忽略的参数。</span><br>            <span class="hljs-comment"># 2. `not _is_fsdp_flattened(param)`：检查该参数是否已经被 FSDP 处理过。</span><br>            <span class="hljs-comment">#    `_is_fsdp_flattened` 会检查参数是否是 `FlatParameter` 的实例，</span><br>            <span class="hljs-comment">#    或者是否已经被合并到另一个 `FlatParameter` 中。这对于处理嵌套 FSDP</span><br>            <span class="hljs-comment">#    至关重要，可以防止重复包装和管理同一个参数。</span><br>            <span class="hljs-keyword">if</span> param <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> ignored_params <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> _is_fsdp_flattened(param):<br>                <span class="hljs-comment"># 如果参数通过了所有检查，就将其 yield 出去</span><br>                <span class="hljs-keyword">yield</span> param<br>    <span class="hljs-keyword">except</span> StopIteration:<br>        <span class="hljs-comment"># 当 `module.parameters()` 迭代完成并抛出 StopIteration 异常时，</span><br>        <span class="hljs-comment"># 捕获它并正常退出循环。`pass` 表示什么也不做。</span><br>        <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure>

<h3 id="init-param-handle-from-params"><a href="#init-param-handle-from-params" class="headerlink" title="_init_param_handle_from_params"></a>_init_param_handle_from_params</h3><p>注意初始化了FlatParamHandle后立刻进行了shard</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_param_handle_from_params</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState, <span class="hljs-comment"># FSDP 状态对象，用于跟踪和管理 FSDP 实例的各种状态</span></span><br><span class="hljs-params">    params: <span class="hljs-built_in">list</span>[nn.Parameter], <span class="hljs-comment"># 从模块中收集到的、需要被 FSDP 管理的原始参数列表</span></span><br><span class="hljs-params">    fully_sharded_module: nn.Module, <span class="hljs-comment"># 这些参数所属的、需要被 FSDP 完全分片的模块</span></span><br><span class="hljs-params"></span>):<br>    <span class="hljs-comment"># 如果没有需要管理的参数，则直接返回，不做任何操作</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(params) == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span><br><br>    <span class="hljs-comment"># 1. 实例化 FlatParamHandle</span><br>    <span class="hljs-comment"># FlatParamHandle 是 FSDP 的核心，它负责将 `params` 列表中的多个参数</span><br>    <span class="hljs-comment"># “展平”（flatten）并合并成一个单一的、连续的张量（FlatParameter）。</span><br>    <span class="hljs-comment"># 这里传入了所有必要的配置，如分片策略、混合精度设置、进程组等。</span><br>    handle = FlatParamHandle(<br>        params, <span class="hljs-comment"># 原始参数列表</span><br>        fully_sharded_module, <span class="hljs-comment"># 所属模块</span><br>        state.compute_device, <span class="hljs-comment"># 计算设备 (例如, &#x27;cuda:0&#x27;)</span><br>        SHARDING_STRATEGY_MAP[state.sharding_strategy], <span class="hljs-comment"># 分片策略</span><br>        state.cpu_offload.offload_params, <span class="hljs-comment"># 是否启用 CPU offload</span><br>        state.mixed_precision.param_dtype, <span class="hljs-comment"># 参数的数据类型 (例如, torch.float16)</span><br>        state.mixed_precision.reduce_dtype, <span class="hljs-comment"># all-reduce 操作的数据类型</span><br>        state.mixed_precision.keep_low_precision_grads, <span class="hljs-comment"># 是否保留低精度梯度</span><br>        state.process_group, <span class="hljs-comment"># 分布式通信的进程组</span><br>        state._use_orig_params, <span class="hljs-comment"># 是否使用原始参数的视图（一种优化）</span><br>        fsdp_extension=state._fsdp_extension, <span class="hljs-comment"># FSDP 扩展</span><br>    )<br><br>    <span class="hljs-comment"># 2. 对 FlatParameter 进行分片</span><br>    <span class="hljs-comment"># 调用 .shard() 方法，根据指定的分片策略，将完整的 FlatParameter 分割成</span><br>    <span class="hljs-comment"># 多个分片，每个 rank 只保留自己负责的那一部分。这是实现显存优化的关键。</span><br>    handle.shard()<br><br>    <span class="hljs-comment"># 3. 更新 FSDP 状态</span><br>    <span class="hljs-comment"># 确保当前 FSDP 实例还没有关联任何 handle</span><br>    <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> state._handle<br>    <span class="hljs-comment"># 将新创建的 FlatParameter 添加到 FSDP 实例的参数列表中，以便优化器可以找到它</span><br>    state.params.append(handle.flat_param)<br>    <span class="hljs-comment"># 将新创建的 handle 保存到 FSDP 状态中</span><br>    state._handle = handle<br>    <span class="hljs-comment"># 建立从模块到其对应 handle 的映射关系</span><br>    state._fully_sharded_module_to_handle[handle._fully_sharded_module] = handle<br><br>    <span class="hljs-comment"># 4. 处理 CPU Offload</span><br>    <span class="hljs-comment"># 如果启用了 CPU offload，并且分片后的 FlatParameter 当前不在 CPU 上</span><br>    cpu_device = torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)<br>    <span class="hljs-keyword">if</span> state.cpu_offload.offload_params <span class="hljs-keyword">and</span> handle.flat_param.device != cpu_device:<br>        <span class="hljs-comment"># 将该分片移动到 CPU，以释放 GPU 显存</span><br>        handle.flat_param_to(cpu_device)<br></code></pre></td></tr></table></figure>

<h3 id="FlatParamHandle"><a href="#FlatParamHandle" class="headerlink" title="FlatParamHandle"></a>FlatParamHandle</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FlatParamHandle</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    一个管理扁平化参数（:class:`FlatParameter`）的句柄。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这包括分片和视图管理。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">        params (Sequence[nn.Parameter]): 要被展平到扁平化参数中的参数序列。</span><br><span class="hljs-string">        fully_sharded_module (nn.Module): 被 FSDP 包装的模块。</span><br><span class="hljs-string">        device (torch.device): 计算和通信设备，通常是 GPU。</span><br><span class="hljs-string">        sharding_strategy (ShardingStrategy): 应用于此句柄的 `FlatParameter` 的分片策略。</span><br><span class="hljs-string">        offload_params (bool): 是否将此句柄的 `FlatParameter` 卸载到 CPU。</span><br><span class="hljs-string">        mp_param_dtype (Optional[torch.dtype]): 用于参数的混合精度类型。</span><br><span class="hljs-string">        mp_reduce_dtype (Optional[torch.dtype]): 用于梯度归约的混合精度类型。</span><br><span class="hljs-string">        keep_low_precision_grads (bool): 是否保持低精度的梯度。</span><br><span class="hljs-string">        use_orig_params (bool): 如果为 True，FSDP 会保留原始参数变量，并从 `named_parameters()` 返回它们。</span><br><span class="hljs-string">                               这允许在同一个 `FlatParameter` 内对不同原始参数使用不同的优化器超参数。</span><br><span class="hljs-string">                               如果为 False，FSDP 会在每次迭代中重新构建参数。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment">##################</span><br>    <span class="hljs-comment"># INITIALIZATION #</span><br>    <span class="hljs-comment">##################</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        params: <span class="hljs-type">Sequence</span>[<span class="hljs-type">Union</span>[nn.Parameter, Tensor]],</span><br><span class="hljs-params">        fully_sharded_module: nn.Module,</span><br><span class="hljs-params">        device: torch.device,</span><br><span class="hljs-params">        sharding_strategy: HandleShardingStrategy,</span><br><span class="hljs-params">        offload_params: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        mp_param_dtype: <span class="hljs-type">Optional</span>[torch.dtype],</span><br><span class="hljs-params">        mp_reduce_dtype: <span class="hljs-type">Optional</span>[torch.dtype],</span><br><span class="hljs-params">        keep_low_precision_grads: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        process_group: dist.ProcessGroup,</span><br><span class="hljs-params">        use_orig_params: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params">        *,</span><br><span class="hljs-params">        fsdp_extension: <span class="hljs-type">Optional</span>[FSDPExtensions] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># 确保传入的参数列表不为空</span><br>        params = <span class="hljs-built_in">list</span>(params)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(params) == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<br>                <span class="hljs-string">f&quot;不能用一个空的参数列表来构造 <span class="hljs-subst">&#123;self.__class__.__name__&#125;</span>&quot;</span><br>            )<br>        <br>        <span class="hljs-comment"># 初始化一些内部函数</span><br>        self._init_setattr_fns()<br><br>        <span class="hljs-comment"># 从环境变量中读取一些用于调试和性能分析的高级配置</span><br>        self._skip_writeback_check = (<br>            os.environ.get(_FSDP_SKIP_WRITEBACK_CHECK, <span class="hljs-string">&quot;&quot;</span>) == <span class="hljs-string">&quot;1&quot;</span><br>        )<br>        self._use_full_prec_in_eval = (<br>            os.environ.get(_FSDP_USE_FULL_PREC_IN_EVAL, <span class="hljs-string">&quot;&quot;</span>) == <span class="hljs-string">&quot;1&quot;</span><br>        )<br>        <span class="hljs-comment"># 这些 &quot;fake&quot; 选项用于性能分析，它们会跳过实际的通信操作</span><br>        self._use_fake_all_gather = os.environ.get(_FSDP_USE_FAKE_ALL_GATHER, <span class="hljs-string">&quot;&quot;</span>) == <span class="hljs-string">&quot;1&quot;</span><br>        self._use_fake_reduce = os.environ.get(_FSDP_USE_FAKE_REDUCE, <span class="hljs-string">&quot;&quot;</span>) == <span class="hljs-string">&quot;1&quot;</span><br>        <br>        <span class="hljs-comment"># ... (处理上述环境变量的警告信息) ...</span><br><br>        <span class="hljs-comment"># 是否对齐内存地址，目前仅在 use_orig_params=True 时启用</span><br>        align_addresses = use_orig_params<br>        self._init_get_unflat_views_fn(align_addresses)<br><br>        <span class="hljs-comment"># --- 初始化核心属性 ---</span><br>        self.device = device  <span class="hljs-comment"># 计算设备 (e.g., &#x27;cuda:0&#x27;)</span><br>        self._device_handle = _FSDPDeviceHandle.from_device(self.device)<br>        self.process_group = process_group  <span class="hljs-comment"># 分布式通信组</span><br>        self.rank = process_group.rank()      <span class="hljs-comment"># 当前进程的排名</span><br>        self.world_size = process_group.size() <span class="hljs-comment"># 总进程数</span><br><br>        <span class="hljs-comment"># --- 存储 FSDP 的主要配置 ---</span><br>        self._sharding_strategy = sharding_strategy <span class="hljs-comment"># 分片策略 (e.g., SHARD_GRAD_OP)</span><br>        self._offload_params = offload_params       <span class="hljs-comment"># 是否卸载到 CPU</span><br>        self._use_orig_params = use_orig_params     <span class="hljs-comment"># 是否使用原始参数</span><br>        self._keep_low_precision_grads = keep_low_precision_grads <span class="hljs-comment"># 是否保留低精度梯度</span><br><br>        <span class="hljs-comment"># --- 初始化状态变量 ---</span><br>        self._training_state = HandleTrainingState.IDLE  <span class="hljs-comment"># 初始状态为空闲</span><br>        self._debug_level = dist.get_debug_level()<br>        self._fully_sharded_module = fully_sharded_module <span class="hljs-comment"># 关联的模块</span><br>        <br>        <span class="hljs-comment"># ... (初始化一些用于 prefetch 和执行顺序跟踪的内部状态变量) ...</span><br>        self._handle_index: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">int</span>] = <span class="hljs-literal">None</span><br>        self._needs_pre_forward_unshard = <span class="hljs-literal">False</span><br>        self._needs_pre_backward_unshard = <span class="hljs-literal">False</span><br>        self._prefetched = <span class="hljs-literal">False</span><br><br>        <span class="hljs-comment"># --- 初始化数据类型 (dtype) ---</span><br>        <span class="hljs-comment"># 原始参数的数据类型</span><br>        self._orig_param_dtype = params[<span class="hljs-number">0</span>].dtype<br>        <span class="hljs-comment"># 初始化用于前向/后向传播和梯度计算的混合精度数据类型</span><br>        self._init_param_reduce_dtypes(mp_param_dtype, mp_reduce_dtype)<br>        <span class="hljs-keyword">assert</span> self._fwd_bwd_param_dtype <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>  <span class="hljs-comment"># mypy</span><br>        <br>        <span class="hljs-comment"># 计算对齐所需的元素数量</span><br>        self._aligned_numel = (<br>            _get_aligned_numel(unsharded_dtype=self._fwd_bwd_param_dtype)<br>            <span class="hljs-keyword">if</span> align_addresses<br>            <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>        )<br>        self._fsdp_extension = fsdp_extension<br><br>        <span class="hljs-comment"># --- 最关键的步骤：创建扁平化参数和元数据 ---</span><br>        <span class="hljs-comment"># 这个方法会执行以下操作:</span><br>        <span class="hljs-comment"># 1. 计算所有参数的总元素数量。</span><br>        <span class="hljs-comment"># 2. 创建一个大的、一维的 `FlatParameter` 来容纳所有参数。</span><br>        <span class="hljs-comment"># 3. 将原始参数的数据复制到这个 `FlatParameter` 中。</span><br>        <span class="hljs-comment"># 4. 记录每个原始参数在 `FlatParameter` 中的位置、形状等元数据。</span><br>        self._init_flat_param_and_metadata(<br>            params,<br>            fully_sharded_module,<br>            self._aligned_numel,<br>            use_orig_params,  <span class="hljs-comment"># type: ignore[arg-<span class="hljs-built_in">type</span>]</span><br>        )<br><br>        <span class="hljs-comment"># --- 最后一步：设置参数视图 ---</span><br>        <span class="hljs-comment"># 让原始模块的参数成为 `FlatParameter` 的“视图”（view）。</span><br>        <span class="hljs-comment"># 这意味着对原始参数的任何修改都会反映在 `FlatParameter` 上，反之亦然。</span><br>        self._use_unsharded_views(as_params=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure>

<h4 id="init-flat-param-and-metadata"><a href="#init-flat-param-and-metadata" class="headerlink" title="_init_flat_param_and_metadata"></a>_init_flat_param_and_metadata</h4><p>这个方法是 FSDP 魔法的起点。它像一个高效的管家，将一堆零散的参数（ params ）整齐地排列、打包，并贴上详细的标签（元数据），最终形成一个易于管理的单一实体（ FlatParameter ）。这个过程不仅处理了复杂的共享参数和内存对齐问题，还为后续的分布式操作（如 reduce-scatter ）做好了准备。一旦这个方法执行完毕， FlatParamHandle 就拥有了一个完整的、随时可以被分片和恢复的扁平化参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_flat_param_and_metadata</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    params: <span class="hljs-built_in">list</span>[<span class="hljs-type">Union</span>[Tensor, nn.Parameter]],</span><br><span class="hljs-params">    module: nn.Module,</span><br><span class="hljs-params">    aligned_numel: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    use_orig_params: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    初始化 ``FlatParameter`` 及其元数据。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    注意：此方法只应在构造时调用一次，之后 ``FlatParameter`` 的元数据被假定为静态的。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># --- 1. 输入验证 ---</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(params) == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;期望非空的 `params`&quot;</span>)<br>    <span class="hljs-keyword">if</span> aligned_numel &lt; <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<br>            <span class="hljs-string">f&quot;期望非负的 `aligned_numel` 但得到了 <span class="hljs-subst">&#123;aligned_numel&#125;</span>&quot;</span><br>        )<br>    <span class="hljs-comment"># 验证所有待展平的张量具有相同的 dtype、requires_grad 和 device</span><br>    (<br>        dtype,<br>        flat_param_requires_grad,<br>        device,<br>    ) = self._validate_tensors_to_flatten(params)<br>    params_set = <span class="hljs-built_in">set</span>(params) <span class="hljs-comment"># 转换为集合以提高查找效率</span><br><br>    <span class="hljs-comment"># --- 2. 初始化用于存储元数据的列表 ---</span><br>    param_infos: <span class="hljs-built_in">list</span>[ParamInfo] = []  <span class="hljs-comment"># 参数信息 (名称, 所属模块, 模块名)</span><br>    numels: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">int</span>] = []             <span class="hljs-comment"># 每个参数的元素数量</span><br>    shapes: <span class="hljs-built_in">list</span>[torch.Size] = []      <span class="hljs-comment"># 每个参数的原始形状</span><br>    strides: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">tuple</span>[<span class="hljs-built_in">int</span>, ...]] = [] <span class="hljs-comment"># 每个参数的原始步长</span><br>    fqns: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">str</span>] = []               <span class="hljs-comment"># 每个参数的完全限定名 (e.g., &#x27;layer1.0.conv.weight&#x27;)</span><br>    shared_param_infos: <span class="hljs-built_in">list</span>[SharedParamInfo] = [] <span class="hljs-comment"># 共享参数的信息</span><br>    <span class="hljs-comment"># 用于跟踪已处理参数，以识别共享参数</span><br>    shared_param_memo: <span class="hljs-built_in">dict</span>[<br>        <span class="hljs-type">Union</span>[Tensor, nn.Parameter], <span class="hljs-built_in">tuple</span>[nn.Module, <span class="hljs-built_in">str</span>, <span class="hljs-built_in">str</span>]<br>    ] = &#123;&#125;<br>    params_to_flatten: <span class="hljs-built_in">list</span>[<span class="hljs-type">Union</span>[Tensor, nn.Parameter]] = [] <span class="hljs-comment"># 最终要展平的张量列表（包括填充）</span><br>    shared_params: <span class="hljs-built_in">list</span>[<span class="hljs-type">Union</span>[Tensor, nn.Parameter]] = []     <span class="hljs-comment"># 识别出的共享参数列表</span><br>    is_padding_mask: <span class="hljs-built_in">list</span>[<span class="hljs-built_in">bool</span>] = [] <span class="hljs-comment"># 标记 `params_to_flatten` 中哪些是填充</span><br>    total_numel = total_numel_without_padding = <span class="hljs-number">0</span> <span class="hljs-comment"># 计数器</span><br><br>    <span class="hljs-comment"># --- 3. 遍历模块，收集参数和元数据 ---</span><br>    <span class="hljs-comment"># 遍历模块的所有子模块和参数，以确保参数的顺序是确定的</span><br>    <span class="hljs-keyword">for</span> submodule_name, submodule <span class="hljs-keyword">in</span> module.named_modules(remove_duplicate=<span class="hljs-literal">False</span>):<br>        <span class="hljs-keyword">for</span> param_name, param <span class="hljs-keyword">in</span> _named_parameters_with_duplicates(<br>            submodule, recurse=<span class="hljs-literal">False</span><br>        ):<br>            <span class="hljs-keyword">if</span> param <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> params_set:<br>                <span class="hljs-keyword">continue</span> <span class="hljs-comment"># 只处理在输入 `params` 列表中的参数</span><br>            <br>            <span class="hljs-comment"># 如果参数已经在 memo 中，说明它是共享参数</span><br>            <span class="hljs-keyword">if</span> param <span class="hljs-keyword">in</span> shared_param_memo:<br>                <span class="hljs-comment"># ... 记录共享参数信息 ...</span><br>                shared_params.append(param)<br>                <span class="hljs-comment"># ...</span><br>            <span class="hljs-keyword">else</span>: <span class="hljs-comment"># 这是一个新的、未见过的参数</span><br>                <span class="hljs-comment"># --- 3a. 处理内存对齐填充 ---</span><br>                <span class="hljs-keyword">if</span> aligned_numel &gt; <span class="hljs-number">0</span>:<br>                    numel_to_pad = aligned_numel - (total_numel % aligned_numel)<br>                    <span class="hljs-keyword">if</span> numel_to_pad &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> numel_to_pad &lt; aligned_numel:<br>                        <span class="hljs-comment"># 如果需要，插入一个填充张量</span><br>                        padding_tensor = _construct_padding_tensor(<br>                            numel_to_pad, dtype, <span class="hljs-literal">False</span>, device<br>                        )<br>                        params_to_flatten.append(padding_tensor)<br>                        is_padding_mask.append(<span class="hljs-literal">True</span>)<br>                        numels.append(numel_to_pad)<br>                        total_numel += numel_to_pad<br>                <br>                <span class="hljs-comment"># --- 3b. 记录主参数的元数据 ---</span><br>                shared_param_memo[param] = (submodule, submodule_name, param_name)<br>                params_to_flatten.append(param)<br>                is_padding_mask.append(<span class="hljs-literal">False</span>)<br>                param_infos.append(ParamInfo(param_name, submodule, submodule_name))<br>                numels.append(param.numel())<br>                shapes.append(param.shape)<br>                strides.append(param.stride())<br>                <span class="hljs-comment"># ... 记录其他元数据 ...</span><br>                total_numel += param.numel()<br>                total_numel_without_padding += param.numel()<br><br>    <span class="hljs-comment"># --- 4. 处理 reduce-scatter 的填充 ---</span><br>    <span class="hljs-comment"># 为了让 reduce-scatter 操作更高效，需要确保总元素数能被 world_size 整除</span><br>    <span class="hljs-keyword">if</span> aligned_numel &gt; <span class="hljs-number">0</span>:<br>        numel_to_pad = self.world_size - (total_numel % self.world_size)<br>        <span class="hljs-keyword">if</span> numel_to_pad &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> numel_to_pad &lt; self.world_size:<br>            <span class="hljs-comment"># ... 如果需要，再次插入填充张量 ...</span><br>            padding_tensor = _construct_padding_tensor(<br>                numel_to_pad, dtype, <span class="hljs-literal">False</span>, device<br>            )<br>            params_to_flatten.append(padding_tensor)<br>            is_padding_mask.append(<span class="hljs-literal">True</span>)<br>            numels.append(numel_to_pad)<br>            total_numel += numel_to_pad<br><br>    <span class="hljs-comment"># --- 5. 执行展平操作 ---</span><br>    <span class="hljs-comment"># 调用 `flatten_tensors_into_flat_param` 将 `params_to_flatten` 列表中的所有张量</span><br>    <span class="hljs-comment"># 合并成一个大的、一维的 `FlatParameter`</span><br>    self.flat_param: FlatParameter = self.flatten_tensors_into_flat_param(<br>        params_to_flatten,<br>        aligned_numel=<span class="hljs-number">0</span>, <span class="hljs-comment"># 此时已手动处理完对齐，故传 0</span><br>        requires_grad=flat_param_requires_grad,<br>    )<br><br>    <span class="hljs-comment"># --- 6. 将元数据附加到 FlatParameter 上 ---</span><br>    <span class="hljs-comment"># 调用 `FlatParameter` 的静态方法，将之前收集的所有元数据（形状、步长、名称等）</span><br>    <span class="hljs-comment"># 作为属性附加到新创建的 `self.flat_param` 对象上。</span><br>    FlatParameter._init_metadata(<br>        self.flat_param,<br>        param_infos,<br>        numels,<br>        shapes,<br>        strides,<br>        <span class="hljs-comment"># ... 传递所有其他元数据列表 ...</span><br>    )<br></code></pre></td></tr></table></figure>

<h5 id="flatten-tensors-into-flat-param"><a href="#flatten-tensors-into-flat-param" class="headerlink" title="flatten_tensors_into_flat_param"></a>flatten_tensors_into_flat_param</h5><p>最后的结果就是进行了张量展开，获得了一个扁平的张量，形状为：[参数数量，参数长度]。即每个参数param都变成了一维，最后各个param都拼接在了一起。所以这些参数最后在物理地址上都是连续的，方便操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">flatten_tensors_into_flat_param</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    tensors: <span class="hljs-built_in">list</span>[Tensor],</span><br><span class="hljs-params">    aligned_numel: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    requires_grad: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params"></span>) -&gt; FlatParameter:<br>    flat_param_data = self.flatten_tensors(tensors, aligned_numel)<br>    <span class="hljs-keyword">return</span> FlatParameter(flat_param_data, requires_grad=requires_grad)<br></code></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">flatten_tensors</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    tensors: <span class="hljs-built_in">list</span>[Tensor], <span class="hljs-comment"># 输入：一个待展平的张量列表</span></span><br><span class="hljs-params">    aligned_numel: <span class="hljs-built_in">int</span>,    <span class="hljs-comment"># 输入：用于内存对齐的元素数量。如果为0，则不进行对齐填充</span></span><br><span class="hljs-params"></span>) -&gt; Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    将 `tensors` 展平为单个扁平张量。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    如果 `aligned_numel` 大于 0，展平过程会包含可选的填充，</span><br><span class="hljs-string">    其中 `aligned_numel` 给出了实现地址对齐所需的元素数量。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    注意：填充对齐算法必须与 `_init_flat_param_metadata` 方法保持同步。</span><br><span class="hljs-string">    我们分离这两个方法是因为初始化只发生一次，而此方法可能在训练过程中</span><br><span class="hljs-string">    被多次调用（例如，用于保存检查点）。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># --- 1. 输入校验 ---</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(tensors) == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;期望 `tensors` 列表不为空&quot;</span>)<br>    <span class="hljs-keyword">if</span> aligned_numel &lt; <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<br>            <span class="hljs-string">f&quot;期望 `aligned_numel` 为非负数，但得到了 <span class="hljs-subst">&#123;aligned_numel&#125;</span>&quot;</span><br>        )<br>    <span class="hljs-comment"># 校验所有输入张量的数据类型(dtype)和设备(device)是否一致</span><br>    dtype, _, device = self._validate_tensors_to_flatten(tensors)<br>    <br>    flat_tensors: <span class="hljs-built_in">list</span>[Tensor] = [] <span class="hljs-comment"># 用于存储最终要拼接的张量（包括填充）</span><br><br>    <span class="hljs-comment"># --- 2. 处理对齐填充 (如果需要) ---</span><br>    <span class="hljs-keyword">if</span> aligned_numel &gt; <span class="hljs-number">0</span>:<br>        total_numel = <span class="hljs-number">0</span> <span class="hljs-comment"># 记录当前已处理的元素总数</span><br>        <span class="hljs-keyword">for</span> tensor <span class="hljs-keyword">in</span> tensors:<br>            <span class="hljs-comment"># 计算在添加当前张量之前需要多少填充，以使其起始位置按 `aligned_numel` 对齐</span><br>            numel_to_pad = aligned_numel - (total_numel % aligned_numel)<br>            <span class="hljs-keyword">if</span> numel_to_pad &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> numel_to_pad &lt; aligned_numel:<br>                <span class="hljs-comment"># 创建一个指定大小的填充张量</span><br>                padding_tensor = _construct_padding_tensor(<br>                    numel_to_pad, dtype, <span class="hljs-literal">False</span>, device<br>                )<br>                flat_tensors.append(padding_tensor)<br>                total_numel += numel_to_pad<br>            <br>            <span class="hljs-comment"># 添加实际的张量。首先将其展平为一维</span><br>            <span class="hljs-comment"># 如果张量是内存连续的，直接 flatten；否则使用 as_strided 避免额外拷贝</span><br>            flat_tensors.append(<br>                torch.flatten(_detach_if_needed(tensor))<br>                <span class="hljs-keyword">if</span> _is_truly_contiguous(tensor)<br>                <span class="hljs-keyword">else</span> _detach_if_needed(tensor).as_strided((tensor.numel(),), (<span class="hljs-number">1</span>,))<br>            )<br>            total_numel += tensor.numel()<br>        <br>        <span class="hljs-comment"># --- 3. 处理分片填充 (为了能被 world_size 整除) ---</span><br>        numel_to_pad = self.world_size - (total_numel % self.world_size)<br>        <span class="hljs-keyword">if</span> numel_to_pad &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> numel_to_pad &lt; self.world_size:<br>            padding_tensor = _construct_padding_tensor(<br>                numel_to_pad, dtype, <span class="hljs-literal">False</span>, device<br>            )<br>            flat_tensors.append(padding_tensor)<br>            total_numel += numel_to_pad<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># --- 4. 无对齐填充的简单情况 ---</span><br>        <span class="hljs-comment"># 直接将每个张量展平并放入列表</span><br>        flat_tensors = [<br>            torch.flatten(_detach_if_needed(tensor))<br>            <span class="hljs-keyword">if</span> _is_truly_contiguous(tensor)<br>            <span class="hljs-keyword">else</span> _detach_if_needed(tensor).as_strided((tensor.numel(),), (<span class="hljs-number">1</span>,))<br>            <span class="hljs-keyword">for</span> tensor <span class="hljs-keyword">in</span> tensors<br>        ]<br>    <br>    <span class="hljs-comment"># --- 5. 最终拼接 ---</span><br>    <span class="hljs-comment"># 将列表中的所有张量（包括原始张量和所有填充张量）拼接成一个最终的扁平化张量</span><br>    <span class="hljs-keyword">return</span> torch.cat(flat_tensors, dim=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>

<h1 id="fsdp模型forward"><a href="#fsdp模型forward" class="headerlink" title="fsdp模型forward"></a>fsdp模型forward</h1><p>最外部的代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, *args: <span class="hljs-type">Any</span>, **kwargs: <span class="hljs-type">Any</span></span>) -&gt; <span class="hljs-type">Any</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Run the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.&quot;&quot;&quot;</span><br>    handle = self._handle<br>    <span class="hljs-keyword">with</span> torch.autograd.profiler.record_function(<br>        <span class="hljs-string">&quot;FullyShardedDataParallel.forward&quot;</span><br>    ):<br>        args, kwargs = _root_pre_forward(self, self, args, kwargs)<br>        unused = <span class="hljs-literal">None</span><br>        args, kwargs = _pre_forward(<br>            self,<br>            handle,<br>            _pre_forward_unshard,<br>            self._fsdp_wrapped_module,<br>            args,<br>            kwargs,<br>        )<br>        <span class="hljs-keyword">if</span> handle:<br>            _p_assert(<br>                handle.flat_param.device == self.compute_device,<br>                <span class="hljs-string">&quot;Expected `FlatParameter` to be on the compute device &quot;</span><br>                <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;self.compute_device&#125;</span> but got <span class="hljs-subst">&#123;handle.flat_param.device&#125;</span>&quot;</span>,<br>            )<br>        output = self._fsdp_wrapped_module(*args, **kwargs)<br>        <span class="hljs-keyword">return</span> _post_forward(<br>            self, handle, _post_forward_reshard, self, unused, output<br>        )<br><br></code></pre></td></tr></table></figure>

<h2 id="pre-forward"><a href="#pre-forward" class="headerlink" title="_pre_forward"></a>_pre_forward</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_pre_forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: <span class="hljs-type">Optional</span>[FlatParamHandle],</span><br><span class="hljs-params">    unshard_fn: <span class="hljs-type">Callable</span>,</span><br><span class="hljs-params">    module: nn.Module,</span><br><span class="hljs-params">    args: <span class="hljs-built_in">tuple</span>[<span class="hljs-type">Any</span>, ...],</span><br><span class="hljs-params">    kwargs: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-built_in">tuple</span>[<span class="hljs-built_in">tuple</span>[<span class="hljs-type">Any</span>, ...], <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>]]:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    执行前向传播前的逻辑。这包括：</span><br><span class="hljs-string">    1. 对当前分片的参数进行反分片（unshard），使其恢复为完整参数。</span><br><span class="hljs-string">    2. 为这些参数注册后向传播钩子（post-backward hooks）。</span><br><span class="hljs-string">    3. 将前向传播的输入（args, kwargs）转换为指定的计算精度。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 使用 PyTorch profiler 记录函数执行，便于性能分析</span><br>    <span class="hljs-keyword">with</span> torch.profiler.record_function(<span class="hljs-string">&quot;FullyShardedDataParallel._pre_forward&quot;</span>):<br>        <span class="hljs-comment"># 这是一个针对梯度检查点（gradient checkpointing）的特殊处理。</span><br>        <span class="hljs-comment"># 在梯度检查点的重计算阶段，模块会再次执行前向传播，但此时参数已经 unshard 过了，</span><br>        <span class="hljs-comment"># 无需重复执行 unshard 和注册 hook 等操作，直接返回即可。</span><br>        <span class="hljs-keyword">if</span> handle <span class="hljs-keyword">and</span> handle._training_state == HandleTrainingState.BACKWARD_PRE:<br>            <span class="hljs-keyword">return</span> args, kwargs<br><br>        <span class="hljs-comment"># 1. 更新 FSDP 状态，标记当前正处于前向或后向传播阶段。</span><br>        state.training_state = TrainingState.FORWARD_BACKWARD<br>        <span class="hljs-comment"># 记录当前模块的执行顺序，这对于后续的预取（prefetching）和梯度同步至关重要。</span><br>        state._exec_order_data.record_pre_forward(handle, module.training)<br>        <span class="hljs-keyword">if</span> handle:<br>            <span class="hljs-comment"># 更新当前参数句柄（handle）的状态为“正在前向传播”。</span><br>            handle._training_state = HandleTrainingState.FORWARD<br><br>        <span class="hljs-comment"># 2. 执行核心操作：反分片（Unsharding）。</span><br>        <span class="hljs-comment"># 这是最关键的一步。如果 unshard_fn 存在，就调用它。</span><br>        <span class="hljs-comment"># 这个函数内部会触发 all-gather 操作，从所有 GPU 上收集参数分片，</span><br>        <span class="hljs-comment"># 在当前设备上重建完整的、未分片的参数，以供模块的 forward 方法使用。</span><br>        <span class="hljs-keyword">if</span> unshard_fn <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            unshard_fn(state, handle)<br><br>        <span class="hljs-comment"># 3. 注册后向传播钩子（Post-Backward Hook）。</span><br>        <span class="hljs-comment"># 这个钩子会在反向传播计算完当前参数的梯度之后被触发。</span><br>        <span class="hljs-comment"># 它的主要作用是：</span><br>        <span class="hljs-comment">#   a. 将参数重新分片（reshard），释放完整参数占用的内存。</span><br>        <span class="hljs-comment">#   b. 对计算出的完整梯度进行 reduce-scatter 操作，完成梯度同步。</span><br>        <span class="hljs-comment"># 因为计算图（grad_fn）每次都可能变化，所以这个钩子需要在每次前向传播时都重新注册。</span><br>        _register_post_backward_hook(state, handle)<br><br>        <span class="hljs-comment"># 针对 CPU Offload 的特殊处理：如果优化器在反向传播中将 CPU 上的梯度清空了，</span><br>        <span class="hljs-comment"># 这里需要重新分配一块内存空间给它，为下一次梯度累积做准备。</span><br>        <span class="hljs-keyword">if</span> handle <span class="hljs-keyword">and</span> handle._offload_params <span class="hljs-keyword">and</span> handle.flat_param._cpu_grad <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            handle.flat_param._cpu_grad = torch.zeros_like(<br>                handle.flat_param._local_shard, device=torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)<br>            ).pin_memory()<br><br>        <span class="hljs-comment"># 检查是否需要将模型输入转换为低精度（如 fp16）。</span><br>        should_cast_forward_inputs = (<br>            state._handle <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> state._handle._force_full_precision<br>        )<br><br>        <span class="hljs-comment"># 4. 如果启用了混合精度（Mixed Precision）并且设置了 cast_forward_inputs，</span><br>        <span class="hljs-comment"># 就将 `args` 和 `kwargs` 中的张量递归地转换为指定的参数数据类型（param_dtype）。</span><br>        <span class="hljs-keyword">if</span> should_cast_forward_inputs <span class="hljs-keyword">and</span> state.mixed_precision.cast_forward_inputs:<br>            input_dtype: <span class="hljs-type">Optional</span>[torch.dtype] = state.mixed_precision.param_dtype<br>            args, kwargs = _cast_forward_inputs(input_dtype, *args, **kwargs)<br><br>        <span class="hljs-comment"># 注册一个只做 reshard 的后向钩子，用于不需要计算梯度的场景。</span><br>        _register_post_backward_reshard_only_hook(state, handle, args, kwargs)<br>        <br>        <span class="hljs-comment"># 返回处理过（可能已转换精度）的输入，传递给原始模块的 forward 方法。</span><br>        <span class="hljs-keyword">return</span> args, kwargs<br></code></pre></td></tr></table></figure>



<h3 id="pre-forward-unshard"><a href="#pre-forward-unshard" class="headerlink" title="_pre_forward_unshard"></a>_pre_forward_unshard</h3><p>对于_pre_forward，可以看到其传入的是<code>_pre_forward_unshard</code>函数，该函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_pre_forward_unshard</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: <span class="hljs-type">Optional</span>[FlatParamHandle],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Unshards parameters in the pre-forward.&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle:<br>        <span class="hljs-keyword">return</span><br>    <span class="hljs-comment"># If the handles have been prefetched, then there is no need to call</span><br>    <span class="hljs-comment"># `_unshard()` again</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle._prefetched:<br>        _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br>    handle._needs_pre_forward_unshard = <span class="hljs-literal">False</span><br>    <span class="hljs-comment"># Don&#x27;t wait during trace</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>        current_stream = state._device_handle.current_stream()<br>        <span class="hljs-keyword">if</span> state._unshard_event <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            current_stream.wait_event(state._unshard_event)<br>            state._unshard_event = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">else</span>:<br>            current_stream.wait_stream(state._unshard_stream)<br>    <span class="hljs-keyword">with</span> torch.profiler.record_function(<br>        <span class="hljs-string">&quot;FullyShardedDataParallel._pre_forward_prefetch&quot;</span><br>    ):<br>        _prefetch_handle(state, handle, _PrefetchMode.FORWARD)<br><br><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_unshard</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params">    unshard_stream: torch.Stream,</span><br><span class="hljs-params">    pre_unshard_stream: torch.Stream,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Unshards the handles in ``handles``. If the handles are in</span><br><span class="hljs-string">    :meth:`summon_full_params` and are using mixed precision, then they are</span><br><span class="hljs-string">    forced to full precision.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Postcondition: handle&#x27;s ``FlatParameter`` &#x27;s data is the padded</span><br><span class="hljs-string">    unsharded flat parameter on the compute device.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle:<br>        <span class="hljs-keyword">return</span><br>    <span class="hljs-keyword">with</span> state._device_handle.stream(pre_unshard_stream):<br>        ran_pre_unshard = handle.pre_unshard()<br>    <span class="hljs-keyword">if</span> ran_pre_unshard:<br>        unshard_stream.wait_stream(pre_unshard_stream)<br>    <span class="hljs-keyword">if</span> state.limit_all_gathers:<br>        event = state._free_event_queue.dequeue_if_needed()<br>        <span class="hljs-keyword">if</span> event:<br>            <span class="hljs-keyword">with</span> torch.profiler.record_function(<br>                <span class="hljs-string">&quot;FullyShardedDataParallel.rate_limiter&quot;</span><br>            ):<br>                event.synchronize()<br>    <span class="hljs-keyword">with</span> state._device_handle.stream(unshard_stream):<br>        handle.unshard()<br>        handle.post_unshard()<br><br></code></pre></td></tr></table></figure>

<p>其中最关键的是<code>unshard</code>函数，该函数用来将参数重新收集回来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python">// ... existing code ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">unshard</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Run the unshard logic.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This includes all-gathering the flat parameter</span><br><span class="hljs-string">        and switching to using the unsharded flat parameter. If the handle does</span><br><span class="hljs-string">        not need unsharding, then this only switches to using the unsharded</span><br><span class="hljs-string">        flat parameter. For ``NO_SHARD``, this is a no-op.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        If FSDP is in :meth:`summon_full_params` and the handle uses parameter</span><br><span class="hljs-string">        mixed precision, then the parameter is forced to full precision.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 1. 检查是否真的需要执行 unshard 操作。</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.needs_unshard():<br>            <span class="hljs-comment"># 如果不需要（例如，sharding 策略是 NO_SHARD，或者参数已经被 unshard），</span><br>            <span class="hljs-comment"># 也要确保后续计算使用的是 unsharded 参数的视图。</span><br>            unsharded_flat_param = (<br>                self._get_padded_unsharded_flat_param()<br>                <span class="hljs-keyword">if</span> self.uses_sharded_strategy<br>                <span class="hljs-keyword">else</span> self.flat_param<br>            )<br>            self._use_unsharded_flat_param(unsharded_flat_param)<br>            <span class="hljs-keyword">return</span><br>        <br>        <span class="hljs-comment"># 2. 如果需要 unshard，则执行以下核心步骤：</span><br>        <span class="hljs-comment"># 2a. 分配内存：为即将聚合的完整参数张量分配空间。</span><br>        unsharded_flat_param = self._alloc_padded_unsharded_flat_param()<br>        <span class="hljs-comment"># 2b. 执行 All-Gather：调用我们之前分析过的 `_all_gather_flat_param` 方法，</span><br>        <span class="hljs-comment">#     从所有进程收集参数分片，并填充到刚刚分配的内存中。</span><br>        padded_unsharded_flat_param = self._all_gather_flat_param(unsharded_flat_param)<br>        <span class="hljs-comment"># 2c. 切换状态：将模块内部的参数指针切换为指向这个刚刚聚合好的完整参数，</span><br>        <span class="hljs-comment">#     以便后续的前向或后向计算可以使用它。</span><br>        self._use_unsharded_flat_param(padded_unsharded_flat_param)<br></code></pre></td></tr></table></figure>

<h4 id="alloc-padded-unsharded-flat-param"><a href="#alloc-padded-unsharded-flat-param" class="headerlink" title="_alloc_padded_unsharded_flat_param"></a>_alloc_padded_unsharded_flat_param</h4><p>该函数负责获取收集全参数的变量，并给他分配足够的内存</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">// ... existing code ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_alloc_padded_unsharded_flat_param</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Allocate the *padded* unsharded flat parameter.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        The unpadded unsharded</span><br><span class="hljs-string">        flat parameter is always a view into the padded one. This padded</span><br><span class="hljs-string">        parameter is saved to a different attribute on the ``FlatParameter``</span><br><span class="hljs-string">        depending on if we force full precision.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 1. 检查：确保当前正在使用分片策略。</span><br>        self._check_sharded_strategy()<br>        flat_param = self.flat_param<br>        <br>        <span class="hljs-comment"># 2. 获取目标张量：获取将要用于存储完整参数的那个张量对象。</span><br>        <span class="hljs-comment">#    此时它可能还只是一个没有分配实际存储空间的“空壳”。</span><br>        unsharded_flat_param = self._get_padded_unsharded_flat_param()<br>        <br>        <span class="hljs-comment"># 3. 检查存储：确保这个张量之前的存储已经被释放，防止内存泄漏。</span><br>        self._check_storage_freed(unsharded_flat_param)<br>        <br>        <span class="hljs-comment"># 4. 分配存储：这是核心操作。调用 `_alloc_storage` 为这个张量分配实际的内存空间。</span><br>        <span class="hljs-comment">#    分配的大小是 `_padded_unsharded_size`，即所有分片聚合后的总大小，可能还包含一些为了对齐而增加的 padding。</span><br>        _alloc_storage(unsharded_flat_param, flat_param._padded_unsharded_size)  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>        <br>        <span class="hljs-comment"># 5. 返回张量：返回这个已经分配好内存、准备好被填充的张量。</span><br>        <span class="hljs-keyword">return</span> unsharded_flat_param<br></code></pre></td></tr></table></figure>

<h5 id="get-padded-unsharded-flat-param"><a href="#get-padded-unsharded-flat-param" class="headerlink" title="_get_padded_unsharded_flat_param"></a>_get_padded_unsharded_flat_param</h5><p>注意这里判断了是否是强制使用全精度，如果是，就会返回<code>flat_param._full_prec_full_param_padded</code>，并且释放掉<code>flat_param._full_param_padded</code>，如果不是，就直接返回<code>flat_param._full_param_padded</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python">// ... existing code ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_padded_unsharded_flat_param</span>(<span class="hljs-params">self</span>) -&gt; torch.Tensor:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Return a reference to the padded unsharded flat parameter depending on the calling context.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        This should only be called if using a sharded strategy.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self._check_sharded_strategy()<br>        flat_param = self.flat_param<br>        <span class="hljs-comment"># 关键的逻辑判断：是否需要强制使用全精度？</span><br>        <span class="hljs-keyword">if</span> self._force_full_precision <span class="hljs-keyword">and</span> self._uses_param_mixed_precision:<br>            <span class="hljs-comment"># --- 情况1: 需要强制全精度 --- </span><br>            <span class="hljs-comment"># 当 FSDP 进入一个需要全精度参数的上下文（例如 `summon_full_params`），</span><br>            <span class="hljs-comment"># 并且当前参数本身是使用混合精度（如 bfloat16）存储的。</span><br>            <br>            <span class="hljs-comment"># 1. 选择一个专门用于存储全精度（如 float32）参数的张量作为目标。</span><br>            unsharded_flat_param = flat_param._full_prec_full_param_padded  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>            _p_assert(<br>// ... existing code ...<br>            )<br>            <span class="hljs-comment"># 2. 释放可能存在的、旧的、低精度的完整参数的存储。</span><br>            <span class="hljs-comment">#    因为全精度版本接下来可能会被修改，这会导致低精度版本失效。</span><br>            <span class="hljs-comment">#    释放它是为了确保下次需要时，会重新执行 all-gather 获取最新的数据。</span><br>            <span class="hljs-keyword">if</span> flat_param._full_param_padded.untyped_storage().size() &gt; <span class="hljs-number">0</span>:<br>                _free_storage(flat_param._full_param_padded)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># --- 情况2: 标准情况 --- </span><br>            <span class="hljs-comment"># 在常规的前向/后向传播中，直接使用默认的、与参数计算类型一致的张量即可。</span><br>            <span class="hljs-comment"># 这个张量的数据类型通常是低精度（如 bfloat16）。</span><br>            unsharded_flat_param = flat_param._full_param_padded  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>        <span class="hljs-keyword">return</span> unsharded_flat_param<br></code></pre></td></tr></table></figure>

<h5 id="alloc-storage"><a href="#alloc-storage" class="headerlink" title="_alloc_storage"></a>_alloc_storage</h5><p>该函数调用了底层存储对象的 <em>resize</em> 方法，将其大小调整为所需的元素数量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_alloc_storage</span>(<span class="hljs-params">tensor: torch.Tensor, size: torch.Size</span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Allocate storage for ``tensor`` with the given size.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        bool: ``True`` if this method allocated storage and ``False`` if the</span><br><span class="hljs-string">        storage was already allocated.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>            already_allocated = tensor._typed_storage()._size() == size.numel()<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> already_allocated:<br>                tensor_storage_size = tensor._typed_storage()._size()<br>                _p_assert(<br>                    tensor_storage_size == <span class="hljs-number">0</span>,<br>                    <span class="hljs-string">&quot;Tensor storage should have been resized to be 0 but got PLACEHOLDEr&quot;</span>,<br>                )<br>                tensor._typed_storage()._resize_(size.numel())<br></code></pre></td></tr></table></figure>

<h4 id="all-gather-flat-param"><a href="#all-gather-flat-param" class="headerlink" title="_all_gather_flat_param"></a>_all_gather_flat_param</h4><p>这个函数是通过all_gather操作来进行实际的参数收集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python">// ... existing code ...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_all_gather_flat_param</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        padded_unsharded_flat_param: Tensor,</span><br><span class="hljs-params">    </span>) -&gt; Tensor:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        All-gather the handle&#x27;s flat parameter to the destination ``padded_unsharded_flat_param``.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Then switch to use the all-gathered tensor.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 1. 断言检查：确保分布式环境已初始化，并且目标张量的大小足以容纳所有分片。</span><br>        _p_assert(<br>// ... existing code ...<br>        )<br><br>        <span class="hljs-comment"># 2. 获取用于通信的进程组（Process Group）。</span><br>        pg = (<br>            self._fake_process_group<br>            <span class="hljs-keyword">if</span> self._use_fake_all_gather<br>            <span class="hljs-keyword">else</span> self.process_group<br>        )<br><br>        <span class="hljs-comment"># 3. 根据张量是在 CPU 还是 GPU 上，执行不同的 all-gather 操作。</span><br>        <span class="hljs-comment"># HACK this should be handled by C10D</span><br>        <span class="hljs-keyword">if</span> sharded_flat_param.is_cpu:  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>            <span class="hljs-comment"># 对于 CPU，将数据收集到一个 tensor 列表中。</span><br>            tensor_list = <span class="hljs-built_in">list</span>(<br>                torch.chunk(<br>                    padded_unsharded_flat_param,<br>                    dist.get_world_size(pg),  <span class="hljs-comment"># type: ignore[arg-<span class="hljs-built_in">type</span>]</span><br>                )<br>            )<br>            dist.all_gather(tensor_list, sharded_flat_param, group=pg)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 对于 GPU，使用更高效的 all_gather_into_tensor 直接填充目标张量。</span><br>            dist.all_gather_into_tensor(<br>                padded_unsharded_flat_param,<br>                sharded_flat_param,<br>                pg,<br>            )<br><br>        <span class="hljs-comment"># 4. 处理参数卸载（Offloading）的特殊情况。</span><br>        <span class="hljs-keyword">if</span> self._offload_params:<br>            <span class="hljs-comment"># 如果参数被卸载到 CPU，需要确保 CUDA stream 正确同步，防止数据竞争。</span><br>            _no_dispatch_record_stream(<br>                sharded_flat_param,<br>                self._device_handle.current_stream(),  <span class="hljs-comment"># unshard_stream</span><br>            )<br>        <span class="hljs-comment"># 5. 返回填充了完整参数的张量。</span><br>        <span class="hljs-keyword">return</span> padded_unsharded_flat_param<br></code></pre></td></tr></table></figure>





<h5 id="all-gather-into-tensor学习"><a href="#all-gather-into-tensor学习" class="headerlink" title="all_gather_into_tensor学习"></a>all_gather_into_tensor学习</h5><p>注意到这里对于GPU使用到了<code>dist.all_gather_into_tensor</code>操作。这个操作的示意图如下，即将各个GPU上的分片按序收集给各个GPU上，使得每个GPU都有一个整体：</p>
<p><img src="/2025/07/02/pytorch-fsdp-1/image.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="/2025/07/02/pytorch-fsdp-1/image-1.png" srcset="/img/loading.gif" lazyload></p>
<p>这里有一个示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.distributed <span class="hljs-keyword">as</span> dist<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>():<br>    dist.init_process_group(backend=<span class="hljs-string">&quot;nccl&quot;</span>)  <span class="hljs-comment"># or &quot;gloo&quot; for CPU</span><br>    rank = dist.get_rank()<br>    world_size = dist.get_world_size()<br><br>    <span class="hljs-comment"># 每个进程构造自己的 input tensor</span><br>    input_tensor = torch.ones(<span class="hljs-number">2</span>, device=<span class="hljs-string">&#x27;cuda&#x27;</span>) * (rank + <span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 所有数据拼接的输出 tensor</span><br>    output_tensor = torch.empty(<span class="hljs-number">2</span> * world_size, device=<span class="hljs-string">&#x27;cuda&#x27;</span>)<br><br>    <span class="hljs-comment"># All-gather into output tensor</span><br>    dist.all_gather_into_tensor(output_tensor, input_tensor)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[rank <span class="hljs-subst">&#123;rank&#125;</span>] output_tensor: <span class="hljs-subst">&#123;output_tensor.cpu().tolist()&#125;</span>&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    torch.cuda.set_device(<span class="hljs-built_in">int</span>(os.environ[<span class="hljs-string">&quot;LOCAL_RANK&quot;</span>]))  <span class="hljs-comment"># torchrun 自动设置</span><br>    run()<br><br><span class="hljs-comment"># 2机，每机2卡的运行指令</span><br><span class="hljs-comment"># 机器1: torchrun   --nproc_per_node=2   --nnodes=2   --node_rank=0   --master_addr=fdbd:dc03:16:266::86   --master_port=12345   test.py</span><br><span class="hljs-comment"># 机器2: torchrun   --nproc_per_node=2   --nnodes=2   --node_rank=1   --master_addr=fdbd:dc03:16:266::86   --master_port=12345   test.py</span><br><br><span class="hljs-comment"># 运行结果</span><br><span class="hljs-comment"># [rank 0] output_tensor: [1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0]</span><br><span class="hljs-comment"># [rank 1] output_tensor: [1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0]</span><br><span class="hljs-comment"># [rank 2] output_tensor: [1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0]</span><br><span class="hljs-comment"># [rank 3] output_tensor: [1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 4.0, 4.0]</span><br><br></code></pre></td></tr></table></figure>

<h5 id="all-gather学习"><a href="#all-gather学习" class="headerlink" title="all_gather学习"></a>all_gather学习</h5><p>此外注意到这里这里对CPU使用了all_gather，它是 PyTorch 最经典的分布式通信原语之一。</p>
<p>它负责把每个进程上的 <code>tensor</code> 收集起来，按 rank 顺序填入 <code>tensor_list</code> 中。</p>
<ul>
<li><code>tensor_list[i]</code> 就是第 i 个进程的 tensor。</li>
</ul>
<p>一个简单的示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.distributed <span class="hljs-keyword">as</span> dist<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>():<br>    dist.init_process_group(backend=<span class="hljs-string">&quot;gloo&quot;</span>)  <span class="hljs-comment"># &quot;nccl&quot; for GPU</span><br>    rank = dist.get_rank()<br>    world_size = dist.get_world_size()<br><br>    <span class="hljs-comment"># 当前进程持有的 tensor，标记自己的 rank</span><br>    input_tensor = torch.full((<span class="hljs-number">2</span>,), rank, dtype=torch.<span class="hljs-built_in">int</span>)<br><br>    <span class="hljs-comment"># tensor_list 是一个 list，会存放所有进程的 tensor</span><br>    tensor_list = [torch.empty_like(input_tensor) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(world_size)]<br><br>    <span class="hljs-comment"># 执行 all_gather：每个进程收集所有人的 tensor</span><br>    dist.all_gather(tensor_list, input_tensor)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[rank <span class="hljs-subst">&#123;rank&#125;</span>] tensor_list = <span class="hljs-subst">&#123;[t.tolist() <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> tensor_list]&#125;</span>&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    run()<br><br><span class="hljs-comment"># 单机2卡的运行指令</span><br><span class="hljs-comment"># torchrun   --nproc_per_node=2   --nnodes=1   --node_rank=0   --master_addr=fdbd:dc03:16:266::86   --master_port=12346   test.py</span><br><br><span class="hljs-comment"># 运行结果</span><br><span class="hljs-comment"># [rank 0] tensor_list = [[0, 0], [1, 1]]</span><br><span class="hljs-comment"># [rank 1] tensor_list = [[0, 0], [1, 1]]</span><br></code></pre></td></tr></table></figure>

<h5 id="record-stream学习"><a href="#record-stream学习" class="headerlink" title="record_stream学习"></a>record_stream学习</h5><p>这里对于off_load模型，即将参数卸载到CPU上的操作，会使用 <code>record_stream(stream)</code> 注册 stream 依赖，从而告诉系统这个 tensor 来自 CPU，是通过 <code>stream X</code> 拷贝到 GPU 的，请不要在这个 stream 执行完之前把它删掉。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> self._offload_params:<br>    <span class="hljs-comment"># In case of offloading, `flat_param.data` (i.e. sharded param) is</span><br>    <span class="hljs-comment"># created on the pre-unshard stream. We need to hand it over to the</span><br>    <span class="hljs-comment"># unshard stream for all-gather</span><br>    _no_dispatch_record_stream(<br>        sharded_flat_param,<br>        self._device_handle.current_stream(),  <span class="hljs-comment"># unshard_stream</span><br>    )<br></code></pre></td></tr></table></figure>

<h3 id="register-post-backward-hook"><a href="#register-post-backward-hook" class="headerlink" title="_register_post_backward_hook"></a>_register_post_backward_hook</h3><p>主要用于在梯度计算完后对参数进行重新分片</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_register_post_backward_hook</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: <span class="hljs-type">Optional</span>[FlatParamHandle],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    在 FlatParameter 的 AccumulateGrad 对象上注册一个后向钩子(post-backward hook)，</span><br><span class="hljs-string">    用于在梯度计算完成后执行梯度的 reduce-scatter 操作以及参数的重新分片(reshard)。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    AccumulateGrad 对象是完成 FlatParameter 梯度计算的最后一个函数，</span><br><span class="hljs-string">    因此钩子能确保在参数的整个梯度计算完成后才运行。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    我们只在 FlatParameter 参与的 *第一个* 前向传播中注册一次钩子。</span><br><span class="hljs-string">    这依赖于 AccumulateGrad 对象在多次前向传播中被保留的特性。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 如果不需要计算梯度，则无需注册后向钩子。</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.is_grad_enabled():<br>        <span class="hljs-keyword">return</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle:<br>        <span class="hljs-keyword">return</span><br>    flat_param = handle.flat_param<br><br>    <span class="hljs-comment"># 根据是否在 TorchDynamo 编译模式下，选择不同的钩子注册方式。</span><br>    <span class="hljs-keyword">if</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>        <span class="hljs-comment"># 检查钩子是否已注册，或者参数是否需要梯度。</span><br>        already_registered = <span class="hljs-built_in">hasattr</span>(flat_param, <span class="hljs-string">&quot;_post_backward_hook_handle&quot;</span>)<br>        <span class="hljs-keyword">if</span> already_registered <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> flat_param.requires_grad:<br>            <span class="hljs-keyword">return</span><br>        <span class="hljs-comment"># 使用 functools.partial 包装钩子函数，传入 FSDP 状态和句柄。</span><br>        hook = functools.partial(_post_backward_hook, state, handle)<br>        <span class="hljs-comment"># 使用为编译模式设计的专用 API 注册钩子。</span><br>        hook_handle = flat_param.register_post_accumulate_grad_hook(hook)<br>        <span class="hljs-comment"># 保存钩子句柄，用于状态检查和可能的卸载。</span><br>        flat_param._post_backward_hook_handle = hook_handle  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>    <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># Eager mode (常规执行模式)</span><br>        <span class="hljs-comment"># 检查钩子是否已注册。</span><br>        already_registered = <span class="hljs-built_in">hasattr</span>(flat_param, <span class="hljs-string">&quot;_post_backward_hook_state&quot;</span>)<br>        <span class="hljs-keyword">if</span> already_registered <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> flat_param.requires_grad:<br>            <span class="hljs-keyword">return</span><br>        <br>        <span class="hljs-comment"># --- 获取 AccumulateGrad 对象 --- #</span><br>        <span class="hljs-comment"># 创建一个临时的、与 flat_param 相同大小的张量，这会创建一个简单的计算图，</span><br>        <span class="hljs-comment"># 从而使我们能够访问其 grad_fn。</span><br>        temp_flat_param = flat_param.expand_as(flat_param)<br>        _p_assert(<br>            temp_flat_param.grad_fn <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>,<br>            <span class="hljs-string">&quot;需要 grad_fn 来访问 AccumulateGrad 对象并注册后向钩子&quot;</span>,<br>        )<br>        <span class="hljs-comment"># AccumulateGrad 对象是与参数直接关联的梯度累积函数。</span><br>        acc_grad = temp_flat_param.grad_fn.next_functions[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]  <span class="hljs-comment"># type: ignore[union-attr]</span><br>        <span class="hljs-keyword">assert</span> acc_grad <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># --- 注册钩子 --- #</span><br>        <span class="hljs-comment"># 在 AccumulateGrad 对象上注册钩子，确保在梯度累积完成后执行。</span><br>        hook_handle = acc_grad.register_hook(<br>            functools.partial(_post_backward_hook, state, handle)<br>        )<br>        <span class="hljs-comment"># 保存 AccumulateGrad 对象和钩子句柄，用于状态检查和后续管理。</span><br>        flat_param._post_backward_hook_state = (acc_grad, hook_handle)  <span class="hljs-comment"># type: ignore[attr-defined]</span><br><br></code></pre></td></tr></table></figure>



<h4 id="acc-grad-register-hook学习"><a href="#acc-grad-register-hook学习" class="headerlink" title="acc_grad.register_hook学习"></a>acc_grad.register_hook学习</h4><p>为了在 <strong>反向传播过程中准确地知道哪些参数梯度已经计算完成</strong>，它会在 <strong>每个参数的 <code>AccumulateGrad</code> 节点上注册钩子（hook）</strong>。</p>
<p>首先需要获取 <code>grad_fn</code>，构建 autograd 路径</p>
<ul>
<li><p><code>flat_param</code> 是一个 leaf tensor（即 <code>requires_grad=True</code> 且没有 <code>grad_fn</code>）；</p>
</li>
<li><p>使用 <code>expand_as()</code> 创建一个临时 view tensor，**这个 view 有 <code>grad_fn</code>**；</p>
</li>
<li><p>这个 <code>grad_fn</code> 会链接到 <strong><code>AccumulateGrad</code> 节点</strong>，而这个节点才允许注册 hook。</p>
</li>
</ul>
<p>需要找到 <code>AccumulateGrad</code></p>
<ul>
<li><p><code>grad_fn.next_functions</code> 是 PyTorch autograd 中的下游节点；</p>
</li>
<li><p><code>.next_functions[0][0]</code> 正好是与参数 <code>flat_param</code> 直接绑定的 <code>AccumulateGrad</code> 节点。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">  grad_fn (ExpandBackward0)<br>        ↓<br>  next_functions[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br>        ↓<br>AccumulateGrad ← hook 就挂在这里！<br></code></pre></td></tr></table></figure>



<p>然后我们需要注册钩子：</p>
<ul>
<li><p>使用 <code>functools.partial()</code> 固定住状态（<code>state</code>, <code>handle</code>）；</p>
</li>
<li><p>注册的 <code>_post_backward_hook</code> 会在梯度写入 <code>.grad</code> 前后触发；</p>
</li>
<li><p>这是 FSDP 判断“这个参数的梯度已经完成了，可以 reshard &#x2F; reduce &#x2F; offload”的触发点。</p>
</li>
</ul>
<p>然后记录下这个钩子对应的对象和句柄，方便：</p>
<ul>
<li><p>判断是否已注册（防止重复）；</p>
</li>
<li><p>后续移除 hook；</p>
</li>
<li><p>debug 或控制生命周期。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>x = torch.tensor([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>], requires_grad=<span class="hljs-literal">True</span>)<br>y = (x * <span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y=&quot;</span>,y)<br><br>temp_x = x.expand_as(x)<br>acc_grad = temp_x.grad_fn.next_functions[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">my_hook</span>(<span class="hljs-params">grad_input, grad_output</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;HOOK: grad_input=<span class="hljs-subst">&#123;grad_input&#125;</span>, grad_output=<span class="hljs-subst">&#123;grad_output&#125;</span>&quot;</span>)<br>    g = grad_output[<span class="hljs-number">0</span>]<br>    g = g * <span class="hljs-number">0.5</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Modified grad:&quot;</span>, g)<br>    <span class="hljs-comment"># 不要 return！Node hook 不能返回任何值</span><br><br>acc_grad.register_hook(my_hook)<br><br>y.backward()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x.grad=&quot;</span>,x.grad)<br><br><span class="hljs-comment"># 运行结果</span><br><span class="hljs-comment"># y= tensor(12., grad_fn=&lt;SumBackward0&gt;)</span><br><span class="hljs-comment"># HOOK: grad_input=(), grad_output=(tensor([2., 2., 2.]),)</span><br><span class="hljs-comment"># Modified grad: tensor([1., 1., 1.])</span><br><span class="hljs-comment"># x.grad= tensor([2., 2., 2.])</span><br></code></pre></td></tr></table></figure>

<h4 id="post-backward-hook"><a href="#post-backward-hook" class="headerlink" title="_post_backward_hook"></a>_post_backward_hook</h4><p>这是 FSDP 的核心反向传播钩子，负责在本地梯度计算完成后，进行跨 GPU 的梯度同步（reduce-scatter）和参数重新分片（reshard）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-meta">@torch.no_grad()</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_backward_hook</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params">    flat_param,  <span class="hljs-comment"># Note: this is a positional argument passed by the hook</span></span><br><span class="hljs-params">    *unused: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    对 `handle` 的 `FlatParameter` 的梯度执行 Reduce-scatter 操作。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这是 FSDP 的核心反向传播钩子，负责在本地梯度计算完成后，</span><br><span class="hljs-string">    进行跨 GPU 的梯度同步（reduce-scatter）和参数重新分片（reshard）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    前置条件:</span><br><span class="hljs-string">    - `FlatParameter` 的 `.grad` 属性包含了本地批次（local batch）的完整（unsharded）梯度。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    后置条件:</span><br><span class="hljs-string">    - 如果使用 `NO_SHARD` 策略，`.grad` 属性将是经过 all-reduce 后的完整梯度。</span><br><span class="hljs-string">    - 否则，`_saved_grad_shard` 属性将是经过 reduce-scatter 后的分片梯度（会与已有的梯度累加）。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    _log_post_backward_hook(state, handle, logger)<br>    flat_param = handle.flat_param<br>    <span class="hljs-comment"># 标记该参数的后向钩子已被调用</span><br>    flat_param._post_backward_called = <span class="hljs-literal">True</span><br>    <span class="hljs-keyword">with</span> torch.autograd.profiler.record_function(<br>        <span class="hljs-string">&quot;FullyShardedDataParallel._post_backward_hook&quot;</span><br>    ):<br>        _assert_in_training_states(state, [TrainingState.FORWARD_BACKWARD])<br>        <span class="hljs-comment"># 当对共享相同 `FlatParameter` 的子模块多次使用可重入的激活检查点（AC）时，</span><br>        <span class="hljs-comment"># 后向钩子可能会在一次反向传播中运行多次。在这种情况下，我们允许句柄的状态</span><br>        <span class="hljs-comment"># 已经是 `BACKWARD_POST`。</span><br>        _p_assert(<br>            handle._training_state<br>            <span class="hljs-keyword">in</span> (HandleTrainingState.BACKWARD_PRE, HandleTrainingState.BACKWARD_POST),<br>            <span class="hljs-string">f&quot;Expects `BACKWARD_PRE` or `BACKWARD_POST` state but got <span class="hljs-subst">&#123;handle._training_state&#125;</span>&quot;</span>,<br>        )<br>        handle._training_state = HandleTrainingState.BACKWARD_POST<br><br>        <span class="hljs-comment"># 如果没有梯度，直接返回</span><br>        <span class="hljs-keyword">if</span> flat_param.grad <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span><br>        <span class="hljs-comment"># FSDP 不支持对梯度本身再求梯度</span><br>        <span class="hljs-keyword">if</span> flat_param.grad.requires_grad:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&quot;FSDP does not support gradients of gradients&quot;</span>)<br><br>        <span class="hljs-comment"># 关键步骤1：在进行梯度通信之前，先尝试重新分片参数，以尽早释放内存</span><br>        _post_backward_reshard(state, handle)<br>        <br>        <span class="hljs-comment"># 如果不进行梯度同步（例如在 `no_sync()` 上下文中），则直接返回</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> state._sync_gradients:<br>            <span class="hljs-keyword">if</span> handle._use_orig_params:<br>                <span class="hljs-comment"># 如果使用了原始（未合并的）参数，需要将梯度视图指向正确的 unsharded grad</span><br>                handle._use_unsharded_grad_views()<br>            <span class="hljs-keyword">return</span><br><br>        <span class="hljs-comment"># 关键步骤2：等待当前计算流中的所有操作（如梯度计算）完成，</span><br>        <span class="hljs-comment"># 然后再开始 reduce-scatter 梯度。这确保了我们拥有完整的本地梯度。</span><br>        <span class="hljs-comment"># TorchDynamo 编译模式下跳过此步。</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>            state._post_backward_stream.wait_stream(<br>                state._device_handle.current_stream()<br>            )<br><br>        <span class="hljs-comment"># 在专用的后向流中执行梯度通信</span><br>        <span class="hljs-keyword">with</span> state._device_handle.stream(state._post_backward_stream):<br>            autograd_computed_grad = flat_param.grad.data<br>            <span class="hljs-comment"># 如果开启了低精度训练，且梯度类型与通信类型不符，则进行类型转换以降低通信开销</span><br>            <span class="hljs-keyword">if</span> (<br>                <span class="hljs-keyword">not</span> _low_precision_hook_enabled(state)<br>                <span class="hljs-keyword">and</span> flat_param.grad.dtype != handle._reduce_dtype<br>                <span class="hljs-comment"># 如果强制全精度（例如在 eval 模式下），则不降低梯度精度</span><br>                <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> handle._force_full_precision<br>            ):<br>                flat_param.grad.data = flat_param.grad.to(handle._reduce_dtype)<br>            <br>            <span class="hljs-comment"># 根据分片策略执行梯度规约</span><br>            <span class="hljs-keyword">if</span> handle.uses_sharded_strategy:<br>                _reduce_grad(state, handle)  <span class="hljs-comment"># Reduce-scatter</span><br>            <span class="hljs-keyword">else</span>:<br>                _reduce_grad_no_shard(state, handle)  <span class="hljs-comment"># All-reduce</span><br>            <br>            <span class="hljs-comment"># 由于未分片的梯度是在计算流中产生的，但在后向流中消耗，</span><br>            <span class="hljs-comment"># 我们需要通知缓存分配器，以避免内存被过早回收。</span><br>            _no_dispatch_record_stream(<br>                autograd_computed_grad, state._post_backward_stream<br>            )<br></code></pre></td></tr></table></figure>

<h5 id="post-backward-reshard"><a href="#post-backward-reshard" class="headerlink" title="_post_backward_reshard"></a>_post_backward_reshard</h5><p>在梯度计算好后我们可以提前将之前unshard的参数进行reshard，从而释放内存</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_backward_reshard</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params">    *unused: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    在反向传播后执行参数的重新分片（reshard）和预取（prefetch）操作。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这个函数是后向钩子（post-backward hook）的核心逻辑之一，负责在梯度计算和</span><br><span class="hljs-string">    聚合之后，管理参数的内存状态，并为下一次迭代（的第一个前向传播）做准备。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 1. 决定在反向传播后是否应该释放当前 handle 的未分片（unsharded）参数内存</span><br>    free_unsharded_flat_param = _should_free_in_backward(state, handle)<br>    <br>    <span class="hljs-comment"># 2. 执行重新分片操作。如果 `free_unsharded_flat_param` 为 True，则会释放内存</span><br>    _reshard(state, handle, free_unsharded_flat_param)<br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 当前的后向预取（Post-backward prefetching）不支持一个模块包含多个 handle 的情况，</span><br>    <span class="hljs-comment"># 因为后向钩子是按 handle 触发的，而不是按一组 handle 触发的。</span><br>    <span class="hljs-keyword">with</span> torch.profiler.record_function(<br>        <span class="hljs-string">&quot;FullyShardedDataParallel._post_backward_prefetch&quot;</span><br>    ):<br>        <span class="hljs-comment"># 3. 为下一次迭代预取参数。这里的模式是 BACKWARD，意味着这个预取是在</span><br>        <span class="hljs-comment">#    反向传播阶段触发的，目的是为下一次迭代的第一个前向传播做准备，</span><br>        <span class="hljs-comment">#    从而实现计算和通信的重叠。</span><br>        _prefetch_handle(state, handle, _PrefetchMode.BACKWARD)<br><br></code></pre></td></tr></table></figure>

<p>此外为了计算和通行的重叠，会为了下一次迭代提前开启unshard。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_prefetch_handle</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    current_handle: <span class="hljs-type">Optional</span>[FlatParamHandle],</span><br><span class="hljs-params">    prefetch_mode: _PrefetchMode,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    根据需要（异步地）预取下一个 handle 的参数。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这个函数是 FSDP 实现计算和通信重叠的关键。它会在当前 handle 计算的同时，</span><br><span class="hljs-string">    提前将下一个 handle 所需的参数从分片状态（sharded）通过 all-gather 恢复为</span><br><span class="hljs-string">    完整状态（unsharded）。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> current_handle:<br>        <span class="hljs-keyword">return</span><br>    <br>    <span class="hljs-comment"># 1. 根据当前 handle 和预取模式（前向或后向），确定下一个需要预取的 handle</span><br>    handle = _get_handle_to_prefetch(state, current_handle)<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle:<br>        <span class="hljs-keyword">return</span><br>    <br>    <span class="hljs-comment"># 2. 临时模拟训练状态，以确保 `_unshard` 能够正确工作。</span><br>    <span class="hljs-comment">#    例如，在 `_unshard` 内部调用的 `_use_unsharded_views()` 需要根据正确的训练状态</span><br>    <span class="hljs-comment">#    来设置参数视图。</span><br>    prev_training_state = handle._training_state<br>    <span class="hljs-keyword">if</span> prefetch_mode == _PrefetchMode.BACKWARD:<br>        <span class="hljs-comment"># 在后向钩子中预取，是为下一次前向传播做准备</span><br>        handle._training_state = HandleTrainingState.BACKWARD_PRE<br>    <span class="hljs-keyword">elif</span> prefetch_mode == _PrefetchMode.FORWARD:<br>        <span class="hljs-comment"># 在前向钩子中预取，是为下一次前向传播做准备</span><br>        handle._training_state = HandleTrainingState.FORWARD<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;Invalid prefetch mode on rank <span class="hljs-subst">&#123;state.rank&#125;</span>: <span class="hljs-subst">&#123;prefetch_mode&#125;</span>&quot;</span>)<br>    <br>    <span class="hljs-comment"># 3. 异步地执行 unshard (all-gather) 操作，但不同步等待操作完成。</span><br>    <span class="hljs-comment">#    这使得 all-gather 通信可以与当前流中的计算（例如，前向/后向计算）重叠。</span><br>    <span class="hljs-comment">#    同步操作（`wait()`）会被推迟到真正需要使用该参数之前执行。</span><br>    _unshard(state, handle, state._unshard_stream, state._pre_unshard_stream)<br>    <br>    <span class="hljs-comment"># 4. 恢复 handle 原始的训练状态</span><br>    handle._training_state = prev_training_state<br>    <span class="hljs-comment"># 5. 标记该 handle 的参数已经被预取</span><br>    handle._prefetched = <span class="hljs-literal">True</span><br></code></pre></td></tr></table></figure>

<h3 id="register-post-backward-reshard-only-hook"><a href="#register-post-backward-reshard-only-hook" class="headerlink" title="_register_post_backward_reshard_only_hook"></a>_register_post_backward_reshard_only_hook</h3><p>对于那些不需要梯度计算的参数，注册一个梯度计算结束后进行重分片的勾子函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_register_post_backward_reshard_only_hook</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: <span class="hljs-type">Optional</span>[FlatParamHandle],</span><br><span class="hljs-params">    args: <span class="hljs-built_in">tuple</span>[<span class="hljs-type">Any</span>, ...],</span><br><span class="hljs-params">    kwargs: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>],</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    为那些不需要计算梯度(requires_grad=False)的扁平化参数(FlatParameter)注册一个</span><br><span class="hljs-string">    仅用于重新分片(reshard)的后向钩子。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    我们通过在模块的输入激活(input activations)上注册一个多重梯度钩子(multi-post-grad hook)</span><br><span class="hljs-string">    来做到这一点。这么做的原因是，对于 requires_grad=False 的参数，我们无法像之前一样</span><br><span class="hljs-string">    在其自身的 AccumulateGrad 对象上注册钩子（因为它不存在）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    通过在输入张量上挂钩，我们可以确保在所有可能依赖于该参数的梯度都计算完毕后，</span><br><span class="hljs-string">    才执行重新分片操作，从而安全地释放内存。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 如果当前上下文不计算梯度，则无需执行任何后向逻辑。</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.is_grad_enabled():<br>        <span class="hljs-keyword">return</span><br>    <br>    <span class="hljs-comment"># `inp_tensors` 会被懒加载，以避免在所有参数都计算梯度的常规情况下产生不必要的CPU开销。</span><br>    inp_tensors: <span class="hljs-type">Optional</span>[<span class="hljs-built_in">list</span>[torch.Tensor]] = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle:<br>        <span class="hljs-keyword">return</span><br>    flat_param = handle.flat_param<br><br>    <span class="hljs-comment"># 检查钩子是否已经注册过。</span><br>    <span class="hljs-keyword">if</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>        already_registered = <span class="hljs-built_in">hasattr</span>(flat_param, <span class="hljs-string">&quot;_post_backward_hook_handle&quot;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        already_registered = <span class="hljs-built_in">hasattr</span>(flat_param, <span class="hljs-string">&quot;_post_backward_hook_state&quot;</span>)<br><br>    <span class="hljs-comment"># 如果钩子已注册，或者参数需要梯度（此函数只处理不需要梯度的参数），则直接返回。</span><br>    <span class="hljs-keyword">if</span> already_registered <span class="hljs-keyword">or</span> flat_param.requires_grad:<br>        <span class="hljs-keyword">return</span><br>    <br>    <span class="hljs-comment"># --- 查找需要梯度的输入张量 --- #</span><br>    <span class="hljs-comment"># 这是此函数的关键逻辑：找到所有需要计算梯度的输入张量，并将钩子挂在它们上面。</span><br>    <span class="hljs-keyword">if</span> inp_tensors <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># 将所有输入参数扁平化为一个列表。</span><br>        args_flat = pytree.arg_tree_leaves(*args, **kwargs)<br>        <span class="hljs-comment"># 筛选出其中是张量(Tensor)且需要梯度(requires_grad=True)的对象。</span><br>        inp_tensors = [<br>            obj <span class="hljs-keyword">for</span> obj <span class="hljs-keyword">in</span> args_flat <span class="hljs-keyword">if</span> torch.is_tensor(obj) <span class="hljs-keyword">and</span> obj.requires_grad<br>        ]<br>    <span class="hljs-keyword">assert</span> inp_tensors <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>  <span class="hljs-comment"># mypy</span><br><br>    <span class="hljs-comment"># --- 注册多重梯度钩子 --- #</span><br>    <span class="hljs-comment"># `register_multi_grad_hook` 会注册一个钩子，该钩子只有在 `inp_tensors` 列表</span><br>    <span class="hljs-comment"># 中所有张量的梯度都计算完毕后才会触发。</span><br>    hook_handle = register_multi_grad_hook(<br>        inp_tensors, functools.partial(_post_backward_reshard_only_hook, state, handle)<br>    )<br><br>    <span class="hljs-comment"># 保存钩子句柄，以防止重复注册。</span><br>    <span class="hljs-keyword">if</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>        flat_param._post_backward_hook_handle = hook_handle  <span class="hljs-comment"># type: ignore[attr-defined, assignment]</span><br>    <span class="hljs-keyword">else</span>:<br>        flat_param._post_backward_hook_state = (hook_handle,)  <span class="hljs-comment"># type: ignore[attr-defined, assignment]</span><br><br></code></pre></td></tr></table></figure>

<h4 id="post-backward-reshard-only-hook"><a href="#post-backward-reshard-only-hook" class="headerlink" title="_post_backward_reshard_only_hook"></a>_post_backward_reshard_only_hook</h4><p>这里是注册的勾子函数，该函数会在梯度计算完成后计算，其作用是对于不需要梯度计算的参数，也在反向传播完成后将参数进行分片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_backward_reshard_only_hook</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params">    *unused: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    仅用于重新分片的后向钩子（post-backward hook）。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这个钩子专门为那些不需要梯度（`requires_grad=False`）的参数服务。</span><br><span class="hljs-string">    它的主要作用是在反向传播完成后，安全地将完整的（unsharded）参数重新分片（reshard），</span><br><span class="hljs-string">    从而释放内存。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">with</span> torch.profiler.record_function(<br>        <span class="hljs-string">&quot;FullyShardedDataParallel._post_backward_hook_reshard_only&quot;</span><br>    ):<br>        <span class="hljs-comment"># 如果前向传播的输出不需要梯度，`_pre_backward_hook` 可能不会被执行。</span><br>        <span class="hljs-comment"># 因此，这里需要显式地更新状态，以确保后续的后向预取（post-backward prefetching）逻辑能够正确运行。</span><br>        state.training_state = TrainingState.FORWARD_BACKWARD<br>        handle._training_state = HandleTrainingState.BACKWARD_POST<br>        <span class="hljs-comment"># 调用核心的重新分片逻辑</span><br>        _post_backward_reshard(state, handle)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_backward_reshard</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params">    *unused: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    执行后向传播后的重新分片和预取操作。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这个函数是后向钩子的核心部分，负责在梯度计算和聚合之后管理参数内存和为下一次迭代做准备。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 决定在反向传播后是否应该释放未分片的扁平参数（flat_param）</span><br>    free_unsharded_flat_param = _should_free_in_backward(state, handle)<br>    <span class="hljs-comment"># 执行重新分片操作，根据上面的标志决定是否释放内存</span><br>    _reshard(state, handle, free_unsharded_flat_param)<br><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> 当前的后向预取不支持一个模块有多个 handle 的情况，</span><br>    <span class="hljs-comment"># 因为后向钩子是按 handle 触发的，而不是按 handle 组触发的。</span><br>    <span class="hljs-keyword">with</span> torch.profiler.record_function(<br>        <span class="hljs-string">&quot;FullyShardedDataParallel._post_backward_prefetch&quot;</span><br>    ):<br>        <span class="hljs-comment"># 为下一次迭代的（前向）传播预取下一个 handle 的参数</span><br>        _prefetch_handle(state, handle, _PrefetchMode.BACKWARD)<br><br><br><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_should_free_in_backward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    决定 FSDP 是否应该在后向钩子中释放未分片的扁平参数。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">        bool: 如果应该释放则返回 True，否则返回 False。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 如果未使用分片策略，则不释放</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle.uses_sharded_strategy:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>    <span class="hljs-comment"># 如果不进行梯度同步（例如，在使用 `no_sync()` 上下文时），</span><br>    <span class="hljs-comment"># 并且参数的分片策略是在前向传播后不重新分片（reshard），</span><br>    <span class="hljs-comment"># 那么我们选择不释放参数。这是一种启发式策略，</span><br>    <span class="hljs-comment"># 目的是用较高的内存占用换取更高的吞吐量（因为避免了额外的 all-gather 操作）。</span><br>    <span class="hljs-comment"># 否则，如果需要同步梯度，或者策略本身就需要重新分片，则释放参数以节省内存。</span><br>    <span class="hljs-keyword">return</span> (<br>        state._sync_gradients<br>        <span class="hljs-keyword">or</span> handle._sharding_strategy <span class="hljs-keyword">in</span> RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES<br>    )<br></code></pre></td></tr></table></figure>

<h2 id="post-forward"><a href="#post-forward" class="headerlink" title="_post_forward"></a>_post_forward</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: <span class="hljs-type">Optional</span>[FlatParamHandle],</span><br><span class="hljs-params">    reshard_fn: <span class="hljs-type">Callable</span>,</span><br><span class="hljs-params">    module: nn.Module,</span><br><span class="hljs-params">    <span class="hljs-built_in">input</span>: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params">    output: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Any</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    运行前向传播后的逻辑。这包括一个机会来重新分片（reshard）当前未分片的参数</span><br><span class="hljs-string">    （例如在当前前向传播中使用的参数），并在前向传播的输出上注册 pre-backward 钩子。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    - 这是 FSDP 前向传播钩子的核心实现，在每个 FSDP 包装的模块的 `forward` 方法之后执行。</span><br><span class="hljs-string">    - 主要负责在前向计算完成后，将不再需要的完整参数重新分片，以释放 GPU 内存。</span><br><span class="hljs-string">    - 同时，在输出张量上注册 pre-backward 钩子，以便在反向传播开始时，能够及时地将分片参数恢复为完整参数，用于梯度计算。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        state (_FSDPState): FSDP 的全局状态。</span><br><span class="hljs-string">        handle (Optional[FlatParamHandle]): 当前前向传播中使用的参数句柄。</span><br><span class="hljs-string">        reshard_fn (Callable): 一个可调用对象，用于重新分片当前未分片的参数。如果为 `None`，则不执行任何重新分片操作。</span><br><span class="hljs-string">        module (nn.Module): 刚刚执行完 `forward` 的模块。</span><br><span class="hljs-string">        input (Any): 模块的输入（未使用，仅为满足钩子签名要求）。</span><br><span class="hljs-string">        output (Any): 前向传播的输出。Pre-backward 钩子会注册在该输出中需要梯度的张量上。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    后置条件：</span><br><span class="hljs-string">    - 每个 `FlatParameter` 的 `data` 属性将指向分片后的扁平化参数，从而释放内存。</span><br><span class="hljs-string">    - 输出张量上已注册 pre-backward 钩子。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    主要逻辑：</span><br><span class="hljs-string">    1. **处理激活检查点（Activation Checkpointing）**：如果与 `fully_shard` 和 `checkpoint` 一起使用，在重新计算的前向传播中会跳过此后向钩子逻辑，因为参数状态由激活检查点管理。</span><br><span class="hljs-string">    2. **记录执行顺序**：记录当前 handle 的前向传播完成事件，用于后续的乱序执行优化。</span><br><span class="hljs-string">    3. **重新分片 (Resharding)**：如果提供了 `reshard_fn`，则调用它来执行参数的重新分片，将完整的参数转换回分片状态，释放内存。</span><br><span class="hljs-string">    4. **注册 Pre-Backward 钩子**：调用 `_register_pre_backward_hooks`，遍历 `output` 中的张量，为那些需要梯度的张量注册一个钩子。这个钩子将在反向传播到达该张量时触发，执行参数的 unshard 操作（all-gather）。</span><br><span class="hljs-string">    5. **更新状态**：将 FSDP 实例和 handle 的训练状态更新为 `IDLE`，表示前向传播阶段已完成。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">with</span> torch.profiler.record_function(<span class="hljs-string">&quot;FullyShardedDataParallel._post_forward&quot;</span>):<br>        <span class="hljs-comment"># 对于 `fully_shard` + `checkpoint`，在重新计算的前向传播中跳过 post-forward 逻辑</span><br>        <span class="hljs-keyword">if</span> handle <span class="hljs-keyword">and</span> handle._training_state == HandleTrainingState.BACKWARD_PRE:<br>            <span class="hljs-keyword">return</span> output<br><br>        state._exec_order_data.record_post_forward(handle)<br>        <span class="hljs-keyword">if</span> reshard_fn <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            reshard_fn(state, handle)<br>        <span class="hljs-comment"># 注册 pre-backward 钩子，以便为梯度计算（如果需要）unshard 扁平化参数</span><br>        output = _register_pre_backward_hooks(state, module, output, handle)<br>        state.training_state = TrainingState.IDLE<br>        <span class="hljs-keyword">if</span> handle:<br>            handle._training_state = HandleTrainingState.IDLE<br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure>

<h3 id="post-forward-reshard"><a href="#post-forward-reshard" class="headerlink" title="_post_forward_reshard"></a>_post_forward_reshard</h3><p>_post_forward中使用的reshard_fn就是_post_forward_reshard。</p>
<p>这是在前向传播后触发重新分片的入口函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_post_forward_reshard</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;在前向传播后重新分片参数。&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 功能：</span><br>    <span class="hljs-comment">#   - 作为前向传播钩子的一部分，决定是否以及如何重新分片（reshard）刚刚在前向计算中使用过的参数。</span><br>    <span class="hljs-comment">#   - 重新分片的目的是及时释放未分片（unsharded）参数占用的 GPU 内存。</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># 主要逻辑：</span><br>    <span class="hljs-comment"># 1. 检查 handle 是否存在，如果不存在则直接返回。</span><br>    <span class="hljs-comment"># 2. 决定是否要释放未分片的扁平化参数（`free_unsharded_flat_param`）。</span><br>    <span class="hljs-comment">#    - 通常情况下，参数在使用后会被立即释放以节省内存。</span><br>    <span class="hljs-comment">#    - 一个重要的例外是根（root）FSDP 模块。在 `FULL_SHARD` 策略下，根模块的参数在</span><br>    <span class="hljs-comment">#      前向传播后不会被立即释放，因为它们很可能马上就要用于反向传播的计算。</span><br>    <span class="hljs-comment">#      这是一种性能优化，避免了在前向后和反向前进行不必要的 `reshard` 和 `unshard` 操作。</span><br>    <span class="hljs-comment">#    - `RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES` 包含了需要这种行为的分片策略。</span><br>    <span class="hljs-comment"># 3. 调用 `_reshard` 函数，传入计算出的 `free_unsharded_flat_param` 标志，执行实际的重新分片操作。</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle:<br>        <span class="hljs-keyword">return</span><br>    <span class="hljs-comment"># 对于 `FULL_SHARD`，不要在 post-forward 中释放根模块的参数，</span><br>    <span class="hljs-comment"># 意图是它们能立即用于反向计算（尽管这可能不总是真的）</span><br>    free_unsharded_flat_param = (<br>        <span class="hljs-keyword">not</span> state._is_root<br>        <span class="hljs-keyword">and</span> handle._sharding_strategy <span class="hljs-keyword">in</span> RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES<br>    )<br>    _reshard(state, handle, free_unsharded_flat_param)<br></code></pre></td></tr></table></figure>

<p>这个函数调用参数句柄（handle）来执行实际的重新分片逻辑。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_reshard</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params">    free_unsharded_flat_param: <span class="hljs-built_in">bool</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    重新分片句柄。`free_unsharded_flat_param` 指示是否释放</span><br><span class="hljs-string">    句柄的带填充的未分片扁平参数。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 功能：</span><br>    <span class="hljs-comment">#   - 协调参数句柄（handle）的重新分片过程。</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># 主要逻辑：</span><br>    <span class="hljs-comment"># 1. 调用 `handle.reshard()` 方法，将 `free_unsharded_flat_param` 标志传递下去，</span><br>    <span class="hljs-comment">#    由 handle 对象自己管理其内部状态和内存。</span><br>    <span class="hljs-comment"># 2. 如果 `limit_all_gathers` 选项被启用并且参数被释放，它会使用一个 CUDA 事件队列（`_free_event_queue`）</span><br>    <span class="hljs-comment">#    来确保在释放内存前，所有在当前流上的操作都已经完成，这是一种更精细的同步机制。</span><br>    <span class="hljs-comment"># 3. 调用 `handle.post_reshard()` 来执行任何 reshard 后的清理工作。</span><br>    <span class="hljs-comment"># 4. 将 `handle._prefetched` 标志设置为 `False`，表示参数现在是分片状态，</span><br>    <span class="hljs-comment">#    下次访问时需要通过 all-gather（即 unshard）来获取完整数据。</span><br>    handle.reshard(free_unsharded_flat_param)<br>    <span class="hljs-keyword">if</span> state.limit_all_gathers <span class="hljs-keyword">and</span> free_unsharded_flat_param:<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>            <span class="hljs-comment"># 在 torch compile 模式下，我们目前不为释放操作运行事件队列</span><br>            <span class="hljs-comment"># 但也许我们需要？TODO(voz): 研究一下</span><br>            free_event = state._device_handle.Event()<br>            free_event.record()<br>            state._free_event_queue.enqueue(free_event)<br>    handle.post_reshard()<br>    <span class="hljs-comment"># 无论扁平参数是否被释放，我们总是在下次访问时“unshard”参数</span><br>    <span class="hljs-comment"># 以获取其正确的形状。</span><br>    handle._prefetched = <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure>

<p>执行重分片逻辑，需要先转为使用本地参数，再安全释放收集的数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reshard</span>(<span class="hljs-params">self, free_unsharded_flat_param: <span class="hljs-built_in">bool</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    运行重新分片逻辑。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    这包括如果 `free_unsharded_flat_param` 为真，则释放未分片的扁平参数，</span><br><span class="hljs-string">    并切换到使用分片的扁平参数。注意，这也隐式地将分片的扁平参数</span><br><span class="hljs-string">    卸载到 CPU（如果启用了 CPU offload），通过将其指向位于 CPU 上的 `_local_shard` 属性。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 功能：</span><br>    <span class="hljs-comment">#   - 在参数句柄（handle）级别上执行重新分片的核心操作。</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># 主要逻辑：</span><br>    <span class="hljs-comment"># 1. **切换指针**：首先调用 `_use_sharded_flat_param()`。这是一个关键步骤，它将 `FlatParameter`</span><br>    <span class="hljs-comment">#    的内部 `data` 指针重新指向分片后的张量（`_sharded_flat_param`）。</span><br>    <span class="hljs-comment">#    这样做可以防止在释放内存后发生“悬空指针”或“use-after-free”的 bug。</span><br>    <span class="hljs-comment"># 2. **释放内存**：如果 `free_unsharded_flat_param` 为 `True`，则调用 `_free_unsharded_flat_param()`</span><br>    <span class="hljs-comment">#    来释放之前未分片的、完整的参数所占用的内存。</span><br>    <span class="hljs-comment"># 在释放之前切换到分片的 `FlatParameter`，以防止外部性能分析工具出现“use-after-free”类型的 bug，</span><br>    <span class="hljs-comment"># 其中对于 `use_orig_params=True`，当在 `_use_sharded_views()` 中设置 `param.data = ...` 时，</span><br>    <span class="hljs-comment"># `param` 不会指向有效的内存。</span><br>    self._use_sharded_flat_param()<br>    <span class="hljs-keyword">if</span> free_unsharded_flat_param:<br>        self._free_unsharded_flat_param()<br></code></pre></td></tr></table></figure>

<p>主要作用是将 self.flat_param (一个 nn.Parameter) 的 .data 属性从指向完整的、未分片的张量，切换为指向本地的分片张量 (self.flat_param._local_shard)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_use_sharded_flat_param</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;切换到使用分片的扁平参数。&quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 功能：</span><br>    <span class="hljs-comment">#   - 这是 reshard（重新分片）过程中的关键步骤。</span><br>    <span class="hljs-comment">#   - 主要作用是将 self.flat_param (一个 nn.Parameter) 的 .data 属性从指向完整的、</span><br>    <span class="hljs-comment">#     未分片的张量，切换为指向本地的分片张量 (self.flat_param._local_shard)。</span><br>    <span class="hljs-comment">#   - 这个切换是实现内存优化的核心：一旦 .data 指向了分片，之前完整张量所占用的</span><br>    <span class="hljs-comment">#     内存就可以被安全地释放。</span><br>    <span class="hljs-comment">#   - 如果 `use_orig_params` 为 True，此方法还负责更新原始模型参数，使其成为</span><br>    <span class="hljs-comment">#     分片张量的“视图”（view），并处理其梯度的视图。</span><br><br>    flat_param = self.flat_param<br>    <span class="hljs-keyword">if</span> self._use_orig_params:<br>        <span class="hljs-comment"># --- 特殊情况处理：决定是否跳过更新原始参数视图 --- #</span><br>        <span class="hljs-comment"># 在某些策略下（如 NO_SHARD），我们不在前向传播后立即重新分片。这是一种优化，</span><br>        <span class="hljs-comment"># 避免在前向和后向之间进行不必要的 unshard/reshard。</span><br>        <span class="hljs-comment"># `skip_use_sharded_views` 用于标识这种情况。</span><br>        in_forward = self._training_state == HandleTrainingState.FORWARD<br>        skip_use_sharded_views = (<br>            torch.is_grad_enabled()<br>            <span class="hljs-keyword">and</span> in_forward<br>            <span class="hljs-keyword">and</span> self._sharding_strategy<br>            <span class="hljs-keyword">in</span> NO_RESHARD_AFTER_FORWARD_HANDLE_STRATEGIES<br>        )<br>        <span class="hljs-comment"># 如果需要跳过，提前保存未分片参数的引用</span><br>        <span class="hljs-keyword">if</span> skip_use_sharded_views:<br>            unsharded_flat_param = flat_param.data<br><br>    <span class="hljs-keyword">if</span> self._offload_params:<br>        <span class="hljs-comment"># --- CPU Offload 断言 --- #</span><br>        <span class="hljs-comment"># 如果启用了参数的 CPU 卸载，那么此时的本地分片理应在 CPU 上。</span><br>        device = flat_param._local_shard.device  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>        _p_assert(<br>            device == torch.device(<span class="hljs-string">&quot;cpu&quot;</span>),<br>            <span class="hljs-string">f&quot;期望本地分片在 CPU 上，但实际在 <span class="hljs-subst">&#123;device&#125;</span>&quot;</span>,<br>        )<br><br>    <span class="hljs-comment"># --- 核心操作：切换 .data 指针 --- #</span><br>    <span class="hljs-comment"># 这是此方法最核心的一行。它将 FlatParameter 的数据指针指向本地分片。</span><br>    <span class="hljs-comment"># 如果启用了 CPU Offload，_local_shard 就在 CPU 上，这个操作也完成了数据到 CPU 的“卸载”。</span><br>    flat_param.data = flat_param._local_shard  <span class="hljs-comment"># type: ignore[attr-defined]</span><br><br>    <span class="hljs-keyword">if</span> self._use_orig_params:<br>        <span class="hljs-comment"># --- 更新原始参数及其梯度视图 --- #</span><br>        <span class="hljs-keyword">if</span> skip_use_sharded_views:  <span class="hljs-comment"># type: ignore[possibly-undefined]</span><br>            <span class="hljs-comment"># 如果跳过了视图更新，只需保存未分片的参数引用即可。</span><br>            self._unsharded_flat_param_for_skipped_views = unsharded_flat_param  <span class="hljs-comment"># type: ignore[possibly-undefined]</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 否则，调用 _use_sharded_views()，将原始参数的 .data 更新为分片张量的视图。</span><br>            self._use_sharded_views()<br><br>        <span class="hljs-comment"># 在前向传播后的 reshard 中，我们可能尝试使用分片的梯度视图</span><br>        <span class="hljs-comment"># （或者，如果在 no_sync() 中累积了梯度，则使用未分片的梯度视图），</span><br>        <span class="hljs-comment"># 但在后向传播后的 reshard 中，我们将此调用推迟到 reduce-scatter 之后。</span><br>        <span class="hljs-keyword">if</span> (<br>            in_forward  <span class="hljs-comment"># type: ignore[possibly-undefined]</span><br>            <span class="hljs-comment"># 如果跳过了使用分片视图，则跳过使用梯度视图，</span><br>            <span class="hljs-comment"># 因为向用户暴露未分片的参数和分片的梯度可能会引起困惑</span><br>            <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> self._skipped_use_sharded_views<br>        ):<br>            <span class="hljs-comment"># 检查在 no_sync() 上下文中是否累积了完整的梯度</span><br>            accumulated_grad_in_no_sync = (<br>                flat_param.grad <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>                <span class="hljs-keyword">and</span> self.uses_sharded_strategy<br>                <span class="hljs-keyword">and</span> flat_param.grad.shape == flat_param._unpadded_unsharded_size<br>            )<br>            <span class="hljs-keyword">if</span> accumulated_grad_in_no_sync:<br>                <span class="hljs-comment"># 如果有完整的梯度，则原始参数的梯度视图也应指向这个完整的梯度。</span><br>                self._use_unsharded_grad_views()<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># 否则，梯度视图应指向分片后的梯度。</span><br>                self._use_sharded_grad_views()<br></code></pre></td></tr></table></figure>

<p>释放放带填充的未分片扁平参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_free_unsharded_flat_param</span>(<span class="hljs-params">self</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    释放带填充的未分片扁平参数。我们允许在存储未分配时也调用此函数。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    要释放的张量取决于调用上下文，因为 unshard 可能强制使用了全精度，</span><br><span class="hljs-string">    在这种情况下，会使用一个不同的张量。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 功能：</span><br>    <span class="hljs-comment">#   - 定位到未分片的、完整的扁平化参数，并准备释放其内存。</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># 主要逻辑：</span><br>    <span class="hljs-comment"># 1. 获取正确的未分片参数张量 `unsharded_flat_param`。</span><br>    <span class="hljs-comment"># 2. 检查该张量是否在计算设备上（例如 GPU）。</span><br>    <span class="hljs-comment"># 3. **同步流**：调用 `_no_dispatch_record_stream()`，确保在释放张量内存之前，</span><br>    <span class="hljs-comment">#    当前 CUDA 流中所有使用该张量的操作都已完成。这是一个重要的同步步骤，</span><br>    <span class="hljs-comment">#    防止在 GPU 操作完成前就释放了其正在使用的内存。</span><br>    <span class="hljs-comment"># 4. 调用底层的 `_free_storage()` 工具函数来执行实际的内存释放。</span><br>    self._check_sharded_strategy()<br>    unsharded_flat_param = self._get_padded_unsharded_flat_param()<br>    self._check_on_compute_device(unsharded_flat_param)<br>    <span class="hljs-comment"># 在当前流中的所有操作完成之前，不要释放内存</span><br>    _no_dispatch_record_stream(<br>        unsharded_flat_param, self._device_handle.current_stream()<br>    )<br>    _free_storage(unsharded_flat_param)<br></code></pre></td></tr></table></figure>

<p>首先获取到之前带填充的、未分片的扁平参数的引用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_padded_unsharded_flat_param</span>(<span class="hljs-params">self</span>) -&gt; torch.Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    根据调用上下文，返回对带填充的、未分片的扁平参数的引用。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    - 此方法是获取用于 all-gather 操作的目标张量的核心逻辑。</span><br><span class="hljs-string">    - 它处理了混合精度训练中的一个重要情况：当需要强制使用全精度参数时，它会返回一个不同的、高精度的张量，并释放可能存在的旧的、低精度的张量，以确保数据一致性。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    主要逻辑：</span><br><span class="hljs-string">    1. **检查分片策略**：确保此方法仅在使用了分片策略（如 `FULL_SHARD` 或 `SHARD_GRAD_OP`）时被调用。</span><br><span class="hljs-string">    2. **处理强制全精度和混合精度**：</span><br><span class="hljs-string">       - 如果 `_force_full_precision`（例如，在 `summon_full_params` 中）和 `_uses_param_mixed_precision` 都为 `True`，则意味着我们需要一个全精度的参数副本进行操作。</span><br><span class="hljs-string">       - 在这种情况下，返回 `_full_prec_full_param_padded`，这是一个专门用于存储全精度参数的张量。</span><br><span class="hljs-string">       - **关键操作**：如果低精度的 `_full_param_padded` 张量仍然占用内存（意味着它可能来自上一次前向传播且未被释放），则必须将其释放。这是因为对全精度参数的修改会使这个低精度副本失效。释放后，下一次计算将强制执行新的 all-gather 来获取最新的数据，而不是使用过时的低精度缓存。</span><br><span class="hljs-string">    3. **标准情况**：</span><br><span class="hljs-string">       - 在其他所有情况下（例如，不强制全精度或不使用混合精度），直接返回标准的 `_full_param_padded` 张量，该张量将作为 all-gather 的目标。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 确认当前正在使用分片策略，因为此方法与获取未分片参数相关</span><br>    self._check_sharded_strategy()<br>    flat_param = self.flat_param<br>    <span class="hljs-comment"># 检查是否需要强制使用全精度参数，并且参数混合精度已启用</span><br>    <span class="hljs-keyword">if</span> self._force_full_precision <span class="hljs-keyword">and</span> self._uses_param_mixed_precision:<br>        <span class="hljs-comment"># 当启用参数混合精度时，我们使用一个不同的张量作为 all-gather 的目标，</span><br>        <span class="hljs-comment"># 以保持 `_full_param_padded` 始终是低精度这一不变性。</span><br>        unsharded_flat_param = flat_param._full_prec_full_param_padded  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>        <span class="hljs-comment"># 断言确保我们获取的确实是全精度张量，其类型不应与前向/后向传播中使用的低精度类型相同</span><br>        _p_assert(<br>            unsharded_flat_param.dtype != self._fwd_bwd_param_dtype,<br>            <span class="hljs-string">f&quot;期望全精度但得到了 <span class="hljs-subst">&#123;self._fwd_bwd_param_dtype&#125;</span>&quot;</span>,<br>        )<br>        <span class="hljs-comment"># 对于在 forward 后不重新分片的策略，`_full_param_padded` 可能仍被分配了内存。</span><br>        <span class="hljs-comment"># 由于我们在这里强制使用全精度，全精度副本可能会被修改，从而使现有的低精度副本失效。</span><br>        <span class="hljs-comment"># 因此，我们在这里释放它，以确保下一次前向/后向计算会进行新的 all-gather，以持久化修改。</span><br>        <span class="hljs-keyword">if</span> flat_param._full_param_padded.untyped_storage().size() &gt; <span class="hljs-number">0</span>:<br>            _free_storage(flat_param._full_param_padded)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># 在标准情况下，直接使用 `_full_param_padded` 作为未分片的参数</span><br>        unsharded_flat_param = flat_param._full_param_padded  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>    <span class="hljs-keyword">return</span> unsharded_flat_param<br></code></pre></td></tr></table></figure>

<p>这是一个通用的底层工具函数，通过将张量的存储大小调整为 0 来释放其内存。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_free_storage</span>(<span class="hljs-params">tensor: torch.Tensor</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    释放 `tensor` 的底层存储。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">        bool: 如果方法释放了存储，则返回 `True`；如果存储已被释放，则返回 `False`。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 功能：</span><br>    <span class="hljs-comment">#   - 这是实际执行内存释放的最低级函数。</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># 主要逻辑：</span><br>    <span class="hljs-comment"># 1. 在 `torch.no_grad()` 上下文中操作，避免不必要的梯度跟踪。</span><br>    <span class="hljs-comment"># 2. 检查存储是否已经被释放（大小是否为 0）。</span><br>    <span class="hljs-comment"># 3. **安全检查**：断言（`_p_assert`）张量的 `storage_offset()` 为 0。这是一个重要的安全措施，</span><br>    <span class="hljs-comment">#    确保我们正在释放的张量是其底层存储的唯一所有者。如果一个存储被多个张量视图（view）共享，</span><br>    <span class="hljs-comment">#    释放它是不安全的。</span><br>    <span class="hljs-comment"># 4. **释放操作**：调用 `tensor._typed_storage()._resize_(0)`。这个内部方法会将张量的底层存储</span><br>    <span class="hljs-comment">#    大小调整为 0，从而有效地将内存返回给 PyTorch 的缓存分配器，使其可以被重用。</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>            already_freed = tensor._typed_storage()._size() == <span class="hljs-number">0</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> already_freed:<br>                _p_assert(<br>                    tensor.storage_offset() == <span class="hljs-number">0</span>,<br>                    <span class="hljs-string">&quot;当张量不是其存储的唯一占用者时，释放它的存储是不安全的\n&quot;</span><br>                    <span class="hljs-string">f&quot;storage offset: <span class="hljs-subst">&#123;tensor.storage_offset()&#125;</span>\n&quot;</span><br>                    <span class="hljs-string">f&quot;storage size: <span class="hljs-subst">&#123;tensor._typed_storage()._size()&#125;</span>\n&quot;</span><br>                    <span class="hljs-string">f&quot;tensor shape: <span class="hljs-subst">&#123;tensor.shape&#125;</span>&quot;</span>,<br>                )<br>                tensor._typed_storage()._resize_(<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>

<h3 id="register-pre-backward-hooks"><a href="#register-pre-backward-hooks" class="headerlink" title="_register_pre_backward_hooks"></a>_register_pre_backward_hooks</h3><p>主要是注册反向传播前置钩子</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_register_pre_backward_hooks</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    module: nn.Module,</span><br><span class="hljs-params">    outputs: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    在 `outputs`（前向传播的输出）中需要梯度的张量上注册反向传播前置钩子（pre-backward hooks）。</span><br><span class="hljs-string">    这些输出是使用 `handle` 的 `FlatParameter` 计算得出的。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    - 这是 FSDP 实现自动、即时（just-in-time）参数 un-sharding 的核心机制。</span><br><span class="hljs-string">    - 通过在模块的输出张量上注册钩子，FSDP 可以在反向传播到达该模块之前，精确地触发相应参数的 all-gather 操作。</span><br><span class="hljs-string">    - 这样可以确保在计算梯度时，完整的、未分片的参数是可用的，同时在其他时间保持分片状态以节省内存。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    主要逻辑：</span><br><span class="hljs-string">    1. **检查梯度计算**：如果当前没有启用梯度计算（例如，在 `torch.no_grad()` 上下文中），则无需注册任何钩子，直接返回。</span><br><span class="hljs-string">    2. **重置状态**：</span><br><span class="hljs-string">       - 对于根模块，重置 `_post_backward_callback_queued` 标志，为新的反向传播做准备。</span><br><span class="hljs-string">       - 对于当前 `handle`，重置 `_needs_pre_backward_unshard` 和 `_ran_pre_backward_hook` 标志，以确保钩子逻辑的正确执行。</span><br><span class="hljs-string">    3. **定义钩子注册函数 `_register_hook`**：</span><br><span class="hljs-string">       - 此内部函数负责在单个张量上注册钩子。</span><br><span class="hljs-string">       - **条件**：仅当张量 `requires_grad` 时才注册，因为只有这些张量会参与反向传播。</span><br><span class="hljs-string">       - **注册**：使用 `t.register_hook()` 将 `_pre_backward_hook`（通过 `functools.partial` 包装）附加到张量上。</span><br><span class="hljs-string">       - **标记需求**：注册钩子后，将 `handle._needs_pre_backward_unshard` 设为 `True`，表明该 `handle` 对应的参数在反向传播中需要被 un-shard。</span><br><span class="hljs-string">    4. **递归应用钩子**：</span><br><span class="hljs-string">       - 使用 `_apply_to_tensors` 工具函数，将 `_register_hook` 应用于 `outputs` 中的所有张量。这可以处理复杂的输出结构（如元组、列表、字典等）。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 如果没有启用梯度计算（例如在 `torch.no_grad()` 中），则不需要反向传播逻辑</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.is_grad_enabled():<br>        <span class="hljs-keyword">return</span> outputs<br>    <span class="hljs-comment"># 如果是根 FSDP 实例，重置 post-backward 回调已排队的标志</span><br>    <span class="hljs-keyword">if</span> state._is_root:<br>        state._post_backward_callback_queued = <span class="hljs-literal">False</span>  <span class="hljs-comment"># 此标志仅在根节点上定义</span><br><br>    <span class="hljs-keyword">if</span> handle:<br>        <span class="hljs-comment"># 初始化标志，表示此 handle 尚不需要在反向传播前进行 un-shard</span><br>        handle._needs_pre_backward_unshard = <span class="hljs-literal">False</span><br>        <span class="hljs-comment"># 由于此 handle 的 FlatParameter 参与了前向传播，我们保守地假设</span><br>        <span class="hljs-comment"># 它将在反向传播中使用。重置此标志，用于跟踪 pre-backward 钩子是否已运行。</span><br>        handle._ran_pre_backward_hook = <span class="hljs-literal">False</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_register_hook</span>(<span class="hljs-params">t: torch.Tensor</span>) -&gt; torch.Tensor:<br>        <span class="hljs-comment"># 只在需要计算梯度的张量上注册钩子</span><br>        <span class="hljs-keyword">if</span> t.requires_grad:<br>            <span class="hljs-comment"># 注册一个不可序列化的钩子。`_pre_backward_hook` 将在反向传播到此张量时被调用。</span><br>            t.register_hook(<br>                torch.utils.hooks.unserializable_hook(<br>                    functools.partial(_pre_backward_hook, state, module, handle)<br>                )<br>            )<br>            <span class="hljs-comment"># 如果注册了钩子，说明这个 handle 对应的参数将需要 un-shard</span><br>            <span class="hljs-keyword">if</span> handle:<br>                handle._needs_pre_backward_unshard = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">return</span> t<br><br>    <span class="hljs-comment"># 递归地将 _register_hook 函数应用于 `outputs` 中的所有张量</span><br>    <span class="hljs-keyword">return</span> _apply_to_tensors(_register_hook, outputs)<br></code></pre></td></tr></table></figure>

<p>具体注册的勾子函数为_pre_backward_hook，该函数主要是执行_unshard来通过 all-gather 操作获取完整的参数，并且为了重叠计算和通信，它会立即触发下一个（在反向传播顺序中）模块参数的 prefetching（预取）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">@no_type_check</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_pre_backward_hook</span>(<span class="hljs-params"></span><br><span class="hljs-params">    state: _FSDPState,</span><br><span class="hljs-params">    module: nn.Module,</span><br><span class="hljs-params">    handle: FlatParamHandle,</span><br><span class="hljs-params">    grad,</span><br><span class="hljs-params">    *unused: <span class="hljs-type">Any</span>,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Any</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    为梯度计算准备 `handle` 的 `FlatParameter`。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    功能：</span><br><span class="hljs-string">    - 这是 FSDP 的核心反向传播钩子，由 `_register_pre_backward_hooks` 注册。</span><br><span class="hljs-string">    - 当反向传播的梯度流到达一个模块的输出张量时，这个钩子被触发。</span><br><span class="hljs-string">    - 它的主要职责是：</span><br><span class="hljs-string">        1. **Un-shard 参数**：执行 all-gather 操作，将当前模块所需的 `FlatParameter` 从分片状态恢复为完整的、未分片的张量，以便进行梯度计算。</span><br><span class="hljs-string">        2. **预取下一个参数**：为了重叠计算和通信，它会立即触发下一个（在反向传播顺序中）模块参数的 prefetching（预取）。</span><br><span class="hljs-string">        3. **状态管理**：管理 FSDP 的内部状态，例如标记钩子已运行，以及为根模块注册最终的 post-backward 回调。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    主要逻辑：</span><br><span class="hljs-string">    1. **钩子执行保护**：检查 `_ran_pre_backward_hook` 标志，确保对于同一次前向计算涉及的同一组参数，此钩子只执行一次。</span><br><span class="hljs-string">    2. **根模块初始化**：如果是根 FSDP 模块，并且是反向传播的第一次调用，它会注册一个 `_post_backward_final_callback`。这个回调将在整个反向传播结束后执行，用于最终的清理工作（如梯度 reshard）。</span><br><span class="hljs-string">    3. **状态转换**：将 FSDP 状态机切换到 `FORWARD_BACKWARD` 和 `BACKWARD_PRE`，用于调试和断言。</span><br><span class="hljs-string">    4. **参数 Un-shard**：</span><br><span class="hljs-string">       - 检查 `_needs_pre_backward_unshard` 标志。</span><br><span class="hljs-string">       - 如果需要 un-shard 且参数尚未被预取（`_prefetched` 为 False），则调用 `_unshard` 执行 all-gather。</span><br><span class="hljs-string">       - 使用 `wait_stream` 确保计算流等待 un-shard 操作完成。</span><br><span class="hljs-string">    5. **反向预取（Backward Prefetch）**：</span><br><span class="hljs-string">       - 调用 `_prefetch_handle` 并传入 `_PrefetchMode.BACKWARD`，以启动下一个句柄的参数 un-sharding。这是 FSDP 的关键性能优化。</span><br><span class="hljs-string">    6. **梯度准备**：调用 `handle.prepare_gradient_for_backward()`，为即将到来的梯度计算做准备。</span><br><span class="hljs-string">    7. **标记完成**：设置 `_ran_pre_backward_hook = True`。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 对于同一次模块前向计算中涉及的同一组句柄，只运行一次 pre-backward 钩子</span><br>    <span class="hljs-keyword">if</span> (<br>        handle<br>        <span class="hljs-keyword">and</span> <span class="hljs-built_in">hasattr</span>(handle, <span class="hljs-string">&quot;_ran_pre_backward_hook&quot;</span>)<br>        <span class="hljs-keyword">and</span> handle._ran_pre_backward_hook<br>    ):<br>        <span class="hljs-keyword">return</span> grad<br><br>    <span class="hljs-keyword">with</span> torch.profiler.record_function(<span class="hljs-string">&quot;FullyShardedDataParallel._pre_backward_hook&quot;</span>):<br>        <span class="hljs-comment"># 为根 FSDP 实例排队一次 post-backward 回调，将其附加到最外层的反向图任务上，</span><br>        <span class="hljs-comment"># 以便在所有反向调用完成后调用它。</span><br>        <span class="hljs-keyword">if</span> state._is_root <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> state._post_backward_callback_queued:<br>            _register_post_backward_final_callback(state, module)<br>            _reset_flat_param_grad_info_if_needed(state._all_handles)<br>        <span class="hljs-keyword">elif</span> handle:<br>            <span class="hljs-comment"># 断言 FSDP 模块处于正确的训练状态</span><br>            allowed_states = [TrainingState.IDLE]<br>            <span class="hljs-keyword">if</span> _is_composable(state):<br>                allowed_states.append(TrainingState.FORWARD_BACKWARD)<br>            _assert_in_training_states(state, allowed_states)<br>        <span class="hljs-comment"># 更新训练状态为正在进行反向传播</span><br>        state.training_state = TrainingState.FORWARD_BACKWARD<br>        <span class="hljs-comment"># 排队 post-backward 回调是 pre-backward 钩子中唯一不是按句柄处理的逻辑，</span><br>        <span class="hljs-comment"># 因此如果没有句柄，我们可以在这里提前返回。</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle:<br>            <span class="hljs-keyword">return</span> grad<br>        <span class="hljs-comment"># 更新句柄的训练状态为反向传播前</span><br>        handle._training_state = HandleTrainingState.BACKWARD_PRE<br><br>        <span class="hljs-keyword">if</span> handle._needs_pre_backward_unshard:<br>            <span class="hljs-comment"># 如果句柄已经被预取，则无需再次调用 `_unshard()`</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> handle._prefetched:<br>                _unshard(<br>                    state,<br>                    handle,<br>                    state._unshard_stream,      <span class="hljs-comment"># 用于 unshard 的 CUDA 流</span><br>                    state._pre_unshard_stream,  <span class="hljs-comment"># 用于 pre-unshard 的 CUDA 流</span><br>                )<br>            <span class="hljs-comment"># 在 tracing 期间不要等待，以避免图中断</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.distributed._functional_collectives.is_torchdynamo_compiling():<br>                <span class="hljs-comment"># 确保计算流等待 unshard 操作完成</span><br>                state._device_handle.current_stream().wait_stream(state._unshard_stream)<br><br>        <span class="hljs-comment"># 将此标志设置为 `False`，以确保目标错误的预取不会实际 unshard 这些句柄</span><br>        handle._needs_pre_backward_unshard = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">with</span> torch.profiler.record_function(<br>            <span class="hljs-string">&quot;FullyShardedDataParallel._pre_backward_prefetch&quot;</span><br>        ):<br>            <span class="hljs-comment"># 预取下一个在反向传播中需要的句柄的参数</span><br>            _prefetch_handle(state, handle, _PrefetchMode.BACKWARD)<br>        <span class="hljs-comment"># 为反向传播准备梯度</span><br>        handle.prepare_gradient_for_backward()<br>        <span class="hljs-comment"># 标记此句柄的 pre-backward 钩子已运行</span><br>        handle._ran_pre_backward_hook = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">return</span> grad<br></code></pre></td></tr></table></figure>

<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="勾子函数及运行流程"><a href="#勾子函数及运行流程" class="headerlink" title="勾子函数及运行流程"></a>勾子函数及运行流程</h2><p><img src="/2025/07/02/pytorch-fsdp-1/diagram.png" srcset="/img/loading.gif" lazyload></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/">https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/485208899">https://zhuanlan.zhihu.com/p/485208899</a></p>
</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/LLM/" class="category-chain-item">LLM</a>
  
  
    <span>></span>
    
  <a href="/categories/LLM/FSDP/" class="category-chain-item">FSDP</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/LLM/" class="print-no-link">#LLM</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>【pytorch-fsdp 源代码阅读（一）】-全流程概览</div>
      <div>http://example.com/2025/07/02/pytorch-fsdp-1/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>滑滑蛋</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年7月2日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2025/07/12/pytorch-fsdp-2/" title="【pytorch-fsdp 源代码阅读（二）】-参数流转">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">【pytorch-fsdp 源代码阅读（二）】-参数流转</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2025/06/28/Picotron-Tutorial%20Context%20parallel/" title="【Picotron-Tutorial】上下文并行">
                        <span class="hidden-mobile">【Picotron-Tutorial】上下文并行</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"WMtHomhQYlrbIodTwoPU3gTY-MdYXbMMI","appKey":"pZeun9WfI1yaQrIoUbvTQrXv","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"https://wmthomhq.api.lncldglobal.com","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 
        <span id="leancloud-site-uv"></span>
         次
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script defer src="/js/leancloud.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
